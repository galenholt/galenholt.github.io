[
  {
    "objectID": "betabinomial/beta_binomial.html",
    "href": "betabinomial/beta_binomial.html",
    "title": "Beta-binomial model testing",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lme4)\n\nWarning: package 'lme4' was built under R version 4.2.2\n\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(lmerTest)\n\nWarning: package 'lmerTest' was built under R version 4.2.2\n\n\n\nAttaching package: 'lmerTest'\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(spaMM)\n\nRegistered S3 methods overwritten by 'registry':\n  method               from \n  print.registry_field proxy\n  print.registry_entry proxy\nspaMM (Rousset & Ferdy, 2014, version 4.1.0) is loaded.\nType 'help(spaMM)' for a short introduction,\n'news(package='spaMM')' for news,\nand 'citation('spaMM')' for proper citation.\nFurther infos, slides, etc. at https://gitlab.mbb.univ-montp2.fr/francois/spamm-ref."
  },
  {
    "objectID": "code_demos.html",
    "href": "code_demos.html",
    "title": "Code Demos",
    "section": "",
    "text": "This section has code and demos that cover many topics and serves several purposes. The pages here are organised thematically, though it will likely take me some iterating on the quarto website yaml to get there.\nThe goal in many of these examples and demos is NOT clean, efficient coding, but exploring HOW the code works and how to accomplish something. That often means creating LOTS of extra variables, copy-paste, and being extremely verbose."
  },
  {
    "objectID": "code_demos.html#clarify-thinking-and-testing",
    "href": "code_demos.html#clarify-thinking-and-testing",
    "title": "Code Demos",
    "section": "Clarify thinking and testing",
    "text": "Clarify thinking and testing\nClarify what I’m actually trying to do, and what the expected outcomes are. Then figuring out a) how do get those, and b) why I sometimes don’t, which can be just as important. Doing this sort of testing here instead of in-project can be very helpful as using minimal examples forces me to isolate the issue I’m trying to solve from all the particulars of a given dataset or project structure."
  },
  {
    "objectID": "code_demos.html#central-location-for-useful-bits",
    "href": "code_demos.html#central-location-for-useful-bits",
    "title": "Code Demos",
    "section": "Central location for useful bits",
    "text": "Central location for useful bits\nA central point for (relatively) clean, complete things that I want to be able to use across many projects (e.g. 2d autocorrelation, the Johnson distribution, how to use certain packages, fonts, colours and other plotting things etc). Having one central reference point keeps me from having to either reinvent the wheel or remember which project I put the wheel in, and having many slightly different variations. And improvements/extensions can then be accessed across projects."
  },
  {
    "objectID": "code_demos.html#understanding-code-testing-beyond-standard-uses",
    "href": "code_demos.html#understanding-code-testing-beyond-standard-uses",
    "title": "Code Demos",
    "section": "Understanding code, testing beyond standard uses",
    "text": "Understanding code, testing beyond standard uses\nI spend quite a lot of time figuring out how to do things in code, understanding how code works, and double-checking everything is working correctly. There are a lot of good demos and tutorials out there (e.g. stackoverflow, some package vignettes and websites), but I often end up needing to figure out weird edge cases. And I often end up doing something similar later, but needing not the final answer, but some intermediate step along the way."
  },
  {
    "objectID": "code_demos.html#the-process-of-coding",
    "href": "code_demos.html#the-process-of-coding",
    "title": "Code Demos",
    "section": "The process of coding",
    "text": "The process of coding\nI also think there can be value in seeing how I’ve solved a problem and tested the various avenues, both for my own future reference and others. For one, if I do later have a need for one of those side avenues, they’re available. For another, it exposes the actual process of coding a bit more than the usual tutorial that has cleaned everything up start to finish. And it gives a better starting point for additional development potentially much later if I can see what I’ve already tried. Maybe most importantly, there are few tutorials/walkthroughs I’ve followed that don’t end up with some sort of error, especially as soon as I try to modify them for my purposes. Seeing where I’ve hit errors, what caused them, and how I solved it can be incredibly helpful, rather than only seeing what worked."
  },
  {
    "objectID": "data_acquisition/bom_gauges.html",
    "href": "data_acquisition/bom_gauges.html",
    "title": "Bom reference stations",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.2.2\n\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nTrying to find BOM gauge locations. Found reference stations. It’s a simple link, but have to use httr2 to download because there’s an error with the user_agent if we try to just download.file.\nMostly including this here as an example of changing user_agent.\nhttp://www.bom.gov.au/waterdata/ has a clickable link to what I want, but the data is buried in a frame so can’t scrape.\nThe below is because I found a link to reference stations and wanted to see what they were.\n\nIs there a url for BOM?\nIt’s just a csv, but have to faff about with httr2 and deparsing back to csv because need to pass a user agent or get a 403 error.\n\nbom2 <- httr2::request(\"http://www.bom.gov.au/water/hrs/content/hrs_station_details.csv\") |>\n  httr2::req_user_agent(\"md-werp\") |> \n  httr2::req_perform() |> \n  httr2::resp_body_string() |> \n  readr::read_csv(skip = 11) |> \n  dplyr::select(site = `Station Name`, \n                gauge = `AWRC Station Number`,  \n                owner = `Data Owner Name`, \n                Latitude, Longitude)\n\nRows: 467 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): AWRC Station Number, Station Name, Jurisdiction, Data Owner Name, D...\ndbl (3): Latitude, Longitude, Catchment Area (km2)\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nbasin <- read_sf(file.path('data','mdb_boundary', 'mdb_boundary.shp'))\n\n\nbom2 <- bom2 |>\n    # lat an long come in as chr because there is a line for 'undefined'\n    dplyr::filter(site != 'undefined') |>\n  st_as_sf(coords = c('Longitude', 'Latitude'), crs = 4326) |> \n  st_transform(crs = st_crs(basin))\n\nThey’re not the gauges I’m looking for. Only 457, instead of 6500, and around the edges of the basin.\n\nggplot() + \n  geom_sf(data = basin) +\n  geom_sf(data = bom2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a placeholder. I am a community ecologist with a focus on aquatic ecology. My background is in theoretical community ecology, though I also do fieldwork in aquatic systems and develop management-focused models. I have a particular interest in scaling, probabalistic modeling, and climate impacts.\nPostdoctoral researcher in the QAEL Lab at Deakin University."
  },
  {
    "objectID": "package/package_creation.html",
    "href": "package/package_creation.html",
    "title": "Creating a package",
    "section": "",
    "text": "I’ve always meant to build packages, but never quite have the time, and often end up with very convoluted projects that are not ideal for shoehorning into a typical package structure, particularly as a first try.\nIn part, I think, that is because my code is often a combination of package-type-things (functions, tests, other software flow) and analyses. It’s unclear what the best approach to this sort of flow is, where we absolutely want functions, but they are very specific to the analyses, of which there are many. Do the analyses go in the package? In two projects, but that’s a hassle? Anyway, that’s a topic for a longer post.\nHere, I have a self-contained, broadly usable bit of code I’m working on to extract information from the Victoria (Australia) waterdata network API. It’s more interesting than a Hello World type package, but also constrained in scope and the analyses can clearly go elsewhere.\nThis doc will be developed as I go, and so like most docs on this site isn’t a tutorial per se, but a sequence of steps, including pitfalls and recoveries (hopefully).\n\n\nFirst, created a repo in git.\nFor the main package development, I’m largely going to follow https://r-pkgs.org/, though I’m hoping I don’t have to read the whole thing (I know I should, but time is time).\nOpened a new Rstudio session (I use renv, but want to adjust some things globally- particularly {devtools}).\ninstall_packages(\"devtools\"), then devtools::dev_sitrep() and install any requested updates (in my case, {roxygen2} was out of date.\n\ndevtools::dev_sitrep()\n\n── R ───────────────────────────────────────────────────────────────────────────\n• version: 4.2.1\n• path: 'C:/Program Files/R/R-4.2.1/'\n\n\n• R is out of date (4.2.1 vs 4.2.2)\n\n\n── Rtools ──────────────────────────────────────────────────────────────────────\n• path: 'C:/rtools42/usr/bin/'\n── devtools ────────────────────────────────────────────────────────────────────\n• version: 2.4.5\n\n\n• devtools or its dependencies out of date:\n  'jsonlite', 'stringr', 'openssl', 'whisker', 'gert'\n  Update them with `devtools::update_packages(\"devtools\")`\n\n\n── dev package ─────────────────────────────────────────────────────────────────\n• package: <unset>\n• path: <unset>\n\n\nR is also out of date (at the time of writing). Fix it with rig, then re-run and update the packages.\n\ndevtools::update_packages('devtools')\n\nCheck the name I used works.\n\navailable::available('vicwater')\n\nWarning: package 'tidytext' was built under R version 4.2.2\n\n\n── vicwater ────────────────────────────────────────────────────────────────────\nName valid: ✔\nAvailable on CRAN: ✔ \nAvailable on Bioconductor: ✔\nAvailable on GitHub:  ✔ \nAbbreviations: http://www.abbreviations.com/vicwater\nWikipedia: https://en.wikipedia.org/wiki/vicwater\nWiktionary: https://en.wiktionary.org/wiki/vicwater\nSentiment:???\n\n\nLooks good.\nQuestion- I typically use Rprojects and renv to manage dependencies and sandbox projects. I also know that I can just devtools::create() (which I think just wraps usethis::create_package(). Can I start with the Rproj and then turn it into a package? Should I want to?\nAnswer- I just needed to read a bit further. Rstudio has devtools and Rprojects working together. So calling usethis::create_package() builds the project and puts all the scaffolding where it needs to be. I’ll need to cross the existing complex Rproj –> package bridge with another project later, but this is fairly straightforward here.\nSo, let’s create the package.\n\nusethis::create_package('~Galen/Documents/vicwater')\n\nAnd that worked with an existing directory. Was kind of worried about that. And it auto-opens a new Rstudio session.\nNow I’m mostly moving over there, but I ran usethis::use_mit_license() to set the license. Looks like description and namespace need work, but do that later.\nLet’s start building.\n\n\n\nI’ve been testing and poking at the API in some qmds here. I expect a lot of that ends up as vignettes in the package, and some is ready to become functions. I’ll likely maintain that flow- test in the qmd, make into functions there, repeat.\nI’m going to go write a function, and then figure out how to use it.\nSwitching to the native pipe |> to see how it goes and reduce dependencies.\n\n\nFor dependencies, I used usethis::use_package(), which installs and auto-populates the DESCRIPTION file. But I think I’m going to try using renv in here too, so I don’t always overwrite system-wide libraries. Hope it doesn’t screw anything up. Usual renv::init().\npackages that are nice to have (e.g. to allow parallelisation) are usethis::use_package('packagename', type = \"suggests\"). And if we want to import a function and not use package::function, use_import_from()- see below for the %dopar%.\n\n\n\nSo, I think usually the thing to do is run devtools::load_all() within the package project. I’m sure I’ll end up doing that. But it is also be possible to run it here, just passing the path, e.g. devtools::load_all(\"path/to/package/dir\"). That lets me work on test and development qmds and scripts here. For a bit. But why? For one, seems like vignettes have to use rmd at least at present. And it keeps all the trial and error out of that repo.\nI got hung up here for a while trying to pre-figure out how I’d install it once it was on github. Turns out it’s super straightforward (see below). It ends up just working as long as the thing on github has basic package structure.\n\n\n\nI’m using roxygen comments, as in the package dev book and roxygen docs for things like inheriting parameters and sections. Running devtools::document() builds the .rd files and means ?function works. There’s a lot of fancy stuff we could do there, but keeping it simple at first.\n\n\n\nI like having actual demonstrations of the code, rather than just function docs, so using usethis::use_vignette to start building some. They have to be in rmd, not qmd. But the visual editor still works, which is nice. Just going to have to re-remember rmd chunk headers.\nI can’t get df_print: paged to work. I think it might be a difference between html and html_vignette, but it is listed as an option in the help. For now using kable even though it’s huge for tables.\nI ended up using the main vignette as an example in the primary github readme. To do that, I did usethis::use_readme_rmd(). Would be good to sort out {pkgdown}, or maybe there’s a streamlined quarto version that builds a website?\n\n\n\nUsing usethis::use_testthat(3) and writing tests was fairly straighforward, but I think there will be a learning curve about what and how to test. I tend to look very granularly at ad-hoc tests, i.e. scanning for weird NA, types, etc. But testthat and the expect_* functions lend themselves to simpler checks.\nIt gets sort of cumbersome if a function takes a while and generates something complex. In that case, I built tests that run the function (and so are fragile to the function just erroring out), and then run multiple different expect_* tests against it to make sure the output is right. As an example,\n\ntest_that(\"derived variables work for ts\", {\n  s3 <- get_response(\"https://data.water.vic.gov.au/cgi/webservice.exe?\",\n                     paramlist = list(\"function\" = 'get_ts_traces',\n                                      \"version\" = \"2\",\n                                      \"params\" = list(\"site_list\" = '233217',\n                                                      \"start_time\" = 20200101,\n                                                      \"varfrom\" = \"100\",\n                                                      \"varto\" = \"140\",\n                                                      \"interval\" = \"day\",\n                                                      \"datasource\" = \"A\",\n                                                      \"end_time\" = 20200105,\n                                                      \"data_type\" = \"mean\",\n                                                      \"multiplier\" = 1)))\n  expect_equal(class(s3), 'list')\n  expect_equal(s3[[1]], 0)\n\n})\n\nAnd then, if I want to hit the function with edge cases, etc, I have to do that over and over. There’s likely a better way, but I’ll need to experiment.\n\n\n\n\nTrying to use %dopar%, but can’t get foreach::%dopar% to work, or with backticks. Putting it in a roxygen comment as @importFrom foreach %dopar% failed too. Seems to have worked to do usethis::use_import_from('foreach', '%dopar%'), which built some new files.\nHaving a hard time testing with doFuture, since it can’t find this package. pause that for a while\n\n\n\nOnce it’s pushed to github, it’s fairly straightforward to install- just\n\ndevtools::install_github(\"galenholt/vicwater\")\n\n\n\n\nIt ended up being pretty straightforward to use devtools::check() and using continuous integration with github to run the checks and put the little badges on, as described in the book."
  },
  {
    "objectID": "plotting/faded_colors.html",
    "href": "plotting/faded_colors.html",
    "title": "Faded colors",
    "section": "",
    "text": "There are a number of reasons we might want bivariate color axes in plots. The particular use I’m looking for now is to use a faded color to indicate less certainty in a result. Other uses will be developed later or elsewhere, but should build on this fairly straightforwardly.\nI’m doing this with colorspace because it’s hue-chroma-luminance approach makes it at least appear logical to shift along those dimensions. We might want hue (or luminance) to show one thing, and intensity to show another. Though we will play around with how that looks in practice. The specific use motivatiung this is to show the predicted amount of something with hue, and certainty with chroma or luminance (in particular, we have a model that makes predictions more accurately in some places than others). But there are many other potential uses.\nIn the HCL exploration file, I figure out HOW to generate faded colors and find some palettes that might work. Here, I’m going to sort out how to go from there to using them in plots, including creating legends."
  },
  {
    "objectID": "plotting/faded_colors.html#plot-the-bivariate-colors",
    "href": "plotting/faded_colors.html#plot-the-bivariate-colors",
    "title": "Faded colors",
    "section": "Plot the bivariate colors",
    "text": "Plot the bivariate colors\nBefore trying to plot with the colors, first I want to actually plot them themselves. One reason is to test how they are being created and specified, and the other is potentially to use the plot as a legend.\nWhy? The legend() part of ggplot may not handle the bivariate nature of the colors well, so need to basically homebrew one. This is the most flexible option- make the plot, then shrink and pretend it’s a legend. But, could also make a legend in vector form, then stack. Just not sure how well that’ll work. The shrunk plot would work better for continuous variables, the legend probably works better to use other parts of ggplot and not always have to screw around with grobs or ggarrange or patchwork or cowplot. I’ll try them all, I guess.\nFirst, make a matrix of colors. Take the base palette, fade it and save the color values for the whole thing. The for loop is lame, should be a function, but I’m just looking right now.\n\nbaseramp <- sequential_hcl(8, 'ag_Sunset')\n\nfadesteps <- seq(0,1, by = 0.25)\n\ncolormat <- matrix(rep(baseramp, length(fadesteps)), nrow = 5, byrow = TRUE)\n\nfor(i in 1:length(fadesteps)) {\n  colormat[i, ] <- lighten(colormat[i, ], amount = fadesteps[i]) %>%\n    desaturate(amount = fadesteps[i])\n}\n\nOption 1 is to make that into a plot that we can then smash on top\n\n# Make a tibble from the matrix to feed to ggplot\ncoltib <- as_tibble(colormat, rownames = 'row') %>%\n  pivot_longer(cols = starts_with('V'), names_to = 'column')\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n# coltib\n\nggplot(coltib, aes(y = row, x = column, fill = value)) + \n  geom_tile() + scale_fill_identity()\n\n\n\n\nThat’s upside-down with how I tend to think about it. How about flipping the construction?\n\nfadesteps <- rev(seq(0,1, by = 0.25))\ncolormat <- matrix(rep(baseramp, length(fadesteps)), nrow = 5, byrow = TRUE)\n\nfor(i in 1:length(fadesteps)) {\n  colormat[i, ] <- lighten(colormat[i, ], amount = fadesteps[i]) %>%\n    desaturate(amount = fadesteps[i])\n}\n\ncoltib <- as_tibble(colormat, rownames = 'row') %>%\n  pivot_longer(cols = starts_with('V'), names_to = 'column')\n\n\nggplot(coltib, aes(y = row, x = column, fill = value)) +\n  geom_tile() + scale_fill_identity()"
  },
  {
    "objectID": "plotting/faded_colors.html#programmatic-color-setting",
    "href": "plotting/faded_colors.html#programmatic-color-setting",
    "title": "Faded colors",
    "section": "Programmatic color setting",
    "text": "Programmatic color setting\nCreate a function basically following the above. But allow it to take palettes by name or raw hue values if they are obtained elsewhere (like from a manually specified hue ramp). hex color vals and pal names are both characters, but hex always starts with ‘#’, so should be able to auto-detect. It can take a number of fades, or a vector of specific fade levels, and returns the matrix of colors.\n\ncol2dmat <- function(pal, n1, n2 = 2, dropwhite = TRUE, fadevals = NULL) {\n  # pal can be either a palette name or a vector of hex colors (or single hex color)\n  # dropwhite is there to by default knock off the bottom row that's all white\n  # fadevals is a way to bypass the n2 and specify specific fade levels (ie if nonlinear)\n\n  if (all(str_detect(pal, '#'))) {\n    baseramp <- pal\n  } else {\n    baseramp <- sequential_hcl(n1, pal)\n  }\n\n  if (is.null(fadevals)) {\n    if (dropwhite) {n2 = n2+1}\n\n    fadesteps <- rev(seq(0,1, length.out = n2))\n\n    if (dropwhite) {fadesteps <- fadesteps[2:length(fadesteps)]}\n\n  }\n\n  if (!is.null(fadevals)) {\n    fadesteps <- sort(fadevals, decreasing = TRUE)\n  }\n\n  colormat <- matrix(rep(baseramp, length(fadesteps)), nrow = length(fadesteps), byrow = TRUE)\n\n\n  for(i in 1:length(fadesteps)) {\n    colormat[i, ] <- lighten(colormat[i, ], amount = fadesteps[i]) %>%\n      desaturate(amount = fadesteps[i])\n  }\n\n  return(colormat)\n}\n\nCreate another function that plots a matrix of colors. Typically that matrix comes out of col2dmat. Why not make one big function? because we will often want to access the color values themselves, and not always just plot them.\n\nplot2dcols <- function(colmat) {\n  coltib <- as_tibble(colmat, rownames = 'row') %>%\n    pivot_longer(cols = starts_with('V'), names_to = 'column') %>%\n    mutate(row = as.numeric(row), column = as.numeric(str_remove(column, 'V')))\n\n  colplot <- ggplot(coltib, aes(y = row, x = column, fill = value)) +\n    geom_tile() + scale_fill_identity()\n\n  return(colplot)\n}\n\nTest that works with a given number of fades\n\nnewcolors <- col2dmat('ag_Sunset', n1 = 8, n2 = 4)\nplot2dcols(newcolors)\n\n\n\n\nTest with set fade levels. REMEMBER FADE is FADE, not intensity. ie 0 is darkest.\n\nnewcolsuneven <- col2dmat('ag_Sunset', n1 = 8, fadevals = c(0, 0.33, 0.8))\nplot2dcols(newcolsuneven)\n\n\n\n\nTest with non-built in palettes- ie setting hue manually. This could be particularly useful if we want quantitative hues. This tests the ability to auto-detect a vector of colors.\nUse the manual-set colors from hcl exploration for testing.\n\nhclmat <- cbind(50, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 50, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg <- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\nWorks!\n\npgmat <- col2dmat(hex(pg), n2 = 4)\nplot2dcols(pgmat)"
  },
  {
    "objectID": "plotting/faded_colors.html#plotting-the-data",
    "href": "plotting/faded_colors.html#plotting-the-data",
    "title": "Faded colors",
    "section": "Plotting the data",
    "text": "Plotting the data\nAbove, we were trying to plot the colors. Now, we want to assign those colors to data so we can plot the data with the appropriate color.\n\nSingle datapoint\nThe above is fine for looking at a color matrix, but in general, we’ll have a dataframe with a value for each dimension, and need to assign it a single color. Step one is figuring out how to do that assignment.\nCan I take a ‘datapoint’ with arbitrary values on both axes and choose its color?\nCan we do that for both color bins or continuous color?\nWe’ll need to relativise the data, since neither hue or fade are defined on the real line, but by their endpoints.\nLet’s fake some data. Don’t use round numbers (e.g. 0, 100) to avoid making stupid mistakes relating to relativising the scale. We need to know the endpoints of the data to match the endpoints of the hue and fade, and then a datapoint somewhere in the middle to create.\n\n# what is the range of the data?\n  # don't use round numbers (e.g. 0, 100)\nmax1 <- 750\nmin1 <- 150\n\nmax2 <- 67\nmin2 <- -55\n\n\n# get color for a single value pair\nval1 <- 455\nval2 <- 8\n\njust use a simple linear transform to get position on the min-max axes. Could use logit or something for either, but keeping it simple. The value above the min divided by the range gives where the data point is on a 0-1 scale from min to max. In reality, we will have two vectors (well, cols in a dataframe), and this is actually easier to do in that case because we can just get the min and max directly.\n\nvalpos1 <- (val1-min1)/(max1-min1)\nvalpos2 <- (val2-min2)/(max2-min2)\n\nThat’s easy to vectorize, which is basically how we’ll do it with a dataframe.\nFor now, can we just get individual colors to assign to a value pair?\nNeed to specify the min and max hue- these are the hue endpoints, not data endpoints.\n\nminhue <- 130\nmaxhue <- 275\n\nfind the hue value at the same relative position as the datapoint\n\nmatchH1 <- (maxhue-minhue)*valpos1 + minhue\n\nUsing the manual colors\n\nsinglehclmat1 <- cbind(50, max_chroma(h = matchH1, l = 50, floor = TRUE),\n                matchH1)\n\npgsingle1 <- polarLUV(singlehclmat1)\nswatchplot(hex(pgsingle1))\n\n\n\n\nalso need the other axis. That’s also just on 0-1 (well, 1-0, since it’s fade, not intensity) and so would be done the same way.\n\nsinglecol <- col2dmat(hex(pgsingle1), fadevals = (1-valpos2))\nswatchplot(singlecol)\n\n\n\n\nIt’s clear we can write all this as functions, and that we’ll need to. So…\n\n\nProgramatically finding colors\nEarlier, we made col2dmat, which found colors and faded them. We want to do something similar here, but the goal isn’t quite the same- we don’t really care about the full matrix, but about a single point. We could modify col2dmat, but probably easier (and fewer horrible logicals) to just write purpose-built functions.\nNeed new functions to 1) find the hue, 2) adjust the fade\n\nFind the hue\nTakes either a number of bins or Inf for continuous.\n\nhuefinder <- function(hueval, minhue, maxhue, n = Inf, palname = NULL) {\n\n  # If continuous, use the value\n  # If binned, find the value of the bin the value is in\n  if (is.infinite(n)) {\n    matchH <- (maxhue-minhue)*hueval + minhue\n  } else if (!is.infinite(n)) {\n\n    nvec <- seq(from = 0, to = 1, length.out = n)\n\n    # The nvecs need to choose the COLOR, but the last one gets dropped in\n    # findInterval, so need an n+1\n    whichbin <- findInterval(hueval,\n                             seq(from = 0, to = 1, length.out = n+1),\n                             rightmost.closed = TRUE)\n\n    # Don't build if using named palette because won't have min and max\n    if (is.null(palname)) {\n      binhue <- nvec[whichbin]\n      matchH <- (maxhue-minhue)*binhue + minhue\n    }\n\n  }\n\n  if (is.null(palname)) {\n    h <- cbind(50, max_chroma(h = matchH, l = 50, floor = TRUE),\n               matchH)\n    h <- hex(polarLUV(h))\n  } else {\n    h <- sequential_hcl(n, palname)[whichbin]\n  }\n\n  return(h)\n}\n\n\n\nFind the fade\nThis takes the just found hue as basehue, and fades it. Again, n specifies either a number of fade bins or if infinite it is continuous and so just fades by whatever the value is.\n\nfadefinder <- function(fadeval, basehue, n = Inf) {\n\n  # If n is infinite, just use fadeval. Otherwise, bin, dropping the all-white level\n  if (is.infinite(n)) {\n    fadeval <- fadeval\n  } else {\n    # The +1 drops the white level\n    fadevec <- seq(from = 0, to = 1, length.out = n + 1)\n\n    # Rightmost closed fixes an issue right at 1\n    fadeval <- fadevec[findInterval(fadeval, fadevec, rightmost.closed = TRUE) + 1]\n  }\n\n  fadedcol <- lighten(basehue, amount = 1-fadeval) %>%\n    desaturate(amount = 1-fadeval)\n}\n\n\n\nHue and fade\nThis is meant to use in a mutate to take two columns of data and find the appropriate color. Should use … to pass, but whatever\n\ncolfinder <- function(hueval, fadeval, minhue, maxhue, nhue = Inf, nfade = Inf, palname = NULL) {\n  thishue <- huefinder(hueval, minhue, maxhue, nhue, palname)\n  thiscolor <- fadefinder(fadeval, thishue, nfade)\n}\n\nQuick tests\n\nfunhue <- huefinder(valpos1, minhue = minhue, maxhue = maxhue)\nfunfaded <- fadefinder(valpos2, funhue)\nswatchplot(funfaded)\n\n\n\n\nshould be the same as\n\nfunboth <- colfinder(valpos1, valpos2, minhue, maxhue)\nswatchplot(funboth)\n\n\n\n\n\n\n\nCalculating for dataframes\nVectorizing the relativization calculations is straightforward.\n\nvec1 <- c(150, 588, 750, 455, 234)\n\n# get it for each value in vectorized way\n(vec1 - min(vec1))/(max(vec1)-min(vec1))\n\n[1] 0.0000000 0.7300000 1.0000000 0.5083333 0.1400000\n\n\nMaking a function to get the relative position. We can use this in the mutate once we move on to dataframes.\n\nrelpos <- function(vec) {\n  (vec - min(vec))/(max(vec)-min(vec))\n}\n\nNow, let’s make a dataframe of fake data, with one column that should map to hue and the other mapping to fade. This just puts points all across the space of both variables so we can make sure everything is getting assigned correctly. Then, we’ll use the functions we just created to do a few different things:\n\ncustom hue ramps and built-in palettes\nbinned hue and fade\ncontinuous hue and binned fade\nboth continuous\n\nThe ‘continuous’ examples using inbuilt palettes are only pseudo-continuous by using large numbers of bins because that’s easier for the moment given the way sequential_hcl() works. There’s probably a way around it, but for the moment I’ll ignore it.\n\ncolortibble <- tibble(rvec1 = runif(10000, min = -20, max = 50),\n       rvec2 = runif(10000, min = 53, max = 99)) %>%\n  mutate(rel1 = relpos(rvec1),\n         rel2 = relpos(rvec2)) %>%\n  mutate(colorval = colfinder(rel1, rel2, minhue, maxhue),\n         binval = colfinder(rel1, rel2, minhue, maxhue, nhue = 8, nfade = 4),\n         # need to bypass some args\n         binsun = colfinder(rel1, rel2, nhue = 8, nfade = 4, palname = 'ag_Sunset',\n                            minhue = NULL, maxhue = NULL),\n         pseudoconsun = colfinder(rel1, rel2, nhue = 1000, nfade = 4, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL),\n         pseudoconsun2 = colfinder(rel1, rel2, nhue = 1000, nfade = Inf, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL))\n\nContinuous in both dimensions, using custom hue ramp\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = colorval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nBinned both dims, custom ramp\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = binval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nInbuilt palette, binned both dims.\nThere is a spot in this ag_Sunset palette that matches the ggplot default grey background and so hard to see, but I’ll ignore that for the moment since it doesn’t affect the main thing we’re doing. THese aren’t production plots.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = binsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nPseudo-continuous, binned fades.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = pseudoconsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nPseudo-continuous both dimensions.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = pseudoconsun2)) +\n  geom_point() +\n  scale_color_identity()"
  },
  {
    "objectID": "plotting/faded_colors.html#plotting-data",
    "href": "plotting/faded_colors.html#plotting-data",
    "title": "Faded colors",
    "section": "Plotting data",
    "text": "Plotting data\nNow, let’s see how that might look for some real data. I’ll use some with point data (iris) and then move on to maps, since that’s originally what this was developed for. It should easily extend to anything we can aes() on, e.g. barplot fills, etc.\n\nScatterplot\nTo keep it simple, let’s use iris\nIt won’t span the full space because of the relationship, but that’s OK, I think. We did that above. Here’s iris- now let’s color this plot.\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width)) + geom_point()\n\n\n\n\n\nFade defined by an axis\nThis is how we did it above when plotting the colors to make sure they were working.\nRelativize the x and y to define colors.\n\ncoloriris <- iris %>%\n  mutate(rel1 = relpos(Sepal.Length),\n         rel2 = relpos(Petal.Width)) %>%\n  mutate(colorval = colfinder(rel1, rel2, minhue, maxhue),\n         binval = colfinder(rel1, rel2, minhue, maxhue, nhue = 8, nfade = 4),\n         # need to bypass some args\n         binsun = colfinder(rel1, rel2, nhue = 8, nfade = 4, palname = 'ag_Sunset',\n                            minhue = NULL, maxhue = NULL),\n         pseudoconsun = colfinder(rel1, rel2, nhue = 1000, nfade = 4, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL),\n         pseudoconsun2 = colfinder(rel1, rel2, nhue = 1000, nfade = Inf, palname = 'ag_Sunset',\n                                   minhue = NULL, maxhue = NULL))\n\nMake some plots to see the colors and fades correspond to the axis values in binned and unbinned ways.\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = colorval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = pseudoconsun2)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = binsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\nFade as a new aesthetic\nTo actually match what I want to use this for, it’s more like we’d say versicolor is less certain. IE Species defines the fade. This is like fade is an aesthetic in ggplot, but we’re sort of manually doing it.\nLet’s set hue by sepal length, and fade by species\n\nuncertainVers <- iris %>%\n  mutate(rel1 = relpos(Sepal.Length),\n         faded = ifelse(Species == 'versicolor', 0.50, 1)) %>%\n  mutate(binhue = huefinder(rel1, n = 8, palname = 'ag_Sunset'),\n         conhue = huefinder(rel1, n = 1000, palname = 'ag_Sunset'),\n         binfade = fadefinder(faded, binhue),\n         confade = fadefinder(faded, conhue))\n\nNow, versicolor should be faded relative to the others\n\nggplot(uncertainVers, aes(x = Sepal.Length, y = Petal.Width, color = binfade)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\nggplot(uncertainVers, aes(x = Sepal.Length, y = Petal.Width, color = confade)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nThat seems to be working, both binned and continous on the hue scale.\n\n\n\nMaps\nWhat I really want this for is a map, with each polygon having a value of the variable of interest mapped to hue, and a ‘certainty’ determining the fade. Though that axis could really be any other value. Can I mock that up?\nRead a map in of catchments in Australia.\n\nallbasins <- read_sf(file.path('data', '42343_shp', 'rbasin_polygon.shp'))\n\nIgnoring fade for the minute, what should we color by? Probably should be random, really, for the demo.\nColoring by centroid will just put a cross-country fade on:\n\nggplot(allbasins, aes(fill = CENTROID_X)) + geom_sf() + scale_fill_continuous_sequential('ag_Sunset')\n\n\n\n\nLet’s make a column representing the value we want to plot for each basin, just chosen at random\n\nallbasins <- allbasins %>%\n  mutate(fakevals = runif(nrow(allbasins))) %>%\n  mutate(rel1 = relpos(fakevals)) %>%\n  mutate(binhue = huefinder(rel1, n = 8, palname = 'ag_Sunset'),\n         conhue = huefinder(rel1, n = 1000, palname = 'ag_Sunset'))\n\nI can use the values directly here with scale_fill_XX if I don’t care about fade\n\nggplot(allbasins, aes(fill = fakevals)) + geom_sf() + scale_fill_continuous_sequential('ag_Sunset')\n\n\n\n\nbut the hues for the faded should match the set hues. Now, I need to use scale_fill_identity(). Works for binned and pseudo-continuous. I’ll save the binned to compare later with the faded version.\n\nhuesonly <- ggplot(allbasins, aes(fill = binhue)) +\n  geom_sf() +\n  scale_fill_identity()\nhuesonly\n\n\n\nggplot(allbasins, aes(fill = conhue)) +\n  geom_sf() +\n  scale_fill_identity()\n\n\n\n\nNow, fade some out (with relatively low probability)\n\nallbasins <- allbasins %>%\n  mutate(faded = sample(x = c(1, 0.5),\n                           size = nrow(allbasins),\n                           replace = TRUE,\n                           prob = c(0.8, 0.2))) %>%\n  mutate(binfade = fadefinder(faded, binhue),\n         confade = fadefinder(faded, conhue))\n\nBinned and continuous. Again, save the binned for comparison\n\nhuefade <- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity()\nhuefade\n\n\n\nggplot(allbasins, aes(fill = confade)) +\n  geom_sf() +\n  scale_fill_identity()\n\n\n\n\nplot the raw and faded next to each other using patchwork. We can now see that some of the catchments are faded versions of the original hue.\n\nhuesonly + huefade\n\n\n\n\n\nLegends\nWe need legends. Could be done by playing with the actual ggplot legend or making mini plot and gluing on.\nQuick attempt at guide fails, because the colors are mixed up because of the RGB sorting.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend') +\n  guides(fill = guide_legend(ncol = 2))\n\n\n\n\nCan I change the order by basing it on the hues and then the fades? Does ‘breaks’ work? Yeah, sort of. And need to sort them in the right way.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = unique(allbasins$binhue))\n\n\n\n\nI think that will basically work, but I’ll need to edit a bit There’s probably a way to write the functions better to just do this all in the mutates, but for now, I can create a tibble of breaks and labels using summarise.\n\nbreaksnlabels <- allbasins %>%\n  st_drop_geometry() %>%\n  group_by(binhue) %>%\n  summarize(minbin = min(fakevals),\n            maxbin = max(fakevals),\n            fromto = paste0(as.character(round(minbin, 2)),\n                            ' to ',\n                            as.character(round(maxbin, 2)))) %>%\n  ungroup() %>%\n  arrange(minbin)\n\nWorks for the unfaded\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = breaksnlabels$binhue,\n                      labels = breaksnlabels$fromto)\n\n\n\n\nI could now ALSO fade those, but I might be able to do it as one summarise using the faded column\n\nfadebreaks <- allbasins %>%\n  st_drop_geometry() %>%\n  # needs to capture the color boundaries, whether or not faded\n  group_by(binhue) %>%\n  mutate(minbin = min(fakevals),\n            maxbin = max(fakevals),\n            fromto = paste0(as.character(round(minbin, 2)),\n                            ' to ',\n                            as.character(round(maxbin, 2)))) %>%\n  ungroup() %>%\n  group_by(binfade, faded) %>%\n  summarize(minbin = first(minbin),\n            maxbin = first(maxbin),\n            fromto = first(fromto)) %>%\n  ungroup() %>%\n  arrange(minbin, desc(faded))\n\n`summarise()` has grouped output by 'binfade'. You can override using the\n`.groups` argument.\n\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fadebreaks$binfade,\n                      labels = fadebreaks$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\nPlot tweaking\nThat’s close. Can I make the labels better? Ideally, drop from the faded, and make them at 45 or something. and fix up the size.\nFirst, drop the labels on the faded, since they are the same as the base hue.\n\nfb2 <- fadebreaks %>%\n  mutate(fromto = ifelse(faded == 0.5, '', fromto))\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom')\n\n\n\n\nFixing up the sizes and angles. The size doesn’t do what I want (square), because the text is too big.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n      legend.background = element_blank(),\n      legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n      legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nCan I fake it on the row labels by inserting line breaks? The number of lines is really unstable across device sizes or saving the figure, so the number of line breaks will have to be adjusted every time this gets saved etc. But it might kind of work.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value\\n\\n\\n\\nCertain\\n\\n\\nUncertain', title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nCan I bold that title?\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = expression(atop(bold('Value'),atop('Certain','Uncertain'))),\n                             title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nThat doesn’t work very well. Does ggtext do it? Allows markdown syntax and HTML (hence the  instead of ). It works, but still, the number of breaks will depend on the size of the figure device or file\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = '**Value**<br><br><br><br>Certain<br><br>Uncertain',\n                             title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'), # This should make them square, but isn't because the angled value labels don't allow it.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nIf we want square legend boxes and readable text for the value labels, might have to go vertical and that means re-doing the breaks and labels dataframe\n\nfbv <- fadebreaks %>%\n  mutate(fromto = ifelse(faded == 1, '', fromto)) %>%\n  arrange(desc(faded), minbin)\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  guides(fill = guide_legend(title = '**Value**<br><br>Certain Uncertain',\n                             title.position = 'top',\n                             ncol = 2, label.position = 'right')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'right',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'))\n\n\n\n\nThat works pretty well. If we wanted multiple levels of uncertainty (fades), a similar thing would work with just having more columns. That basically works. If I want to label the fades more robustly, I think I’ll likely need to resort to grobs, in which case I probably might as well do the figure as legend method.\n\n\nMini-figure legends\nSometimes we want to create a legend and then add it back into a figure (maybe if it’s shared, or we want a standard legend across a group of figures). Here, we might want to create a different legend for the certian and uncertain, glue them together, and then glue them back on the main figure.\nto show how this might make sense, let’s make three plots- one with just the certain, one with uncertain, and one with no legend, and then glue together Making this as vertical, but easy enough to swap\nMake the map alone\n\njustmap <- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  theme(legend.position = 'none')\n\n# used later- continuous specification of color and fade\njustmapcon <- ggplot(allbasins, aes(fill = confade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  theme(legend.position = 'none')\n\nGet the indices for the two fades\n\ncerts <- which(fbv$faded == 1)\nuncerts <- which(fbv$faded == 0.5)\n\nMake the legend for the unfaded\n\ncertleg <- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade[certs],\n                      labels = fbv$fromto[certs]) +\n  guides(fill = guide_legend(title = 'Certain',\n                             title.position = 'top',\n                             ncol = 1, label.position = 'right')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'right',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'))\n\n# I don't actually want the plot, just the legend, so\n certleg <- ggpubr::get_legend(certleg)\n\nAnd the faded\n\n uncertleg <- ggplot(allbasins, aes(fill = binfade)) +\n   geom_sf() +\n   scale_fill_identity(guide = 'legend',\n                       breaks = fbv$binfade[uncerts],\n                       labels = fbv$fromto[uncerts]) +\n   guides(fill = guide_legend(title = 'Uncertain',\n                              title.position = 'top',\n                              ncol = 1, label.position = 'right')) +\n   theme(legend.title = ggtext::element_markdown(),\n         legend.position = 'right',\n         legend.background = element_blank(),\n         legend.key.size = unit(0.5, 'cm'))\n\n # I don't actually want the plot, just the legend, so\n uncertleg <- ggpubr::get_legend(uncertleg)\n\nGlue those legends\n\nbothleg <- ggpubr::ggarrange(certleg, uncertleg)\n\nand glue on the plot\n\n plotpluslegs <- ggpubr::ggarrange(justmap, bothleg, widths = c(8,2))\n plotpluslegs\n\n\n\n\nThat’s not really any better than what I had before. It is useful to have this level of control sometimes though. In particular, we might want to use a PLOT as a legend, either binned or not.\nTo use a plot as a legend\nHere, binned is obviously the way to go, especially for the two fade levels, but let’s demo both.\nabove, we defined a function col2dmat that makes a plot of the color matrix. Let’s use that to demo a few options. First create the figures that will be the legends.\nBinned both dims, two fades, but just low-high labels\n\nbinnedplotmat <- col2dmat('ag_Sunset', n1 = 8, fadevals = c(0, 0.5))\n bin2legqual <- plot2dcols(binnedplotmat) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = c(1, 2), labels = c('Uncertain', 'Certain')) +\n   # Vague levels\n   scale_x_continuous(breaks = c(1, 8), labels = c('Low', 'High')) +\n   theme(axis.text = element_text())\n bin2legqual\n\n\n\n\nBinned both dims, but now the hue values are quantitatively labeled\n\nnamedlabs <- filter(fb2, fromto != '') %>% select(fromto) %>% pull()\n bin2legquant <- plot2dcols(binnedplotmat) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = c(1, 2), labels = c('Uncertain', 'Certain')) +\n   # Vague levels\n   scale_x_continuous(breaks = 1:8, labels = namedlabs) +\n   theme(axis.text.y = element_text(),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n   ggtitle('Value')\n bin2legquant\n\n\n\n\nA few levels of fade. Very similar to above\n\nmat4fade <- col2dmat('ag_Sunset', n1 = 8, n2 = 4)\n\n fadevals <- rev(seq(0,1, length.out = 4+1))[1:4]\n bin4leg <- plot2dcols(mat4fade) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = 1:4, labels = rev(fadevals), name = 'Certainty') +\n   scale_x_continuous(breaks = 1:8, labels = namedlabs, name = 'Value') +\n   theme(axis.text.y = element_text(),\n         axis.title.y = element_text(angle = 90),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n         axis.title.x = element_text())\n bin4leg\n\n\n\n\npseudo-continuous. put the x-axis on top, because that’s what we’d expect for a legend, really. Labels can take a lambda function of the breaks, allowing us to use auto-chosen breaks. But probably better to reference the values they correspond to. It’s just that for this silly demo they are 0-1. Let’s pretend for the minute that they’re logged just for fun and to demo how to do it.\n\nmatcfade <- col2dmat('ag_Sunset', n1 = 100, n2 = 100)\n conleg <- plot2dcols(matcfade) +\n   theme_void() +\n   scale_y_continuous(name = 'Certainty %') +\n   #\n   scale_x_continuous(labels = ~round(log(.), 2), name = 'Value', position = 'top') +\n   theme(axis.text.y = element_text(),\n         axis.title.y = element_text(angle = 90),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n         axis.title.x = element_text())\n conleg\n\n\n\n\nNow, attach those to the map as legends.\nI’ll use patchwork for most of them, but ggpubr::ggarrange would work too, just with different tweaking. The way patchwork does insets and sizes is working better for me right now, so that’s what I’ll use.\nTaking the grey background off because it’s distracting with inset legends.\nTwo-level binned legend with high-low\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element(bin2legqual, left = 0.1, bottom = 0.1, right = 0.5, top = 0.2)\n\n\n\n\nSame, but quantitative legend labels. Text is a bit absurd.\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((bin2legquant + theme(axis.text = element_text(size = 8),\n                                       title = element_text(size = 8))),\n                 left = 0.1, bottom = 0, right = 0.5, top = 0.25)\n\n\n\n\nA 4-fade example with quantitative fades as well. That’s not our immediate need, but good to be able to do. maybe fade according to standard error or something.\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((bin4leg + theme(axis.text = element_text(size = 8),\n                                       title = element_text(size = 8))),\n                 left = 0.1, bottom = 0, right = 0.5, top = 0.25)\n\n\n\n\nContinuous values in both dimensions. Here, we use a map where colors and fades are both defined continuously.\n\n(justmapcon + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((conleg + coord_fixed() +\n                    theme(axis.text = element_text(size = 8),\n                          title = element_text(size = 8))),\n                 left = 0.1, bottom = 0.05, right = 0.5, top = 0.25)\n\n\n\n\nCan I put the legend off to the side just by specifying bigger coords? sort of- it goes but gets lost\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((conleg + coord_fixed() +\n                    theme(axis.text = element_text(size = 8),\n                          title = element_text(size = 8))),\n                 left = 1, bottom = 0.4, right = 1.5, top = 0.75)\n\n\n\n\nWorks with making a small plot with spacers and then glueing that onto the big plot\n\nguidespot <- plot_spacer() /\n   (conleg + coord_fixed() +\n   theme(axis.text = element_text(size = 8),\n         title = element_text(size = 8))) /\n   plot_spacer()\n\n (justmap + theme_bw() + theme(legend.position = 'none')) +\n   guidespot +\n   plot_layout(widths = c(9, 1))\n\n\n\n\nDoes that work with the simpler ones? Yeah, although the 2-fades makes more sense horizontal, so do that\n\n# I can't fiugre out why this creates a dataframe. results = 'hide' doesn't hide it, wrapping with invisible(), etc. I give up. Giving it its own code block\nguidespot2 <- plot_spacer() |\n   (bin2legquant + theme(axis.text = element_text(size = 8),\n                         title = element_text(size = 8))) |\n   plot_spacer()\n\n\n (justmap + theme_bw() + theme(legend.position = 'none')) /\n   guidespot2 +\n   plot_layout(heights = c(9, 1))\n\n\n\n\nA very similar approach would work for ggpubr::ggarrange\nThere’s quite a lot more that could be done here, but this gets me what I need for now."
  },
  {
    "objectID": "plotting/faded_colors.html#notes",
    "href": "plotting/faded_colors.html#notes",
    "title": "Faded colors",
    "section": "Notes",
    "text": "Notes\nif this were truly bivariate (ie two variables of interest), could rotate 45 degrees to equally weight (and likely use different color ramps). But it’s not- it’s certainty along one axis, so leaving horiz and having a lightness axis fits what we’re doing here better."
  },
  {
    "objectID": "plotting/fonts.html",
    "href": "plotting/fonts.html",
    "title": "Fonts",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\nUsing knitr::inline_expr(r message = FALSE, warning = FALSE) hopefully stops it printing all the package messages\n\nlibrary(tidyverse) # Overkill, but easier than picking and choosing\n\nThese are mostly little plot tweaks and small things that I find and forget all the time.\n\nAccessing fonts\nIn the past, I’ve used extrafonts to use fonts within figures, but it’s failing for me (‘No FontName, skipping’ error as in https://github.com/wch/extrafont/issues/88).\nTry sysfonts. Actually, showtext on top of sysfonts. First, look at how it finds the fonts.\n\nlibrary(showtext)\n\nWarning: package 'showtext' was built under R version 4.2.2\n\n\nLoading required package: sysfonts\n\n\nWarning: package 'sysfonts' was built under R version 4.2.2\n\n\nLoading required package: showtextdb\n\n\nWarning: package 'showtextdb' was built under R version 4.2.2\n\nfontsIhave <- font_files()\nfontsIhave\n\n\n\n  \n\n\nstr(fontsIhave)\n\n'data.frame':   349 obs. of  6 variables:\n $ path   : chr  \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" ...\n $ file   : chr  \"AGENCYB.TTF\" \"AGENCYR.TTF\" \"ALGER.TTF\" \"ANTQUAB.TTF\" ...\n $ family : chr  \"Agency FB\" \"Agency FB\" \"Algerian\" \"Book Antiqua\" ...\n $ face   : chr  \"Bold\" \"Regular\" \"Regular\" \"Bold\" ...\n $ version: chr  \"Version 1.01\" \"Version 1.01\" \"Version 1.57\" \"Version 2.35\" ...\n $ ps_name: chr  \"AgencyFB-Bold\" \"AgencyFB-Reg\" \"Algerian\" \"BookAntiqua-Bold\" ...\n\n\nI should be able to use font_add\nFirst, what fonts are CURRENTLY available in R?\n\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\nfont_families()\n\n[1] \"sans\"         \"serif\"        \"mono\"         \"wqy-microhei\"\n\n\nTest with one of the\n\nfont_add('Bookman Old Style', regular = 'BOOKOS.TTF', \n         italic = 'BOOKOSI.TTF', \n         bold = 'BOOKOSB.TTF', \n         bolditalic = 'BOOKOSBI.TTF')\n\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\nfont_families()\n\n[1] \"sans\"              \"serif\"             \"mono\"             \n[4] \"wqy-microhei\"      \"Bookman Old Style\"\n\n\nI’m not quite understanding how this object is organised. What is that last line? are the $xxxx the defaults?\nTo test, let’s make a plot and try to change font.\nThe help (https://cran.rstudio.com/web/packages/showtext/vignettes/introduction.html) says we need to tell R to use showtext for text.\n\nshowtext_auto()\n\n\nbaseiris <- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\nbaseiris + theme(axis.title = element_text(family = 'Bookman Old Style'),\n                 legend.title = element_text(family = 'Bookman Old Style', face = 'bold.italic'))\n\n\n\n\nThat seems to work. Can I feed in all the fonts on my system automaticallly? Is that a bad idea? Might be if it takes a while and we only want one.\nFirst, though, can I give it a font name as a character and it load all of the faces automatically?\nNote: some of the fonts I have have weird faces. For now, just fail on those and stick with the ones supported by showtext. That should be fine.\n\nunique(fontsIhave$face)\n\n [1] \"Bold\"            \"Regular\"         \"Bold Italic\"     \"Italic\"         \n [5] \"Demibold\"        \"Demibold Italic\" \"Demibold Roman\"  \"Bold Oblique\"   \n [9] \"Oblique\"         \"Light\"          \n\n\nThis is a) a useful thing to simplify adding single fonts, and b) precursor to loading them all.\n\n# chosen more or less at random for testing\nfamilyname <- 'Candara'\n\n# I could use dplyr but this seems better to just use base logical indexing.\n# fontsIhave %>%\n#   filter(family == familyname & face == 'Regular') %>%\n#   select(file) %>%\n#   pull()\n\n# Could do all the indexing in the function call to font_add(), but it just gets ridiculous\nregfile <- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Regular'), 'file']\n\nitalfile <- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Italic'), 'file']\n\nboldfile <- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Bold'), 'file']\n\nbifile <- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Bold Italic'), 'file']\n\n\n# NEED TO TEST WHEN THE FACE DOESN'T EXIST AND THROW NULL\n  # If not there, the value will be character(0). testing for that and returning NULL (which is what the function needs) is a bit tricky:\nnoface <- function(x) {ifelse(rlang::is_empty(x), return(NULL), return(x))}\n\nfont_add(familyname, \n         regular = noface(regfile), \n         italic = noface(italfile), \n         bold = noface(boldfile), \n         bolditalic = noface(bifile))\n\nTest that with a figure\n\nbaseiris + theme(axis.title.x = element_text(family = familyname, face = 'italic'),\n                 axis.title.y = element_text(family = familyname, face = 'bold'),\n                 legend.text = element_text(family = familyname),\n                 legend.title = element_text(family = familyname, face = 'bold.italic'))\n\n\n\n\nHow bad an idea is it to just read them ALL in at once?\nEasy enough to feed the font_add above in a loop. Probably vectorizable too, but why bother?\nWrite it as a function, then it will work for all fonts or a subset if that’s a bad idea. Either feed it a dataframe of fonts or just use font_files() directly. It can also take NULL for fontvec, in which case it loads all the fonts.\n\nloadfonts <- function(fontvec = NULL, fontframe = NULL) {\n  \n  # Get all fonts if no fontframe\n  if (is.null(fontframe)) {\n    fontframe <- font_files()\n  }\n  \n  # Load all fonts if no fontvec\n  if (is.null(fontvec)) {\n    fontvec <- unique(fontframe$family)\n  }\n  \n  # Loop over the font families\n  for (i in 1:length(fontvec)) {\n    familyname <- fontvec[i]\n    regfile <- fontframe[which(fontframe$family == familyname &\n                   fontframe$face == 'Regular'), 'file']\n\n    italfile <- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Italic'), 'file']\n    \n    boldfile <- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Bold'), 'file']\n    \n    bifile <- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Bold Italic'), 'file']\n    \n## TODO: THROW A TRYCATCH ON HERE TO BYPASS AND ALERT FOR FAILURES\n    # For example, Berlin Sans FB Demi has no 'regular' and so fails. let's just skip those, this isn't supposed to be the most robust thing ever that handles all cases flawlessly.\n    try(font_add(fontvec[i], \n         regular = noface(regfile), \n         italic = noface(italfile), \n         bold = noface(boldfile), \n         bolditalic = noface(bifile)))\n    \n    # To avoid unforeseen carryover through the loop\n    rm(familyname, regfile, italfile, boldfile, bifile)\n  }\n  \n}\n\nLet’s try just reading everything in. Use try in the function above because there are failures for a few reasons, and instead of dealing with them I just want to quickly read in what’s easy to read in. I don’t have a ton of interest here in figuring out corner cases for weird fonts.\nTest the function with a vector of fontnames first, because can’t do that after try with everything\n\nloadfonts(fontvec = c('Consolas', 'Comic Sans MS', 'Tahoma'))\nfont_families()\n\n[1] \"sans\"              \"serif\"             \"mono\"             \n[4] \"wqy-microhei\"      \"Bookman Old Style\" \"Candara\"          \n[7] \"Consolas\"          \"Comic Sans MS\"     \"Tahoma\"           \n\n\nNow, go for it with everything. There are a million errors, so I’ve turned error reporting off for this code chunk.\n\nsystem.time(loadfonts())\n\nThat was pretty quick. What do I have?\n\nfont_families()\n\n  [1] \"sans\"                            \"serif\"                          \n  [3] \"mono\"                            \"wqy-microhei\"                   \n  [5] \"Bookman Old Style\"               \"Candara\"                        \n  [7] \"Consolas\"                        \"Comic Sans MS\"                  \n  [9] \"Tahoma\"                          \"Agency FB\"                      \n [11] \"Algerian\"                        \"Book Antiqua\"                   \n [13] \"Arial\"                           \"Arial Narrow\"                   \n [15] \"Arial Black\"                     \"Arial Rounded MT Bold\"          \n [17] \"Bahnschrift\"                     \"Baskerville Old Face\"           \n [19] \"Bauhaus 93\"                      \"Bell MT\"                        \n [21] \"Bernard MT Condensed\"            \"Bodoni MT\"                      \n [23] \"Bodoni MT Black\"                 \"Bodoni MT Condensed\"            \n [25] \"Bodoni MT Poster Compressed\"     \"Bradley Hand ITC\"               \n [27] \"Britannic Bold\"                  \"Berlin Sans FB\"                 \n [29] \"Broadway\"                        \"Bookshelf Symbol 7\"             \n [31] \"Calibri\"                         \"Calibri Light\"                  \n [33] \"Californian FB\"                  \"Calisto MT\"                     \n [35] \"Cambria\"                         \"Candara Light\"                  \n [37] \"Cascadia Code\"                   \"Cascadia Mono\"                  \n [39] \"Castellar\"                       \"Century Schoolbook\"             \n [41] \"Centaur\"                         \"Century\"                        \n [43] \"Chiller\"                         \"Colonna MT\"                     \n [45] \"Constantia\"                      \"Cooper Black\"                   \n [47] \"Copperplate Gothic Bold\"         \"Copperplate Gothic Light\"       \n [49] \"Corbel\"                          \"Corbel Light\"                   \n [51] \"Courier New\"                     \"Curlz MT\"                       \n [53] \"Dubai\"                           \"Dubai Light\"                    \n [55] \"Dubai Medium\"                    \"Ebrima\"                         \n [57] \"Elephant\"                        \"Engravers MT\"                   \n [59] \"Eras Bold ITC\"                   \"Eras Demi ITC\"                  \n [61] \"Eras Light ITC\"                  \"Eras Medium ITC\"                \n [63] \"Felix Titling\"                   \"Forte\"                          \n [65] \"Franklin Gothic Book\"            \"Franklin Gothic Demi\"           \n [67] \"Franklin Gothic Demi Cond\"       \"Franklin Gothic Heavy\"          \n [69] \"Franklin Gothic Medium\"          \"Franklin Gothic Medium Cond\"    \n [71] \"Freestyle Script\"                \"French Script MT\"               \n [73] \"Footlight MT Light\"              \"Gabriola\"                       \n [75] \"Gadugi\"                          \"Garamond\"                       \n [77] \"Georgia\"                         \"Gigi\"                           \n [79] \"Gill Sans MT\"                    \"Gill Sans MT Condensed\"         \n [81] \"Gill Sans Ultra Bold Condensed\"  \"Gill Sans Ultra Bold\"           \n [83] \"Gloucester MT Extra Condensed\"   \"Gill Sans MT Ext Condensed Bold\"\n [85] \"Century Gothic\"                  \"Goudy Old Style\"                \n [87] \"Goudy Stout\"                     \"Harrington\"                     \n [89] \"Haettenschweiler\"                \"Microsoft Himalaya\"             \n [91] \"HoloLens MDL2 Assets\"            \"HP Simplified\"                  \n [93] \"HP Simplified Light\"             \"HP Simplified Hans Light\"       \n [95] \"HP Simplified Hans\"              \"HP Simplified Jpan Light\"       \n [97] \"HP Simplified Jpan\"              \"High Tower Text\"                \n [99] \"Impact\"                          \"Imprint MT Shadow\"              \n[101] \"Informal Roman\"                  \"Ink Free\"                       \n[103] \"Blackadder ITC\"                  \"Edwardian Script ITC\"           \n[105] \"Kristen ITC\"                     \"Javanese Text\"                  \n[107] \"Jokerman\"                        \"Juice ITC\"                      \n[109] \"Kunstler Script\"                 \"Lucida Sans Unicode\"            \n[111] \"Wide Latin\"                      \"Lucida Bright\"                  \n[113] \"Leelawadee UI\"                   \"Leelawadee UI Semilight\"        \n[115] \"Lucida Fax\"                      \"Lucida Sans\"                    \n[117] \"Lucida Sans Typewriter\"          \"Lucida Console\"                 \n[119] \"Maiandra GD\"                     \"Malgun Gothic\"                  \n[121] \"Malgun Gothic Semilight\"         \"Marlett\"                        \n[123] \"Matura MT Script Capitals\"       \"Microsoft Sans Serif\"           \n[125] \"MingLiU-ExtB\"                    \"Mistral\"                        \n[127] \"Myanmar Text\"                    \"Modern No. 20\"                  \n[129] \"Mongolian Baiti\"                 \"MS Gothic\"                      \n[131] \"Microsoft JhengHei\"              \"Microsoft JhengHei Light\"       \n[133] \"Microsoft YaHei\"                 \"Microsoft YaHei Light\"          \n[135] \"Microsoft Yi Baiti\"              \"Monotype Corsiva\"               \n[137] \"MT Extra\"                        \"MV Boli\"                        \n[139] \"Niagara Engraved\"                \"Niagara Solid\"                  \n[141] \"Nirmala UI\"                      \"Nirmala UI Semilight\"           \n[143] \"Microsoft New Tai Lue\"           \"OCR A Extended\"                 \n[145] \"Old English Text MT\"             \"Onyx\"                           \n[147] \"MS Outlook\"                      \"Palatino Linotype\"              \n[149] \"Palace Script MT\"                \"Papyrus\"                        \n[151] \"Parchment\"                       \"Perpetua\"                       \n[153] \"Microsoft PhagsPa\"               \"Playbill\"                       \n[155] \"Poor Richard\"                    \"Pristina\"                       \n[157] \"Rage Italic\"                     \"Ravie\"                          \n[159] \"MS Reference Sans Serif\"         \"MS Reference Specialty\"         \n[161] \"Rockwell Condensed\"              \"Rockwell\"                       \n[163] \"Rockwell Extra Bold\"             \"Sans Serif Collection\"          \n[165] \"Script MT Bold\"                  \"Segoe MDL2 Assets\"              \n[167] \"Segoe Fluent Icons\"              \"Segoe Print\"                    \n[169] \"Segoe Script\"                    \"Segoe UI\"                       \n[171] \"Segoe UI Light\"                  \"Segoe UI Semilight\"             \n[173] \"Segoe UI Black\"                  \"Segoe UI Emoji\"                 \n[175] \"Segoe UI Historic\"               \"Segoe UI Semibold\"              \n[177] \"Segoe UI Symbol\"                 \"Segoe UI Variable\"              \n[179] \"Showcard Gothic\"                 \"SimSun\"                         \n[181] \"SimSun-ExtB\"                     \"Sitka Text\"                     \n[183] \"Snap ITC\"                        \"Stencil\"                        \n[185] \"Sylfaen\"                         \"Symbol\"                         \n[187] \"Microsoft Tai Le\"                \"Tw Cen MT\"                      \n[189] \"Tw Cen MT Condensed\"             \"Tw Cen MT Condensed Extra Bold\" \n[191] \"Tempus Sans ITC\"                 \"Times New Roman\"                \n[193] \"Trebuchet MS\"                    \"Verdana\"                        \n[195] \"Viner Hand ITC\"                  \"Vladimir Script\"                \n[197] \"Webdings\"                        \"Wingdings\"                      \n[199] \"Wingdings 2\"                     \"Wingdings 3\"                    \n[201] \"Yu Gothic\"                       \"Yu Gothic Light\"                \n[203] \"Yu Gothic Medium\"                \"ZWAdobeF\"                       \n\n\nI’m sure if there were something that got bypassed that I really needed I could get it directly with font_add(), but this is sure quick to get them all. Test a couple of the new ones.\n\nbaseiris + theme(axis.title.x = element_text(family = 'Poor Richard', face = 'italic'),\n                 axis.title.y = element_text(family = 'Stencil', face = 'bold'),\n                 legend.text = element_text(family = 'Papyrus'),\n                 legend.title = element_text(family = 'Onyx', face = 'bold.italic'))\n\n\n\n\nI have also put loadfonts in the functions folder so I can use it elsewhere."
  },
  {
    "objectID": "plotting/hcl_exploration.html",
    "href": "plotting/hcl_exploration.html",
    "title": "hcl exploration",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\nUsing knitr::inline_expr(r message = FALSE, warning = FALSE) hopefully stops it printing all the package messages\nFinding colors to use for a given plot can be a pain. I’m trying to find some good color ramps for a project, and also sort out manipulating those colors to allow fading. This is me playing around to try to understand how to do those manipulations and looking at the various potential color palettes.\nColorspace (https://colorspace.r-forge.r-project.org/index.html) is a particularly useful package (though it is not the only color package I use).\nColorspace uses a hue-chroma-luminance specification for colors that is really powerful. It also has built-in palettes. For some other work, I was interested in exploring moving along those dimensions and generating color palettes for plotting.\nPreviously (for the project that gave rise to looking at fading colors), I was using purples and emerald, so let’s start there. But for simplicity switch to greens so constant hue.\nI actually like those single-hue fades a lot for showing more or less of something. But it SHOULD be possible to do a hue shift from green to purple for one axis? will that make sense?"
  },
  {
    "objectID": "plotting/hcl_exploration.html#hue-sequences",
    "href": "plotting/hcl_exploration.html#hue-sequences",
    "title": "hcl exploration",
    "section": "Hue sequences",
    "text": "Hue sequences\nI’d like to be able to specify the endpoints of a hue sequence and just shift along that axis. I’ll try it out with the purple and green above.\nFirst, I want to try to get the hue values (and the L and C as well) to make the endpoints. I can’t find a straightforward extraction in colorspace to get the HCLs though. So, since I know the endpoints are coming from those palettes above, I want their values. Make the palette, turn it into RGB, then turn the RGB into polarLUV to get the three axis values. Here, rows are the 8 fades in the palettes above.\n\nrgbpurps <- hex2RGB(sequential_hcl(8, 'Purples'))\n\nluvpurps <- as(rgbpurps, 'polarLUV')\nluvpurps\n\n            L         C        H\n[1,] 19.88570 55.128356 274.8415\n[2,] 34.37280 69.304529 274.3131\n[3,] 47.99202 56.799744 273.3506\n[4,] 60.90031 43.200021 272.3221\n[5,] 72.77975 31.772302 271.4182\n[6,] 83.46538 21.076091 271.6285\n[7,] 92.78865 10.863733 268.7090\n[8,] 98.79258  2.985742 276.3941\n\n\nThat’s sure roundabout, going palette that’s polarLUV under the hood but returns in hex to rgb and then back to polarLUV. Seems to work though.\n\nswatchplot(hex(luvpurps))\n\n\n\n\n\nrgbgrns <- hex2RGB(sequential_hcl(8, 'Greens'))\n\nluvgrns <- as(rgbgrns, 'polarLUV')\nluvgrns\n\n            L         C        H\n[1,] 25.06952 33.792199 132.8916\n[2,] 40.15678 49.456834 132.0640\n[3,] 54.06676 63.854764 129.4059\n[4,] 66.47833 62.340742 126.5380\n[5,] 77.49000 47.581607 123.8001\n[6,] 86.86700 33.248323 120.6451\n[7,] 93.95644 19.112933 117.4570\n[8,] 98.08100  5.367478 116.7639\n\n\nI can swatchplot them up together.\n\nswatchplot(hex(luvpurps), hex(luvgrns))\n\n\n\n\nNow, the goal is actually to identify those dark colors and transition between them. Now, can I get from purple to green? The L and C are quite different, unfortunately. Pick something the middle?\nHardcode numbers for now, though ideally we’ll get to a function that takes a start and end value.\n\npg <- polarLUV(L = 20, C = 40, H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\nWarning in max(nchar(rnam) - 1L): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\nThat fails. So, now we learned the ranges of the other axes matter. Likely chroma?\n\n# Fails\nmax_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20)\n\n[1] 29.55000 19.90429 16.62571 16.05429 17.74000 23.18000 41.07429 66.11000\n\n\nCan I just use the minimum max_chroma? Not really…\n\n# Guessing I can't just go with 16, but let's try\npg <- polarLUV(L = 20, C = 16, H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\n\n\n\nIf I try to fix how dark that is with chroma, it doesn’t work very well and I still lose one.\n\npg <- polarLUV(L = 20, \n               C = max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20),\n               H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\n\n\n\nUsing a matrix isn’t the answer- same thing, though a floor argument puts the missing color back\n\nhclmat <- cbind(20, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20, floor = TRUE),\n      seq(from = 130, to = 275, length.out = 8))\n\npg <- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\nGuessing I don’t want to just turn up luminance, but let’s see what that does to get a better sense how this all works.\n\nhclmat <- cbind(80, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 80, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg <- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\nLower luminance does work OK, but it’s still ‘darker’ in the middle and the shift to blue on the right is abrupt. The darker middle is likely why a lot of the colorspace palettes have triangular luminance. I don’t particularly want to get so fine-tuned here. I was looking for a way to programatically define these sequences, and getting into tweaking luminance in a nonlinear and nonmonotonic way could get very bespoke very quickly. Likely better to just use the built-in palettes where someone who understands color theory has already done that.\n\nhclmat <- cbind(50, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 50, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg <- polarLUV(hclmat)\nswatchplot(hex(pg))"
  },
  {
    "objectID": "plotting/hcl_exploration.html#fading",
    "href": "plotting/hcl_exploration.html#fading",
    "title": "hcl exploration",
    "section": "Fading",
    "text": "Fading\nI also want to make faded versions of palettes, and control levels of fade. The particular use I have in mind is to illustrate levels of uncertainty, but it could be any bivariate outcomes.\nI originally thought that I would need to manually adjust the chroma and luminance manually. But the exploration above suggests they interact and so it’s unlikely to just shift one or the other. Still, colorspace provides lighten, darken (which both shift luminance), and desaturate, which shifts chroma. I should be able to play with these to see how they work using either a homebrew base palette as above or the inbuilt ones.\nIn either case, we need the hex values\n\nhexcols <- hex(pg)\n\nLighten (increase luminance)\n\nswatchplot('orig' = hexcols,\n           '25' = lighten(hexcols, amount = 0.25),\n           '50' = lighten(hexcols, amount = 0.5),\n           '75' = lighten(hexcols, amount = 0.75),\n           '100' = lighten(hexcols, amount = 1))\n\n\n\n\nDarken (decrease luminance)\n\nswatchplot('orig' = hexcols,\n           '25' = darken(hexcols, amount = 0.25),\n           '50' = darken(hexcols, amount = 0.5),\n           '75' = darken(hexcols, amount = 0.75),\n           '100' = darken(hexcols, amount = 1))\n\n\n\n\nDesaturate (adjust chroma)\n\nswatchplot('orig' = hexcols,\n           '25' = desaturate(hexcols, amount = 0.25),\n           '50' = desaturate(hexcols, amount = 0.5),\n           '75' = desaturate(hexcols, amount = 0.75),\n           '100' = desaturate(hexcols, amount = 1))\n\n\n\n\nFor my particular use, I like desaturating better, in that it implies less information. But it also makes the values look more similar across the range, and we don’t want that. That gets captured better by lightening.\nAs a bit of an aside, the ends of the lightened versions are effectively ‘Purples’ and ‘Greens’, reading down instead of across. What does it look like if I desaturate those built-in palettes?\n\npurp8 <- sequential_hcl(8, 'Purples')\nswatchplot('orig' = purp8,\n           '25' = desaturate(purp8, amount = 0.25),\n           '50' = desaturate(purp8, amount = 0.5),\n           '75' = desaturate(purp8, amount = 0.75),\n           '100' = desaturate(purp8, amount = 1))\n\n\n\n\nIt does remove color, but it perceptually darkens as well, which is NOT what I want.\nWhat about choosing a pre-built set of colors and lightening/darkening? Start with viridis, we know it has good properties in greyscale, etc.\n\nvir8 <- sequential_hcl(8, 'Viridis')\nswatchplot('orig' = vir8,\n                 '25' = lighten(vir8, amount = 0.25),\n                 '50' = lighten(vir8, amount = 0.5),\n                 '75' = lighten(vir8, amount = 0.75),\n                 '100' = lighten(vir8, amount = 1))\n\n\n\n\nThat actually works pretty well, even though the original had a luminance ramp on it already (https://colorspace.r-forge.r-project.org/articles/approximations.html), this just shifts it each time, I think. We can compare using specplot.\n\nspecplot(vir8, lighten(vir8, amount = 0.75))\n\n\n\n\nWhat does a desaturated viridis look like?\n\nswatchplot('orig' = vir8,\n           '25' = desaturate(vir8, amount = 0.25),\n           '50' = desaturate(vir8, amount = 0.5),\n           '75' = desaturate(vir8, amount = 0.75),\n           '100' = desaturate(vir8, amount = 1))\n\n\n\n\nAgain, makes them more similar, though the underlying luminance ramp helps. I don’t like that the first level still ends up darker though.\n\nInteracting chroma and luminance\nSo, changing luminance makes colors brighter or darker, while adjusting chroma removes color but tends to make them darker. Neither is exactly what I want- a color ramp that look the same, just “faded”. Is the answer to control this interaction? Does a simultaneous lighten and desaturate give me what I want by avoiding the perceptual darkening from the desaturation?\n\nswatchplot('orig' = vir8,\n           '25' = desaturate(vir8, amount = 0.25) %>%\n             lighten(amount = 0.25),\n           '50' = desaturate(vir8, amount = 0.5) %>%\n             lighten(amount = 0.5),\n           '75' = desaturate(vir8, amount = 0.75) %>%\n             lighten(amount = 0.75),\n           '100' = desaturate(vir8, amount = 1) %>%\n             lighten(amount = 1))\n\n\n\n\nThat works really well, actually. Does the order of operations matter? No:\n\nswatchplot('orig' = vir8,\n           '25' = lighten(vir8, amount = 0.25) %>%\n             desaturate(amount = 0.25),\n           '50' = lighten(vir8, amount = 0.5) %>%\n             desaturate(amount = 0.5),\n           '75' = lighten(vir8, amount = 0.75) %>%\n             desaturate(amount = 0.75),\n           '100' = lighten(vir8, amount = 1) %>%\n             desaturate(amount = 1))\n\n\n\n\nDid I just get lucky with viridis, or does it work with other palettes too? how about my ramp that I made from green to purple? Seems to:\n\nswatchplot('orig' = hexcols,\n           '25' = lighten(hexcols, amount = 0.25) %>%\n             desaturate(amount = 0.25),\n           '50' = lighten(hexcols, amount = 0.5) %>%\n             desaturate(amount = 0.5),\n           '75' = lighten(hexcols, amount = 0.75) %>%\n             desaturate(amount = 0.75),\n           '100' = lighten(hexcols, amount = 1) %>%\n             desaturate(amount = 1))\n\n\n\n\nDoes the lighten and desat work for the single-hue scales? Seems like it shouldn’t because they’re already changing along those axes.\n\nswatchplot('orig' = purp8,\n           '25' = lighten(purp8, amount = 0.25) %>%\n             desaturate(amount = 0.25),\n           '50' = lighten(purp8, amount = 0.5) %>%\n             desaturate(amount = 0.5),\n           '75' = lighten(purp8, amount = 0.75) %>%\n             desaturate(amount = 0.75),\n           '100' = lighten(purp8, amount = 1) %>%\n             desaturate(amount = 1))\n\n\n\n\nNot really. It basically does what it should, but the light end is just always light and so doesn’t contain info in the faded dimension and very similar colors appear in both dimensions- values at row n and col m are frequently very similar to row n + 1 and col m - 1.\nI suppose that might be OK for particular situations, but still not ideal. Might work ok though if we limited that lower end? ie don’t let it fall all the way to white in the original? Getting pretty hacky at that point and the diagonals are still too similar.\n\nswatchplot('orig' = purp8[1:6],\n           '25' = lighten(purp8[1:6], amount = 0.25) %>%\n             desaturate(amount = 0.25),\n           '50' = lighten(purp8[1:6], amount = 0.5) %>%\n             desaturate(amount = 0.5),\n           '75' = lighten(purp8[1:6], amount = 0.75) %>%\n             desaturate(amount = 0.75),\n           '100' = lighten(purp8[1:6], amount = 1) %>%\n             desaturate(amount = 1))\n\n\n\n\n\nTesting with other palettes\nViridis and the one I made are both fine, but look at a couple other palettes too. This is not comprehensive, mostly looking at those that have greens and purples for the use I have in mind.\nWrite a little function to do the fade and make this less cut and paste\n\npalcheck <- function(palname, n = 8) {\n pal8 <- sequential_hcl(n, palname)\n \n swatchplot('orig' = pal8,\n            '25' = lighten(pal8, amount = 0.25) %>%\n              desaturate(amount = 0.25),\n            '50' = lighten(pal8, amount = 0.5) %>%\n              desaturate(amount = 0.5),\n            '75' = lighten(pal8, amount = 0.75) %>%\n              desaturate(amount = 0.75),\n            '100' = lighten(pal8, amount = 1) %>%\n              desaturate(amount = 1))\n \n}\n\nPlasma\n\npalcheck('Plasma')\n\n\n\n\nGreen-based\nag_GrnYl is OK, but does get a bit of the diagonal issue\n\npalcheck('ag_GrnYl')\n\n\n\n\nditto Emrld, but might work?\n\npalcheck('Emrld')\n\n\n\n\nTerrains might be OK? 2 is less gaudy\n\npalcheck('Terrain')\n\n\n\npalcheck('Terrain2')\n\n\n\n\nmints and TealGrn fail diagonal test\n\npalcheck('Dark Mint')\n\n\n\npalcheck('Mint')\n\n\n\npalcheck('TealGrn')\n\n\n\n\nYlGn is pretty good, actually.\n\npalcheck('YlGn')\n\n\n\n\nFor the specific use, keep in mind that it will be two levels of fade, and so I can do something like orig and 75% and it’ll be pretty different. But here I’m trying to be fairly general.\nMeh\n\npalcheck('BluGrn')\n\n\n\n\nas expected, batlow and Hawaii are extreme, though might be OK?\n\npalcheck('Batlow')\n\n\n\npalcheck('Hawaii')\n\n\n\n\nPurple-based\nsingle hue doesn’t work\n\npalcheck('Purples')\n\n\n\npalcheck('Purples 3')\n\n\n\n\nthese are all maybes with tricky diagonals\n\npalcheck('Purple-Blu')\n\n\n\npalcheck('Purple-Ora')\n\n\n\npalcheck('Purp')\n\n\n\npalcheck('PurpOr')\n\n\n\npalcheck('Sunset')\n\n\n\npalcheck('Magenta')\n\n\n\npalcheck('SunsetDark')\n\n\n\n\npretty good, but have a fair amount of green in, so could be confusing\n\npalcheck('Purple-Yellow')\n\n\n\npalcheck('Viridis')\n\n\n\npalcheck('Mako')\n\n\n\n\nPlasma pretty good\n\npalcheck('Plasma')\n\n\n\n\nInferno might actually be pretty good if I cut off the first one\n\npalcheck('Inferno')\n\n\n\n\nag_Sunset is better on the diagonals than similar hue sequences\n\npalcheck('ag_Sunset')\n\n\n\n\nGood, but would need to cut the last one; too white. It is less gaudy/ more obviously a hue ramp than ag sunset. Diagonals are tricky too\n\npalcheck('RdPu')\n\n\n\n\nPretty good, but blue could be an issue getting confused with water for this project.\n\npalcheck('BuPu')\n\n\n\n\n\n\n\nContinuous hue from specified palettes\nIf I want to map values to colors continously, that gets tricky using the specified palettes because sequential_hcl takes an n argument.\nCan I get the endpoints and make my own (as I did above with green and purple?)\ndoes the one I’m using use a linear hue scale\n\nspecplot(sequential_hcl(8, 'ag_Sunset'))\n\n\n\n\nIt does, but doesn’t use linear chroma. and it has luminance shift too.\nCan I extract the hue from the ends? The same way I did right at the beginning for the greens and purples.\n\nspecplot(sequential_hcl(2, 'ag_Sunset'))\n\n\n\nrgbsun <- hex2RGB(sequential_hcl(8, 'ag_Sunset'))\n\nluvsun <- as(rgbsun, 'polarLUV')\nluvsun\n\n            L         C          H\n[1,] 25.00933  69.80052 274.922758\n[2,] 33.57582  78.49556 296.995075\n[3,] 42.09671  87.16488 318.944488\n[4,] 50.70304  96.81962 341.141446\n[5,] 59.33484 102.07413   3.730076\n[6,] 67.89723  89.83472  25.626328\n[7,] 76.47041  74.80664  47.677726\n[8,] 84.95182  45.16493  69.593540\n\n\nThis generates the wrong thing (roughly, viridis) because the hue crosses 0\n\nsunmat <- cbind(seq(from = 85, to = 25, length.out = 8), \n                max_chroma(h = seq(from = 69, to = 275, length.out = 8), \n                           l = seq(from = 85, to = 25, length.out = 8), \n                           floor = TRUE),\n                seq(from = 69, to = 275, length.out = 8))\n\npgsun <- polarLUV(sunmat)\nswatchplot(hex(pgsun))\n\n\n\n\nCan I fix the zero-crossing? I’m sure there’s a polar coord package, but for now, add a 360 and take it off\n\nhvec <- seq(from = luvsun@coords[1, 3], to = 360+luvsun@coords[8,3], length.out = 8)\nhvec[hvec > 360] <- hvec[hvec>360]-360\n\nlvec <- seq(from = luvsun@coords[1, 1], to = luvsun@coords[8, 1], length.out = 8)\n\nThe max_chroma is intense, but not sure how else to choose the chromas if we’re trying to build a continuous ramp. Could just use n = 1000 or something to get pseudo-continuous\n\nsunmat <- cbind(lvec, \n                max_chroma(h = hvec, \n                           l = lvec, \n                           floor = TRUE),\n                hvec)\n\npgsun <- polarLUV(sunmat)\nswatchplot(hex(pgsun))\n\n\n\n\nSo, one option is just treating the built-in palettes as their endmembers like that and then doing it as I did before. But it does lose the actual built-in palettes, especially chroma or nonlinearity. Likely better to just use a large n for now and call it good."
  },
  {
    "objectID": "plotting/math_in_ggplot.html#using-latex2exp",
    "href": "plotting/math_in_ggplot.html#using-latex2exp",
    "title": "Math and greek in legends",
    "section": "Using latex2exp",
    "text": "Using latex2exp\n\nlibrary(ggplot2) \nlibrary(latex2exp)\n\nLet’s just try to do some things with that. Make the usual iris plot.\n\ntestplot <- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\ntestplot\n\n\n\n\nNow let’s add some math. I’d typically use labs for the x,y, and color, so try that. Spacing not great, but it works.\nBasically following the manual, it’s pretty self-explanatory.\nThe r says to use raw strings so don’t have to escape slashes.\n\ntestplot <- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho})'),\n       color = TeX(r'($\\left{ \\int_0^\\inf \\exp{\\eta x} dx \\right})'))\n\ntestplot\n\n\n\n\nCan I use amsmath in latex? Maybe, but not for linebreaks- this errors.\n\ntestplot <- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho}$)'),\n       color = TeX(r'($\\begin{split}\\left{ \\int_0^\\inf \\\\ \\exp{\\eta x} dx \\right}\\end{split}$)'))\n\ntestplot"
  },
  {
    "objectID": "plotting/tweaks_tricks.html#theming",
    "href": "plotting/tweaks_tricks.html#theming",
    "title": "Theming and saving",
    "section": "Theming",
    "text": "Theming\nI tend to establish a theme to set the basic plot look, including font sizes. I start with theme_bw() because the default ggplot grey background doesn’t look good in pubs. I used to set the sizes separately for each sort of text (commented out), but that is typically easier to just use the base_size argument and let ggplot handle the relative adjustments.\nCan also set theme differently for presentations, including doing things like setting font to match a ppt theme.\nTypically, very few fonts are loaded into R and available for use. See fonts.Rmd for figuring out how to work with that. The short answer is that we use showtext to load what we need (if anything). If this step is skipped, will default to the default font and throw a warning about “fontfamily not found” because we haven’t loaded the selected font yet.\nWe could load fonts by hand Using functions from showtext and sysfonts, and specify the text = element_text(family=\"Ink Free\") with a manual character vector. It’s way easier to automate though, and saves issues of loading the wrong font.\nFirst, load the function I wrote that simplifies finding the files and their names to load them. And tell R to use showtext to render fonts.\n\nprint(file.path('functions', 'loadfonts.R'))\n\n[1] \"functions/loadfonts.R\"\n\nsource(file.path('functions', 'loadfonts.R'))\nshowtext::showtext_auto()\n\nThen, load the font(s) we want\n\ntalkfont <- 'Ink Free'\npubfont <- 'Cambria'\nloadfonts(fontvec = c(talkfont, pubfont))\n# loadfonts()\n\nNote that we could also just loadfonts() with no arguments to read in ALL available fonts\nPass talkfont and pubfont to the themes.\n\ntalktheme <- theme_bw(base_size = 18) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=talkfont)) # Replace with fontname used in PPT\n\n# \n        # axis.text = element_text(size = 18),\n        # axis.title = element_text(size = 24),\n        # strip.text = element_text(size = 24),\n        # plot.title = element_text(size = 24))\n\npubtheme <- theme_bw(base_size = 10) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=pubfont))\n\nAs an example, let’s make a simple plot with iris, and then look at the themed versions.\n\nbaseiris <- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\nbaseiris\n\n\n\n\nNow, what does a publication version look like?\n\nbaseiris + pubtheme\n\n\n\n\nNote that further theme changes can happen later on, e.g. Note that it’s easy to get in trouble with the internal legend positions when it comes time to save- as the dimensions change on export vs whatever arbitrary size you have the Rstudio plot pane, what looks good will changes as well.\n\nbaseiris + pubtheme +\n  theme(legend.title = element_text(face = 'bold'),\n        legend.position = c(0.8,0.2))\n\n\n\n\nFor talks, we use talktheme. Terrible font, but easy to see that it’s been shifted from default.\n\nbaseiris + talktheme\n\n\n\n\nWe can update parts of the theme including the font while keeping the rest. Though if we haven’t loaded all fonts, will need to load the new ones now.\n\n# load new font\nloadfonts(fontvec = 'Elephant')\n\ntalktheme <- talktheme + \n  theme(text = element_text(family = 'Elephant')) # Replace with fontname used in PPT\n\nAnd to show that it worked, plot again.\n\nbaseiris + talktheme"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Placeholder, research goes here in sections, needs pics and short blurbs"
  },
  {
    "objectID": "research.html#cv-google-scholar-etc",
    "href": "research.html#cv-google-scholar-etc",
    "title": "Research",
    "section": "CV, google scholar, etc",
    "text": "CV, google scholar, etc"
  },
  {
    "objectID": "RpyEnvs/managingprivate.html",
    "href": "RpyEnvs/managingprivate.html",
    "title": "Private data and website",
    "section": "",
    "text": "I’m getting set up to use github pages to host a website. But some content I (might) host needs to be private. A clear option is to simply mock-up data matching that private data, and that’s the way I’ll go. But because a large part of the content here will be sorting through issues, the initial sort-through will likely depend on figuring out what it is about the private data that needs to be mocked-up, and some portion of the testing will depend on that data. And I want all of that to be version controlled, but not shared publicly. In short, I need a private development location, and then go through, make a clean version based on mocked-up data, and publish that. So, how?\nThe first thing that came to mind is to just have a local-only branch that I keep private. I could have a private/ folder, where I do dev, with that folder ignored in the master .gitignore. And have a different .gitignore in another branch so development would be tracked in that other branch. Then, as things were ready to make public, I could just drop them over. However, because github requires the whole github pages repo to be public, I would never be able to push this branch. Sure, people would be unlikely to poke around in it, but it would all be there. And if I ever forgot the process, I would expose things. And I don’t want to lose a cloud-hosted version- only storing locally isn’t so great, even if it is backed up or dropboxed.\nI’m now leaning towards having a second, private repo for development, and then drag and drop into the github pages repo once the doc I’m working on is clean. That’s basically the same idea as the internal private/ folder, but as a whole different repo, and so could actually be held as a private repo on github. There are two main catches that I can see with this approach at the outset-\n\nThe actual development history of files won’t be available on the public repo. That’s kind of the point, but it is a bit annoying\nKeeping the two repos synced will be a pain. If I make a small change to a file that’s already public in the public repo, I’d need to get it back into the other. The obvious solution is to do everything in the private, and then move things over. But if I make a small change to a file that’s already moved to the public repo in the private repo, I need to make sure it moves.\n\nI think I’ve dealt with this issue before, but can’t remember the details. I had a repo as a fork of another that was upstream or something. Will need to sort that out. It’s essentially a repo-diff, but needing to check what should be diff (still private), vs. what shouldn’t be a diff (a change that needs to move over).\n\n\nIs there a better solution that allows building from somewhere other than github pages, and so could use a private repo? Netlify would work. And might be better anyway. But if the whole point is to make messy code public, then we want it on a public repo, right? And just hold the private stuff back/ do it elsewhere."
  },
  {
    "objectID": "RpyEnvs/python_setup.html",
    "href": "RpyEnvs/python_setup.html",
    "title": "Python setup",
    "section": "",
    "text": "I’m working on a Python project, and trying to figure out how to set up and get started. I’m used to R, where most simply, all I have to do is download R, Rstudio, and then start coding. R doesn’t need any environment manager to get going, but I do tend to use renv to manage packages, but that’s pretty lightweight and straightforward. And I can start coding without it.\nPython, on the other hand, is more opaque. In part it’s because I’m new to it, but a bit of googling suggests I’m not the only one. It’s likely also because there’s no one dedicated IDE/workflow that almost everyone uses, a la Rstudio (maybe that will change with the Rstudio–> Posit move?).\nSo, I’m going to work out getting setup to code in Python (I sorta did it before, but I’m trying a new way with fewer black boxes). And using this as a place to write notes/what I did as I go. That means this might end up being less a tutorial and more a series of pitfalls, but we’ll see how it goes.\n\n\nI’m trying to get set up to manage Python versions themselves with pyenv and packages with poetry. As far as I can tell, poetry does approximately similar things to renv (but more complicated because python). And I haven’t used something similar to pyenv to manage R versions themselves, but I am about to have to figure that out too because a lot of packages broke when I moved to R 4.2. Will probably try rig, and write another one of these. I’m assuming I’ll code primarily in VS Code, unless Posit suddenly runs python like R (without reticulate). Even then, remote work will all use VS Code for the time being. I’m loosely following https://briansunter.com/blog/python-setup-pyenv-poetry and https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/, and doing all the actual setup in VS Code, not Rstudio.\nRealising after I wrote this that I probably could have actually done all of this inside quarto- I think I can run powershell/system code in code blocks?"
  },
  {
    "objectID": "RpyEnvs/python_setup.html#getting-started--systemwide-installations",
    "href": "RpyEnvs/python_setup.html#getting-started--systemwide-installations",
    "title": "Python setup",
    "section": "Getting started- systemwide installations",
    "text": "Getting started- systemwide installations\nBoth those websites are working on Unix and Mac, so while step 1 is install pyenv, we aren’t going to apt-get or brew install. In fact, the pyenvgithub says we need to use a windows fork. Things already getting nonstandard. Should I just run everything in Windows Subsystem for Linux? Maybe, but I’d like to just use windows if possible, as much as I like WSL.\n\npyenv\nGuess I’ll just start at Quick start. Going to use powershell directly rather than inside vs code here, because vs code likes to open in recent directories instead of globally, and I think I want pyenv system-wide.\nStep 1- install in powershell with Invoke-WebRequest -UseBasicParsing -Uri \"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\" -OutFile \"./install-pyenv-win.ps1\"; &\"./install-pyenv-win.ps1\"\nCan’t run scripts (new computer). Sends me to https:/go.microsoft.com/fwlink/?LinkID=135170.\n\nSetting the policy to just the running process. Will probably regret that when I next try to run a script, but for now I don’t really want universal unrestricted powershell scripts.\n\nPowershell script permissions\nAside- it starts to get really annoying because pyenv runs scripts, so will need to fix. Get an error when I try to change Scope to CurrentUser because of a group policy. Setting it to Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope LocalMachine seems to work, despite apparently being a larger set than my User.\nIt says it wasn’t installed successfully, but when I try again it says it’s there. I guess push on?\n\nStep 2 was just to shut down and reopen powershell\nStep 3- Run pyenv --version. If you haven’t changed policy to something larger than Process, this will fail because of the ExecutionPolicy. Guess I need to turn it back on. I kept doing one-offs for a while until I got annoyed and then set it to LocalMachine (see above).\nStep 4, pyenv install -l lists a million python versions. Seems like a good thing. I’m going to need older versions in projects, but for now, let’s install the latest.\nStep 5- the install of python. There’s a 3.12.a1, which I’m assuming means alpha, so I’ll go with pyenv install 3.11.0, which is the most recent without the .a1.\nThe install seems to have worked.\nStep 6- set global pyenv global 3.11.0\nStep 7- check it worked pyenv version \nStep 8- check python with python -c \"import sys; print(sys.executable)\"\nworks, gives me the path, \\.pyenv\\pyenv-win\\versions\\3.11.0\\python.exe\nNow it says to validate by closing and reopening- do that. Now pyenv --version gives the version, while pyenv alone tells us the commands. They’re also all at the github page I’m following.\nThen try in a vs code terminal, also works. Note that using the git bash terminal instead of powershell bypasses the script permissions issue if it hasn’t been set larger than Process.\nNow, we have Python 3.11 as the global python version, but should be able to install other versions and use them in local projects. Assuming I’ll get there once I set up poetry.\n\n\n\npoetry\nOnce again, the websites I’m following have commands for mac/unix, so back to the main poetry page to sort this out on windows.\nAgain, powershell command on windows. Could I use the git bash in vs? Maybe? Just stick with powershell. (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -.\nThe instructions say that last bit should be py instead of python if python not installed from Microsoft Store, but had to use python anyway.\nseems to have worked- \nThe instructions then have an advanced section I’m skipping, but that install message above seems to match step 3, where we add Poetry to the PATH in order to run Poetry by just running poetry and not the full path.\nI could change it with powershell, but the instructions I found involved a bunch of regex. Instead, search for “Advanced System Settings”, then in the bottom right, click Environment Variables, then in the System Variables box, click on Path, then Edit button, then New. That creates a blank line, paste in the path from poetry install, or use the one from their website, %APPDATA%\\Python\\Scripts. OK out of all the system settings boxes.\nShut down powershell, then fire it back up and run poetry --version. If the path setting failed, it won’t be able to find poetry, if it worked, it’ll give the version number.\nThat worked for me once, but now isn’t working the next day. Actually, it works in powershell, but not vs code powershell. It wasn’t unpacking the %APPDATA% correctly - running (Get-ChildItem env:path).Value lists %APPDATA% instead of the expanded directory. I stuffed the full path as in the screenshot above in the PATH, restarted VS Code and it works. Interestingly though, I left the %APPDATA% version there too, and it’s now unpacked when I run (Get-ChildItem env:path).Value."
  },
  {
    "objectID": "RpyEnvs/python_setup.html#setting-up-a-project",
    "href": "RpyEnvs/python_setup.html#setting-up-a-project",
    "title": "Python setup",
    "section": "Setting up a project",
    "text": "Setting up a project\nLet’s first say we’re going to use a different-than-standard version of python, so install that with pyenv. For this test, let’s just use 3.8.9. Not really any particular reason. So, run pyenv install 3.8.9\nNow, pyenv versions (NO FLAGS- the “--” flag will give the version of pyenv) shwos two versions with an asterisk by global.\n\n\nCreate the project\nJust need a directory and cd inside it, I think. I’ll make it inside the directory with this qmd. mkdir pytesting, cd pytesting. Actually, this yields too much nesting. poetry builds a directory for the project, and another directory in that, so this just yields annoying levels of nesting. Call poetry new (see below) from the directory you want to contain the main project directory.\n\n\nSet the python version\nI think just pyenv local 3.8.9. That creates a .python-version file in the directory, which seems to be the idea.\n\n\nSet poetry\nAm I going to completely screw up my R project having this inside it? Guess we’ll find out.\npoetry new pytesting then creates another directory and returns “Created package pytesting in pytesting”. It builds the outer directory, so don’t make one first or the nesting gets silly.\nThat directory seems to be establishing a standard package structure and the lockfiles etc. Opening the .toml looks like it didn’t pick up the python version though- it’s using 3.11. Hmmm. Tried killing and restarting powershell and it’s still doing that. Not sure why it’s not picking up the local python.\nIf I move up a directory, the pyenv versions returns back to 3.11. So pyenv seems to be working, but poetry’s not picking it up. I guess I can change in manually, but that’s annoying.\nSeems to be a long-running known issue- recent posts here https://github.com/python-poetry/poetry/issues/651. Ignore for now, maybe fix manually if it becomes an issue. I tried the solution in the last post (poetry config virtualenvs.prefer-active-python true), and it didn’t fix it. Tried completely starting over a few times. No luck. Worry about that later. I guess that means the pyenv stuff might be useless for the moment- will need to use the version poetry thinks it has in the directory. It does look like can reinstall poetry, but that seems like a pain. (No one else seems to have issues with the config above).\nSo, I’ve just set the pyenv to the global for now, and moving ahead with poetry for the project. I guess I could change pyenv global each time I switch projects as an annoying workaround if it becomes an issue. An answer might be poetry env use , see https://python-poetry.org/docs/managing-environments/.\n\n\nUsing poetry for dependencies\nMuch like renv can install all dependencies from info in renv.lock, we could build the project with dependencies from pyproject.toml. Or, also like renv, as we’re developing a project, we can add iteratively. Let’s do that, since that’s what we’re doing.\nThere seems to be an intermediate step here though, running poetry install to initialise a virtual environment from the .toml. I assume this would install whatever’s in the toml, but we don’t have anything at present. That apparently creates a virtual environment somewhere globally (.cache/, according to https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/).\nAnd now I have poetry.lock . According to the docs, this takes precedence over the .toml, though I doubt that’s true for python version itself. This is what gets committed to share the project.\nTyping poetry shell at the command line activates the environment. But how do we activate it for a VS code session?\nSeems to just be active once we open a .py file in that directory (e.g. if we open the file, then a powershell at that location, it appears with pyenv shell already going.\nTo test adding a dependency, i’ll try numpy. First, I can run simply python code- a = 1 etc. But import numpy as np fails (as expected)- ModuleNotFoundError: No module named ‘numpy’. So, click back to the powershell terminal, and try poetry add numpy. It resolves dependencies and writes a lock file.\nAnd yet, if I try import numpy as np, same error. The poetry show command lists it as installed.\nSo, it’s because VScode doesn’t know where to find the venvs. On a one-off basis, can use poetry env info --path to get the path, then in VS code select interpreter (ctrl-shift-p for the search thingy), then paste in that path. But that’s annoying.\nI’m trying getting to the search thing, then User settings, then adding C:\\Users\\galen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ to the venv Folders. That seems to work, but it may just be remembering from last time.\nThe most robust option might be to not keep the venvs in .cache but instead local to the project, as described here. Looks like that’s done with poetry config virtualenvs.in-project true at the very outset, and then rebuilding the project (and it should persist for future projects).\nAnd that’s what we’re doing in the project I’m doing this for. So let’s do that. First, run poetry env list to get the name, then poetry env remove NAME to delete. But it failed, seemingly partway. So also go to AppData\\Local\\pypoetry\\Cache\\virtualenvs and delete the folder. Restart vs and now poetry env list doesn’t return anything.\nTo set the config, poetry config virtualenvs.in-project true . I believe that’s global (I ran it outside the project).\nNow rebuild from .toml in the project with poetry install. The .venv is there now, but VS still can’t find it.\nOK, shut everything down, and instead of opening VS again as usual, I opened it and then opened a new window, and now it works. It was somehow setting the root based on where VS happened to open, rather than based on where files were. That probably makes sense for a git repo with just python, but broke here. I assume we need to add the venv directory to the gitignore.\nJust did poetry add pandas and it works and I immediately have access to it in a python script.\n\n\nVS code note\nSometimes VS seems to find the poetry venv and use it, and other times (I think if it’s not at the head dir of the workspace?) it needs to be pointed at the python.exe. To do that, open the command palette, (ctrl-shift-p), select python interpreter, then .venv\\scripts\\python.exe wherever that venv is."
  },
  {
    "objectID": "RpyEnvs/python_updated_functions.html",
    "href": "RpyEnvs/python_updated_functions.html",
    "title": "Updating function defs",
    "section": "",
    "text": "As I develop, I often try a function, tweak it, try again, etc. In R, I can just run the function definition to have access, or source(filewithfunction.R). In python, I could tweak the function, but just trying to use them elsewhere (e.g. in a .qmd) wasn’t working, even if I re-ran import filename. Clearly, there are differences between import in python and source in R. After poking around a bit, it looks like python caches on first import, and so subsequent ones don’t refresh.\nWhat does seem to work is to run importlib.reload(filename). Obviously we wouldn’t put that in a script, but when using an interactive session, it’s really helpful. Not sure why this requires a whole separate package, but it works. See stackoverflow. It appears to be typical to just restart, but that is really prohibitive if the testing involves processing data that took a long time to create."
  },
  {
    "objectID": "RpyEnvs/quarto_website_github.html",
    "href": "RpyEnvs/quarto_website_github.html",
    "title": "Quarto website",
    "section": "",
    "text": "I want to use quarto to build a website hosted on github pages. I have a few goals for it, but step one is to figure out how to do it.\nI’ve already started a github pages repo, and had started putting things in it before I realised I was probably not working in the best way (and some things needed to be private). So before any commits, I moved the work out and want to just start clean and see how to do it. I’ll walk through the process here.\nI’ll start by following the quarto docs, but may diverge. Using the Rstudio version, but will likely use a bit of VS too for python."
  },
  {
    "objectID": "RpyEnvs/quarto_website_github.html#set-up-github-pages",
    "href": "RpyEnvs/quarto_website_github.html#set-up-github-pages",
    "title": "Quarto website",
    "section": "Set up github pages",
    "text": "Set up github pages\nI did this a while ago, will come back to it.\nClone the repo locally."
  },
  {
    "objectID": "RpyEnvs/quarto_website_github.html#start-as-a-website-project",
    "href": "RpyEnvs/quarto_website_github.html#start-as-a-website-project",
    "title": "Quarto website",
    "section": "Start as a website project",
    "text": "Start as a website project\nCould I have converted from a normal project? Probably. And I could only get to the ‘quarto website’ option if I made a new directory. So even though I already cloned the repo from github, I put the project in a new dir, and then will copy it into the repo (I guess?).\nI actually made it in a dir, then started a git repo in that dir and set its remote to hit the url of my github.io repo following the instructions on github for starting a local repo and pointing it to github.\nProject seems to work when I click render, though it renders in browser not in Viewer pane (which is actually nicer, just not what the docs say).\nI had my repo in dropbox, as that seems like it usually works fine for other repos and gives another layer of backup. But it was failing here with lots of errors about files being in use by other processes. Moved it to Documents and seems to work fine."
  },
  {
    "objectID": "RpyEnvs/quarto_website_github.html#setting-up-nav",
    "href": "RpyEnvs/quarto_website_github.html#setting-up-nav",
    "title": "Quarto website",
    "section": "Setting up nav",
    "text": "Setting up nav\nI’m not entirely sure what I want the structure to be, but likely a brief home page, navbar at top with things like ‘Research’, ‘About’, ‘Code examples’, etc. Lots of options here, I guess just cobble something together quickly.\nOne question I have is what happens when I start committing to git. Does it auto-publish? It looks like no, according to quarto, if I set up to render to docs and don’t push master. Or if I publish from a gh-pages branch though that’s not working on windows.\nThe .yaml seems to be where all the website structure goes- nav bars, search, etc.\nI think I’m going to end up with something fairly complex for nav, but for now, maybe try broad categories across the top, then specifics down the side. Add additional nesting later.\nSeems reasonably ok, with ability to have sections within contents in the sidebar (I think)."
  },
  {
    "objectID": "RpyEnvs/quarto_website_github.html#questions",
    "href": "RpyEnvs/quarto_website_github.html#questions",
    "title": "Quarto website",
    "section": "Questions",
    "text": "Questions\nIf I render a single file, does it render the whole website? seems like yes. If I want to render single pages (like to test them without having to re-render everything), can use the terminal quarto render filename.qmd or a subdir quarto render subdir/. The output ends up in the _site directory."
  },
  {
    "objectID": "RpyEnvs/quarto_website_github.html#pushing-to-github",
    "href": "RpyEnvs/quarto_website_github.html#pushing-to-github",
    "title": "Quarto website",
    "section": "Pushing to github",
    "text": "Pushing to github\nThe publishing the gh-pages branch seems the nicest, but there’s a bit warning not to do that on Windows. So, I guess I’ll do the render to docs way.\nadd output-dir: docs to the _quarto.yml and then create a .nojekyll file. Then quarto render to render to docs. I think I’ll also add a _quarto.yml.local with\nexecute:\n  cache: true\nto cache output and avoid long re-renders ( I hope). Seems to- re-clicking render was much faster.\nTo set to docs, go to repo, then settings –> Pages (on left) –> deploy from a branch, and choose the branch (likely Main) and /docs instead of /root.\nSo, I’ve been developing on dev, I guess I’ll merge main and see what happens.\nDon’t forget to merge and push ‘main’ if using the publish to /docs method. Otherwise no changes will actually appear."
  },
  {
    "objectID": "RpyEnvs/RandPython.html",
    "href": "RpyEnvs/RandPython.html",
    "title": "Using R and python together",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#the-issue",
    "href": "RpyEnvs/RandPython.html#the-issue",
    "title": "Using R and python together",
    "section": "The issue",
    "text": "The issue\nI have a project primarily in R, but needs some python. For the big python work, I’ll have a directory with a poetry environment and python code. But I’ve run into the issue that I want to run just one or two lines of python from R. The specific case is that I have python code for extracting river gauge data, and I’ve filtered some river gauges in R for something else, and rather than do the finding of the gauges again in python, I’d rather just do the extraction in R. I think that means I have to sort out {reticulate}, but also how to point reticulate at my python environment. The situation I have is a poetry project inside a directory with an Rproj (which probably needs to be split up, but it’s what I have now).\nMy python_setup sets up a very similar situation, so let’s see if I can use it."
  },
  {
    "objectID": "RpyEnvs/RandPython.html#set-up-reticulate-from-r",
    "href": "RpyEnvs/RandPython.html#set-up-reticulate-from-r",
    "title": "Using R and python together",
    "section": "Set up reticulate from R",
    "text": "Set up reticulate from R\nPoint reticulate at the venv. See stackoverflow. This seems to not be necessary if the .venv is in the outer project directory.\n\nreticulate::use_virtualenv(file.path('RpyEnvs', 'pytesting', '.venv'), required = TRUE)\n\nLoad the library. Interestingly, the python code chunks will run without loading the library, but I can’t access their values using py$pythonobject unless I load it.\n\n# library(reticulate)"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#r",
    "href": "RpyEnvs/RandPython.html#r",
    "title": "Using R and python together",
    "section": "R",
    "text": "R\nFirst, let’s create some things in R.\n\na <- 1\nb <- 2"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#python",
    "href": "RpyEnvs/RandPython.html#python",
    "title": "Using R and python together",
    "section": "Python",
    "text": "Python\nDoes not just inherit the values from R, but runs.\n\na = 1\nb = 2\na+b\n\n3\n\n\nDo I have access to packages? Yes.\n\nimport numpy as np\n\nx = np.arange(15, dtype=np.int64).reshape(3, 5)\nx[1:, ::2] = -99\nx\n\narray([[  0,   1,   2,   3,   4],\n       [-99,   6, -99,   8, -99],\n       [-99,  11, -99,  13, -99]], dtype=int64)\n\n\nDoes access to python objects persist? Yes\n\nx.max(axis=1)\n\narray([ 4,  8, 13], dtype=int64)"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#moving-data-back-and-forth",
    "href": "RpyEnvs/RandPython.html#moving-data-back-and-forth",
    "title": "Using R and python together",
    "section": "Moving data back and forth",
    "text": "Moving data back and forth\n\nPython to R\nCan I access objects with R? Yes, but not quite directly. Have to use the py$pythonObject notation. But only if I’ve loaded library(reticulate) or specified with reticulate::py. That’s a pain, so probably almost always better to load the library. Even though the python chunks run fine without explictly loading it, I can’t seem to access py without loading it.\n\n# x\nreticulate::py$x\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    2    3    4\n[2,]  -99    6  -99    8  -99\n[3,]  -99   11  -99   13  -99\n\n\n\n\nR to python\nSimilar to python objects being in py, R objects are in r, and are accessed with . instead of $.\n\nc <- 17\n\nInterestingly, the r. notation to get R into python does not need reticulate:: on it. Which I guess makes some sense- this block is actually running in python and python doesn’t know what reticulate is. But it does know what r. is, somehow. Pretty cool.\n\nr.c + b\n\n19.0"
  },
  {
    "objectID": "RpyEnvs/rig.html",
    "href": "RpyEnvs/rig.html",
    "title": "Managing R versions",
    "section": "",
    "text": "I’ve update to R 4.2, but have projects that were built with 3.x. Some new versions of packages for R 4.x don’t work in 3.x, so I would need to update packages, but I know doing that will break things, and I don’t have time to do a full update of the project.\nI use renv to manage the packages, but not currently anything to switch/manage R versions itself. In python, there’s pyenv to manage python versions. I’ve run across rig (https://github.com/r-lib/rig)."
  },
  {
    "objectID": "RpyEnvs/rig.html#install",
    "href": "RpyEnvs/rig.html#install",
    "title": "Managing R versions",
    "section": "Install",
    "text": "Install\nclick on windows installer. Restart terminal. Type rig list to see what R is available."
  },
  {
    "objectID": "RpyEnvs/rig.html#using-it",
    "href": "RpyEnvs/rig.html#using-it",
    "title": "Managing R versions",
    "section": "Using it",
    "text": "Using it\nGo to the project I want to run, and figure out what version of R it was using. Interesting- it says 4.0.2 so maybe I didn’t need to worry about this? Whoops. Still interesting, I guess. And, just to avoid any issues, I still think I might downgrade to that to run the project because I really just need things to work.\nSo, try rig add 4.0.2. Seems to have worked. Set the default to current, though.\nrig default 4.2.1.\n\nChoosing for a project\nWhat if I just open the project file by double clicking? There’s no obvious way to change the R version just by opening Rstudio- it uses the default.\nI think there’s probably a way to use the CLI to change the R version and then double click, but what seems to be easiest is cd path/to/repo and then rig rstudio renv.lock to open with the version in the lockfile.\nAnd do I keep using other R versions elsewhere? Seem to. For now, this should do what I need."
  },
  {
    "objectID": "RpyEnvs/rig.html#installing-rtools",
    "href": "RpyEnvs/rig.html#installing-rtools",
    "title": "Managing R versions",
    "section": "Installing rtools",
    "text": "Installing rtools\nWe need rtools to install packages with compiled components. R 4.2 has updated to Rtools 42 (from 40), and so using previous versions of R need older Rtools. The telltale is when trying to install a package, we get errors about ‘make’ not being found. The rig documents imply that rig system update-rtools40 should work, but I get “Error: the system cannot find the path specified”. I’m not sure what path that is, so hard to fix. So, I seem to be OK until I need something that needs ‘make’, and then I’m out of luck.\n\nThe solution\nTo install Rtools40, needed for R 4.0- 4.1, run rig add rtools40. Seems to be all it took, now I can compile. I assume there’s a similar command for even older Rtools if need to downgrade to R 3.x, but haven’t tried."
  },
  {
    "objectID": "RpyEnvs/R_py_shared_projects.html",
    "href": "RpyEnvs/R_py_shared_projects.html",
    "title": "R and python envs in same project",
    "section": "",
    "text": "Both renv and poetry want to set up project structures and work within them. And I want to do both in the same git repo because I’m using both for the same project. And have access to both everywhere within the project (ie I want to be able to use reticulate, but more importantly I want the different parts of the project to be able to have both py and R components.\nI’ve done a bit getting them to work in the same script, and setting up clean python environments with poetry. With the shared scripts, I did it with a subdirectory, and that sort of worked for testing, but won’t work for a project where they’re both used a fair amount and in both places.\n\n\nCan I just initiate them both in the base git directory? I know i can with renv, but does poetry let us stick a project in an existing repo? When I was sorting out poetry, I always made a new dir with poetry new dirname.\nIt looks like py code should be in the inner directory of the poetry structure. Let’s assume that. Which roughly matches R structure, where we’ll have code in an R/ dir if it’s a package or in some other dir structure. IE, if we can just get the environment management into the outer dir of the repo, and then all other code inside. I’m not sure though that I’ll want to split py from R at present. Think about that.\nSo, really, the question is whether I can poetry new and poetry install in a dir that already exists.\nMaybe poetry init instead of poetry new? Asks a bunch of questions.\nIt creates a pyproject.toml file, and then poetry install creates the poetry.lockand .venv, but the rest of the structure’s not there (tests dir, second level of the project dir). Will it work? Probably. Do we want that structure? Probably.\n\n\n\nSo, maybe better to poetry new somewhere else and drag over, then poetry install. Does that work? I copied over everything inside the outer dir, since I want the whole project to share the outer dir. It makes the lock and venv, but I get ‘dirname does not contain any element’. I’m guessing because I made a poetry env with a different name. Try using the same name, then again copying over the internals.\nThat seems to work. Now to build the env so everything actually works. But that’s about project details, so I’ll leave this here.\nThat seemed to have made vs code happy- it can find a venv in the workspace and use it. It didn’t do that automatically when the venv was in a subdir. (I had to command palette- select python interpreter)."
  },
  {
    "objectID": "setup/rstudio_themes.html",
    "href": "setup/rstudio_themes.html",
    "title": "Editing Rstudio themes",
    "section": "",
    "text": "I really don’t like how faint the selection colours are for all of the dark themes. When I do a ‘find’, I don’t want to hunt around for dark grey on black. So to fix that, I need to edit the theme.\n\n\nI went to this massive list of themes, clicked ‘gallery’, chose the one I wanted (just a simple edit of Tomorrow Night, will save a more complex hunt for another day). Then the pane in the middle lets us change all the colours. I clicked the ‘General’ tab, and changed the lineHighlight and selection to a nice blue. I also changed the comment colour to green, not sure if I like that or not.\n\nThen, go to the ‘Info’ tab and change the name before downloading. Otherwise Rstudio won’t find it under a new name.\nDownload. This saves as a .tmTheme file, which I think might just be able to be used directly (see new Posit documentation, but I was looking at something old and so used rstudioapi::convertTheme('setup/Tomorrow Night HL.tmTheme', outputLocation = 'setup') to create a .rstheme file.\nThen global options, add, and select the theme. I had to restart Rstudio a couple times for it to take. The edited theme is available in the git for this website."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html",
    "href": "simmodelling/twoDautocorr.html",
    "title": "2d autocorrelation",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#motivation",
    "href": "simmodelling/twoDautocorr.html#motivation",
    "title": "2d autocorrelation",
    "section": "Motivation",
    "text": "Motivation\nI often need to simulate processes that are autocorrelated in two dimensions. Sometimes that’s time and 1d space, sometimes 2d space. Clearly 3d is likely needed as well, and I’ll update this with that once I get to it.\nThis is code that builds on work I’ve done in a couple projects, both across matlab and R. I’m doing it here in R because that’s the most up to date and open-source, but the matlab translation is straightforward.\nWe want to be able to generate a set of values with given statistical properties- means, standard deviations, and correlations in both dimensions. For the moment, I’m developing this with a gaussian random variable, but extensions to other random variables that are transforms from gaussian are relatively straightforward by backcalculating the needed \\(\\mu\\) and \\(\\sigma\\). Care must be taken if the correlations need to also be defined on the final scale.\n\nFuture/elsewhere\nI’ve done the back-calculations for the lognormal to allow setting desired correlations, means, and variances on the lognormal scale, and will add it in here later as an example. Likewise, we might want to set the correlation length \\(\\tau\\) rather than the correlation \\(\\rho\\), and in that case we need to back-calculate \\(\\rho\\) from the desired \\(\\tau\\). I’ve done that as well and will add it in. Finally, I have written up the math to obtain the equations used in this function, and will add that later as well."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#process",
    "href": "simmodelling/twoDautocorr.html#process",
    "title": "2d autocorrelation",
    "section": "Process",
    "text": "Process\nThe goal is a U matrix that is 2d AC, on the normal scale\n\nSet up autocorrelation in the y dimension in U with a usual \\(y+1 = y*\\rho + a\\) formulation, where \\(a\\) is uncorrelated errors\nSet up autocorrelation in the x dimension\n\n\n\nthe errors here (\\(\\varepsilon\\) matrix) need to be correlated in the y dimension\nthese errors are thus generated by an AC process and so need their own set of errors (which are uncorrelated) for that AC\n\nVariances are set for all error matrices (\\(a\\), \\(\\varepsilon\\), and sub-errors (\\(z\\) matrix)) according to the relationships between normVar (the desired \\(\\sigma^2\\) of the final distribution) and the \\(\\rho_y\\) and \\(\\rho_x\\) (the desired correlations in both dimensions)."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#function",
    "href": "simmodelling/twoDautocorr.html#function",
    "title": "2d autocorrelation",
    "section": "Function",
    "text": "Function\nI usually do a bunch of demos, but here I’ve developed this and just want it available more easily. So I’ll lead with the function and then demonstrate it and a few extensions.\n\nac2d <- function(n_x, n_y, \n                 rho_y = 0, rho_x = 0, \n                 normVar = 1,\n                 printStats = FALSE,\n                 returnStats = FALSE) {\n  # n_x = number of sites along the x-dimension\n  # n_y = number of sites along the y-dimension\n  # rho_y = desired autocorr in the x direction\n  # rho_x = desired autocorr in the y direction\n  # normVar = desired variance of the underlying normal distribution\n  \n  # The goal is a U matrix that is 2d AC, on the normal scale\n  \n  # make the U matrix as rnorms to initialise\n  U <- matrix(rnorm(n_x*n_y)*sqrt(normVar), nrow = n_y)\n  \n  # Set up the errors for the y process alone\n  # generate the errors - set the SD of these (hence the sqrt around the\n  # variance)\n  a <- rnorm(n_y) * sqrt((normVar*(1-rho_y^2)))\n  \n  # Make the y ac for the U matrix\n  for (i in 1:(n_y-1)) {\n    U[i+1, ] <- (rho_y * U[i, ]) + a[i]\n  }\n  \n  # Set up for the x-autocorr, which needs to have errors autocorred in the y-dimension\n  \n  # first, generate a z error matrix- these are the errors for epsilon, which\n  # are in turn the errors for U(t,x).\n  # What should var(z) be theoretically?\n  varZ <- normVar*(1-rho_y^2)*(1-rho_x^2)\n  \n  # Make z, adjusting its standard deviation\n  # should have 'y' rows\n  z <- matrix(rnorm(n_x*n_y), nrow = n_y) * \n    (sqrt(normVar * (1-rho_y^2) * (1-rho_x^2)))\n  \n  # now let's generate an epsilon matrix\n  # These are the errors for x part of the 2d ac process. These errors are\n  # themselves autocorrelated in the y dimension.\n  vareps <- normVar * (1-rho_x^2)\n  eps <- matrix(rnorm(n_x*n_y), nrow = n_y) * sqrt(vareps)\n  \n  # Now, generate the eps matrix y-autocorrelated (that is, going down rows within each column)\n  # eps is already created, so just write into the rows\n  for (i in 1:(n_y-1)) {\n    eps[i+1, ] <- (rho_y * eps[i, ]) + z[i, ]\n  }\n  \n  # Now, make the U matrix x-autocorrelated\n  for (t in 1:(n_x-1)) {\n    U[ ,t+1] <- (rho_x * U[ ,t]) + eps[ ,t]\n    \n  }\n  \n  # Check the stats if asked\n  if (printStats | returnStats) {\n    # calc stats in both dimensions\n    acstats <- ac2dstats(U)\n    \n    if (printStats) {\n      print(paste0('Mean of all points is ', round(mean(c(U)), 3)))\n      print(paste0('Var of all points is ', round(var(c(U)), 3)))\n      print(paste0('Mean y AC is ', round(mean(acstats$ac_y), 3)))\n      print(paste0('Mean x AC is ', round(mean(acstats$ac_x), 3)))\n    }\n  }\n  \n  # usually don't want a list with the stats, and can always get later if needed, I suppose\n  if (returnStats) {\n    return(lst(U, acstats))\n  } else {\n    return(U)\n  }\n  \n}\n\nThat potentially calls another function to get the stats, which is here.\n\n# 2d ac stats function, useful for calling elsewhere\nac2dstats <- function(acmatrix) {\n  # Calculate the autocorrs in both dimensions\n  \n  # Conditionals on 0 variance are because ar throws an error if there's no variance. Could have set up a try, but this is clearer\n  # Using 1 as the ac in that case because with no variance each value is the same as previous and so perfectly correlated. NA would be another option.\n  \n  # Get the ac in x-dimension: do this for each y (row)\n  ac_x <- vector(mode = 'numeric', length = nrow(acmatrix)-1)\n  for (i in 1:(nrow(acmatrix)-1)) {\n    if (sd(acmatrix[i, ]) == 0) {\n      ac_x <- 1\n    } else {\n      ac_x[i] <- acf(acmatrix[i, ], lag.max = 1, type = 'correlation', plot = FALSE, demean = TRUE)$acf[2]\n    }\n    \n  } \n  \n  # Get the ac acorss the stream: do this for each x (column)\n  ac_y <- vector(mode = 'numeric', length = ncol(acmatrix)-1)\n  for (i in 1:(ncol(acmatrix)-1)) {\n    \n    if (sd(acmatrix[,i]) == 0) {\n      ac_y[i] <- 1\n    } else {\n      ac_y[i] <- acf(acmatrix[ ,i], lag.max = 1, type = 'correlation', plot = FALSE, demean = TRUE)$acf[2]\n    }\n    \n  } \n  \n  return(lst(ac_y, ac_x))\n}"
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#testing",
    "href": "simmodelling/twoDautocorr.html#testing",
    "title": "2d autocorrelation",
    "section": "Testing",
    "text": "Testing\nA couple edge cases to make sure it doesn’t break. 0 and 1 correlations.\n\nacmatrix_0_1 <- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0, rho_y = 1,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is 0.028\"\n[1] \"Var of all points is 0.976\"\n[1] \"Mean y AC is 1\"\n[1] \"Mean x AC is 0.043\"\n\n\n0 variance, but try to set autocorrelations- forces all points equal, which is right.\n\nacmatrix_0_1 <- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 0, printStats = TRUE)\n\n[1] \"Mean of all points is 0\"\n[1] \"Var of all points is 0\"\n[1] \"Mean y AC is 1\"\n[1] \"Mean x AC is 1\""
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#demonstration",
    "href": "simmodelling/twoDautocorr.html#demonstration",
    "title": "2d autocorrelation",
    "section": "Demonstration",
    "text": "Demonstration\nHow do we use that? Let’s say we want to create an environment that is 1000 x 500 sites, with \\(\\rho_y = 0.9\\) and \\(\\rho_x = 0.7\\), with the whole environment having a variance of 1 (for simplicity).\nSetting printstats = TRUE prints out the statistics and confirms the final matrix has been created with the desired correlations.\n\nacmatrix_7_9 <- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is 0.021\"\n[1] \"Var of all points is 0.993\"\n[1] \"Mean y AC is 0.89\"\n[1] \"Mean x AC is 0.698\"\n\n\nWe can plot that up, easiest is to use ggplot because that’s what I’m used to. First, make it a tibble\n\nactib_7_9 <- tibble::as_tibble(acmatrix_7_9) %>%\n  mutate(y = row_number()) %>%\n  pivot_longer(cols = starts_with('V')) %>%\n  mutate(x = as.numeric(str_remove(name, 'V'))) %>%\n  select(-name)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\nPlot it two different ways. It’s a monster though, so cut it to just a 100x100 block.\nFirst, a contour\n\nggplot(filter(actib_7_9, x > 100 & x <= 200 & y > 300 & y < 400), aes(x = x, y = y, z = value)) +\n  geom_contour_filled()\n\n\n\n\nAnd a tiled version, which is more precisely the data.\n\nggplot(filter(actib_7_9, x > 100 & x <= 200 & y > 300 & y < 400), aes(x = x, y = y, fill = value)) + \n  geom_tile() +\n  viridis::scale_fill_viridis(option = 'viridis')"
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#extensions",
    "href": "simmodelling/twoDautocorr.html#extensions",
    "title": "2d autocorrelation",
    "section": "Extensions",
    "text": "Extensions\n\n2 species\nA crude step toward 3d autocorr is to say we want 2d autocorr for two species (or really, just a second set of 2d autocorrelated values) with known correlation to the first set. I’ve done that, but it’s very task-specific and so not including here until I generalise a bit better.\n\n\nCross-correlation\nBy definition, the 2d autocorrelated matrices here have embedded nonzero cross-correlations at different lags (see analytical work for what they are once I put it in here). As a quick example, we can use ccf to get the cross correlation between two adjacent vectors along the x-dimension (columns), or the same along the y-dimension (rows).\nColumns\n\nccf(x = acmatrix_7_9[,100], y = acmatrix_7_9[,101], lag.max = 10, type = 'correlation')\n\n\n\n\nRows\n\nccf(x = acmatrix_7_9[100,], y = acmatrix_7_9[101,], lag.max = 10, type = 'correlation')"
  },
  {
    "objectID": "small_helpers/smallpieces.html",
    "href": "small_helpers/smallpieces.html",
    "title": "Small pieces",
    "section": "",
    "text": "This is mostly quick little code snippets to copy-paste and avoid re-writing. load tidyverse and get going.\n\nlibrary(tidyverse)\n\n\n\nWe often want to set the root directory not to the file but to the project. In Rmarkdown, we use the following in the setup chunk. Quarto typically uses a different method, but see the Quarto notes for some exceptions. Converting from Rmarkdown to quarto with knitr::convert_chunk_header kills this block, and it’s annoying to always have the header. In both Rmarkdown and Quarto, this has to be in a setup chunk.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nI thought it’d be easiest to set in the global options, but that doesn’t seem to persist to render or knit.\n\n\n\n\n\nnewdir <- file.path('output', 'testdir')\nif (!dir.exists(newdir)) {dir.create(newdir, recursive = TRUE)}\n\n\n\n\nWindows paths come in with \\, which R treats as an escape character. We can use file.path to just avoid them, or replace them with / or \\\\. But sometimes we just want to paste a path in quickly and be done with it. As of R 4.0, we can do that with r. It requires the parentheses to be in a funny place- inside the quotes.\n\npastepath <- r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\"\npastepath\n\n[1] \"C:\\\\Users\\\\username\\\\path\\\\to\\\\somewhere.csv\"\n\n\nAnd we can feed that straight into functions that need paths, eg.\n\nreadr::read_csv(r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\")\n\n\n\n\nFunctions like duplicated give the second (and greater) values that match. e.g.\n\nx <- c(1,2,1,3,4,2)\nduplicated(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nBut we often want to grab all values that are repeated- ie if everything matches in one column what’s going on in the others. do do that we can use group_by and filter to get those with > 1 row.\nIE, let’s compare cars with duplicated mpg values\n\nmtcars %>%\n  dplyr::group_by(mpg) %>%\n  dplyr::filter(n() > 1) %>%\n  dplyr::arrange(mpg) # makes the comparisons easier\n\n\n\n  \n\n\n\nWhy is that useful? We can see not only that these aren’t fully duplicated rows (which we also could have done with duplicated on the whole table), but also actually look at what differs easily.\n\n\n\nSometimes with long csvs, readr’s guess of col type based on the first thousand rows is wrong. But only for some cols. If we want to not have to specify all of them, we can use .default and only specify the offending col.\nFirst, save dummy data\n\ndumtib <- tibble(c1 = 1:3000, c2 = rep(letters, length.out = 3000), c3 = c(c1[1:2000], c2[2001:3000]))\n\nwrite_csv(dumtib, file = file.path(newdir, 'colspectest.csv'))\n\nIf we read in without the cols, it assumes c3 is numeric and we get errors. But it doesn’t. why not? It keeps getting me elsewhere, but now I can’t create the problem. FIgure this out later, I guess\n\nfilein <- read_csv(file.path(newdir, 'colspectest.csv'), guess_max = 100)\n\nRows: 3000 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): c2, c3\ndbl (1): c1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTell it the third col is character.\n\nfilein <- readr::read_csv(file.path(newdir, 'colspectest.csv'), col_types = cols(.default = \"?\", c3 = \"c\"))\n\n\n\n\nYes, we should be building as a library in this case, but it’s often easier at least initially to not deal with the overhead. If, for example, all functions are in the ‘functions’ directory,\n\n# Location-setting header\n# source everything in the functions folder. This really is turning into a package\nfunfiles <- list.files('functions')\nfor (s in 1:length(funfiles)) {\n  source(file.path('functions', funfiles[s])) \n}\n\n\n\n\nRender in quarto defaults to making dfs text, and so often we can’t see all the columns (or rows), or access them. setting the df-print option to paged allows them to work. The header should look like this (commented out because this isn’t a header)\n\n# title: \"TITLE\"\n# author: \"AUTHOR\"\n# format:\n#   html:\n#     df-print: paged\n\n\n\n\nconvert_chunk_headers is the main thing, but I want to apply it to a full directory. Let’s get the dir for here.\n\nallrmd <- list.files(rprojroot::find_rstudio_root_file(), pattern = '.Rmd', recursive = TRUE, full.names = TRUE)\n\nallrmd <- allrmd[!stringr::str_detect(allrmd, 'renv')]\n\nallqmd <- stringr::str_replace(allrmd, '.Rmd', '.qmd')\n\nCan I vectorize? No, but a loop works. Git commit first!\n\nfor (i in 1:length(allrmd)) {\n  knitr::convert_chunk_header(input = allrmd[i], output = allqmd[i])\n}\n\nNow, if you want to really go for it, delete the rmds. That makes git happier because then it can treat this as a rename and keep tracking the files.\nDangerous- make sure you’ve git-committed. I’m commenting out and eval: false ing this\n\n# file.remove(allrmd)"
  },
  {
    "objectID": "small_helpers/zip_downloading.html",
    "href": "small_helpers/zip_downloading.html",
    "title": "Download Zip helper",
    "section": "",
    "text": "When we download files from the internet, we often feed in a url, and it returns a zip, which we then want to unzip to access. There’s a fairly simple way to do that, but we can write a quicky function to do it and clean up the directory afterwards."
  },
  {
    "objectID": "small_helpers/zip_downloading.html#the-function",
    "href": "small_helpers/zip_downloading.html#the-function",
    "title": "Download Zip helper",
    "section": "The function",
    "text": "The function\nwe want to give it the dirname for the file(s), the datadir that contains our data, and the URL. Then it checks if it exists, and downloads, unzips, and cleans up.\n\nzip_load <- function(dirname, datadir, sourceurl,  \n                      existing_dirs = list.files(datadir)) {\n  print(existing_dirs)\n  if (!(dirname %in% existing_dirs)) {\n    \n    zippath <- file.path(datadir, paste0(dirname, '.zip'))\n    download.file(sourceurl, destfile = zippath)\n    \n    unzip(zippath, exdir = file.path(datadir, dirname))\n    \n    file.remove(zippath)\n  }\n}"
  },
  {
    "objectID": "small_helpers/zip_downloading.html#an-example",
    "href": "small_helpers/zip_downloading.html#an-example",
    "title": "Download Zip helper",
    "section": "An example",
    "text": "An example\nGet the Murray-Darling basin boundary\n\nzip_load('mdb_boundary', 'data', \"https://data.gov.au/data/dataset/4ede9aed-5620-47db-a72b-0b3aa0a3ced0/resource/8a6d889d-723b-492d-8c12-b8b0d1ba4b5a/download/sworkingadhocjobsj4430dataoutputsmdb_boundarymdb_boundary.zip\")\n\ncharacter(0)\n\n\nWarning in download.file(sourceurl, destfile = zippath): URL\nhttps://data.gov.au/data/dataset/4ede9aed-5620-47db-a72b-0b3aa0a3ced0/resource/8a6d889d-723b-492d-8c12-b8b0d1ba4b5a/download/sworkingadhocjobsj4430dataoutputsmdb_boundarymdb_boundary.zip:\ncannot open destfile 'data/mdb_boundary.zip', reason 'No such file or directory'\n\n\nWarning in download.file(sourceurl, destfile = zippath): download had nonzero\nexit status\n\n\nWarning in unzip(zippath, exdir = file.path(datadir, dirname)): error 1 in\nextracting from zip file\n\n\nWarning in file.remove(zippath): cannot remove file 'data/mdb_boundary.zip',\nreason 'No such file or directory'\n\n\n[1] FALSE\n\n\nI’ve saved this in functions/ so I have easy access to it everywhere."
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html",
    "href": "vicwater/vicwater_api_howtocall.html",
    "title": "Vicwater api crude testing",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#access-victoria-water-data-through-api",
    "href": "vicwater/vicwater_api_howtocall.html#access-victoria-water-data-through-api",
    "title": "Vicwater api crude testing",
    "section": "Access Victoria water data through API",
    "text": "Access Victoria water data through API\nWe want to access victorian water data for a set of sites. That requires using the api at https://data.water.vic.gov.au/cgi/webservice.exe?[JSON_request] , but it’s poorly documented, and I’ve maybe done one API call ever. Time to figure this out. Will start by piggybacking on the mdba-gauge-getter python that gets water levels as a starting point and then try to get other data.\nFirst, how do we make an API request? Most tutorials use twitter or github, which are well-documented. But let’s try something similar.\npurrr conflicts with jsonlite::flatten, so don’t load tidyverse.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(httr)\nlibrary(jsonlite)\n\n# Actually end up using\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.2.2\n\n\nLooks like the first thing is the base url. The web says it’s this, and the mdba-gauge-getter uses the same, and then appends json_data\n\nvicurl <- \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\n\nI guess I need to specify something to get. But there is no documentation I can find for what the parameters are. The gauge-getter has a few, so I guess start picking things apart.\n\nparams <- list(\"site_list\" = '232202')\n\n\nresponse <- GET(vicurl, query = params)\nresponse\n\nResponse [https://data.water.vic.gov.au/cgi/webservice.exe?site_list=232202]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: text/html\n  Size: 103 B\n\n\nFollowing the R api vignette,\n\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nInteresting. It looked like it returned 200 (good) when I printed response and when I look at it in the View, but actually had errors. Where ARE these results?\nso, can we add those missing ‘top-level’ items? I see now that the gauge-getter has a two-level dict\n\nparams <- list(\"version\" = '2')\nresponse <- GET(vicurl, params = params)\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\nparsed\n\nTry the example. can’t get it to even be a character vector\n\n# demourl <- https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}\n\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\nresponse <- GET(vicurl, query = params)\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 122\n\n$error_msg\n[1] \"Parameter error(s) for function get_db_info:Missing: table_name\"\n\n\nWell, that’s a start. at least I’m not getting the top-level errors. Can i just smash that whole demo into a single params list? without the sublists of ‘params’ and ‘filter_values’?\n\n# params <- list(\"function\" = 'get_db_info',\n#                \"version\" = \"3\",\n#                \"table_name\" = \"site\",\n#                \"station\" = \"221001\")\n# response <- GET(vicurl, query = params)\n# \n# # The parsed barfs\n# # parsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n# # parsed\n# \n# response\n\nWhat am I actually asking for here? GET is using modify_url\n\nmodify_url(vicurl, query = params)\n\n[1] \"https://data.water.vic.gov.au/cgi/webservice.exe?function=get_db_info&version=3\"\n\n\nSo that’s using the ’conventional parameter pairs’ option here, not the json . How do I generate some json so I can see if I’m matching the format? auto_unbox = TRUE is needed to not wrap the second values in brackets.\n\ntoJSON(params, auto_unbox = TRUE)\n\n{\"function\":\"get_db_info\",\"version\":\"3\"} \n\n\nOK, so that looks vaguely right, but not leveled. Can we do lists of lists?\n\nnestparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"hash\",\n                               \"filter_values\" = list(\"station\" = \"221001\")))\ntoJSON(nestparams, auto_unbox = TRUE)\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}} \n\n\nWhoa! that looks right. Now, let’s try it. It immediately fails to just use GET. try looking at modify_url to see why.\n\njsonbit <- toJSON(nestparams, auto_unbox = TRUE)\n\n\nmodify_url(vicurl, query = jsonbit)\n\n[1] \"https://data.water.vic.gov.au/cgi/webservice.exe?{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\n\nmodify_url(vicurl, path = jsonbit)\n\n[1] \"https://data.water.vic.gov.au/{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\nGetting a lot of slashes. does it matter? Maybe?\n\nmodify_url(vicurl, scheme = nestparams)\n\n[1] \"get_db_info://data.water.vic.gov.au/cgi/webservice.exe\"                                                                                    \n[2] \"3://data.water.vic.gov.au/cgi/webservice.exe\"                                                                                              \n[3] \"list(table_name = \\\"site\\\", return_type = \\\"hash\\\", filter_values = list(station = \\\"221001\\\"))://data.water.vic.gov.au/cgi/webservice.exe\"\n\n\nIs httpbin a way to test?\n\nbinurl <- \"http://httpbin.org/get\"\n\nbinr <- GET(binurl, query = jsonbit)\nbinr\n\nResponse [http://httpbin.org/get?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: application/json\n  Size: 689 B\n{\n  \"args\": {\n    \"{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name...\n  }, \n  \"headers\": {\n    \"Accept\": \"application/json, text/xml, application/xml, */*\", \n    \"Accept-Encoding\": \"deflate, gzip\", \n    \"Host\": \"httpbin.org\", \n    \"User-Agent\": \"libcurl/7.64.1 r-curl/4.3.3 httr/1.4.4\", \n    \"X-Amzn-Trace-Id\": \"Root=1-638ffc12-7dab7950495d406217e5071c\"\n...\n\n\n\nparsedB <- fromJSON(content(binr, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsedB\n\n$args\n$args$`{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}`\n[1] \"\"\n\n\n$headers\n$headers$Accept\n[1] \"application/json, text/xml, application/xml, */*\"\n\n$headers$`Accept-Encoding`\n[1] \"deflate, gzip\"\n\n$headers$Host\n[1] \"httpbin.org\"\n\n$headers$`User-Agent`\n[1] \"libcurl/7.64.1 r-curl/4.3.3 httr/1.4.4\"\n\n$headers$`X-Amzn-Trace-Id`\n[1] \"Root=1-638ffc12-7dab7950495d406217e5071c\"\n\n\n$origin\n[1] \"180.222.17.154\"\n\n$url\n[1] \"http://httpbin.org/get?{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\nThat call looks right.\n\nresponse <- GET(vicurl, query = jsonbit, encode = 'json')\nresponse\n\nResponse [https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: text/html\n  Size: 99 B\n\n\n\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Request is not well-formed JSON\\r\\nInput request was not valid JSON\"\n\n\nI pasted it in to notebook++ and it’s exactly the same as the example. So, why isn’t it working?\n\nmodurl <- modify_url(vicurl, query = jsonbit)\nresponse <- GET(modurl)\nresponse\n\nResponse [https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: text/html\n  Size: 99 B\n\n\n\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Request is not well-formed JSON\\r\\nInput request was not valid JSON\""
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#httr2",
    "href": "vicwater/vicwater_api_howtocall.html#httr2",
    "title": "Vicwater api crude testing",
    "section": "httr2",
    "text": "httr2\nHmmm. I see Hadley has released a v2. And it has a req_body_json. See if that works\n\nlibrary(httr2)\n\nThe req_dry_run lets us see what it’s passing. THat looks right? I think?\n\nreq <- request(vicurl)\nreq %>% \n  req_body_json(nestparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 129\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}\n\n\n\nresp <- req %>% \n  req_body_json(nestparams) %>% \n  req_perform()\n\n\nresp %>% resp_raw()\n\nHTTP/1.1 200 OK\nDate: Wed, 07 Dec 2022 02:36:05 GMT\nContent-Type: text/html\nContent-Length: 972\nConnection: keep-alive\nContent-Encoding: gzip\nVary: Accept-Encoding\nServer: Microsoft-IIS/10.0\nContent: \nAccess-Control-Allow-Origin: *\n\n{\"error_num\":0,\"return\":{\"rows\":{\"221001\":{\"category20\":\"\",\"category19\":\"\",\"category18\":\"\",\"category17\":\"\",\"category16\":\"\",\"category15\":\"\",\"category14\":\"\",\"category13\":\"\",\"category12\":\"\",\"category11\":\"\",\"category10\":\"N\\/A\",\"active\":false,\"northing\":\"5887218.000\",\"timezone\":\"10.0\",\"shortname\":\"GENOA R @ ROCKTON\",\"datecreate\":18991230,\"elevdatum\":\"\",\"stname\":\"GENOA RIVER @ ROCKTON\",\"category9\":\"N\\/A\",\"category8\":\"G(S)\",\"category7\":\"G\",\"category6\":\"2WD\\/4WD\",\"category5\":\"10\",\"category4\":\"150\",\"category3\":\"GRAVEL\",\"category2\":\"V_70D3\",\"category1\":\"23\",\"elevacc\":\"1\",\"dbver47\":false,\"quarter\":\"Y\",\"section\":0,\"commence\":19930526,\"parent\":\"\",\"mapname\":\"MAFFRA\",\"meridian\":\"\",\"spare5\":\"\",\"spare4\":\"\",\"spare3\":\"\",\"spare2\":\"N\\/A\",\"spare1\":\"N\\/A\",\"posacc\":\"9\",\"timemod\":1642,\"region\":\"221\",\"grdatum\":\"UTM\",\"township\":\"\",\"longitude\":\"149.317296100\",\"comment\":\" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011\\/07\\/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\",\"lldatum\":\"WGS84\",\"station\":\"221001\",\"datemod\":20221117,\"timecreate\":0,\"orgcode\":\"DSE\",\"barcode\":\"Bombala\",\"zone\":55,\"elev\":\"429.000\",\"cease\":18991230,\"local_map\":\"CRAIGIE\",\"latitude\":\"-37.138654990\",\"range\":\"\",\"qquarter\":\"Y\",\"easting\":\"705825.000\",\"stntype\":\"VIR\"}}}}\n\n\nI thought it was json? but resp_body_json fails with defaults\n\nrj <- resp %>% resp_body_json(check_type = FALSE)\nrj\n\n$error_num\n[1] 0\n\n$return\n$return$rows\n$return$rows$`221001`\n$return$rows$`221001`$category20\n[1] \"\"\n\n$return$rows$`221001`$category19\n[1] \"\"\n\n$return$rows$`221001`$category18\n[1] \"\"\n\n$return$rows$`221001`$category17\n[1] \"\"\n\n$return$rows$`221001`$category16\n[1] \"\"\n\n$return$rows$`221001`$category15\n[1] \"\"\n\n$return$rows$`221001`$category14\n[1] \"\"\n\n$return$rows$`221001`$category13\n[1] \"\"\n\n$return$rows$`221001`$category12\n[1] \"\"\n\n$return$rows$`221001`$category11\n[1] \"\"\n\n$return$rows$`221001`$category10\n[1] \"N/A\"\n\n$return$rows$`221001`$active\n[1] FALSE\n\n$return$rows$`221001`$northing\n[1] \"5887218.000\"\n\n$return$rows$`221001`$timezone\n[1] \"10.0\"\n\n$return$rows$`221001`$shortname\n[1] \"GENOA R @ ROCKTON\"\n\n$return$rows$`221001`$datecreate\n[1] 18991230\n\n$return$rows$`221001`$elevdatum\n[1] \"\"\n\n$return$rows$`221001`$stname\n[1] \"GENOA RIVER @ ROCKTON\"\n\n$return$rows$`221001`$category9\n[1] \"N/A\"\n\n$return$rows$`221001`$category8\n[1] \"G(S)\"\n\n$return$rows$`221001`$category7\n[1] \"G\"\n\n$return$rows$`221001`$category6\n[1] \"2WD/4WD\"\n\n$return$rows$`221001`$category5\n[1] \"10\"\n\n$return$rows$`221001`$category4\n[1] \"150\"\n\n$return$rows$`221001`$category3\n[1] \"GRAVEL\"\n\n$return$rows$`221001`$category2\n[1] \"V_70D3\"\n\n$return$rows$`221001`$category1\n[1] \"23\"\n\n$return$rows$`221001`$elevacc\n[1] \"1\"\n\n$return$rows$`221001`$dbver47\n[1] FALSE\n\n$return$rows$`221001`$quarter\n[1] \"Y\"\n\n$return$rows$`221001`$section\n[1] 0\n\n$return$rows$`221001`$commence\n[1] 19930526\n\n$return$rows$`221001`$parent\n[1] \"\"\n\n$return$rows$`221001`$mapname\n[1] \"MAFFRA\"\n\n$return$rows$`221001`$meridian\n[1] \"\"\n\n$return$rows$`221001`$spare5\n[1] \"\"\n\n$return$rows$`221001`$spare4\n[1] \"\"\n\n$return$rows$`221001`$spare3\n[1] \"\"\n\n$return$rows$`221001`$spare2\n[1] \"N/A\"\n\n$return$rows$`221001`$spare1\n[1] \"N/A\"\n\n$return$rows$`221001`$posacc\n[1] \"9\"\n\n$return$rows$`221001`$timemod\n[1] 1642\n\n$return$rows$`221001`$region\n[1] \"221\"\n\n$return$rows$`221001`$grdatum\n[1] \"UTM\"\n\n$return$rows$`221001`$township\n[1] \"\"\n\n$return$rows$`221001`$longitude\n[1] \"149.317296100\"\n\n$return$rows$`221001`$comment\n[1] \" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011/07/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\"\n\n$return$rows$`221001`$lldatum\n[1] \"WGS84\"\n\n$return$rows$`221001`$station\n[1] \"221001\"\n\n$return$rows$`221001`$datemod\n[1] 20221117\n\n$return$rows$`221001`$timecreate\n[1] 0\n\n$return$rows$`221001`$orgcode\n[1] \"DSE\"\n\n$return$rows$`221001`$barcode\n[1] \"Bombala\"\n\n$return$rows$`221001`$zone\n[1] 55\n\n$return$rows$`221001`$elev\n[1] \"429.000\"\n\n$return$rows$`221001`$cease\n[1] 18991230\n\n$return$rows$`221001`$local_map\n[1] \"CRAIGIE\"\n\n$return$rows$`221001`$latitude\n[1] \"-37.138654990\"\n\n$return$rows$`221001`$range\n[1] \"\"\n\n$return$rows$`221001`$qquarter\n[1] \"Y\"\n\n$return$rows$`221001`$easting\n[1] \"705825.000\"\n\n$return$rows$`221001`$stntype\n[1] \"VIR\"\n\n\nIf I hack it together to check, first note that the resp_body is raw, as we see in the str of resp_raw and in resp_body_raw\n\nresp %>% resp_raw() %>% str()\n\nHTTP/1.1 200 OK\nDate: Wed, 07 Dec 2022 02:36:05 GMT\nContent-Type: text/html\nContent-Length: 972\nConnection: keep-alive\nContent-Encoding: gzip\nVary: Accept-Encoding\nServer: Microsoft-IIS/10.0\nContent: \nAccess-Control-Allow-Origin: *\n\n{\"error_num\":0,\"return\":{\"rows\":{\"221001\":{\"category20\":\"\",\"category19\":\"\",\"category18\":\"\",\"category17\":\"\",\"category16\":\"\",\"category15\":\"\",\"category14\":\"\",\"category13\":\"\",\"category12\":\"\",\"category11\":\"\",\"category10\":\"N\\/A\",\"active\":false,\"northing\":\"5887218.000\",\"timezone\":\"10.0\",\"shortname\":\"GENOA R @ ROCKTON\",\"datecreate\":18991230,\"elevdatum\":\"\",\"stname\":\"GENOA RIVER @ ROCKTON\",\"category9\":\"N\\/A\",\"category8\":\"G(S)\",\"category7\":\"G\",\"category6\":\"2WD\\/4WD\",\"category5\":\"10\",\"category4\":\"150\",\"category3\":\"GRAVEL\",\"category2\":\"V_70D3\",\"category1\":\"23\",\"elevacc\":\"1\",\"dbver47\":false,\"quarter\":\"Y\",\"section\":0,\"commence\":19930526,\"parent\":\"\",\"mapname\":\"MAFFRA\",\"meridian\":\"\",\"spare5\":\"\",\"spare4\":\"\",\"spare3\":\"\",\"spare2\":\"N\\/A\",\"spare1\":\"N\\/A\",\"posacc\":\"9\",\"timemod\":1642,\"region\":\"221\",\"grdatum\":\"UTM\",\"township\":\"\",\"longitude\":\"149.317296100\",\"comment\":\" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011\\/07\\/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\",\"lldatum\":\"WGS84\",\"station\":\"221001\",\"datemod\":20221117,\"timecreate\":0,\"orgcode\":\"DSE\",\"barcode\":\"Bombala\",\"zone\":55,\"elev\":\"429.000\",\"cease\":18991230,\"local_map\":\"CRAIGIE\",\"latitude\":\"-37.138654990\",\"range\":\"\",\"qquarter\":\"Y\",\"easting\":\"705825.000\",\"stntype\":\"VIR\"}}}}\nList of 5\n $ method     : chr \"POST\"\n $ url        : chr \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\n $ status_code: int 200\n $ headers    :List of 9\n  ..$ Date                       : chr \"Wed, 07 Dec 2022 02:36:05 GMT\"\n  ..$ Content-Type               : chr \"text/html\"\n  ..$ Content-Length             : chr \"972\"\n  ..$ Connection                 : chr \"keep-alive\"\n  ..$ Content-Encoding           : chr \"gzip\"\n  ..$ Vary                       : chr \"Accept-Encoding\"\n  ..$ Server                     : chr \"Microsoft-IIS/10.0\"\n  ..$ Content                    : chr \"\"\n  ..$ Access-Control-Allow-Origin: chr \"*\"\n  ..- attr(*, \"class\")= chr \"httr2_headers\"\n $ body       : raw [1:1460] 7b 22 65 72 ...\n - attr(*, \"class\")= chr \"httr2_response\"\n\n\n\nresp %>% resp_body_raw()\n\n   [1] 7b 22 65 72 72 6f 72 5f 6e 75 6d 22 3a 30 2c 22 72 65 74 75 72 6e 22 3a\n  [25] 7b 22 72 6f 77 73 22 3a 7b 22 32 32 31 30 30 31 22 3a 7b 22 63 61 74 65\n  [49] 67 6f 72 79 32 30 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 39 22 3a\n  [73] 22 22 2c 22 63 61 74 65 67 6f 72 79 31 38 22 3a 22 22 2c 22 63 61 74 65\n  [97] 67 6f 72 79 31 37 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 36 22 3a\n [121] 22 22 2c 22 63 61 74 65 67 6f 72 79 31 35 22 3a 22 22 2c 22 63 61 74 65\n [145] 67 6f 72 79 31 34 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 33 22 3a\n [169] 22 22 2c 22 63 61 74 65 67 6f 72 79 31 32 22 3a 22 22 2c 22 63 61 74 65\n [193] 67 6f 72 79 31 31 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 30 22 3a\n [217] 22 4e 5c 2f 41 22 2c 22 61 63 74 69 76 65 22 3a 66 61 6c 73 65 2c 22 6e\n [241] 6f 72 74 68 69 6e 67 22 3a 22 35 38 38 37 32 31 38 2e 30 30 30 22 2c 22\n [265] 74 69 6d 65 7a 6f 6e 65 22 3a 22 31 30 2e 30 22 2c 22 73 68 6f 72 74 6e\n [289] 61 6d 65 22 3a 22 47 45 4e 4f 41 20 52 20 40 20 52 4f 43 4b 54 4f 4e 22\n [313] 2c 22 64 61 74 65 63 72 65 61 74 65 22 3a 31 38 39 39 31 32 33 30 2c 22\n [337] 65 6c 65 76 64 61 74 75 6d 22 3a 22 22 2c 22 73 74 6e 61 6d 65 22 3a 22\n [361] 47 45 4e 4f 41 20 52 49 56 45 52 20 40 20 52 4f 43 4b 54 4f 4e 22 2c 22\n [385] 63 61 74 65 67 6f 72 79 39 22 3a 22 4e 5c 2f 41 22 2c 22 63 61 74 65 67\n [409] 6f 72 79 38 22 3a 22 47 28 53 29 22 2c 22 63 61 74 65 67 6f 72 79 37 22\n [433] 3a 22 47 22 2c 22 63 61 74 65 67 6f 72 79 36 22 3a 22 32 57 44 5c 2f 34\n [457] 57 44 22 2c 22 63 61 74 65 67 6f 72 79 35 22 3a 22 31 30 22 2c 22 63 61\n [481] 74 65 67 6f 72 79 34 22 3a 22 31 35 30 22 2c 22 63 61 74 65 67 6f 72 79\n [505] 33 22 3a 22 47 52 41 56 45 4c 22 2c 22 63 61 74 65 67 6f 72 79 32 22 3a\n [529] 22 56 5f 37 30 44 33 22 2c 22 63 61 74 65 67 6f 72 79 31 22 3a 22 32 33\n [553] 22 2c 22 65 6c 65 76 61 63 63 22 3a 22 31 22 2c 22 64 62 76 65 72 34 37\n [577] 22 3a 66 61 6c 73 65 2c 22 71 75 61 72 74 65 72 22 3a 22 59 22 2c 22 73\n [601] 65 63 74 69 6f 6e 22 3a 30 2c 22 63 6f 6d 6d 65 6e 63 65 22 3a 31 39 39\n [625] 33 30 35 32 36 2c 22 70 61 72 65 6e 74 22 3a 22 22 2c 22 6d 61 70 6e 61\n [649] 6d 65 22 3a 22 4d 41 46 46 52 41 22 2c 22 6d 65 72 69 64 69 61 6e 22 3a\n [673] 22 22 2c 22 73 70 61 72 65 35 22 3a 22 22 2c 22 73 70 61 72 65 34 22 3a\n [697] 22 22 2c 22 73 70 61 72 65 33 22 3a 22 22 2c 22 73 70 61 72 65 32 22 3a\n [721] 22 4e 5c 2f 41 22 2c 22 73 70 61 72 65 31 22 3a 22 4e 5c 2f 41 22 2c 22\n [745] 70 6f 73 61 63 63 22 3a 22 39 22 2c 22 74 69 6d 65 6d 6f 64 22 3a 31 36\n [769] 34 32 2c 22 72 65 67 69 6f 6e 22 3a 22 32 32 31 22 2c 22 67 72 64 61 74\n [793] 75 6d 22 3a 22 55 54 4d 22 2c 22 74 6f 77 6e 73 68 69 70 22 3a 22 22 2c\n [817] 22 6c 6f 6e 67 69 74 75 64 65 22 3a 22 31 34 39 2e 33 31 37 32 39 36 31\n [841] 30 30 22 2c 22 63 6f 6d 6d 65 6e 74 22 3a 22 20 5c 72 5c 6e 54 75 72 6e\n [865] 20 72 69 67 68 74 20 66 72 6f 6d 20 4d 6f 6e 61 72 6f 20 48 69 67 68 77\n [889] 61 79 20 6f 6e 74 6f 20 49 6d 6c 61 79 20 72 6f 61 64 20 73 69 67 6e 70\n [913] 6f 73 74 65 64 20 38 32 20 6b 6d 20 74 6f 20 45 64 65 6e 2e 20 20 54 72\n [937] 61 76 65 6c 20 6f 76 65 72 20 6c 6f 77 20 6c 65 76 65 6c 20 62 72 69 64\n [961] 67 65 20 20 74 6f 20 61 20 73 68 61 72 70 20 6c 65 66 74 20 62 65 6e 64\n [985] 20 69 6e 20 74 68 65 20 72 6f 61 64 2e 20 20 41 20 70 69 6e 65 20 70 6c\n[1009] 61 6e 74 61 74 69 6f 6e 20 20 77 69 6c 6c 20 62 65 20 64 69 72 65 63 74\n[1033] 6c 79 20 6f 6e 20 74 68 65 20 72 69 67 68 74 2e 20 20 54 75 72 6e 20 64\n[1057] 6f 77 6e 20 69 6e 74 6f 20 70 6c 61 6e 74 61 74 69 6f 6e 20 61 6e 64 20\n[1081] 74 72 61 76 65 6c 20 61 70 70 72 6f 78 2e 20 31 30 30 6d 32 30 31 31 5c\n[1105] 2f 30 37 5c 2f 31 37 20 56 69 72 74 75 61 6c 20 73 69 74 65 20 65 6e 74\n[1129] 72 79 20 67 65 6e 65 72 61 74 65 64 20 62 79 20 44 53 45 56 49 52 54 55\n[1153] 41 4c 53 49 54 45 2e 48 53 43 20 66 72 6f 6d 20 64 65 74 61 69 6c 73 20\n[1177] 69 6e 20 32 32 31 30 30 31 41 5c 72 5c 6e 22 2c 22 6c 6c 64 61 74 75 6d\n[1201] 22 3a 22 57 47 53 38 34 22 2c 22 73 74 61 74 69 6f 6e 22 3a 22 32 32 31\n[1225] 30 30 31 22 2c 22 64 61 74 65 6d 6f 64 22 3a 32 30 32 32 31 31 31 37 2c\n[1249] 22 74 69 6d 65 63 72 65 61 74 65 22 3a 30 2c 22 6f 72 67 63 6f 64 65 22\n[1273] 3a 22 44 53 45 22 2c 22 62 61 72 63 6f 64 65 22 3a 22 42 6f 6d 62 61 6c\n[1297] 61 22 2c 22 7a 6f 6e 65 22 3a 35 35 2c 22 65 6c 65 76 22 3a 22 34 32 39\n[1321] 2e 30 30 30 22 2c 22 63 65 61 73 65 22 3a 31 38 39 39 31 32 33 30 2c 22\n[1345] 6c 6f 63 61 6c 5f 6d 61 70 22 3a 22 43 52 41 49 47 49 45 22 2c 22 6c 61\n[1369] 74 69 74 75 64 65 22 3a 22 2d 33 37 2e 31 33 38 36 35 34 39 39 30 22 2c\n[1393] 22 72 61 6e 67 65 22 3a 22 22 2c 22 71 71 75 61 72 74 65 72 22 3a 22 59\n[1417] 22 2c 22 65 61 73 74 69 6e 67 22 3a 22 37 30 35 38 32 35 2e 30 30 30 22\n[1441] 2c 22 73 74 6e 74 79 70 65 22 3a 22 56 49 52 22 7d 7d 7d 7d\n\n\nSo, if we get the raw, convert to char, then pass to JSON, it looks the same as what I’m getting out of resp_body_json.\n\nresp %>% resp_body_raw() %>% rawToChar() %>% fromJSON()\n\n$error_num\n[1] 0\n\n$return\n$return$rows\n$return$rows$`221001`\n$return$rows$`221001`$category20\n[1] \"\"\n\n$return$rows$`221001`$category19\n[1] \"\"\n\n$return$rows$`221001`$category18\n[1] \"\"\n\n$return$rows$`221001`$category17\n[1] \"\"\n\n$return$rows$`221001`$category16\n[1] \"\"\n\n$return$rows$`221001`$category15\n[1] \"\"\n\n$return$rows$`221001`$category14\n[1] \"\"\n\n$return$rows$`221001`$category13\n[1] \"\"\n\n$return$rows$`221001`$category12\n[1] \"\"\n\n$return$rows$`221001`$category11\n[1] \"\"\n\n$return$rows$`221001`$category10\n[1] \"N/A\"\n\n$return$rows$`221001`$active\n[1] FALSE\n\n$return$rows$`221001`$northing\n[1] \"5887218.000\"\n\n$return$rows$`221001`$timezone\n[1] \"10.0\"\n\n$return$rows$`221001`$shortname\n[1] \"GENOA R @ ROCKTON\"\n\n$return$rows$`221001`$datecreate\n[1] 18991230\n\n$return$rows$`221001`$elevdatum\n[1] \"\"\n\n$return$rows$`221001`$stname\n[1] \"GENOA RIVER @ ROCKTON\"\n\n$return$rows$`221001`$category9\n[1] \"N/A\"\n\n$return$rows$`221001`$category8\n[1] \"G(S)\"\n\n$return$rows$`221001`$category7\n[1] \"G\"\n\n$return$rows$`221001`$category6\n[1] \"2WD/4WD\"\n\n$return$rows$`221001`$category5\n[1] \"10\"\n\n$return$rows$`221001`$category4\n[1] \"150\"\n\n$return$rows$`221001`$category3\n[1] \"GRAVEL\"\n\n$return$rows$`221001`$category2\n[1] \"V_70D3\"\n\n$return$rows$`221001`$category1\n[1] \"23\"\n\n$return$rows$`221001`$elevacc\n[1] \"1\"\n\n$return$rows$`221001`$dbver47\n[1] FALSE\n\n$return$rows$`221001`$quarter\n[1] \"Y\"\n\n$return$rows$`221001`$section\n[1] 0\n\n$return$rows$`221001`$commence\n[1] 19930526\n\n$return$rows$`221001`$parent\n[1] \"\"\n\n$return$rows$`221001`$mapname\n[1] \"MAFFRA\"\n\n$return$rows$`221001`$meridian\n[1] \"\"\n\n$return$rows$`221001`$spare5\n[1] \"\"\n\n$return$rows$`221001`$spare4\n[1] \"\"\n\n$return$rows$`221001`$spare3\n[1] \"\"\n\n$return$rows$`221001`$spare2\n[1] \"N/A\"\n\n$return$rows$`221001`$spare1\n[1] \"N/A\"\n\n$return$rows$`221001`$posacc\n[1] \"9\"\n\n$return$rows$`221001`$timemod\n[1] 1642\n\n$return$rows$`221001`$region\n[1] \"221\"\n\n$return$rows$`221001`$grdatum\n[1] \"UTM\"\n\n$return$rows$`221001`$township\n[1] \"\"\n\n$return$rows$`221001`$longitude\n[1] \"149.317296100\"\n\n$return$rows$`221001`$comment\n[1] \" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011/07/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\"\n\n$return$rows$`221001`$lldatum\n[1] \"WGS84\"\n\n$return$rows$`221001`$station\n[1] \"221001\"\n\n$return$rows$`221001`$datemod\n[1] 20221117\n\n$return$rows$`221001`$timecreate\n[1] 0\n\n$return$rows$`221001`$orgcode\n[1] \"DSE\"\n\n$return$rows$`221001`$barcode\n[1] \"Bombala\"\n\n$return$rows$`221001`$zone\n[1] 55\n\n$return$rows$`221001`$elev\n[1] \"429.000\"\n\n$return$rows$`221001`$cease\n[1] 18991230\n\n$return$rows$`221001`$local_map\n[1] \"CRAIGIE\"\n\n$return$rows$`221001`$latitude\n[1] \"-37.138654990\"\n\n$return$rows$`221001`$range\n[1] \"\"\n\n$return$rows$`221001`$qquarter\n[1] \"Y\"\n\n$return$rows$`221001`$easting\n[1] \"705825.000\"\n\n$return$rows$`221001`$stntype\n[1] \"VIR\"\n\n\nNow can I clean that up? THere’s a lot of nesting but some of it is pointless?\n\nnames(rj)\n\n[1] \"error_num\" \"return\"   \n\n\nand we know error_num is a single int from the str above. And $return is length 1 with only rows and rows only has the gauge number. After that there’s actually some data.\n\nnames(rj$return)\n\n[1] \"rows\"\n\nnames(rj$return$rows)\n\n[1] \"221001\"\n\nnames(rj$return$rows$`221001`)\n\n [1] \"category20\" \"category19\" \"category18\" \"category17\" \"category16\"\n [6] \"category15\" \"category14\" \"category13\" \"category12\" \"category11\"\n[11] \"category10\" \"active\"     \"northing\"   \"timezone\"   \"shortname\" \n[16] \"datecreate\" \"elevdatum\"  \"stname\"     \"category9\"  \"category8\" \n[21] \"category7\"  \"category6\"  \"category5\"  \"category4\"  \"category3\" \n[26] \"category2\"  \"category1\"  \"elevacc\"    \"dbver47\"    \"quarter\"   \n[31] \"section\"    \"commence\"   \"parent\"     \"mapname\"    \"meridian\"  \n[36] \"spare5\"     \"spare4\"     \"spare3\"     \"spare2\"     \"spare1\"    \n[41] \"posacc\"     \"timemod\"    \"region\"     \"grdatum\"    \"township\"  \n[46] \"longitude\"  \"comment\"    \"lldatum\"    \"station\"    \"datemod\"   \n[51] \"timecreate\" \"orgcode\"    \"barcode\"    \"zone\"       \"elev\"      \n[56] \"cease\"      \"local_map\"  \"latitude\"   \"range\"      \"qquarter\"  \n[61] \"easting\"    \"stntype\"   \n\n\nNow, can we put that in a dataframe? Is each one length 1?\n\nactualdata <- rj$return$rows$`221001`\n\nall(purrr::map_int(actualdata, length) == 1)\n\n[1] TRUE\n\n\n\ntibout <- as_tibble(actualdata)\ntibout\n\n# A tibble: 1 × 62\n  category20 category19 catego…¹ categ…² categ…³ categ…⁴ categ…⁵ categ…⁶ categ…⁷\n  <chr>      <chr>      <chr>    <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n1 \"\"         \"\"         \"\"       \"\"      \"\"      \"\"      \"\"      \"\"      \"\"     \n# … with 53 more variables: category11 <chr>, category10 <chr>, active <lgl>,\n#   northing <chr>, timezone <chr>, shortname <chr>, datecreate <int>,\n#   elevdatum <chr>, stname <chr>, category9 <chr>, category8 <chr>,\n#   category7 <chr>, category6 <chr>, category5 <chr>, category4 <chr>,\n#   category3 <chr>, category2 <chr>, category1 <chr>, elevacc <chr>,\n#   dbver47 <lgl>, quarter <chr>, section <int>, commence <int>, parent <chr>,\n#   mapname <chr>, meridian <chr>, spare5 <chr>, spare4 <chr>, spare3 <chr>, …\n\n\nand toss cols with no data\n\ntibout %>% select(where(~!all(. == '')))\n\n# A tibble: 1 × 44\n  catego…¹ active north…² timez…³ short…⁴ datec…⁵ stname categ…⁶ categ…⁷ categ…⁸\n  <chr>    <lgl>  <chr>   <chr>   <chr>     <int> <chr>  <chr>   <chr>   <chr>  \n1 N/A      FALSE  588721… 10.0    GENOA …  1.90e7 GENOA… N/A     G(S)    G      \n# … with 34 more variables: category6 <chr>, category5 <chr>, category4 <chr>,\n#   category3 <chr>, category2 <chr>, category1 <chr>, elevacc <chr>,\n#   dbver47 <lgl>, quarter <chr>, section <int>, commence <int>, mapname <chr>,\n#   spare2 <chr>, spare1 <chr>, posacc <chr>, timemod <int>, region <chr>,\n#   grdatum <chr>, longitude <chr>, comment <chr>, lldatum <chr>,\n#   station <chr>, datemod <int>, timecreate <int>, orgcode <chr>,\n#   barcode <chr>, zone <int>, elev <chr>, cease <int>, local_map <chr>, …\n\n\nCool, I have data and can do stuff with it. NOW, how do I get the data I actually want, vs the data that happens to be in the one demo on the website?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#querying-options",
    "href": "vicwater/vicwater_api_howtocall.html#querying-options",
    "title": "Vicwater api crude testing",
    "section": "Querying options",
    "text": "Querying options\nBefore we go get data, we need to figure out what we can ask for. First, sort out those functions.\n\nDatasources\nLet’s try get_datasources_by_site. Takes site_list params. Dunno what versions it works for? Tried 2, gave error says has to be 1. I assume the “A” datasource means archive here, since that’s what it means in QLD.\n\nds_s_params <- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217\"))\n\n# req <- request(vicurl)\n\nreq %>% \n  req_body_json(ds_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 84\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217\"}}\n\nresp_ds_s <- req %>% \n  req_body_json(ds_s_params) %>% \n  req_perform()\n\nrbody_ds_s <- resp_ds_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 1\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\n\n\nSitelist\nOk, I could make that into a tibble easily enough. It tells me what that site has for datasources, how about another? Can I figure out how to use a sitelist? That’d be really nice, and applies in a lot of places. Cool. I had tried to do \"sitelist\" = c('site', 'site') , and that failed. But it works to have \"site, site\"\n\nds_s_params <- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\"))\n\n# req <- request(vicurl)\n\nreq %>% \n  req_body_json(ds_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 92\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328\"}}\n\nresp_ds_s <- req %>% \n  req_body_json(ds_s_params) %>% \n  req_perform()\n\nrbody_ds_s <- resp_ds_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 2\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405328\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\n\n\nVariables\nNow, for a given site, we want to know what variables are available. (and I also eventually want to know what all possible variables are, and what happens if we ask for variables that aren’t there). Let’s start with the same two sites. I’m\n\nv_s_params <- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\",\n                               \"datasource\" = \"A\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(v_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 103\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328\",\"datasource\":\"A\"}}\n\nresp_v_s <- req %>% \n  req_body_json(v_s_params) %>% \n  req_perform()\n\nrbody_v_s <- resp_v_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 2\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ variables   :List of 6\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"19610306171500\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"210.00\"\n  .. .. .. .. ..$ units       : chr \"pH\"\n  .. .. .. .. ..$ name        : chr \"Acidity/Alkalinity (pH)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"215.00\"\n  .. .. .. .. ..$ units       : chr \"ppm\"\n  .. .. .. .. ..$ name        : chr \"Dissolved Oxygen (ppm)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"233217\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221111094500\"\n  .. .. .. .. ..$ period_start: chr \"20091119170800\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. ..$ site        : chr \"405328\"\n\n\nSo, that gives me the number and name of each variable at each site. But it does not give derived variables (discharge being the main one)."
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#location-etc",
    "href": "vicwater/vicwater_api_howtocall.html#location-etc",
    "title": "Vicwater api crude testing",
    "section": "Location etc",
    "text": "Location etc\nSo, get_db_info seems like it should be useful, but kind of isn’t (see above). Maybe I’ll come back to that. It does let us do geofilters, but they seem both crude and complex. I think I’d probably rather do geofiltering myself and then turn that into a site_list. But might come back to this. The complex_filter might be useful if we can use it to choose sites based on something. But again, I’d probably do that myself? Again, come back to this maybe?\n\nCan we get a list of all sites and all variables?\nMaybe? Do we want to?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#getting-timeseries",
    "href": "vicwater/vicwater_api_howtocall.html#getting-timeseries",
    "title": "Vicwater api crude testing",
    "section": "Getting timeseries",
    "text": "Getting timeseries\nNow, let’s go back to get timeseries, now we know what the variables are. Just for the Barwon at first, and way fewer days. There’s a var_list option, or varfrom and varto. It’s unclear whether the from and to version is numerivally inclusive- ie if we have 100 and 10000, does it get everything? I’ll try with varto = 820, since that’s the highest number avail at the barwon. Gives cryptic error. Try 210? Also, why isn’t 141 available in teh lsit above? Again, cryptic “Assumed fail to reload varcon for 233217: 100.00-> 210.00, as we failed loading it last time”. does it work for 141? Yes.\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"100\",\n                               \"interval\" = \"day\",\n                               \"varto\" = \"141\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"100\",\"interval\":\"day\",\"varto\":\"141\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 150: chr \"Rating extrapolated above 1.5x maximum flow gauged.\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"15.93\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"14.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"11.23\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.81\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.75\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.09\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"4.13\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"2.48\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.88\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"12.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"9.87\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.42\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.45\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"9.74\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Discharge (Ml/d)\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"141.00\"\n  .. .. .. ..$ units     : chr \"megalitres/day\"\n  .. .. .. ..$ name      : chr \"Stream Discharge (Ml/d)\"\n\n\nDo the other vars work if we ask for them separately? Try pH (which failed above)\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"210\",\n                               \"interval\" = \"day\",\n                               \"varto\" = \"210\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"210\",\"interval\":\"day\",\"varto\":\"210\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nInteresting. How about a var_list?\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nThat works, seems to set the varfrom and varto to each value in the list. I wonder if things like 141 are done in the from/to way because they are derived from 100. But how do we find them when they don’t appear in the get_variable_list? Can I include them in var_list? Hmm. No. what’s going on? see table 3 in qld doc- they are derived, and it gives numbers. Is there a get_available_varcons or similar?\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,141,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\",\n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 227\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,141,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nHow about asking for variables that don’t exist- ie can we just ask for all of them, and it just gives us whatever’s available? The other site (Steavenson, 405328) only has variable 100, so ask for some others. Just gives 100.\n\nsparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"405328\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(sparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"405328\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nresps <- req %>% \n  req_body_json(sparams) %>% \n  req_perform()\n\nrbodys <- resps %>% resp_body_json(check_type = FALSE)\n\nstr(rbodys)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ longitude : chr \"145.773503100\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. .. ..$ latitude  : chr \"-37.525797590\"\n  .. .. .. ..$ org_name  : chr \"Victorian Rural Water Corporation\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.738\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.736\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.734\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.755\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.739\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.735\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.759\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.742\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.757\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"405328\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\n\nCan we get derived (141, etc) with varlist\nIt looks like it’s really just 140 (cumecs) and 141 (ML/d) we’d want, if Vic matches QLD. There are a couple other varcons, but they’re about groundwater.\nLet’s see what get_varcon gives us. I can’t get this not to error, and the examples online have square brackets mixed in the json. I think some combo of c and list might do it but not worth it.\n\nvc_params <- list(\"function\" = 'get_varcon',\n               \"version\" = \"2\",\n               \"params\" = list(\"varcons\" = list(\"varfrom\" = \"100\",\n                               \"varto\" = \"141\",\n                               \"site_list\" = \"233217, 405328\",\n                               \"datasource\" = \"A\",\n                               \"requests\" = list(\"qf1\" = \"1\", \n                                                 \"t1\" = \"20200101000000\",\n                                                 \"t2\" = \"20200131000000\"))))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(vc_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 205\n\n{\"function\":\"get_varcon\",\"version\":\"2\",\"params\":{\"varcons\":{\"varfrom\":\"100\",\"varto\":\"141\",\"site_list\":\"233217, 405328\",\"datasource\":\"A\",\"requests\":{\"qf1\":\"1\",\"t1\":\"20200101000000\",\"t2\":\"20200131000000\"}}}}\n\nresp_vc <- req %>% \n  req_body_json(vc_params) %>% \n  req_perform()\n\nrbody_vc <- resp_vc %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_vc)\n\nList of 3\n $ error_num: int 124\n $ error_msg: chr \"Access violation at address 005203ED in module 'webservice.exe'. Read of address 00000008\"\n $ return   :List of 1\n  ..$ varcons: list()\n\n\nThis isn’t worth it. If we ask for 141 or 140, just do another round with varfrom and varto. Or always get 100, 140, 141, then only sometimes get the others if asked?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#geolocation",
    "href": "vicwater/vicwater_api_howtocall.html#geolocation",
    "title": "Vicwater api crude testing",
    "section": "Geolocation",
    "text": "Geolocation\nSo, get_db_info seems to have a way to get sites by radius or bounding box.. And the flipside is we might want to get sites within a polygon, and so need their locations, which should be available as geoJSON. Try to figure both those out.\nI think get_site_geojson is going to be simpler. start there. Not sure why the site_list can’t have a c(), but the fields has to use it. Works though, gives a feature list. I think those are readable by sf, so that’s good. Not sure what the fields even are though. The help says “Any field that is part of the site table”. So I guess we need to sort that out. On to get_db_info.\n\ng_j_params <- list(\"function\" = 'get_site_geojson',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\",\n                               \"get_elev\" = \"1\",\n                               \"fields\" = c(\"zone\",\"region\")))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(g_j_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 127\n\n{\"function\":\"get_site_geojson\",\"version\":\"2\",\"params\":{\"site_list\":\"233217, 405328\",\"get_elev\":\"1\",\"fields\":[\"zone\",\"region\"]}}\n\nresp_g_j <- req %>% \n  req_body_json(g_j_params) %>% \n  req_perform()\n\nrbody_g_j <- resp_g_j %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_g_j)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 2\n  ..$ features:List of 2\n  .. ..$ :List of 4\n  .. .. ..$ properties:List of 2\n  .. .. .. ..$ region: chr \"233\"\n  .. .. .. ..$ zone  : int 55\n  .. .. ..$ geometry  :List of 2\n  .. .. .. ..$ coordinates:List of 3\n  .. .. .. .. ..$ : num 144\n  .. .. .. .. ..$ : num -38.2\n  .. .. .. .. ..$ : int 0\n  .. .. .. ..$ type       : chr \"Point\"\n  .. .. ..$ id        : chr \"233217\"\n  .. .. ..$ type      : chr \"Feature\"\n  .. ..$ :List of 4\n  .. .. ..$ properties:List of 2\n  .. .. .. ..$ region: chr \"405\"\n  .. .. .. ..$ zone  : int 55\n  .. .. ..$ geometry  :List of 2\n  .. .. .. ..$ coordinates:List of 3\n  .. .. .. .. ..$ : num 146\n  .. .. .. .. ..$ : num -37.5\n  .. .. .. .. ..$ : int 0\n  .. .. .. ..$ type       : chr \"Point\"\n  .. .. ..$ id        : chr \"405328\"\n  .. .. ..$ type      : chr \"Feature\"\n  ..$ type    : chr \"FeatureCollection\""
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#db-info",
    "href": "vicwater/vicwater_api_howtocall.html#db-info",
    "title": "Vicwater api crude testing",
    "section": "DB info",
    "text": "DB info\nI was using get_db_info to test above, so let’s just go back to that as a start and think a bit more about what we want. look at the barown. Cannot feed it a list of sites for fitler_values. It does give lat/long/northing, etc, so could use this instead of geoJSON, but geoJSON probably better if we want geo. Using return_type = hash is not noticably different than return_type = array. All examples use hash, so I guess keep using that moving forward. I think we can filter on lots of things in this list, both here and in the geojson.\n\ndbparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"station\" = \"233217\")))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(dbparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 130\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"station\":\"233217\"}}}\n\nrespdb <- req %>% \n  req_body_json(dbparams) %>% \n  req_perform()\n\nrbodydb <- respdb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodydb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 1\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5772691.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON @ GEELONG\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"0\"\n  .. .. ..$ category4 : chr \"150\"\n  .. .. ..$ category3 : chr \"SEALED\"\n  .. .. ..$ category2 : chr \"V_93G4\"\n  .. .. ..$ category1 : chr \"0\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19601118\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"BW\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. ..$ comment   : chr \"\\r\\n\\r\\n\\r\\nBarwon Water flood monitoring stationFrom the intersection of the Fyans St and La Trobe Terrace, he\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233217\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Geelong\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"267562.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n\n\n\nGeofiltering the db to select sites\nOK, so there are lots of ways to filter (sitename, date, name, etc). Some of those like Name or region or active might be useful, but for now let’s try the geo filters (boudning box and radius).\nTry circle (lat, long, radius in degrees)- use the Barwon lat/long.\nKeeps crashing with timeouts. is it just too much to ask for? Or is the json not right?\n\n# dbparams <- list(\"function\" = 'get_db_info',\n#                \"version\" = \"3\",\n#                \"params\" = list(\"table_name\" = \"site\",\n#                                \"return_type\" = \"hash\",\n#                                \"geo_filter\" = list(\"circle\" = c(\"-38.16\", \"144.35\", \"0.25\"))))\n# \n# req <- request(vicurl)\n# \n# req %>% \n#   req_body_json(dbparams) %>% \n#   req_dry_run()\n# \n# respdb <- req %>% \n#   req_body_json(dbparams) %>% \n#   req_perform()\n# \n# rbodydb <- respdb %>% resp_body_json(check_type = FALSE)\n# \n# str(rbodydb)\n\nLet’s try one of the other geofilters. Otherwise this will work better to write my own if I can et the geojson of all the sites.\nUgh. the rectangle (and region) need nested square brackets. I can make one with c(),\n\ndbparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"hash\",\n                               \"geo_filter\" = list(\"rectangle\" = \n                                                     c(c(\"-38.126\", \"144.282\"),\n                                                       c(\"-38.223\", \"144.406\")))))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(dbparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 161\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"geo_filter\":{\"rectangle\":[\"-38.126\",\"144.282\",\"-38.223\",\"144.406\"]}}}\n\n# \n# respdb <- req %>% \n#   req_body_json(dbparams) %>% \n#   req_perform()\n# \n# rbodydb <- respdb %>% resp_body_json(check_type = FALSE)\n# \n# str(rbodydb)\n\nthe locations of all guages it’d be\nOK, generating the json for these geo selections is horrible. If I can pull faster to do it myself.\nCan I get a complete gaugelist, nad then pull geojson?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#get-all-gauges",
    "href": "vicwater/vicwater_api_howtocall.html#get-all-gauges",
    "title": "Vicwater api crude testing",
    "section": "Get all gauges",
    "text": "Get all gauges\nWhat’s the best way? with get_db_info? With get_sites_by_datasource? The latter would assume we know all datasources. We probably do just want those in ‘A’ but not positive.\nSo, how about db_info, but maybe not all columns? Tempted to get lat/long or easting/northing.\nTakes a while, but it does run. 189,464 sites??? Yikes. WHY? Clearly i need to filter on something.\n\ndbparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"field_list\" = c(\"station\", \"stname\", \"shortname\")))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(dbparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 139\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"field_list\":[\"station\",\"stname\",\"shortname\"]}}\n\nrespdb <- req %>% \n  req_body_json(dbparams) %>% \n  req_perform()\n\nrbodydb <- respdb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodydb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 189491\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"0\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"044079\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"044387\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045407\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045621\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045652\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045717\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"092825\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"097421\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"097962\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100000\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100001\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100002\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100003\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100004\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100005\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100006\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100007\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100008\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100009\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100010\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100011\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100012\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100013\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100014\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100015\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100016\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100017\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100018\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100019\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100020\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100021\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100022\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100023\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100024\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100025\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100026\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100028\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100029\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100030\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100031\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100032\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100033\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100034\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100035\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100036\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100037\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100038\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100039\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100040\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100041\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100042\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100043\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100044\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100045\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100046\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100047\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100048\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100049\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100050\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100051\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100054\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100055\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100056\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100057\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100058\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100059\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100060\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100061\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100062\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100063\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100064\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100065\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100066\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100067\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100068\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100069\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100070\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100071\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100072\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100073\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100074\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100075\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100076\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100077\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100078\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100079\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100080\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100081\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100082\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100083\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100084\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100085\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100086\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100087\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100088\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100089\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100090\"\n  .. .. [list output truncated]\n\n\nwhat are the variables at some of those sites? Can we figure out what’s up that way? I have a feeling some are groundewater, but there’s no obvious field for that.\n\nv_s_params <- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"100089, 100079\",\n                               \"datasource\" = \"A\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(v_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 103\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"100089, 100079\",\"datasource\":\"A\"}}\n\nresp_v_s <- req %>% \n  req_body_json(v_s_params) %>% \n  req_perform()\n\nrbody_v_s <- resp_v_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 2\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"\"\n  .. .. .. ..$ name      : chr \"\"\n  .. .. ..$ variables   : list()\n  .. .. ..$ site        : chr \"100089\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"\"\n  .. .. .. ..$ name      : chr \"\"\n  .. .. ..$ variables   : list()\n  .. .. ..$ site        : chr \"100079\"\n\n\nUhhh, those have no variables? WHat’s going on here?"
  },
  {
    "objectID": "vicwater/vicwater_testing.html",
    "href": "vicwater/vicwater_testing.html",
    "title": "Testing VicWater API",
    "section": "",
    "text": "# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#access-victoria-water-data-through-api",
    "href": "vicwater/vicwater_testing.html#access-victoria-water-data-through-api",
    "title": "Testing VicWater API",
    "section": "Access Victoria water data through API",
    "text": "Access Victoria water data through API\nThis document is my testing and development of functions to include in the {vicwater} package. Basically, it’s where I interactively sorted through how to hit the API functions, the formats of the lists, and how to unpack the returned lists. It is a work in progress, since that package is under development.\nWe want to access victorian water data for a set of sites. That requires using the api at https://data.water.vic.gov.au/cgi/webservice.exe?[JSON_request] , but it’s poorly documented. I think I got it mostly figured out in a testing document, but there’s a lot of extra testing in there that needs to be skipped over and cleaned up. My plan is to make this a package, but it needs more development. I’m moving further testing here so I can get to the point a bit quicker.\nLibraries. Do I still need jsonlite now that I’ve moved ot httr2?\n\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.2.2\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.2.2\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#set-up-params",
    "href": "vicwater/vicwater_testing.html#set-up-params",
    "title": "Testing VicWater API",
    "section": "Set up params",
    "text": "Set up params\n\nURL\nThe base url that everything gets attached to is: and we use httr2::request to start building the request.\n\nvicurl <- \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\nreqvic <- request(vicurl)\n\n\n\nSite lists\nI want to test with one, two, and several sites in a site list. I had tried to do \"sitelist\" = c('site', 'site') , and that failed. But it works to have \"site, site\"\nThe upper steavenson is 405328, Barwon is 233217 (and has Temp), Taggerty 405331 only ran 2010-2013, And the Marysville golf course 405837 (only rainfall). That hits some things we want to make sure we pick up- no longer running gauges, gauges with only rain, gauges with lots of variables, etc.\nI only make one site_list here with multiple, but can do the str_c inside the calls usually.\n\nbarwon <- '233217'\nsteavenson <- '405328'\ntaggerty <- '405331'\ngolf <- '405837'\n\nallsites <- str_c(barwon, steavenson, taggerty, golf, sep = \", \")"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#api-functions",
    "href": "vicwater/vicwater_testing.html#api-functions",
    "title": "Testing VicWater API",
    "section": "API functions",
    "text": "API functions\nAnd how to call each- including multiple values.\nI finally found a couple sources of documentation that will hopefully be helpful: https://kisters.com.au/doco/hydllp.htm and https://water-monitoring.information.qld.gov.au/wini/Documents/RDMW_API_doco.pdf.\nThe first thing to do is to figure out what basic information is there, so we can ask for it. What we really want is get_ts_traces, but it has a lot of parameters (see Kisters docs). Some are relatively straightforward to meaning, though how to get them to be correct JSON can be tricky (e.g. site_list, while others are opaque, e.g. varfrom, varto, datasource, either to their meaning or what the options are we can ask for. We can try to figure that out with some querying of the other functions.\n\nDatasources\nCan we figure out what datasource means by asking for some by site?\nversion has to be 1.\n\nds_s_params <- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites))\n\nreqvic %>% \n  req_body_json(ds_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 108\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\"}}\n\nresp_ds_s <- reqvic %>% \n  req_body_json(ds_s_params) %>% \n  req_perform()\n\nrbody_ds_s <- resp_ds_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 4\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405328\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405331\"\n  .. .. ..$ datasources:List of 2\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405837\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\nI’ll need to sort out how to unpack that list later, but for now, let’s just look at it and see that all of them have options A and TELEM, and a couple have TELEMCOPY.\nAccording to the QLD pdf, the datasource distinguishes things like Archive and Telemetry. That’s similar in Vic, though QLD also had codes for back-filled holes, which don’t seem to be here (at least at these sites).\nSeems like it will be safest to ask for ‘A’ or ‘TELEM’.\nAnd the different variables can be in a var_list or varto and varfrom (though not always- see below). The numbers are for different variables, but again, no guarantee they’re the same in Vic.\n\nunpacking the list\nI might as well do this and build the function. Should be able to do it with unnest, and then maybe drop dumb columns? Nope, some of the lists unpack into lists of mixed type. But unnest_wider and unnest_longer might be the trick. Will need to test with single sites in case the structure changes.\nFor the function, we probably want to just print the error value or something, and not return it in the df. Or if it errors, return that, if it doesn’t, just give df. That’s probably best.\n\na <- as_tibble(rbody_ds_s[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # error and a `return` list\n  unnest_wider(col = where(is.list)) %>% # error, site, and a `datasources` list\n  unnest_longer(col = where(is.list)) # fully unpacked into a long df\na\n\n# A tibble: 11 × 2\n   site   datasources\n   <chr>  <chr>      \n 1 233217 A          \n 2 233217 TELEM      \n 3 233217 TELEMCOPY  \n 4 405328 A          \n 5 405328 TELEM      \n 6 405328 TELEMCOPY  \n 7 405331 A          \n 8 405331 TELEM      \n 9 405837 A          \n10 405837 TELEM      \n11 405837 TELEMCOPY  \n\n\nMight actually be better as a table or pivot_wider? Depends what the point is? Pivot wider is kind of a pain, use table? But table actually unpacks longer when I as_tibble or as.data.frame it. Which is annoying.\n\nb <- table(a$site, a$datasources)\nb\n\n        \n         A TELEM TELEMCOPY\n  233217 1     1         1\n  405328 1     1         1\n  405331 1     1         0\n  405837 1     1         1\n\n\nI think just return the long tibble, and do the table as a plot or explicitly a table or something. Which orientation makes most sense? Not sure. Probably gauges on x? But plots of actual data will have gauges on y, time on x, so maybe stay consistent.\n\nc <- b %>% \n  as_tibble(.name_repair = 'unique') %>% \n  rename(gauge = `...1`, datasource = `...2`)\n\nNew names:\n• `` -> `...1`\n• `` -> `...2`\n\nc %>% \n  mutate(n = as.logical(n)) %>% \nggplot2::ggplot(ggplot2::aes(x = datasource, y = gauge, fill = n)) + \n  ggplot2::geom_tile(colour=\"white\", size=0.25) +\n  ggplot2::scale_fill_discrete(type = c('firebrick', 'dodgerblue')) +\n  ggplot2::labs(fill = NULL) +\n  ggplot2::coord_equal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nCan I just build that plot from a? Yes, but would be just as annoying. maybe.\n\nallopts <- a %>%\n  expand(site, datasources)\n\na2 <- a %>% \n  mutate(indata = TRUE) %>% \n  dplyr::right_join(allopts) %>% \n  mutate(indata = ifelse(is.na(indata), FALSE, indata))\n\nJoining, by = c(\"site\", \"datasources\")\n\n  ggplot(a2, aes(x = datasources, y = site, fill = indata)) + \n  ggplot2::geom_tile(colour=\"white\", linewidth=0.25) +\n  ggplot2::scale_fill_discrete(type = c('firebrick', 'dodgerblue')) +\n  ggplot2::labs(fill = NULL) +\n  ggplot2::coord_equal()\n\n\n\n\nGood enough for this one- turn that into a function in a package.\n\n\n\nTest from the package\nI’m using devtools::load_all() to load the package repo in here for interactive testing and poking.\nObviously, hard paths are a bad idea, but I’m going to do it here since I typically do relative within repos, and these are across repos. Still, temporary and hacky.\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\n\n\nreturntib <- get_datasources_by_site(vicurl, allsites)\n\n\nreturntib\n\n# A tibble: 11 × 2\n   site   datasource\n   <chr>  <chr>     \n 1 233217 A         \n 2 233217 TELEM     \n 3 233217 TELEMCOPY \n 4 405328 A         \n 5 405328 TELEM     \n 6 405328 TELEMCOPY \n 7 405331 A         \n 8 405331 TELEM     \n 9 405837 A         \n10 405837 TELEM     \n11 405837 TELEMCOPY \n\nplot_datasources_by_site(returntib)\n\nJoining, by = c(\"site\", \"datasource\")\n\n\n\n\n\n\n\nSites by datasource\nHaven’t written this one before, might blow things up. But if we want a list of sites, it might be better than the way I did this before of just asking for everything in the db, lots of which had no data.\nAnd now we know the datasource options. I think? I suppose it’s possible there’s another type I’m not aware of.\nFor some weird reason, sitelists should be \"site, site, site\", while the datasources should be c('source', 'source'). The latter yields JSON array ['source', 'source'], while the former yields JSON 'site', 'site'\nThis works. The list truncates, but it did work.\n\nds_wanted <- c('A', 'TELEM')\ns_ds_params <- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = ds_wanted))\n\nreqvic %>% \n  req_body_json(s_ds_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}\n\nresp_s_ds <- reqvic %>% \n  req_body_json(s_ds_params) %>% \n  req_perform()\n\nrbody_s_ds <- resp_s_ds %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_s_ds)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ datasources:List of 2\n  .. ..$ :List of 2\n  .. .. ..$ datasource: chr \"A\"\n  .. .. ..$ sites     :List of 4044\n  .. .. .. ..$ : chr \"100023\"\n  .. .. .. ..$ : chr \"100500\"\n  .. .. .. ..$ : chr \"100503\"\n  .. .. .. ..$ : chr \"100504\"\n  .. .. .. ..$ : chr \"101708\"\n  .. .. .. ..$ : chr \"102621\"\n  .. .. .. ..$ : chr \"102827\"\n  .. .. .. ..$ : chr \"102828\"\n  .. .. .. ..$ : chr \"102829\"\n  .. .. .. ..$ : chr \"102830\"\n  .. .. .. ..$ : chr \"102831\"\n  .. .. .. ..$ : chr \"103811\"\n  .. .. .. ..$ : chr \"104929\"\n  .. .. .. ..$ : chr \"104930\"\n  .. .. .. ..$ : chr \"105134\"\n  .. .. .. ..$ : chr \"105222\"\n  .. .. .. ..$ : chr \"105287\"\n  .. .. .. ..$ : chr \"105317\"\n  .. .. .. ..$ : chr \"105480\"\n  .. .. .. ..$ : chr \"105484\"\n  .. .. .. ..$ : chr \"105936\"\n  .. .. .. ..$ : chr \"107631\"\n  .. .. .. ..$ : chr \"107970\"\n  .. .. .. ..$ : chr \"107971\"\n  .. .. .. ..$ : chr \"108201\"\n  .. .. .. ..$ : chr \"108203\"\n  .. .. .. ..$ : chr \"108319\"\n  .. .. .. ..$ : chr \"108320\"\n  .. .. .. ..$ : chr \"108321\"\n  .. .. .. ..$ : chr \"108898\"\n  .. .. .. ..$ : chr \"108899\"\n  .. .. .. ..$ : chr \"108917\"\n  .. .. .. ..$ : chr \"108944\"\n  .. .. .. ..$ : chr \"109133\"\n  .. .. .. ..$ : chr \"109461\"\n  .. .. .. ..$ : chr \"109462\"\n  .. .. .. ..$ : chr \"109644\"\n  .. .. .. ..$ : chr \"109645\"\n  .. .. .. ..$ : chr \"109769\"\n  .. .. .. ..$ : chr \"109770\"\n  .. .. .. ..$ : chr \"109778\"\n  .. .. .. ..$ : chr \"109779\"\n  .. .. .. ..$ : chr \"110151\"\n  .. .. .. ..$ : chr \"110152\"\n  .. .. .. ..$ : chr \"110153\"\n  .. .. .. ..$ : chr \"110171\"\n  .. .. .. ..$ : chr \"110186\"\n  .. .. .. ..$ : chr \"110464\"\n  .. .. .. ..$ : chr \"110721\"\n  .. .. .. ..$ : chr \"110724\"\n  .. .. .. ..$ : chr \"110731\"\n  .. .. .. ..$ : chr \"110739\"\n  .. .. .. ..$ : chr \"110745\"\n  .. .. .. ..$ : chr \"110943\"\n  .. .. .. ..$ : chr \"110978\"\n  .. .. .. ..$ : chr \"111543\"\n  .. .. .. ..$ : chr \"111549\"\n  .. .. .. ..$ : chr \"111551\"\n  .. .. .. ..$ : chr \"111691\"\n  .. .. .. ..$ : chr \"111692\"\n  .. .. .. ..$ : chr \"112182\"\n  .. .. .. ..$ : chr \"112235\"\n  .. .. .. ..$ : chr \"112236\"\n  .. .. .. ..$ : chr \"112237\"\n  .. .. .. ..$ : chr \"112459\"\n  .. .. .. ..$ : chr \"112708\"\n  .. .. .. ..$ : chr \"112803\"\n  .. .. .. ..$ : chr \"112804\"\n  .. .. .. ..$ : chr \"113004\"\n  .. .. .. ..$ : chr \"113124\"\n  .. .. .. ..$ : chr \"113125\"\n  .. .. .. ..$ : chr \"113467\"\n  .. .. .. ..$ : chr \"113694\"\n  .. .. .. ..$ : chr \"113695\"\n  .. .. .. ..$ : chr \"113705\"\n  .. .. .. ..$ : chr \"113706\"\n  .. .. .. ..$ : chr \"114158\"\n  .. .. .. ..$ : chr \"114169\"\n  .. .. .. ..$ : chr \"115732\"\n  .. .. .. ..$ : chr \"115872\"\n  .. .. .. ..$ : chr \"116382\"\n  .. .. .. ..$ : chr \"116459\"\n  .. .. .. ..$ : chr \"116460\"\n  .. .. .. ..$ : chr \"116802\"\n  .. .. .. ..$ : chr \"116803\"\n  .. .. .. ..$ : chr \"119329\"\n  .. .. .. ..$ : chr \"119330\"\n  .. .. .. ..$ : chr \"119337\"\n  .. .. .. ..$ : chr \"119338\"\n  .. .. .. ..$ : chr \"119339\"\n  .. .. .. ..$ : chr \"119340\"\n  .. .. .. ..$ : chr \"119341\"\n  .. .. .. ..$ : chr \"119342\"\n  .. .. .. ..$ : chr \"119347\"\n  .. .. .. ..$ : chr \"119348\"\n  .. .. .. ..$ : chr \"119366\"\n  .. .. .. ..$ : chr \"119367\"\n  .. .. .. ..$ : chr \"119377\"\n  .. .. .. ..$ : chr \"120248\"\n  .. .. .. .. [list output truncated]\n  .. ..$ :List of 2\n  .. .. ..$ datasource: chr \"TELEM\"\n  .. .. ..$ sites     :List of 2093\n  .. .. .. ..$ : chr \"100023\"\n  .. .. .. ..$ : chr \"100500\"\n  .. .. .. ..$ : chr \"100503\"\n  .. .. .. ..$ : chr \"100504\"\n  .. .. .. ..$ : chr \"100731\"\n  .. .. .. ..$ : chr \"101708\"\n  .. .. .. ..$ : chr \"102621\"\n  .. .. .. ..$ : chr \"102827\"\n  .. .. .. ..$ : chr \"102828\"\n  .. .. .. ..$ : chr \"102829\"\n  .. .. .. ..$ : chr \"102830\"\n  .. .. .. ..$ : chr \"102831\"\n  .. .. .. ..$ : chr \"103811\"\n  .. .. .. ..$ : chr \"104929\"\n  .. .. .. ..$ : chr \"104930\"\n  .. .. .. ..$ : chr \"105134\"\n  .. .. .. ..$ : chr \"105222\"\n  .. .. .. ..$ : chr \"105484\"\n  .. .. .. ..$ : chr \"105936\"\n  .. .. .. ..$ : chr \"107631\"\n  .. .. .. ..$ : chr \"107970\"\n  .. .. .. ..$ : chr \"108201\"\n  .. .. .. ..$ : chr \"108203\"\n  .. .. .. ..$ : chr \"108319\"\n  .. .. .. ..$ : chr \"108320\"\n  .. .. .. ..$ : chr \"108321\"\n  .. .. .. ..$ : chr \"108944\"\n  .. .. .. ..$ : chr \"109133\"\n  .. .. .. ..$ : chr \"109462\"\n  .. .. .. ..$ : chr \"109644\"\n  .. .. .. ..$ : chr \"109645\"\n  .. .. .. ..$ : chr \"109769\"\n  .. .. .. ..$ : chr \"109770\"\n  .. .. .. ..$ : chr \"110151\"\n  .. .. .. ..$ : chr \"110152\"\n  .. .. .. ..$ : chr \"110153\"\n  .. .. .. ..$ : chr \"110171\"\n  .. .. .. ..$ : chr \"110186\"\n  .. .. .. ..$ : chr \"110464\"\n  .. .. .. ..$ : chr \"110721\"\n  .. .. .. ..$ : chr \"110724\"\n  .. .. .. ..$ : chr \"110731\"\n  .. .. .. ..$ : chr \"110739\"\n  .. .. .. ..$ : chr \"110745\"\n  .. .. .. ..$ : chr \"110943\"\n  .. .. .. ..$ : chr \"110978\"\n  .. .. .. ..$ : chr \"111543\"\n  .. .. .. ..$ : chr \"111549\"\n  .. .. .. ..$ : chr \"111551\"\n  .. .. .. ..$ : chr \"111691\"\n  .. .. .. ..$ : chr \"111692\"\n  .. .. .. ..$ : chr \"112182\"\n  .. .. .. ..$ : chr \"112185\"\n  .. .. .. ..$ : chr \"112235\"\n  .. .. .. ..$ : chr \"112236\"\n  .. .. .. ..$ : chr \"112237\"\n  .. .. .. ..$ : chr \"112459\"\n  .. .. .. ..$ : chr \"112708\"\n  .. .. .. ..$ : chr \"112803\"\n  .. .. .. ..$ : chr \"112804\"\n  .. .. .. ..$ : chr \"113004\"\n  .. .. .. ..$ : chr \"113124\"\n  .. .. .. ..$ : chr \"113125\"\n  .. .. .. ..$ : chr \"113467\"\n  .. .. .. ..$ : chr \"113694\"\n  .. .. .. ..$ : chr \"113695\"\n  .. .. .. ..$ : chr \"113705\"\n  .. .. .. ..$ : chr \"113706\"\n  .. .. .. ..$ : chr \"114158\"\n  .. .. .. ..$ : chr \"114169\"\n  .. .. .. ..$ : chr \"114975\"\n  .. .. .. ..$ : chr \"115732\"\n  .. .. .. ..$ : chr \"116382\"\n  .. .. .. ..$ : chr \"116802\"\n  .. .. .. ..$ : chr \"116803\"\n  .. .. .. ..$ : chr \"119329\"\n  .. .. .. ..$ : chr \"119337\"\n  .. .. .. ..$ : chr \"119340\"\n  .. .. .. ..$ : chr \"119341\"\n  .. .. .. ..$ : chr \"119342\"\n  .. .. .. ..$ : chr \"119347\"\n  .. .. .. ..$ : chr \"119348\"\n  .. .. .. ..$ : chr \"119366\"\n  .. .. .. ..$ : chr \"119367\"\n  .. .. .. ..$ : chr \"119377\"\n  .. .. .. ..$ : chr \"120248\"\n  .. .. .. ..$ : chr \"121019\"\n  .. .. .. ..$ : chr \"122152\"\n  .. .. .. ..$ : chr \"123140\"\n  .. .. .. ..$ : chr \"126975\"\n  .. .. .. ..$ : chr \"129744\"\n  .. .. .. ..$ : chr \"129746\"\n  .. .. .. ..$ : chr \"134949\"\n  .. .. .. ..$ : chr \"137194\"\n  .. .. .. ..$ : chr \"137195\"\n  .. .. .. ..$ : chr \"137197\"\n  .. .. .. ..$ : chr \"137198\"\n  .. .. .. ..$ : chr \"137199\"\n  .. .. .. ..$ : chr \"137200\"\n  .. .. .. .. [list output truncated]\n\n\nCan I tibble that up?\n\ns <- as_tibble(rbody_s_ds[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # sites, and a `datasource` list\n  unnest_longer(col = where(is.list)) # fully unpacked into a long df\ns\n\n# A tibble: 6,137 × 2\n   datasource sites \n   <chr>      <chr> \n 1 A          100023\n 2 A          100500\n 3 A          100503\n 4 A          100504\n 5 A          101708\n 6 A          102621\n 7 A          102827\n 8 A          102828\n 9 A          102829\n10 A          102830\n# … with 6,127 more rows\n\n\nWhat are the number of sites in each?\n\ns %>% group_by(datasource) %>% summarise(n = n())\n\n# A tibble: 2 × 2\n  datasource     n\n  <chr>      <int>\n1 A           4044\n2 TELEM       2093\n\n\nWay more in Archive. Are there any in Telem that aren’t in A?\n\ntsites <- s %>% \n  filter(datasource == 'TELEM') %>% \n  select(sites) %>% \n  pull()\n\nasites <- s %>% \n  filter(datasource == 'A') %>% \n  select(sites) %>% \n  pull()\n\nall(tsites %in% asites)\n\n[1] FALSE\n\nsum(!(tsites %in% asites))\n\n[1] 96\n\n\nsee if I can blow up ggplot. Oof the plurals\n\ns <- s %>% \n  rename(site = sites, datasources = datasource)\n\nUnreadable. Not surprisingly.\n\nplot_datasources_by_site(s) + coord_flip()\n\nyikes.\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\nsxd <- get_sites_by_datasource(datasources = c('A', 'TELEM'))\n\n\nsxd\n\n# A tibble: 6,172 × 2\n   datasource site  \n   <chr>      <chr> \n 1 A          100023\n 2 A          100500\n 3 A          100503\n 4 A          100504\n 5 A          101708\n 6 A          102621\n 7 A          102827\n 8 A          102828\n 9 A          102829\n10 A          102830\n# … with 6,162 more rows\n\n\nPlot still needs work.\n\nplot_datasources_by_site(sxd) + coord_flip()\n\nJoining, by = c(\"datasource\", \"site\")"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#variables",
    "href": "vicwater/vicwater_testing.html#variables",
    "title": "Testing VicWater API",
    "section": "Variables",
    "text": "Variables\nor do I go straight for get_ts_ and then back this back out? Or get_db_info?\nI think I’m going to want to use get_variable_list both in get_ts_traces and to generate a set of possible variables.\nI’d like to get all sites, then make a master list of datasources and variables. But I need to get this usable for get_ts_traces, I think.\n\nGet_variable_list\nFeeding this a c(datasource, datasource) makes it return only some of the results, but throws no errors. So do one at a time.\n\nv_s_params <- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites,\n                               \"datasource\" = \"A\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(v_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 119\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\",\"datasource\":\"A\"}}\n\nresp_v_s <- req %>% \n  req_body_json(v_s_params) %>% \n  req_perform()\n\nrbody_v_s <- resp_v_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 4\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ variables   :List of 6\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"19610306171500\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"210.00\"\n  .. .. .. .. ..$ units       : chr \"pH\"\n  .. .. .. .. ..$ name        : chr \"Acidity/Alkalinity (pH)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"215.00\"\n  .. .. .. .. ..$ units       : chr \"ppm\"\n  .. .. .. .. ..$ name        : chr \"Dissolved Oxygen (ppm)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"233217\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221202090000\"\n  .. .. .. .. ..$ period_start: chr \"20091119170800\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. ..$ site        : chr \"405328\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"TAGGERTY R LADY TALT\"\n  .. .. .. ..$ name      : chr \"TAGGERTY RV @ LADY TALBOT DRIVE NEAR MARYSVILLE\"\n  .. .. ..$ variables   :List of 4\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"405331\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"R.G. MARYSVILLE\"\n  .. .. .. ..$ name      : chr \"RAINGAUGE @ MARYSVILLE GOLF CLUB\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221202081730\"\n  .. .. .. .. ..$ period_start: chr \"20010621142700\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"10.00\"\n  .. .. .. .. ..$ units       : chr \"mm\"\n  .. .. .. .. ..$ name        : chr \"Rainfall (mm)\"\n  .. .. ..$ site        : chr \"405837\"\n\n\nUnpack that\n\ns <- as_tibble(rbody_v_s[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # sites, and a `datasource` list\n  unnest_wider(col = site_details) %>% # site details in new cols\n  unnest_longer(col = variables) %>% # one line per variable, details of variables in a list\n  rename(long_name = name) %>% # variables have names too, avoid conflicts\n  unnest_wider(col = variables) %>% # columns for each attribute of the variables\n  rename(var_name = name)\ns\n\n# A tibble: 12 × 10\n   timezone short_…¹ long_…² perio…³ perio…⁴ subdesc varia…⁵ units var_n…⁶ site \n   <chr>    <chr>    <chr>   <chr>   <chr>   <chr>   <chr>   <chr> <chr>   <chr>\n 1 10.0     BARWON … BARWON… 202211… 196103… Availa… 100.00  metr… Stream… 2332…\n 2 10.0     BARWON … BARWON… 202211… 201007… Availa… 210.00  pH    Acidit… 2332…\n 3 10.0     BARWON … BARWON… 202211… 201007… Availa… 215.00  ppm   Dissol… 2332…\n 4 10.0     BARWON … BARWON… 202211… 201007… Availa… 450.00  Degr… Water … 2332…\n 5 10.0     BARWON … BARWON… 202211… 201007… Availa… 810.00  NTU   Turbid… 2332…\n 6 10.0     BARWON … BARWON… 202211… 201007… Availa… 820.00  µS/c… Conduc… 2332…\n 7 10.0     STEAVEN… STEAVE… 202212… 200911… Availa… 100.00  metr… Stream… 4053…\n 8 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 100.00  metr… Stream… 4053…\n 9 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 450.00  Degr… Water … 4053…\n10 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 810.00  NTU   Turbid… 4053…\n11 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 820.00  µS/c… Conduc… 4053…\n12 10.0     R.G. MA… RAINGA… 202212… 200106… Availa… 10.00   mm    Rainfa… 4058…\n# … with abbreviated variable names ¹​short_name, ²​long_name, ³​period_end,\n#   ⁴​period_start, ⁵​variable, ⁶​var_name\n\n\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\nvl <- get_variable_list(site_list = allsites, datasource = 'A')\n\n\nvl\n\n# A tibble: 12 × 11\n   site   short_…¹ long_…² varia…³ units var_n…⁴ perio…⁵ perio…⁶ subdesc datas…⁷\n   <chr>  <chr>    <chr>   <chr>   <chr> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 233217 BARWON … BARWON… 100.00  metr… Stream… 196103… 202211… Availa… A      \n 2 233217 BARWON … BARWON… 210.00  pH    Acidit… 201007… 202211… Availa… A      \n 3 233217 BARWON … BARWON… 215.00  ppm   Dissol… 201007… 202211… Availa… A      \n 4 233217 BARWON … BARWON… 450.00  Degr… Water … 201007… 202211… Availa… A      \n 5 233217 BARWON … BARWON… 810.00  NTU   Turbid… 201007… 202211… Availa… A      \n 6 233217 BARWON … BARWON… 820.00  µS/c… Conduc… 201007… 202211… Availa… A      \n 7 405328 STEAVEN… STEAVE… 100.00  metr… Stream… 200911… 202212… Availa… A      \n 8 405331 TAGGERT… TAGGER… 100.00  metr… Stream… 201007… 201302… Availa… A      \n 9 405331 TAGGERT… TAGGER… 450.00  Degr… Water … 201007… 201302… Availa… A      \n10 405331 TAGGERT… TAGGER… 810.00  NTU   Turbid… 201007… 201302… Availa… A      \n11 405331 TAGGERT… TAGGER… 820.00  µS/c… Conduc… 201007… 201302… Availa… A      \n12 405837 R.G. MA… RAINGA… 10.00   mm    Rainfa… 200106… 202212… Availa… A      \n# … with 1 more variable: timezone <chr>, and abbreviated variable names\n#   ¹​short_name, ²​long_name, ³​variable, ⁴​var_name, ⁵​period_start, ⁶​period_end,\n#   ⁷​datasource\n\n\nTry with two datasources\n\nv2 <- get_variable_list(site_list = allsites, datasource = c('A', 'TELEM'))\n\n\nv2\n\n# A tibble: 27 × 11\n   site   short_…¹ long_…² varia…³ units var_n…⁴ perio…⁵ perio…⁶ subdesc datas…⁷\n   <chr>  <chr>    <chr>   <chr>   <chr> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 233217 BARWON … BARWON… 100.00  metr… Stream… 196103… 202211… Availa… A      \n 2 233217 BARWON … BARWON… 210.00  pH    Acidit… 201007… 202211… Availa… A      \n 3 233217 BARWON … BARWON… 215.00  ppm   Dissol… 201007… 202211… Availa… A      \n 4 233217 BARWON … BARWON… 450.00  Degr… Water … 201007… 202211… Availa… A      \n 5 233217 BARWON … BARWON… 810.00  NTU   Turbid… 201007… 202211… Availa… A      \n 6 233217 BARWON … BARWON… 820.00  µS/c… Conduc… 201007… 202211… Availa… A      \n 7 405328 STEAVEN… STEAVE… 100.00  metr… Stream… 200911… 202212… Availa… A      \n 8 405331 TAGGERT… TAGGER… 100.00  metr… Stream… 201007… 201302… Availa… A      \n 9 405331 TAGGERT… TAGGER… 450.00  Degr… Water … 201007… 201302… Availa… A      \n10 405331 TAGGERT… TAGGER… 810.00  NTU   Turbid… 201007… 201302… Availa… A      \n# … with 17 more rows, 1 more variable: timezone <chr>, and abbreviated\n#   variable names ¹​short_name, ²​long_name, ³​variable, ⁴​var_name,\n#   ⁵​period_start, ⁶​period_end, ⁷​datasource\n\n\nI think now go to get_ts_traces, and then go back and write some helpers that can call get_datasources and get_variable and geo-locate, and use them to allow passing things like variables = ‘all’"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#get-traces",
    "href": "vicwater/vicwater_testing.html#get-traces",
    "title": "Testing VicWater API",
    "section": "get traces",
    "text": "get traces\nThe basic format is this, will need to do some testing.\n\nb1params <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(b1params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 219\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb1 <- req %>% \n  req_body_json(b1params) %>% \n  req_perform()\n\nrbodyb1 <- respb1 %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb1)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nand with two params\n\nb2params <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(b2params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb2 <- req %>% \n  req_body_json(b2params) %>% \n  req_perform()\n\nrbodyb2 <- respb2 %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb2)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nTwo params, two sites (one site only has one var, but not an error, I hope)\n\nb22params <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = str_c(barwon, steavenson, sep = \", \"),\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(b22params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 231\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217, 405328\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb22 <- req %>% \n  req_body_json(b22params) %>% \n  req_perform()\n\nrbodyb22 <- respb22 %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb22)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 3\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ longitude : chr \"145.773503100\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. .. ..$ latitude  : chr \"-37.525797590\"\n  .. .. .. ..$ org_name  : chr \"Victorian Rural Water Corporation\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.738\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.736\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.734\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.755\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.739\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.735\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.759\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.742\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.757\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"405328\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nI wasn’t able to get discharge (140, 141) from a varlist. Double check\n\nbparamsd <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,140\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparamsd) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,140\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbd <- req %>% \n  req_body_json(bparamsd) %>% \n  req_perform()\n\nrbodybd <- respbd %>% resp_body_json(check_type = FALSE)\n\nstr(rbodybd)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nKisters has an example of asking for 140.01? Try that? No, just do the varfrom/varto method for discharge and stage.\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,140.01\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 226\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,140.01\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nvarfrom-varto check. Does it give us both, or do we have to ask for the from separately`?\n\nbparamsft <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"100\",\n                               \"varto\" = \"140\", \n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparamsft) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"100\",\"varto\":\"140\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbft <- req %>% \n  req_body_json(bparamsft) %>% \n  req_perform()\n\nrbodybft <- respbft %>% resp_body_json(check_type = FALSE)\n\nstr(rbodybft)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 150: chr \"Rating extrapolated above 1.5x maximum flow gauged.\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.18443\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.16332\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.12997\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.10202\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.08706\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07044\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.04782\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.02872\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07961\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.14628\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.11425\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.09740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07464\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.11269\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Discharge (m3/sec)\"\n  .. .. .. ..$ precision : chr \"0.000010\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"140.00\"\n  .. .. .. ..$ units     : chr \"cubic metres/second\"\n  .. .. .. ..$ name      : chr \"Stream Discharge (m3/s)\"\n\n\nThat looks like it might have only given us varto.\nWhat happens if we ask for a varto/from for a site that doesn’t have it? Earlier we saw that the varlist just skips, but does this?\n\nbparamse <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = golf,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparamse) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"405837\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbe <- req %>% \n  req_body_json(bparamse) %>% \n  req_perform()\n\nrbodybe <- respbe %>% resp_body_json(check_type = FALSE)\n\nstr(rbodybe)\n\nList of 2\n $ error_num: int 125\n $ error_msg: chr \"No data for specified variable in file\"\n\n\nOk, that errors. So I’ll need to capture and skip errors.\n\nunpack different formats\n(varlist with multiple, varfrom/to, etc). b1params (one param, one site), b2params (two params, varlist), b22params (two params, two sites- only one param at one site) bparamsft (varfrom/varto), bparamse (error),\nstart simple- grab errors. Should have been doing this all along.\n\ner1 <- rbodyb1[1]\nere <- rbodybe[1]\ner1\n\n$error_num\n[1] 0\n\nere\n\n$error_num\n[1] 125\n\n\nUnpack the single. Yeesh\nI’m ignoring quality codes for the moment. They’re a definition list, and so maybe should be unpacked separately, matched to varto (I think not varfrom), and then joined? But I want to see how they work with more complex structures first.\n\ns <- as_tibble(rbodyb1[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %>% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\nTry two variables. still works. still not sure why I need both varfrom and varto when they match.\n\ns <- as_tibble(rbodyb2[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %>% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\nTwo variables, two sites\n\ns <- as_tibble(rbodyb22[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %>% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\nAnd finally, the one where we do have a varto\n\ns <- as_tibble(rbodybft[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %>% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\nFinally, some new q values. and pretty clear we don’t need the varfroms.\nNeed to change the time column to dates\nDo we want to split up or return stacked or return wide? Make an option.\nDo we want to return NA days as NA or skip them?\n\n\nCleaning that up\nsafest is code x site x varto. though I think don’t need site, we’ll have expanded there by the time we get varto.\n\ns <- as_tibble(rbodyb22[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. \n  # and we can drop varfrom\n  select(-varfrom_details) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('variable_', .)), \n              c(short_name, name)) \n\n# break in here to get the quality codes to match\nqc <- s %>% \n  select(quality_codes, site, variable) %>% \n  unnest_longer(col = quality_codes) %>% \n  mutate(quality_codes_id = as.integer(quality_codes_id))\n\n# finish unpacking\ns <- s %>%\n  select(-quality_codes) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\n# clean up\ns <- s %>% \n  rename(value = v, time = t, quality_codes_id = q) %>% \n  mutate(time = lubridate::ymd_hms(time)) %>% \n  left_join(qc, by = c('quality_codes_id', 'site', 'variable')) %>% \n  mutate(across(c(longitude, latitude, value), as.numeric)) # leaving some others because they either are names (gauges, variable) or display better (precision)\n\nCan I make that wide? Works without using id_cols but messy because too many info cols. Would end up being better to cut and join the info back on. But then I lose the quality codes, because they apply to each variable differently. Just return like this for now with a warning.\n\nsw <- s %>% pivot_wider(names_from = variable, values_from = value, id_cols = c(time, site))\n\nWhat I could do though is break it up into a list, potentially by sites and/or variables.\n\nslist <- split(s, s$site)\nvlist <- split(s, s$variable)\nsvlist <- split(s, interaction(s$site, s$variable))\n\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\n\nOne site, one variable\n\nbs <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = '100', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nWarning: executing %dopar% sequentially: no parallel backend registered\n\n\nCan I pass decimals? it’s how they come out of get_variable_list\n\nbsdec <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = '100.00', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOne site, derived variables\n\nbsd <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = c('100', '140'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOnly derived\n\nbsod <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = '140', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nSome more variables, derived and not\n\nbsdv <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = c('100', '140', '210', '450'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nAnd multi-sites too- does it correctly collapse the vector?\n\nbsdvs <- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nAnd finally everything including rain at golf. Careful though- does mean make sense for that? Probably better as a sum? Tried that and threw an error but told me the options:\nMean/Max/Min/Start/End/First/Last/Tot/MaxMin/Point/Cum\nDefinitely need two calls if need two different values at least for now- total temp is nonsense.\n\nbsdvs <- get_ts_traces(site_list = allsites, \n                       datasource = 'A', \n                       var_list = c('10', '100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nand return a list\n\nbsdvsl <- get_ts_traces(site_list = allsites, \n                       datasource = 'A', \n                       var_list = c('10', '100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'sxvlist')"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#a-variable-and-time-aware-version",
    "href": "vicwater/vicwater_testing.html#a-variable-and-time-aware-version",
    "title": "Testing VicWater API",
    "section": "A variable and time-aware version",
    "text": "A variable and time-aware version\n\npossibles <- get_variable_list(site_list = allsites, datasource = 'A') %>% \n  dplyr::select(site, datasource, variable, period_start, period_end)\n\n\nposs140 <- possibles[possibles$variable == '100.00', ] \n\nposs141 <- poss140\nposs140$variable <- '140.00'\nposs141$variable <- '141.00'\n\npossibles <- bind_rows(possibles, poss140, poss141)\n\nall the tests above should run\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\n\nOne site, one variable\n\nbs <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '100', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nCan I pass decimals? it’s how they come out of get_variable_list\n\nbsdec <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '100.00', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOne site, derived variables\n\nbsd <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = c('100', '140', '141'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOnly derived\n\nbsod <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '140', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nSome more variables, derived and not\n\nbsdv <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = c('100', '140', '210', '450'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nAnd multi-sites too- does it correctly collapse the vector?\n\nbsdvs <- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nDo the ‘all’ settings work? Let’s bump to year so I don’t have so much data\n\nbsdvs <- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = \"all\", \n                       start_time = \"all\", \n                       end_time = \"all\", \n                       interval = 'year', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nWarning: `var_list = 'all'` is *very* dangerous, since it applies the same\n`data_type` to all variables, which is rarely appropriate. Check the variables\navailable for your sites and make sure you want to do this.\n\n\nCan I throw something wrong to interval to see if it tells me what it can do? Kisters says\nyear, month, day, hour, minute, second,\nperiod,\ndefault\n\nbiw <- get_ts_traces2(site_list = barwon, \n                       datasource = 'A', \n                       var_list = \"100\", \n                       start_time = \"20200101\", \n                       end_time = \"20211231\", \n                       interval = 'eon', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nerror is Invalid interval, must be YEAR, MONTH, DAY, HOUR, MINUTE or SECOND"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#benchmark",
    "href": "vicwater/vicwater_testing.html#benchmark",
    "title": "Testing VicWater API",
    "section": "Benchmark",
    "text": "Benchmark\nThis likely varies a lot depending on what I’m asking for. Should be done more systematically, and use microbenchmark.\nThey should be roughly the same for a single?\n\nsystem.time(b1 <- get_ts_traces(site_list = barwon, \n                       datasource = 'A', \n                       var_list = '100', \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.10    0.00    0.86 \n\nsystem.time(b2 <- get_ts_traces2(site_list = barwon, \n                       datasource = 'A', \n                       var_list = '100', \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.17    0.01    1.50 \n\n\ninteresting. so the second is faster locally, but higher network, I think.\n\nsystem.time(bsdvs1 <- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.16    0.02    1.71 \n\nsystem.time(bsdvs2 <- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.25    0.01    5.69 \n\n\nOof. That’s pretty bad. Can I speed it up? probably.\nHow about parallel?\n\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nWarning: package 'foreach' was built under R version 4.2.2\n\n\nLoading required package: future\n\n\nWarning: package 'future' was built under R version 4.2.2\n\nregisterDoFuture()\nplan(multisession)\n\nsystem.time(bsdvs1p <- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.04    0.00    2.81 \n\nsystem.time(bsdvs2p <- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.06    0.00    3.67"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html",
    "href": "RpyEnvs/py_r_dates.html",
    "title": "Complex passing py-R",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#general-problem",
    "href": "RpyEnvs/py_r_dates.html#general-problem",
    "title": "Complex passing py-R",
    "section": "General problem",
    "text": "General problem\nThe real issue here is when we bring a df into R from python and it has a list column with ‘environment’ in it. Then the conversion hasn’t really happened, and doing that conversion post-hoc has to convert every single environment, which is all rows. And that takes forever. It’s not necessarily dates (and as we can see below, sometimes they do work just fine). So, if that happens, rather than taking an eternity to purrr or otherwise go through the column to translate, just put it in something easier in py, bring it over, and put it back.\nI do wish I had a better idea about why it happened. I’m sure as it happens more I’ll start to figure it out."
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#a-demonstration-sort-of",
    "href": "RpyEnvs/py_r_dates.html#a-demonstration-sort-of",
    "title": "Complex passing py-R",
    "section": "A demonstration (sort of)",
    "text": "A demonstration (sort of)\nSay we have a pandas dataframe with a few columns of simple types (numeric, character) and 1000 rows\n\nimport pandas as pd\nimport random\nimport string\n\nrandnums = [random.gauss(0, 1) for _ in range(1000)]\nallchars = list(string.ascii_lowercase) + list(string.ascii_uppercase)\n\nrandchars = [random.choice(allchars) for _ in range(1000)]\n\nsimpledf = pd.DataFrame({'rand_nums': randnums, 'rand_chars': randchars}, columns=['rand_nums', 'rand_chars'], index=range(1000))\n\nsimpledf\n\n     rand_nums rand_chars\n0     1.577293          H\n1    -2.399360          U\n2    -0.232699          m\n3     0.482192          W\n4     0.384289          W\n..         ...        ...\n995   0.131491          q\n996   0.623685          p\n997   0.936023          w\n998  -0.472836          f\n999   0.020971          w\n\n[1000 rows x 2 columns]\n\n\nNow, we can get that into R without too much fuss using py$.\n\nrsimple <- py$simpledf\nhead(rsimple)\n\nNULL\n\n\nQuick and easy. I think py_to_r is supposed to do some of this, but I can never get it to work. I think maybe it would make more sense in a script where we’re moving back and forth than here where we have separate code chunks?"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#now-with-time",
    "href": "RpyEnvs/py_r_dates.html#now-with-time",
    "title": "Complex passing py-R",
    "section": "Now, with time",
    "text": "Now, with time\nLet’s add a column of dates to simpledf. First, create the dates, then add to simpledf.\n\nimport datetime\n\ndates = []\n\nd1 = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n\n# Because i starts at 0, the first loop is the start date\nfor i in range(1000):\n    # Add i days to the start date\n    day_new = d1 + datetime.timedelta(days=i)\n    # Append the current date string to the list of dates\n    dates.append(day_new)\n\ntimedf = simpledf.assign(date = dates)\ntimedf\n\n     rand_nums rand_chars       date\n0     1.577293          H 2000-01-01\n1    -2.399360          U 2000-01-02\n2    -0.232699          m 2000-01-03\n3     0.482192          W 2000-01-04\n4     0.384289          W 2000-01-05\n..         ...        ...        ...\n995   0.131491          q 2002-09-22\n996   0.623685          p 2002-09-23\n997   0.936023          w 2002-09-24\n998  -0.472836          f 2002-09-25\n999   0.020971          w 2002-09-26\n\n[1000 rows x 3 columns]\n\n\nNow, when we bring it into R, it just works? That’s not at all what happened to me when I had the original issue.\n\ntimedfR <- py$timedf\nhead(timedfR)\n\nNULL"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#for-future-reference",
    "href": "RpyEnvs/py_r_dates.html#for-future-reference",
    "title": "Complex passing py-R",
    "section": "For future reference",
    "text": "For future reference\nSo, I can’t seem to replicate the issue. Previously, the datetime col came in as a list-col into a tibble, and was wrapped in a python environment. It was possible to parse it with purrr (or lapply, but purrr was much faster (and weirdly, furrr was slower). Not running, because this isn’t in a python env, and so this doesn’t actually work.\n\ndemodates <- timedfR$date\nrdates <- purrr::map(demodates, py_to_r) %>%\n  tibble(.name_repair = ~'Date') %>%\n  unnest(cols = Date)\n\nBut what was much faster was to comvert the column to strings in python with\n\ntimedf['date_str'] = timedf['date'].astype(str)\ntimedf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   rand_nums   1000 non-null   float64       \n 1   rand_chars  1000 non-null   object        \n 2   date        1000 non-null   datetime64[ns]\n 3   date_str    1000 non-null   object        \ndtypes: datetime64[ns](1), float64(1), object(2)\nmemory usage: 31.4+ KB\n\n\nAnd then lubridate back to dates in R\n\ntimedfRstr <- py$timedf\ntimedfRstr <- dplyr::select(timedfRstr, -date) |> \n  dplyr::mutate(date = lubridate::ymd(date_str))\nstr(timedfRstr)\n\n'data.frame':   1000 obs. of  4 variables:\n $ rand_nums : num  0.226 -0.11 1.652 0.729 -1.55 ...\n $ rand_chars: chr  \"g\" \"h\" \"r\" \"K\" ...\n $ date_str  : chr  \"2000-01-01\" \"2000-01-02\" \"2000-01-03\" \"2000-01-04\" ...\n $ date      : Date, format: \"2000-01-01\" \"2000-01-02\" ...\n - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=1000, step=1)"
  },
  {
    "objectID": "setup/R in VS.html",
    "href": "setup/R in VS.html",
    "title": "R in VS code",
    "section": "",
    "text": "I typically use Rstudio, and am very used to it. But I need to use VScode for working on Azure, and am trying to sort that out. There are also idiosyncracies with using Azure, but I’ll try to hold those for somewhere else and just keep this about VS."
  },
  {
    "objectID": "setup/R in VS.html#the-basics",
    "href": "setup/R in VS.html#the-basics",
    "title": "R in VS code",
    "section": "The basics",
    "text": "The basics\nThe VS code documentation gives a pretty good overview of the basics- install VScode, install languageserver, and install the R extension. That gets us up and running. Though it is worth noting that if you tend to work in renv for everything, it’s probably better to install languageserver globally (ie in a non-renv-managed session).\nNow, supposedly that provides linting, debugging, code completion, help, etc. And the add-ons (radian and httpgd look good too in terms of nicer terminal and visualisations). The question now is, HOW do we actually use all that functionality. I’m so used to Rstudio, it’ll take some playing. I’ll try to write down here what I try and how to get it to work.\nI tried to install.packages('languageserver') globally, but it gets grumpy sometimes and can’t find it inside a renv- managed repo because the .libPaths don’t have wherever the global package directory is. It seems to have worked on Windows, but not Azure/Unix.\n\nRadian\nI tried installing radian as the terminal. Assuming I don’t want it to mess up the project-level python environments, I installed it globally through the git-bash in windows terminal. It works when I type radian into VS bash, but does not run when I try to actually run something from a .R file. In the command pallette -> settings -> extensions -> R, there’s an option for Rterm:Windows that says it can be the path to radian. I tried where radian in git-bash (which radian on unix), and it gave me two paths in py-env/shims. The one with the .bat works on windows (C:\\Users\\galen\\.pyenv\\pyenv-win\\shims\\radian.bat). On Unix, it’s just a standard usr/bin/…. I’ve turned radian back off, though, because it throws a really annoying amount of weird errors that don’t actually stop the code from running, and that don’t appear in the base R terminal. Things like ‘unexpected & in }’, when there are no ‘&’ symbols in the code (and it still completes (usually). I think it works in .R scripts, but not in quarto. It’s something about bracketed paste not working right in notebooks I think. Working on sorting that out.\n\n\nlinting\nThis is something that (weirdly, I think) isn’t included in Rstudio. It supposedly is in VScode, but I’m not seeing obvious signs of it.\nInteresting. I don’t know what changed, but it has suddenly started linting. Maybe I turned something on in the settings->extensions->R section?\nAnd now all the blue lines are super annoying. Would be nice to at least have a turn off for comments setting. Or a good way to wrap comments a la Rstudio ctrl-shift /.\nGuess I need to figure out how to step through lintr and fix issues."
  },
  {
    "objectID": "observable/trying_observable.html",
    "href": "observable/trying_observable.html",
    "title": "Trying observablejs chunks",
    "section": "",
    "text": "Quarto has the option of using Observable JS for code chunks. This gives the ability to add interactivity in-browser, without needing to do server-side calcs, as we do with Shiny. The catch is with Shiny, I can serve R objects (e.g. ggplots) that I’m familiar with. Using observable means I need to figure out how that system works. I’m also a bit unclear how much processing can happen. My understanding is that any actual processing that happens needs to happen in the ojs, not R chunks, so we can’t interactively change a setting in the observable chunk and have that kick off some R. Though I might be wrong.\nI have quite a few use cases in mind if I can get this to work- serving maps, as an interface to {vicwater}, some drone settings, playing with population models, etc."
  },
  {
    "objectID": "observable/trying_observable.html#r-setup",
    "href": "observable/trying_observable.html#r-setup",
    "title": "Trying observablejs chunks",
    "section": "R setup",
    "text": "R setup\n\nlibrary(ggplot2)\n\nI know the cool thing to do is {palmerpenguins}, but I’m just going to use {iris}.\nI have a feeling ojs is likely just as happy plotting vectors, but I’ll tend to have dataframes from analyses, so let’s stick with that.\nTo start, can we reproduce a simple ggplot?\n\nggplot(iris, aes(x = Sepal_Length, y = Sepal_Width, color = Species)) + geom_point()\n\n\n\n\nLet’s try that without reactivity to start."
  },
  {
    "objectID": "observable/trying_observable.html#data-to-ojs",
    "href": "observable/trying_observable.html#data-to-ojs",
    "title": "Trying observablejs chunks",
    "section": "Data to ojs",
    "text": "Data to ojs\nIt seems like Arquero makes a lot of sense, since it’s dplyr-like. But the example (and all other examples I can find) use it to read data in from an external file (csv, json, etc). That’s almost never what I’m going to want to do. So, how do I get a dataframe into Arquero? I’m guessing I can’t just grab it. The data sources documentation says we need to use ojs_define in R to make things available. Let’s see if we can do that and then make it an arquero object?\nNOTE I’ve not seen this mentioned anywhere, but ojs_define cannot be found in an interactive session- it’s only available on render. So interactively it throws “Error in ojs_define(iris = iris) : could not find function”ojs_define”“. AND, if caching is on for the quarto project, it won’t render because the ojs_defined object can’t be cached.\n\nojs_define(iris = iris)\n\nCan I see that in ojs? I thought .view made tables? Maybe not if we haven’t imported arquero? But I also thought order didn’t matter for ojs.\n\niris.view()\n\n\n\n\n\n\nAnyway, we can see it as an object (once we preview instead of render). I’m still grumpy about that.\n\niris\n\n\n\n\n\n\nI think we usually need to transpose according to various stackexchanges.\n\ntiris = transpose(iris)\ntiris\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, the arquero docs seem to suggest I might be able to use from to make it arquero?\n\nimport { aq, op } from '@uwdata/arquero'\nirtab = aq.from(iris)\n\nirtab.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat does look like it’s the wrong dims, let’s used the transposed.\n\ntirtab = aq.from(tiris)\ntirtab.view()"
  },
  {
    "objectID": "observable/ojs_penguins.html",
    "href": "observable/ojs_penguins.html",
    "title": "ojs_penguins",
    "section": "",
    "text": "I’m struggling to get ojs code chunks to output anything. So starting over just straight replicating the penguins.\n\ndata = FileAttachment(\"palmer-penguins.csv\").csv({ typed: true })\n\n\n\n\n\n\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltered = data.filter(function(penguin) {\n  return bill_length_min < penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\n\n\n\n\n\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  ))\n  .plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\npdata = FileAttachment(\"palmer-penguins.csv\").csv({typed: true})\n\nPlot.plot({\n  facet: {\n    data: pdata,\n    x: \"sex\",\n    y: \"species\",\n    marginRight: 80\n  },\n  marks: [\n    Plot.frame(),\n    Plot.rectY(pdata, \n      Plot.binX(\n        {y: \"count\"}, \n        {x: \"body_mass_g\", thresholds: 20, fill: \"species\"}\n      )\n    ),\n    Plot.tickX(pdata, \n      Plot.groupZ(\n        {x: \"median\"}, \n        {x: \"body_mass_g\",\n         z: d => d.sex + d.species,\n         stroke: \"#333\",\n         strokeWidth: 2\n        }\n      )\n    )\n  ]\n})"
  },
  {
    "objectID": "observable/ojs_penguins_fromR.html",
    "href": "observable/ojs_penguins_fromR.html",
    "title": "ojs_penguins_fromR",
    "section": "",
    "text": "ojs_define(data = palmerpenguins::penguins)\n\n\nviewof bill_length_min = Inputs.range(\n  [32, 50],\n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"],\n  { value: [\"Torgersen\", \"Biscoe\"],\n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltered = transpose(data).filter(function(penguin) {\n  return bill_length_min < penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\n\n\n\n\n\n\nPlot.rectY(filtered,\n  Plot.binX(\n    {y: \"count\"},\n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  ))\n  .plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)"
  },
  {
    "objectID": "observable/ojs_demo.html",
    "href": "observable/ojs_demo.html",
    "title": "ojs_demo",
    "section": "",
    "text": "This worked in a clean project. Does it work here?"
  },
  {
    "objectID": "observable/ojs_demo.html#trying",
    "href": "observable/ojs_demo.html#trying",
    "title": "ojs_demo",
    "section": "trying",
    "text": "trying\nhttps://github.com/quarto-dev/quarto-web/blob/main/docs/interactive/ojs/ojs-cells.qmd\nOJS code cells {ojs} behave a bit differently than cells in traditional notebooks, and have many options available to control their display and layout."
  },
  {
    "objectID": "observable/ojs_demo.html#cell-execution",
    "href": "observable/ojs_demo.html#cell-execution",
    "title": "ojs_demo",
    "section": "Cell Execution",
    "text": "Cell Execution\nA critical difference between OJS cell execution and traditional notebooks is that in OJS cells do not need to be defined in any particular order.\nBecause execution is fully reactive, the runtime will automatically execute cells in the correct order based on how they reference each other. This is more akin to a spreadsheet than a traditional notebook with linear cell execution.\nFor example, in this cell we reference a variable that is not yet defined (it’s defined immediately below):\n\nx + 5\n\n\n\n\n\n\n\nx = 10\n\n\n\n\n\n\nThis code works because the Observable runtime automatically determines the correct order of execution for the cells.\nThis doesn’t work because we don’t have the penguins csv\n```{ojs}\ndata = FileAttachment(\"palmer-penguins.csv\").csv({ typed: true })\n```\nIf I give it a hard path, does it work? “C:/Users/galen/Documents/clean_ojs_test/renv/library/R-4.2/x86_64-w64-mingw32/palmerpenguins/extdata/penguins.csv”\nNo, and I’m just going to skip\n```{ojs}\ndata = FileAttachment(\"C:/Users/galen/Documents/clean_ojs_test/renv/library/R-4.2/x86_64-w64-mingw32/palmerpenguins/extdata/penguins.csv\").csv({ typed: true })\n```\nUgh. Can I get it from R?\n\nojs_define(pdata = palmerpenguins::penguins)\n\n\npdata\n\n\n\n\n\n\nCan I plot? I think I have to transpose\n\nPlot.plot({\n  facet: {\n    data: transpose(pdata),\n    x: \"sex\",\n    y: \"species\",\n    marginRight: 80\n  },\n  marks: [\n    Plot.frame(),\n    Plot.rectY(transpose(pdata), \n      Plot.binX(\n        {y: \"count\"}, \n        {x: \"body_mass_g\", thresholds: 20, fill: \"species\"}\n      )\n    ),\n    Plot.tickX(transpose(pdata), \n      Plot.groupZ(\n        {x: \"median\"}, \n        {x: \"body_mass_g\",\n         z: d => d.sex + d.species,\n         stroke: \"#333\",\n         strokeWidth: 2\n        }\n      )\n    )\n  ]\n})\n\n\n\n\n\n\nSo, that worked fine in a clean project, but does NOT work here. Why? Is it possible that I need to click the render button to render the whole project, and not render just this doc at command line? That’s getting really expensive if I have to do full-project renders."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html",
    "href": "pix4d/read_pix4d_outputs.html",
    "title": "Pix4d outputs",
    "section": "",
    "text": "Trying to read in pix4d outputs- specifically point clouds, orthophotos, dsm, and dtm.\nI’m going to assume I need stars and sf.\nTypically the projdir would have a better name (likely similar/same as the project name)"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#to-do",
    "href": "pix4d/read_pix4d_outputs.html#to-do",
    "title": "Pix4d outputs",
    "section": "To do",
    "text": "To do\nmake a package. Have funs for xyz and funs for tifs. But call those within specific funs for each output (e.g. dsm and point cloud should have their own funs, and the DSM should allow returning a raster in addition to points, but pc shouldn’t). Similar for a standard set of plot returns"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#can-i-get-rayshader-to-work",
    "href": "pix4d/read_pix4d_outputs.html#can-i-get-rayshader-to-work",
    "title": "Pix4d outputs",
    "section": "Can I get rayshader to work?",
    "text": "Can I get rayshader to work?\nWould be cool to do height with z and color with hexcol to actually map the stream in 3d with photo overlay. Pix4d does it, but would be nice to do here too."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#why-does-the-point-cloud-not-include-the-water",
    "href": "pix4d/read_pix4d_outputs.html#why-does-the-point-cloud-not-include-the-water",
    "title": "Pix4d outputs",
    "section": "Why does the point cloud not include the water?",
    "text": "Why does the point cloud not include the water?\nDoes that make our lives easier in some ways?"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#where-is-this-stuff-in-space-is-it-in-the-right-place",
    "href": "pix4d/read_pix4d_outputs.html#where-is-this-stuff-in-space-is-it-in-the-right-place",
    "title": "Pix4d outputs",
    "section": "Where is this stuff in space? is it in the right place?",
    "text": "Where is this stuff in space? is it in the right place?"
  },
  {
    "objectID": "small_helpers/json_api_construction.html",
    "href": "small_helpers/json_api_construction.html",
    "title": "JSON API coding",
    "section": "",
    "text": "I’m working on an API that uses JSON in the body, but getting it to come out right with square brackets, curly brackets, commas, etc where they’re supposed to be has been trial and error. I’m going to put what I’ve figured out here. I’m using examples from the {vicwater} package, but the main point is to show how to get different sorts of output.\nUsing the httr2 examples with req_dry_run to see what the request looks like and check the formats.\n\nlibrary(httr2)\nreq <- request(\"http://httpbin.org/post\")\n\n\n\nTo pass simple one to one key-value pairs, wrapped in {} use a list.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\n\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 40\n\n{\"function\":\"get_db_info\",\"version\":\"3\"}\n\n\n\n\n\nTo pass nested key-value pairs, use nested lists\n\nparams <- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '123abc',\n                               \"datasource\" = \"A\"))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 95\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"123abc\",\"datasource\":\"A\"}}\n\n\n\n\n\nThese cannot be created with c(), because that does something else (square brackets- see below).\n\nparams <- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '233217, 405328, 405331'))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 100\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331\"}}\n\n\n\n\n\nTo get square brackets, we need a vector. So, typically c() the bits together in the call (or previously).\n\nparams <- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = c('A', 'TELEM')))\n\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}\n\n\n\n\n\nTo get patterns like [['a', 'b'],['c', 'd']], use a matrix (and I think maybe a df). Which makes sense if we think of that as a group of vectors. The pattern is [[row1], [row2], [row_n]].\n\ntopleft <- c('-35', '148')\nbottomright <- c('-36', '149')\n\nrectbox <- rbind(topleft, bottomright)\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"148\"],[\"-36\",\"149\"]]}}}\n\n\n\n\nGives some horrible combination of curly and square braces including column and row names.\n\nrectdf <- data.frame(rectbox)\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 208\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"X1\":\"-35\",\"X2\":\"148\",\"_row\":\"topleft\"},{\"X1\":\"-36\",\"X2\":\"149\",\"_row\":\"bottomright\"}]}}}\n\n\nTibbles aren’t really any different, but the names are a bit cleaner\n\nrectdf <- tibble::as_tibble(rectbox)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}\n\n\n\n\n\n\nThere are arguments to toJSON that alter how matrices and dfs get parsed. Matrices are by default row-wise, but we can change to cols (e.g. [['col1'], ['col2']] with matrix = 'columnmajor'.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %>%\n  req_body_json(params, matrix = 'columnmajor') %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"-36\"],[\"148\",\"149\"]]}}}\n\n\nSimilarly, we can alter how dfs work, which might actually be fairly useful in the way it handles named columns especially. The default (above) is dataframe = 'rows' , which is kind of a mess (or at least not how my brain parses what a dataframe means). But dataframe = 'columns' ends up with named vectors. I don’t currently need that, but it sure makes more sense.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %>%\n  req_body_json(params, dataframe = 'columns') %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 160\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":{\"V1\":[\"-35\",\"-36\"],\"V2\":[\"148\",\"149\"]}}}}\n\n\nUsing dataframe = 'values' is again a confusing list.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %>%\n  req_body_json(params, datafraem = 'values') %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}\n\n\n\n\n\nTo get square brackets around multiple sets of curlies, e.g. `[{‘key’: ‘value’}, {‘key2’: ‘value2’}], use a list of lists.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \n                               \"complex_filter\" = list(list('fieldname' = 'stntype', \n                                                       'value' = 'HYD'),\n                                                   list('combine' = 'OR',\n                                                        'fieldname' = 'stntype',\n                                                        'value' = 'VIR'))))\nreq %>%\n  req_body_json(params, datafraem = 'values') %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 203\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"complex_filter\":[{\"fieldname\":\"stntype\",\"value\":\"HYD\"},{\"combine\":\"OR\",\"fieldname\":\"stntype\",\"value\":\"VIR\"}]}}"
  },
  {
    "objectID": "small_helpers/quarto_notes.html",
    "href": "small_helpers/quarto_notes.html",
    "title": "Quarto notes",
    "section": "",
    "text": "If there’s not a Quarto project (which is not the same as an Rproject), i.e. the .qmds are standalone, then the above rmarkdown method works to set the root to the Rproject. But if there is a Quarto project (I’ve moved to almost always doing this), then we can set the Quarto root directory in the _quarto.yml for the project. There are actually two useful settings we can make there- a project-wide output directory, and what to use to execute. There are (to my knowledge) two options for execute-dir- execute-dir: file and execute-dir: project, which set the root for rendering as the file location or the project. It almost always makes most sense to use project, because then everything uses the same reference for relative paths. For this website, that yaml is\n\nproject:\n  type: website\n  output-dir: docs\n  execute-dir: project\n\n\nThe only exception that I’ve run into so far is a weird situation where I have a Quarto project with an Rproject in a subdirectory because I want the Quarto to have access to several different code projects. The catch is that if I use paths relative to the R project and ask Quarto to render, all the paths are wrong. They are also wrong if I Run or run the code cells interactively. If I add the Rmarkdown setup chunk above knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()), then Run and interactive stuff works, but when I try to quarto render, it can’t find the Rproject because it’s looking at the same level and up, and the Rproj is in a directory down.\nSo, the workaround I’ve come up with is to set execute-dir: file, so the dir is set to the file dir, which is inside the Rproject. Then, use an Rmarkdown-style setup chunk with knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) to reset the directory to the Rproject root dir.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nThen the code both Runs and renders, and we don’t have to keep track of paths relative to two different nested projects manually. This approach works if the internal projects are self-contained (and we often want them to be). But if we need to render something across several, we’ll need to do something different.\nOther options that might work but I haven’t tried are parameterised notebooks (with conditional params?) Or using freeze and virtual environments (see docs)\nWhat I really want is a way to set the execute-dir on a file-by-file basis in the header yaml, but that doesn’t seem to work."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#headers-in-vs-code-vs-rstudio",
    "href": "small_helpers/quarto_notes.html#headers-in-vs-code-vs-rstudio",
    "title": "Quarto notes",
    "section": "Headers in VS code vs Rstudio",
    "text": "Headers in VS code vs Rstudio\nI had a few notebooks that ran fine in Rstudio, but wouldn’t render in VS (or from command line). I got the very cryptic error YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key with a line and col number that didn’t seem to correspond to anything with YAML in the project- not the file header, not the _quarto.yml, not _quarto.yaml.local, and nothing particularly useful showed up on google.\nThe solution seems to be to add an explicit line for editor: visual in the file header, e.g.\ntitle: Test\nauthor: Galen Holt\nformat:\n  html:\n    df-print: paged\neditor: visual\nIt doesn’t seem to matter if the title and author are wrapped in double-quotes (which was one suggestion online).\nI think what’s happening is that Rstudio’s visual editor is adding some hidden formatting syntax that gets exposed and looks like YAML to VS and the quarto CLI."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#python-paths",
    "href": "small_helpers/quarto_notes.html#python-paths",
    "title": "Quarto notes",
    "section": "Python paths",
    "text": "Python paths\nIf we use {reticulate}, R wants to know where the python lives, and gets grumpy if it can’t find it. Sometimes it silently points somewhere we don’t want, and other times we see errors like\nError creating conda environment 'C:/Users/galen/Documents/Website/galen_website/renv/python/r-reticulate' [exit code 1]\n\nRprofile\nThe main solution is to edit .Rprofile to set the RETICULATE_PYTHON environment variable,\nSys.setenv(RETICULATE_PYTHON = '../werptoolkitpy/.venv/Scripts/python.exe')\nNote that this can either be a full path, or relative to the R project directory- in the situation above, I have the R project nested in a Quarto project that also contains a py project, so we need to go up and over to get to the .venv.\nThis is needed to get any of the reticulate code to work (though you can set it on a file-by-file basis.\nAdditional issues can come up with Quarto though.\n\n\nQuarto environment\nWhen we run Quarto inside Rstudio in parallel with an R project, everything works fine. But, if Quarto is run through VScode, for some reason it doesn’t hit the .Rprofile and so doesn’t run the line we just added, and we’re back to using the wrong python and errors about conda. To make it more complex, Quarto has its own python environment variable QUARTO_PYTHON. This is all more complicated when the Quarto project directory doesn’t match the R project directory.\nThe solution is to set up a _environment file in the Quarto project directory to set those variables according to the docs (and maybe PY_PYTHON too, just to be safe- I can’t find the docs to know how these differ). For my case, my _environment file looks like\nRETICULATE_PYTHON='../werptoolkitpy/.venv/Scripts/python.exe'\nQUARTO_PYTHON='werptoolkitpy/.venv/Scripts/python.exe'\nPY_PYTHON='werptoolkitpy/.venv/Scripts/python.exe'\nHere, again, I have Rproject nested in Quarto proj, but they access this file differently so the paths differ even though they point to the same place. RETICULATE_PYTHON needs to get up and out of the R subdir and over to the py side of things, while the Quarto project is the outer dir and so QUARTO_PYTHON can go straight in to the py subdir.\nAs far as I can tell, this does NOT supersede the necessity of setting .Rprofile, which is needed for the R code to run. This stuff in _environment is in addition so Quarto (especially in VS) can access the right info."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#table-printing",
    "href": "small_helpers/quarto_notes.html#table-printing",
    "title": "Quarto notes",
    "section": "Table printing",
    "text": "Table printing\n\nStyle\nUse the yaml header to declare style, one of paged, kable, tibble, and default as in documentation. In a twist, default seems to be the way to do some customisation using S3, see https://debruine.github.io/quarto_demo/table.html, though I haven’t played with that.\nSo, typically the yaml header would be something like\nformat:\n  html:\n    df-print: paged\nCan I change that for a single chunk? Work on that later. Putting it in as #| df-print: option isn’t recognized.\n\n\nRow number\nThe different df-print options have different defaults of how much they print (and with paged it doesn’t matter so much). As far as I can tell, tibble prints the whole thing, and kable prints 10 rows. Sometimes we want to control that though - maybe we have a df with 13 rows, and we want to just print the whole thing.\nThe default is\n\niris\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\n\nTo print more rows, we can use the rows.print option.\n\niris\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica"
  },
  {
    "objectID": "observable/trying_observable.html#issues",
    "href": "observable/trying_observable.html#issues",
    "title": "Trying observablejs chunks",
    "section": "ISSUES",
    "text": "ISSUES\n\nInteractive notebooks\nThe ojs chunks dont work in interactive mode, and throw errors like “Error in ojs_define(iris = iris) : could not find function”ojs_define”“. So to work on anything past the first ojs chunk requires rendering. But that brings us to the next issue:\n\n\nNo output\nObservable chunks don’t have output unless you use quarto preview, not just quarto render. And the ‘Render’ button in Rstudio renders and previews, making this more confusing. JUST RUNNING quarto render doc.qmd at the terminal yields a document with code chunks but no output. This is expected behavior, but is super counter intuitive, espcially given the Render button’s name.\nUnfortunately, there is no per-document quarto preview at the terminal like there is for quarto render. So if you’re working in a quarto project (website, book, etc), you have to preview the whole thing just to check a document.\nThat means you’ll almost certainly want to turn caching on for the project (probably do anyway if it’s big), but if caching is on for the quarto project, it won’t render because the ojs_defined object can’t be cached. So the chunk with ojs_define needs to have #| cache: false added to it. Or perhaps just turn caching off in the yaml headers for pages using ojs. Depends on how much pre-processing happens in R, probably.\n\n\nChunk options\nPython and R both use #| option: value for setting chunk options. ojs cells use //| option: value.\n\n\nColumn names\nObservable uses object.thing notation like python, but it also uses the . to reference columns in arquero. That means Sepal_Length is confusing, because it gets referenced as d.Sepal_Length. So change the names.\n\nnames(iris) <- stringr::str_replace_all(names(iris), '\\\\.', '_')\n\n\n\nCode changes\nI’m running into issues where I change some R code, and it works when I run it interactively, but then when I go to render, the new R code just doesn’t happen. E.g. I’ll add code that makes a dataframe with more values, and I can see them interactively in R, but they don’t appear in the render. I think it has something to do with the cache not resetting with changes, but I’m not positive."
  },
  {
    "objectID": "observable/trying_observable.html#plots",
    "href": "observable/trying_observable.html#plots",
    "title": "Trying observablejs chunks",
    "section": "Plots",
    "text": "Plots\nTo make a first plot, do something I’m pretty sure should work, stolen directly from the penguins example that starts with a dataframe and just modifying the name and removing a facet level.\n\nPlot.rectY(tiris,\n  Plot.binX(\n    {y: \"count\"},\n    {x: \"Sepal_Width\", fill: \"Species\", thresholds: 10}\n  ))\n  .plot({\n    facet: {\n      data: tiris,\n      y: \"Species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\nScatter\nNow, can we make a scatter?\n\nPlot.dot(tiris, {x: \"Sepal_Length\", y: \"Sepal_Width\", fill: \"Species\"}).plot()\n\n\n\n\n\n\nThere’s lots of cleanup we could do to make that look different, but let’s go with that for now.\nCan I make a line? It’ll be jumbled, but whatever. Maybe I can sort it at least with arquero.\nRemember to use the arquero version of the data- this barfs with tiris.\n\ntirtab\n  .orderby('Sepal_Length')\n  .view()\n\n\n\n\n\n\nNow, how to plot that? Does the chunk above order tirtab permanently? Doesn’t seem to\n\ntirtab.view()\n\n\n\n\n\n\n\n\nLine\nLet’s try the line with the orderby\n\nPlot.line(tirtab.orderby('Sepal_Length'), {x: \"Sepal_Length\", y: \"Sepal_Width\", fill: \"Species\"}).plot()\n\n\n\n\n\n\nThat seems to have worked, but it sure is goofy looking. Oh. Is it because i’m using fill? Use stroke (not color- this isn’t ggplot).\n\nPlot.line(tirtab.orderby('Sepal_Length'), {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}).plot()\n\n\n\n\n\n\nWould be good to not do the data processing inside the plot call.\nI assume that’s as easy as\n\nirorder = tirtab.orderby('Sepal_Length')\n\nPlot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}).plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoint and line\nNeed to figure this out. Can I just do both? I think the answer might be to use a Plot.plot with mutliple marks?\nFirst, how does that syntax work? This should just recreate the above, right?\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"})\n  ]\n})\n\n\n\n\n\n\ncan we just add more Plot.marktypes?\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nCool. Would be nice if there was a ggplot-esque way to use the same x,y,color and just change the marks. Maybe there is? Look for that later.\nMoving toward reactivity, let’s say I only want dots where Sepal_Length > 5 and < 6\nI don’t quite seem to know the filter syntax. Not entirely sure what the d=> means. Seems to be an internal reference to the data, but that feels weird and extra. I can get it to work, but doing anything complicated will require more thinking I think. Note that almost all the examples I can find use op.operation and so confused me for a bit thinking I needed op. The op access mathematical operations like abs, round, etc, and here I just need a simple ><.\n\nsl_filter = irorder\n  .filter(d => (d.Sepal_Length < 6 & d.Sepal_Length > 5))\n  \nsl_filter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow use that in a plot\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(sl_filter, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nThat seems to work"
  },
  {
    "objectID": "observable/trying_observable.html#reactivity",
    "href": "observable/trying_observable.html#reactivity",
    "title": "Trying observablejs chunks",
    "section": "Reactivity",
    "text": "Reactivity\nThe thing here is to use viewof and Inputs.typeofinput. But what are those types? The observable docs seem to have a good overview.\nLet’s replicate the above, but also allow selecting the species. Basically following the quarto docs, but with a couple modifications. There’s got to be a way to obtain the ranges, species names, etc in code and not hardcode them in.\n\nviewof min_sl = Inputs.range(\n  [4, 8],\n  {value: 5, step: 0.1, label: \"Min Sepal Length:\"}\n)\n\nviewof max_sl = Inputs.range(\n  [4, 8],\n  {value: 6, step: 0.1, label: \"Max Sepal Length:\"}\n)\n\nviewof sp = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI like the tabset thing they do in the help, but to keep it simple just make the plot.\nI’m going to filter the data in its own chunk to try to aid figuring this out.\nThe .params here is needed to use the reactive values, and then gets referenced as $, while the data is d.\n\nspfilter = irorder\n  .params({\n  spf: sp\n})\n  .filter((d, $) => op.includes($.spf, d.Species))\n  \nspfilter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsizefilter = spfilter\n  .params({\n  minsl: min_sl,\n  maxsl: max_sl\n})\n  .filter((d, $) => d.Sepal_Length > $.minsl && d.Sepal_Length < $.maxsl)\n  \nsizefilter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd plot\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(sizefilter, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nThat seems to work. Can I package it up pretty like in the example?"
  },
  {
    "objectID": "observable/trying_observable.html#making-better-ux",
    "href": "observable/trying_observable.html#making-better-ux",
    "title": "Trying observablejs chunks",
    "section": "Making better UX",
    "text": "Making better UX\nLet’s build that same thing, but at least kill off displaying code. We need to use different names here because ojs is reactive and so you can’t define variables in two places. Maybe I’ll just use petals instead of sepals.\n\nviewof min_pl = Inputs.range(\n  [1, 7],\n  {value: 5, step: 0.1, label: \"Min Petal Length:\"}\n)\n\nviewof max_pl = Inputs.range(\n  [1, 7],\n  {value: 6, step: 0.1, label: \"Max Petal Length:\"}\n)\n\nviewof sp2 = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [\"versicolor\"],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspfilter2 = irorder\n  .orderby('Petal_Length')\n  .params({\n  spf: sp2\n})\n  .filter((d, $) => op.includes($.spf, d.Species))\n\npetfilter = spfilter2\n  .params({\n  minpl: min_pl,\n  maxpl: max_pl\n})\n  .filter((d, $) => d.Petal_Length > $.minpl && d.Petal_Length < $.maxpl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter2, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\nFancy layouts\nSee the quarto layouts docs for help here, I’ll only try a couple things.\n\nTabset\nMake a tabset with the plot and data\n\nPlotData\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter2, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\npetfilter.view()\n\n\n\n\n\n\n\n\n\n\n\nSidebar panel\nNeed to rename the inputs to avoid double-naming\n\n\nviewof min_pl3 = Inputs.range(\n  [1, 7],\n  {value: 5, step: 0.1, label: \"Min Petal Length:\"}\n)\n\nviewof max_pl3 = Inputs.range(\n  [1, 7],\n  {value: 6, step: 0.1, label: \"Max Petal Length:\"}\n)\n\nviewof sp3 = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [\"versicolor\"],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter3, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter3, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\nspfilter3 = irorder\n  .orderby('Petal_Length')\n  .params({\n  spf: sp3\n})\n  .filter((d, $) => op.includes($.spf, d.Species))\n\npetfilter3 = spfilter3\n  .params({\n  minpl: min_pl3,\n  maxpl: max_pl3\n})\n  .filter((d, $) => d.Petal_Length > $.minpl && d.Petal_Length < $.maxpl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI think I’ll stop there. There’s lots more I could do to get better at using observable, and lots more I could do to integrate that with Quarto layouts, but my goal here was to figure out how to get it to work, and those other things will make more sense with specific use cases or their own quartos or something."
  },
  {
    "objectID": "drones/overlaps_reactive.html",
    "href": "drones/overlaps_reactive.html",
    "title": "Drone flight calculations",
    "section": "",
    "text": "This is an interactive page based on more detailed exploration of drone overlaps.\nThis gives us the opportunity to plug in flight parameters and back-calculate others. For example, give us the size of the photos on the ground given height, the needed drone flight distance for a desired overlap at a given height, or the speed we need to fly at a given height to yield the right overlap given the photo interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport { aq, op } from '@uwdata/arquero'\n\ndroned_aq = aq.from(transpose(droned))\ndronev_aq = aq.from(transpose(dronev))\ndronei_aq = aq.from(transpose(dronei))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof dronetype = Inputs.checkbox(\n  [\"Mini 2\", \"Phantom 4 Pro V2\"],\n  {value: [\"Mini 2\"],\n    label: \"Drone\"\n  }\n)\n\nviewof aspect = Inputs.checkbox(\n  [\"3:2\", \"4:3\", \"16:9\"],\n  {value: [\"4:3\"],\n    label: \"Aspect Ratio\"\n  }\n)\n\nviewof overlap_p = Inputs.range(\n  [0.7, 0.95],\n  {value: 0.8, step: 0.05, label: \"Overlap:\"}\n)\n\nviewof height = Inputs.range(\n  [1, 50],\n  {value: 5, step: 0.1, label: \"Altitude (m):\"}\n)\n\nviewof velocity = Inputs.range(\n  [0.1, 5],\n  {value: 1, step: 0.1, label: \"Velocity (m/s):\"}\n)\n\nviewof interval = Inputs.range(\n  [0.1, 5],\n  {value: 2, step: 0.1, label: \"Photo interval (s):\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the selected values\nGround distances (photo footprints) and needed photo distances to achieve overlap are\n\ndist_h\n  .select('altitude_m', 'overlap', 'direction', 'photo_distance_m', 'ground_distance_m')\n  .view()\n\n\n\n\n\n\nFor the selected interval and height, the needed velocity is\n\nvel_h\n  .select('altitude_m', 'overlap', 'intervals', 'velocity_ms')\n  .view()\n\n\n\n\n\n\nFor the selected velocity, the needed photo interval is\n\ni_h\n  .select('altitude_m', 'overlap', 'velocity_ms', 'photo_interval')\n  .view()\n\n\n\n\n\n\n\n\n\n\n\ndistfilter = droned_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect\n})\n  .filter((d, $) => op.includes(d.drone, $.dr))\n  .filter((d, $) => op.equal(d.overlap, $.ov))\n  .filter((d, $) => op.includes(d.aspect, $.asp))\n\ndist_h = distfilter\n  .params({\n  h: height\n})\n  .filter((d, $) => op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvfilter = dronev_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect,\n  inter: interval\n})\n  .filter((d, $) => op.includes(d.drone, $.dr))\n  .filter((d, $) => op.equal(d.overlap, $.ov))\n  .filter((d, $) => op.includes(d.aspect, $.asp))\n  .filter((d, $) => op.equal(d.intervals, $.inter))\n\nvel_h = vfilter\n  .params({\n  h: height\n})\n  .filter((d, $) => op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nifilter = dronei_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect,\n  v: velocity\n})\n  .filter((d, $) => op.includes(d.drone, $.dr))\n  .filter((d, $) => op.equal(d.overlap, $.ov))\n  .filter((d, $) => op.includes(d.aspect, $.asp))\n  .filter((d, $) => op.equal(d.velocity_ms, $.v))\n\ni_h = ifilter\n  .params({\n  h: height\n})\n  .filter((d, $) => op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGround distancePhoto distanceVelocityPhoto interval\n\n\n\nPlot.plot({\n  grid: false,\n  color: {\n    legend: true\n  },\n  marks: [\n    Plot.line(distfilter, {x: \"altitude_m\", y: \"ground_distance_m\", stroke: \"direction\"}),\n    Plot.dot(dist_h, {x: \"altitude_m\", y: \"ground_distance_m\", stroke: \"drone\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  color: {\n    legend: true\n  },\n  marks: [\n    Plot.line(distfilter, {x: \"altitude_m\", y: \"photo_distance_m\", stroke: \"direction\"}),\n    Plot.dot(dist_h, {x: \"altitude_m\", y: \"photo_distance_m\", stroke: \"drone\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(vfilter, {x: \"altitude_m\", y: \"velocity_ms\"}),\n    Plot.dot(vel_h, {x: \"altitude_m\", y: \"velocity_ms\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(ifilter, {x: \"altitude_m\", y: \"photo_interval\"}),\n    Plot.dot(i_h, {x: \"altitude_m\", y: \"photo_interval\"})\n  ]\n})"
  },
  {
    "objectID": "drones/overlaps.html",
    "href": "drones/overlaps.html",
    "title": "Overlaps",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "drones/overlaps.html#calculating-photo-separation-for-desired-overlaps",
    "href": "drones/overlaps.html#calculating-photo-separation-for-desired-overlaps",
    "title": "Overlaps",
    "section": "Calculating photo separation for desired overlaps",
    "text": "Calculating photo separation for desired overlaps\nThere are a lot of variables that go into this. My goal here is to start by making a plot of how this varies with height. And then sort out how to calculate photo frequency given speed (or vice versa). Though Litchi allows photos by distance interval, which is nice."
  },
  {
    "objectID": "drones/overlaps.html#gsd",
    "href": "drones/overlaps.html#gsd",
    "title": "Overlaps",
    "section": "GSD",
    "text": "GSD\nGround sampling distance. Lots of people determine heights to fly for a desired ground sampling distance (GSD) in cm/pixel. That’s useful to know, but we’re almost certainly always going to be flying low enough for really good GSD. Our main need here is that the overlap calculations rely on GSD. So, let’s find it. The equation is \\[GSD_w=\\frac{HS_w}{FI_w}\\]\nand\n\\[GSD_h=\\frac{HS_h}{FI_h}\\]\nwhere the subscripts h and w are the dimensions height (along path) and width (across path), H is flight altitude in meters, S is sensor width or height (ie physical size of the sensor in mm), F is focal length (true, in mm. not 35mm equivalent), and I is image width or height in pixels (be careful here- often the default photo settings are 16:9 but the sensor dims are given for 4:3, and the 16:9 crops to get there. Easiest to change to shoot in 4:3, and get better coverage to boot. This gives GSD in m/px. We typically want it in cm/px, so we adjust H in the calculations below.\n\nWhere to get those values?\n\nSensor size I\nDJI sensor sizes are given here https://www.djzphoto.com/blog/2018/12/5/dji-drone-quick-specs-amp-comparison-page. For the drones we have, the Mini 2 is 6.3w x 4.7h (called 1/2.3” sensor), and the Phantom 4 Pro V2 is 13.2w x 8.8h (called 1” sensor).\nParameterise for Mini 2\n\nsw <- 6.3\nsh <- 4.7\n\n\n\nImage size\nImage sizes depend on photo settings, and are given for the Mini 2 and Phantom 4 Pro V2 by DJI. You can also get them from the photo’s EXIF data (either in Properties, or more completely from https://jimpl.com/. It’s a good idea to check, because the set aspect ratio can change. Mini can be 16:9 or 4:3 (best), and Phantom can be 16:9, 4:3, or 3:2 (best).\nMini 2\n\n4:3: 4000×3000\n16:9: 4000×2250\n\nPhantom 4 Pro V2\n\n3:2 Aspect Ratio: 5472×3648\n4:3 Aspect Ratio: 4864×3648\n16:9 Aspect Ratio: 5472×3078\n\nParameterise for Mini 2 at 4:3\n\nIw <- 4000\nIh <- 3000\n\n\n\nFocal length\nI found the true focal length from the EXIF data, though it can be back-calculated from the 35mm format equivalent on the specs pages above.\nThe EXIF had Mini 2 at 4.5, Phantom at 8.8.\nTo back-calculate, the expression is \\(Focal*Scale=35mmequiv\\) where the scale factor is found in the EXIF, or is known and can be looked up for the sensor size. Both drones have a 24mm 35mm equivalent, but scale factors differ.\nParameterise for Mini 2\n\nFocal <- 4.5 \n\n\n\n\nCalculate GSD\nWe now have what we need to get the two GSDs. Let’s get one as a function of height, and another to solve for height given GSD\n\nget_gsd <- function(H_m, focal_mm, sensor_mm, image_px, h_units = 'm') {\n  # make cm\n  if (h_units == 'm') {H_m = H_m * 100} else if (h_units != 'cm') {stop(\"units not supported\")}\n  gsd <- (H_m * sensor_mm)/(focal_mm * image_px)\n}\n\nAnd typically, the recommendation is to use the maximum of the height and width GSDs, as that’s the worst resolution.\n\nworst_gsd <- function(H_m, focal_mm, sensor_w, sensor_h, image_w, image_h, h_units = 'm') {\n  gsd_w <- get_gsd(H_m, focal_mm, sensor_w, image_w)\n  gsd_h <- get_gsd(H_m, focal_mm, sensor_h, image_h)\n  \n  gsd <- pmax(gsd_w, gsd_h)\n}\n\nWhat is that for the mini 2 for a range of heights? I’ll look at all of them, even if that’s not typically what we’d do.\n\nheights <- seq(0, 100, by = 0.1)\nmini_gsd_w <- get_gsd(heights, Focal, sw, Iw)\nmini_gsd_h <- get_gsd(heights, Focal, sh, Ih)\n\nmini_gsd_worst <- worst_gsd(heights, Focal, sw, sh, Iw, Ih)\n\nPlot\n\nmini_gsd <- tibble(altitude_m = heights, width_gsd = mini_gsd_w, height_gsd = mini_gsd_w, worst_gsd = mini_gsd_worst)\n\n\nmini_gsd |> \n  tidyr::pivot_longer(cols = ends_with('gsd'), names_to = 'gsd_type', values_to = 'GSD_cmperpx') |> \nggplot(aes(x = altitude_m, y = GSD_cmperpx, color = gsd_type)) + geom_line()\n\n\n\n\nNot much difference there between h and w.\nDoes it match the Pix4d calculator?\nThat gives a GSD of 0.07 for a height of 2m, I get\n\nformat(mini_gsd_w[which(heights == 2)], scientific = FALSE)\n\n[1] \"0.07\""
  },
  {
    "objectID": "drones/overlaps.html#distance-and-overlap",
    "href": "drones/overlaps.html#distance-and-overlap",
    "title": "Overlaps",
    "section": "Distance and overlap",
    "text": "Distance and overlap\nWhat we really want is to get the flight distance we need to get a desired overlap. That will depend on height and GSD (from which we get the ground area covered per photo). And if we want photo timings, we need speed as well. Using the equations from pix4d, but rearranged so the order of operations makes sense to me.\nThe image size on the ground in meters (again using h and w for height (along path) and width (across path), given image height in px and GSD in cm/px is \\[D_h=(I_hGSD)/100\\]\nThen the flight distance needed for an overlap % O_p expressed in 0-1 is \\[d = D_h-O_pD_h = D_h(1-O_p)\\]\nThe pix4d then goes on to back that back out to the definition of D_h, \\[d=((I_hGSD)/100)(1-O_p)\\]\nIf we can’t set a travel distance d for the drone, we will need to adjust it’s velocity v in m/s and the photo interval t in seconds. In practice, well want to adjust them in tandem (and for a given height). To get the photo interval for a given velocity, it’s simply the desired distance divided by velocity, \\[t = d/v\\]\nand so the velocity for a given interval is\n\\[\nv=d/t\n\\]\nWe can obviously break this down into the equation for d, e.g. \\[t = D_h(1-O_p)/v\\].\n\nFunctions\n\nGiven GSD\nFirst, the ground distance in m\n\nground_dist <- function(image_px, gsd_cmpx) {\n  D <- image_px*gsd_cmpx/100\n}\n\nNext, the distance the drone should travel, given overlap\n\ndrone_dist <- function(ground_dist_m, overlap_prop) {\n  d <- ground_dist_m * (1-overlap_prop)\n}\n\nThe time interval, given velocity\n\nphoto_interval <- function(drone_dist_m, v_ms) {\n  t <- drone_dist_m/v_ms\n}\n\nThe velocity, given interval (not typical, but we might want it since we can only set intervals down to 2 seconds when hand-flying.\n\nvelocity <- function(drone_dist_m, p_s) {\n  v <- drone_dist_m/p_s\n}\n\n\n\nWrap those up\nDrone dist is just one level\n\ndrone_dist_from_gsd <- function(image_px, gsd_cmpx, overlap_prop) {\n  D <- ground_dist(image_px, gsd_cmpx)\n  dd <- drone_dist(D, overlap_prop)\n}\n\nPhoto interval and velocity need to depend on that\n\nphoto_interval_from_gsd <- function(image_px, gsd_cmpx, overlap_prop, v_ms) {\n  dd <- drone_dist_from_gsd(image_px, gsd_cmpx, overlap_prop)\n  t <- photo_interval(dd, v_ms)\n}\n\n\nvelocity_from_gsd <- function(image_px, gsd_cmpx, overlap_prop, p_s) {\n  dd <- drone_dist_from_gsd(image_px, gsd_cmpx, overlap_prop)\n  v <- velocity(dd, p_s)\n}"
  },
  {
    "objectID": "drones/overlaps.html#values-from-h-and-overlap",
    "href": "drones/overlaps.html#values-from-h-and-overlap",
    "title": "Overlaps",
    "section": "Values from H and overlap",
    "text": "Values from H and overlap\nDo this separately for h and w, I guess?\n\nDrone distance\nThis might be all we need for litchi, and regardless, it will tell us about how close flightpaths need to be.\n\ndrone_dist_H_o <- function(H_m, overlap_prop, \n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  gsd <- get_gsd(H_m, focal_mm, \n                   sensor_mm, \n                   image_px, \n                   h_units = 'm')\n  \n  gD <- ground_dist(image_px, gsd)\n  \n  dd <- drone_dist(gD, overlap_prop)\n  \n  return(dd)\n}\n\n\n\nVelocity and photo intervals\n\nv_H_o <- function(H_m, overlap_prop, p_s,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  dd <- drone_dist_H_o(H_m, overlap_prop,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm')\n  v <- dd/p_s # could use velocity(dd/ps), but not worth it here.\n  return(v)\n}\n\n\nt_H_o <- function(H_m, overlap_prop, v_ms,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  dd <- drone_dist_H_o(H_m, overlap_prop,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm')\n  t <- dd/v_ms # could use velocity(dd/ps), but not worth it here.\n  \n  return(t)\n}"
  },
  {
    "objectID": "drones/overlaps.html#plots",
    "href": "drones/overlaps.html#plots",
    "title": "Overlaps",
    "section": "Plots",
    "text": "Plots\n\nDrone distance\nHow does the drone distance depend on height and overlap?\nFor a fixed overlap of 80%, and the same heights sequences as above, for the h dimension (along flight path) and w (across),\n\ndd_h <- drone_dist_H_o(heights, 0.8,\n                       Focal, sh, Ih)\n\ndd_w <- drone_dist_H_o(heights, 0.8,\n                       Focal, sw, Iw)\n\nThose are fairly different, actually\n\ndd_hw <- tibble(altitude_m = heights, width_dd = dd_w, height_dd = dd_h) |> \n  tidyr::pivot_longer(cols = ends_with('dd'), names_to = 'dd_direction', values_to = 'drone_photo_distance')\n\n\nggplot(dd_hw, aes(x = altitude_m, y = drone_photo_distance, color = dd_direction)) + geom_line()\n\n\n\n\nCould use plotly here to have mouseover. Or use observable or shiny.\nWhat if we zoom in on the lower altitudes (< 10m)?\n\ndd_hw |> \n  dplyr::filter(altitude_m <= 10) |> \nggplot(aes(x = altitude_m, y = drone_photo_distance, color = dd_direction)) + geom_line()\n\n\n\n\nWe could make a heatmap with overlaps, but I’m not sure we really care that much? We’d really only be interested in maybe 75, 80, 85 or something, and this is for rule of thumb. Do that later.\nWhat are those at 2 and 4 m?\n\ndd_hw |> \n  dplyr::filter(altitude_m %in% c(2,4))\n\n# A tibble: 4 × 3\n  altitude_m dd_direction drone_photo_distance\n       <dbl> <chr>                       <dbl>\n1          2 width_dd                    0.56 \n2          2 height_dd                   0.418\n3          4 width_dd                    1.12 \n4          4 height_dd                   0.836\n\n\n\n\nVelocity from interval\nHere, let’s say we have a fixed overlap, and want to know the velocity we need to fly to get that at a given height and photo interval. This sounds contrived, but is pretty much exactly our situation when hand flying- the shortest interval we have is 2seconds, so how fast/slow do we need to fly to get 80% overlap at a range of heights?\nHere, we’ll focus on the h dimension. While we could fly sideways, we usually will fly with forward velocity.\nLet’s say 2 seconds, and then look at a heatmap.\n\nv_h <- v_H_o(heights, 0.8, 2,\n                       Focal, sh, Ih)\n\nv_tib <- tibble(altitude_m = heights, velocity_ms = v_h)\n\n\nggplot(v_tib, aes(x = altitude_m, y = velocity_ms)) + geom_line()\n\n\n\n\nAnd again zoom in\n\nv_tib |> \n  dplyr::filter(altitude_m <= 10) |> \nggplot(aes(x = altitude_m, y = velocity_ms)) + geom_line()\n\n\n\n\nSo, to get 80% overlap if we’re limited to intervals of 2 seconds, we’d need to fly at about 0.2m/s at 2m or 0.4 at 4m. Should do a tooltip kinda thing. But for now\n\nv_tib |> \n  dplyr::filter(altitude_m %in% c(2, 4)) \n\n# A tibble: 2 × 2\n  altitude_m velocity_ms\n       <dbl>       <dbl>\n1          2       0.209\n2          4       0.418\n\n\nHow about a heatmap of heights and intervals?\n\nintervals <- seq(0.1, 5, by = 0.1)\n\nvel_map <- tidyr::expand_grid(altitude_m = heights, photo_intervals = intervals) |> \n  mutate(velocity_ms = v_H_o(altitude_m, 0.8, photo_intervals,\n                       Focal, sh, Ih))\n\n\nvel_map |> \nggplot(aes(x = altitude_m, y = photo_intervals, fill = velocity_ms)) + geom_raster()\n\n\n\n\nThat’s a dumb scale. Obviously we can fly super fast at 100m and super fast photo intervals\n\nvel_map |> \n  dplyr::filter(altitude_m <= 10) |> \nggplot(aes(x = altitude_m, y = photo_intervals, fill = velocity_ms)) + geom_raster()\n\n\n\n\nStill not particularly useful. Moving on.\n\n\nInterval from velocity\nHow often do we need to take photos given a velocity and height? Let’s start by saying velocity of 1m/s (3.6km/h).\n\nt_h <- t_H_o(heights, 0.8, 1,\n                       Focal, sh, Ih)\n\nt_tib <- tibble(altitude_m = heights, photo_interval = t_h)\n\n\nt_tib |> \n  ggplot(aes(x = altitude_m, y = photo_interval)) + geom_line()\n\n\n\n\n\nt_tib |> \n  dplyr::filter(altitude_m <= 10) |> \n  ggplot(aes(x = altitude_m, y = photo_interval)) + geom_line()\n\n\n\n\nSo, at that velocity, the photo interval can’t be as long as 2 seconds without flying at 10m. Clearly that would change as we fly slower.\nMake a heatmap again, I guess, even if it wasn’t very useful before and we know this is just flipping axes and colors.\n\nspeeds <- seq(0.1, 5, by = 0.1)\n\ninterval_map <- tidyr::expand_grid(altitude_m = heights, velocity_m = speeds) |> \n  mutate(photo_interval = t_H_o(altitude_m, 0.8, velocity_m,\n                       Focal, sh, Ih))\n\n\ninterval_map |> \n  dplyr::filter(altitude_m <= 10) |> \nggplot(aes(x = altitude_m, y = velocity_m, fill = photo_interval)) + geom_raster()"
  },
  {
    "objectID": "drones/overlaps.html#next-steps",
    "href": "drones/overlaps.html#next-steps",
    "title": "Overlaps",
    "section": "Next steps",
    "text": "Next steps\nThe obvious thing to do here is to make an observable quarto where we can select drone type, desired overlap, things we want to set, and it makes the plot and returns values we want. For now though, these plots get us most of the way there.\nWhat do those distances look like for the phantom?"
  },
  {
    "objectID": "RpyEnvs/R_py_package.html",
    "href": "RpyEnvs/R_py_package.html",
    "title": "Wrapping python in R package",
    "section": "",
    "text": "I have a package that is mostly R, but needs to call some python functions. Those functions are themselves fairly lightweight wrappers of functions from another python package. So, the basic structure is that I have a package with the usual R package structure, and a python script in inst/python/pyscript.py that contains some function definitions.\nThere are two main issues I need to address here\nSolving these was surprisingly quick for dev, using devtools::load_all-"
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#works-with-load_all",
    "href": "RpyEnvs/R_py_package.html#works-with-load_all",
    "title": "Wrapping python in R package",
    "section": "Works with load_all",
    "text": "Works with load_all\nI began dev with the following setup, and it just worked with load_all\n\nEnvironment\nThe python environment I’m using is in a separate directory in the same repo, so I can point to it by going up and over with ../pydirectory\nIn .Rprofile, set Sys.setenv(RETICULATE_PYTHON = '../pydir/.venv/Scripts/python.exe')\nI also followed instructions from reticulate to handle dependencies in the .onLoad function and DESCRIPTION file, but am pretty sure that doesn’t come into play when I use load_all.\n\n\nExposing functions\nThe reticulate::source_python('pyfile.py') function makes functions in pyfile.py directly callable. So I naively created R/loadpys.R that contains only this line, since everything in R gets read on package load: reticulate::source_python(system.file(\"python/pyfile.py\", package = 'packagename')) .\n\n\nBreaking issues\nWhen I try to actually install the package (or check, build, etc), I first got errors about not being able to find the python environment at ../pydir/.venv/Scripts/python.exe .\nWhen the package is built, I do not have access to the python functions. Presumably this is because load_all puts everything into memory, including private functions, but actually installing only gets the exported ones."
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#fixing-environment-path",
    "href": "RpyEnvs/R_py_package.html#fixing-environment-path",
    "title": "Wrapping python in R package",
    "section": "Fixing environment path",
    "text": "Fixing environment path\nRunning devtools::check seems to have worked to just replace the relative path to the environment with a full path, e.g.\nSys.setenv(RETICULATE_PYTHON = 'C:/Users/USER/Documents/REPO_DIR/pydir/.venv/Scripts/python.exe').\nIt’s not clear to me how this will work for installing the package elsewhere (e.g. by a user, not from within the repo during dev). Obviously they’ll need their own python env, and so will need to set that up. Will come back to that once I can actually get the package built with the functions."
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#exporting-python-functions",
    "href": "RpyEnvs/R_py_package.html#exporting-python-functions",
    "title": "Wrapping python in R package",
    "section": "Exporting python functions",
    "text": "Exporting python functions\nThere are brief instructions at the reticulate vignettes for wrapping python modules. Presumably this would work, but it’s unclear how to call them, whether they would be exported, and whether they’d need to be called python-style with module.function, or just the bare names. It also conflicts with the advice in the vignette about package dependencies which seems to be more recent. It’s not completely clear what still applies- clearly the ways of specifying environments has changed, but has the way to access functions changed as well? And how do we make them available to a user (and not just internally)?\n\nWrite exported R wrappers\nIn loadpys.R, instead of just having reticulate::source_python(system.file(\"python/pyfile.py\", package = 'packagename')) call, write an R wrapper for the functions we want to export. NOTE that we have to put the source_python inside the R function (it does not work to have it at the head of the file).\npyfun_R <- function(...) {\n  reticulate::source_python(system.file(\"python/pyfile.py\", \n                                        package = 'packagename'))\n  pyfun(...)\n}\nThat seems to work if we make sure there’s a @export in the roxygen documentation. But it’s not exactly pretty.\nNote- it also works if we bring over the full arguments instead of dots, which would be better for documenting, probably. Guess it depends if we want to read R or python documentation.\nNote- the following error happens when the source_python isn’t in the function definition.\nError in `(function (command = NULL, args = character(), error_on_status = TRUE, …`:"
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#can-i-use-onload",
    "href": "RpyEnvs/R_py_package.html#can-i-use-onload",
    "title": "Wrapping python in R package",
    "section": "Can I use onLoad?",
    "text": "Can I use onLoad?\nThe instructions from reticulate describe bringing in a module. I think that is ultimately how we should do things, but unclear how difficult.\n\nBack to load_all\nTo test, let’s just see what we get when we devtools::load_all()\n\n\nImport module\nThe instructions above reference importing a module that is installed. We should be able to do that by making our python code a small package. But in the meantime, let’s use reticulate::import_from_path.\nUsing load_all, py_functions <<- reticulate::import_from_path(\"pyfile\", path = \"inst/python\", delay_load = TRUE) works. I get an object py_functions and can access the individual functions with py_functions$func1 etc. Note- it also works to pass the path to the python directory py_functions <<- reticulate::import_from_path(\"pyfile\", path = \"inst/python\", delay_load = TRUE).\n\n\nRunning\nThat runs after a load_all by calling pyfile$pyfun(args). But does it still need to be exported in an R wrapper to actually use as a package? YES- building the package can’t find the function.\nAnd if we do it this way, are the python functions just floating around in the user’s global environment? That would be bad. NO- they don’t seem to be.\nIt does seem to fail when I try to run the vignette in a different directory after devtools::install_github(). So that implies the method really isn’t working right. Why does it work for load_all?\n\n\nAnother issue\nThe approach above seems to work, I can devtools::install()` and run the package, using the exported R functions.\nThe catch is that devtools::check() hangs on building a vignette that uses those functions (even though I can run that same vignette after devtools::install().\nIs it that the python env gets called in one of them but not the other? setting RETICULATE_PYTHON in the vignette doesn’t help.\nDoes it actually not work, and I’m just carrying around info to make it work somewhere? Nope, it doesn’t work, after using devtools::install_local to install into a separate R session."
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#solution-for-now",
    "href": "RpyEnvs/R_py_package.html#solution-for-now",
    "title": "Wrapping python in R package",
    "section": "Solution (for now)",
    "text": "Solution (for now)\nUse reticulate::source_python, but with envir = globalenv(), which seems to accomplish what the <<- was doing with reticulate::import in .onLoad. And in fact we can do that in .onLoad. so my .onLoadnow looks like\n.onLoad <- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n\n  reticulate::source_python(system.file(\"python/controller_functions.py\", \n                                        package = 'werptoolkitr'), \n                            envir = globalenv())\n\n}\nThat doesn’t allow the user to library(package) and then set up their python- they’ll need to have python sorted ahead of time. Using the delay_load = TRUEin reticulate::importwould be better, but it doesn’t seem to work without a full py package.\n\nHow to test (relatively) quickly in a clean session-\n\nStart a new Rproject\nRun devtools::install_local(\"path/to/package\", force = TRUE) .\n\nWithout the force = TRUE this almost never rebuilds even with changes.\n\nThen library(packagename)\nSys.setenv(RETICULATE_PYTHON = 'path/to/venv') (otherwise it will try to install conda and barf)\n\nI would have thought I’d have to do this before the library, but it seems to work.\nBetter is to have a path to venv in .Rprofile, anyway\nNOTE if the .venv is in the outer directory, I’m getting weird errors when I try to Sys.setenv or otherwise set the path to the venv (it’s either prepending ~/virtualenvs or C::/ unless I pass a full fixed path. Seems to work in that case to just not set the environment variable though, and {reticulate} sorts it out correctly.\n\nTry to use the functions and see if they break."
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#passing-types",
    "href": "RpyEnvs/R_py_package.html#passing-types",
    "title": "Wrapping python in R package",
    "section": "Passing types",
    "text": "Passing types\nWe might need to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. See my testing of type-passing."
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html",
    "href": "RpyEnvs/py_r_project_overview.html",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "I’m working on a project that needs both Python and R, and they need to talk to each other. Most of the work is in R, but the first bit is Python, and more importantly, it has to call a python package (more in future, likely). I’m packaging everything up so the user doesn’t need to do everything through the repo, but can just library(package) and be off and running, but that’s complicated by the fact that any actual use of this project needs both languages.\nIt seems like there are a couple options to how I deal with the two languages.\n\nHave a python package and an R package, and make the user install both and manage the bilinguilism in their own scripts using the packages.\n\nSeems most cumbersome for user, but the users are fairly involved in the project, so could work\n\nSkip creating my own python package entirely, and just have the bits of python wrapper in inst/python\n\nLikely how I’ll start to figure out how to include python in package\n\nAnd see below- I think it’ll be easier to rapidly iterate the py code.\n\nAs python side grows, this will get very cumbersome\nIs it slower than the first option?\n\nMake my own python package, and wrap that in inst/python\n\nI think this is actually the best option, since I won’t have to maintain two copies of py code\nWill also allow a user to just use option 1 (or for the whole project to move that way in future, or shifting functions over to python).\nIt will make the R package more cumbersome than option 1, but I think it’s the best tradeoff.\nIt will also slow down initial dev- as I change the python side, I’ll have to rebuild the py package and re-install it into my environment. There’s no obvious analogue to devtools::load_all that can reach across and do all that. Whereas I think if I have my py code in inst it’ll get refreshed when I load_all.\n\n\nAll of these will follow {reticulate} docs, but those are fairly sketchy as to how to actually write code that works.\n\nhttps://rstudio.github.io/reticulate/articles/package.html (seems to conflict a bit with the next)\nhttps://rstudio.github.io/reticulate/articles/python_dependencies.html\nhttps://rstudio.github.io/reticulate/articles/calling_python.html\n\n\n\nhttps://stackoverflow.com/questions/72185273/reticulate-fails-automatic-configuration-in-r-package\nhttps://github.com/rstudio/reticulate/issues/997\nWill need to test how this works both when we already have a venv and when we don’t.\n\n\n\nas I’m developing\nThis works from console, after I put the file in the inst/python\nreticulate::source_python(file.path('inst', 'python', 'controller_functions.py'))\ntested with\nscene_namer(file.path('inst', 'extdata', 'testsmall'))\nAt first it didn’t work to have the Config/retictulate in description and the source_python in .onLoad. I didn’t get anything. Then I fixed it by adding it to the package’s global environment with the argument envir = globalenv(), and it started working. But only when I already have a python environment. If I try to get the Config/reticulate to build a python environment with the necessary dependencies in a bare project, it won’t even install the package.\nI think the issue there is that the way the config/reticulate and onload are set up, it doesn’t try to install dependencies until Python is used, and for some reason running that file in onLoad doesn’t trigger it.\nSo, somehow I need to be able to install the package without needing the py dependencies, and then install them the first time someone types library(packagename) or otherwise tries to use it.\nThat’s a whole new thing to figure out, I think.\nIf we assume the user has the python dependencies installed and accessible to reticulate, then the Config/reticulate works with the following .onLoad.\n.onLoad <- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n\n  reticulate::source_python(system.file(\"python/py_functions.py\", package = 'packagename'), envir = globalenv())\n\n}\nSee some initial work I’ve done sorting this out.\n\n\n\nThe above method works to expose python functions, but the specific ones I have take dicts and lists as arguments. How do I pass those from R? We can’t just assign them to a variable in R, because those formats don’t work- e.g. we cannot create the lists and dicts in R to pass.\n\noutputType = ['summary', 'all']\n\nallowance ={'minThreshold': MINT, 'maxThreshold': MAXT, 'duration': DUR, 'drawdown': DRAW}\n\nI have sorted out how to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. See my testing of type-passing (well, if I could get it to render in quarto).\n\n\n\nIs there a way to auto-build the Config/reticulate part of DESCRIPTION? Maybe from pyproject.toml? Similar to the way usethis::use_package installs automatically add them to renv? That’s the other way though, so maybe it’s moot. Still, would be nice to automate."
  },
  {
    "objectID": "setup/R_in_VS.html",
    "href": "setup/R_in_VS.html",
    "title": "R in VS code",
    "section": "",
    "text": "I typically use Rstudio, and am very used to it. But I need to use VScode for working on Azure, and am trying to sort that out. There are also idiosyncracies with using Azure, but I’ll try to hold those for somewhere else and just keep this about VS."
  },
  {
    "objectID": "setup/R_in_VS.html#the-basics",
    "href": "setup/R_in_VS.html#the-basics",
    "title": "R in VS code",
    "section": "The basics",
    "text": "The basics\nThe VS code documentation gives a pretty good overview of the basics- install VScode, install languageserver, and install the R extension. That gets us up and running. Though it is worth noting that if you tend to work in renv for everything, it’s probably better to install languageserver globally (ie in a non-renv-managed session).\nNow, supposedly that provides linting, debugging, code completion, help, etc. And the add-ons (radian and httpgd look good too in terms of nicer terminal and visualisations). The question now is, HOW do we actually use all that functionality. I’m so used to Rstudio, it’ll take some playing. I’ll try to write down here what I try and how to get it to work.\nI tried to install.packages('languageserver') globally, but it gets grumpy sometimes and can’t find it inside a renv- managed repo because the .libPaths don’t have wherever the global package directory is. It seems to have worked on Windows, but not Azure/Unix.\n\nRadian\nI tried installing radian as the terminal. Assuming I don’t want it to mess up the project-level python environments, I installed it globally through the git-bash in windows terminal. It works when I type radian into VS bash, but does not run when I try to actually run something from a .R file. In the command pallette -> settings -> extensions -> R, there’s an option for Rterm:Windows that says it can be the path to radian. I tried where radian in git-bash (which radian on unix), and it gave me two paths in py-env/shims. The one with the .bat works on windows (C:\\Users\\galen\\.pyenv\\pyenv-win\\shims\\radian.bat). On Unix, it’s just a standard usr/bin/…. I’ve turned radian back off, though, because it throws a really annoying amount of weird errors that don’t actually stop the code from running, and that don’t appear in the base R terminal. Things like ‘unexpected & in }’, when there are no ‘&’ symbols in the code (and it still completes (usually). I think it works in .R scripts, but not in quarto. It’s something about bracketed paste not working right in notebooks I think. Working on sorting that out.\n\n\nlinting\nThis is something that (weirdly, I think) isn’t included in Rstudio. It supposedly is in VScode, but I’m not seeing obvious signs of it.\nInteresting. I don’t know what changed, but it has suddenly started linting. Maybe I turned something on in the settings->extensions->R section?\nAnd now all the blue lines are super annoying. Would be nice to at least have a turn off for comments setting. Or a good way to wrap comments a la Rstudio ctrl-shift /.\nGuess I need to figure out how to step through lintr and fix issues."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#website",
    "href": "small_helpers/quarto_notes.html#website",
    "title": "Quarto notes",
    "section": "Website",
    "text": "Website\n\nFinding new pages\nI often work on a bunch of pages and actually merge and push rarely. To identify pages that are present in the repo but not yet in the yaml (and so will not be included in the website), I wrote a little function that finds the files not in the yaml and also qmds in the yml but without a file, as when we change filenames. It’s available in the functions dir and here:\n\nfind_missing_pages <- function() {\n  qmd_in_proj <- list.files(pattern = \"*.qmd\", recursive = TRUE)\n  qmd_in_yml <- readLines(\"_quarto.yml\") |> \n    purrr::keep(\\(x) grepl(\".qmd\", x)) |> \n    gsub('\\\\s', '', x = _) |> \n    gsub('-', '', x = _) |> \n    gsub('href:', '', x = _)\n  # The values in the `render` section are still there, but that's ok\n  \n  not_present <- which(!(qmd_in_proj %in% qmd_in_yml))\n  \n  missing_pages <- qmd_in_proj[not_present]\n  \n  no_file <- which(!(qmd_in_yml %in% qmd_in_proj))\n  \n  broken_links <- qmd_in_yml[no_file]\n\n  \n  return(list(missing_pages = missing_pages, no_file_present = broken_links))\n}"
  }
]