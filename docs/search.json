[
  {
    "objectID": "betabinomial/beta_binomial.html",
    "href": "betabinomial/beta_binomial.html",
    "title": "Beta-binomial model testing",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lme4)\n\nWarning: package 'lme4' was built under R version 4.2.2\n\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(lmerTest)\n\nWarning: package 'lmerTest' was built under R version 4.2.2\n\n\n\nAttaching package: 'lmerTest'\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(spaMM)\n\nRegistered S3 methods overwritten by 'registry':\n  method               from \n  print.registry_field proxy\n  print.registry_entry proxy\nspaMM (Rousset & Ferdy, 2014, version 4.1.0) is loaded.\nType 'help(spaMM)' for a short introduction,\n'news(package='spaMM')' for news,\nand 'citation('spaMM')' for proper citation.\nFurther infos, slides, etc. at https://gitlab.mbb.univ-montp2.fr/francois/spamm-ref."
  },
  {
    "objectID": "code_demos.html",
    "href": "code_demos.html",
    "title": "Code Demos",
    "section": "",
    "text": "This section has code and demos that cover many topics and serves several purposes. The pages here are organised thematically, though it will likely take me some iterating on the quarto website yaml to get there.\nThe goal in many of these examples and demos is NOT clean, efficient coding, but exploring HOW the code works and how to accomplish something. That often means creating LOTS of extra variables, copy-paste, and being extremely verbose."
  },
  {
    "objectID": "code_demos.html#clarify-thinking-and-testing",
    "href": "code_demos.html#clarify-thinking-and-testing",
    "title": "Code Demos",
    "section": "Clarify thinking and testing",
    "text": "Clarify thinking and testing\nClarify what I’m actually trying to do, and what the expected outcomes are. Then figuring out a) how do get those, and b) why I sometimes don’t, which can be just as important. Doing this sort of testing here instead of in-project can be very helpful as using minimal examples forces me to isolate the issue I’m trying to solve from all the particulars of a given dataset or project structure."
  },
  {
    "objectID": "code_demos.html#central-location-for-useful-bits",
    "href": "code_demos.html#central-location-for-useful-bits",
    "title": "Code Demos",
    "section": "Central location for useful bits",
    "text": "Central location for useful bits\nA central point for (relatively) clean, complete things that I want to be able to use across many projects (e.g. 2d autocorrelation, the Johnson distribution, how to use certain packages, fonts, colours and other plotting things etc). Having one central reference point keeps me from having to either reinvent the wheel or remember which project I put the wheel in, and having many slightly different variations. And improvements/extensions can then be accessed across projects."
  },
  {
    "objectID": "code_demos.html#understanding-code-testing-beyond-standard-uses",
    "href": "code_demos.html#understanding-code-testing-beyond-standard-uses",
    "title": "Code Demos",
    "section": "Understanding code, testing beyond standard uses",
    "text": "Understanding code, testing beyond standard uses\nI spend quite a lot of time figuring out how to do things in code, understanding how code works, and double-checking everything is working correctly. There are a lot of good demos and tutorials out there (e.g. stackoverflow, some package vignettes and websites), but I often end up needing to figure out weird edge cases. And I often end up doing something similar later, but needing not the final answer, but some intermediate step along the way."
  },
  {
    "objectID": "code_demos.html#the-process-of-coding",
    "href": "code_demos.html#the-process-of-coding",
    "title": "Code Demos",
    "section": "The process of coding",
    "text": "The process of coding\nI also think there can be value in seeing how I’ve solved a problem and tested the various avenues, both for my own future reference and others. For one, if I do later have a need for one of those side avenues, they’re available. For another, it exposes the actual process of coding a bit more than the usual tutorial that has cleaned everything up start to finish. And it gives a better starting point for additional development potentially much later if I can see what I’ve already tried. Maybe most importantly, there are few tutorials/walkthroughs I’ve followed that don’t end up with some sort of error, especially as soon as I try to modify them for my purposes. Seeing where I’ve hit errors, what caused them, and how I solved it can be incredibly helpful, rather than only seeing what worked."
  },
  {
    "objectID": "data_acquisition/bom_gauges.html",
    "href": "data_acquisition/bom_gauges.html",
    "title": "Bom reference stations",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.2.2\n\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nTrying to find BOM gauge locations. Found reference stations. It’s a simple link, but have to use httr2 to download because there’s an error with the user_agent if we try to just download.file.\nMostly including this here as an example of changing user_agent.\nhttp://www.bom.gov.au/waterdata/ has a clickable link to what I want, but the data is buried in a frame so can’t scrape.\nThe below is because I found a link to reference stations and wanted to see what they were.\n\nIs there a url for BOM?\nIt’s just a csv, but have to faff about with httr2 and deparsing back to csv because need to pass a user agent or get a 403 error.\n\nbom2 <- httr2::request(\"http://www.bom.gov.au/water/hrs/content/hrs_station_details.csv\") |>\n  httr2::req_user_agent(\"md-werp\") |> \n  httr2::req_perform() |> \n  httr2::resp_body_string() |> \n  readr::read_csv(skip = 11) |> \n  dplyr::select(site = `Station Name`, \n                gauge = `AWRC Station Number`,  \n                owner = `Data Owner Name`, \n                Latitude, Longitude)\n\nRows: 467 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): AWRC Station Number, Station Name, Jurisdiction, Data Owner Name, D...\ndbl (3): Latitude, Longitude, Catchment Area (km2)\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nbasin <- read_sf(file.path('data','mdb_boundary', 'mdb_boundary.shp'))\n\n\nbom2 <- bom2 |>\n    # lat an long come in as chr because there is a line for 'undefined'\n    dplyr::filter(site != 'undefined') |>\n  st_as_sf(coords = c('Longitude', 'Latitude'), crs = 4326) |> \n  st_transform(crs = st_crs(basin))\n\nThey’re not the gauges I’m looking for. Only 457, instead of 6500, and around the edges of the basin.\n\nggplot() + \n  geom_sf(data = basin) +\n  geom_sf(data = bom2)"
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html",
    "href": "data_acquisition/gauge_data_pre_gauge.html",
    "title": "Gauge data and gauged period",
    "section": "",
    "text": "library(foreach)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(vicwater)\nlibrary(reticulate)\nWe often end up needing to pull gauge data for lots of projects. If we just want flow data, and especially if we’re working in python, there’s the mdba_gauge_getter, and I’ve written {vicwater} for R and expanded it to cover Victoria, NSW, and Qld. A primary advantage of {vicwater} is that it can access more of the API calls and return any desired variables.\nOne issue we often end up having is wanting to grab gauge data for a range of dates that may or may not go earlier than a gauge was put in place. There’s actually an issue with the Kisters API for that, where it silently returns values of 0 for those dates, which is not good. Because it comes from the API itself, it affects both packages.\nFor example, we can go to the website and find the period of record for NSW gauge 410007 (gauges_to_pull[138]) is 10/01/1979 - present. We’ll show first what happens for three situations with both mdba_gauge_getter and vicwater\nand then we’ll show how to handle it in {vicwater}."
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html#all-dates-pre-gauge",
    "href": "data_acquisition/gauge_data_pre_gauge.html#all-dates-pre-gauge",
    "title": "Gauge data and gauged period",
    "section": "All dates pre-gauge",
    "text": "All dates pre-gauge\nIf we ask for the period before the gauge is operational {vicwater} passes the API error through.\n\nget_ts_traces(state = 'NSW', \n                site_list = gaugenum, \n                var_list = '141',\n                start_time = weekbefore,\n                end_time = daybefore,\n                interval = 'day',\n                data_type = 'mean')\n\nWarning: executing %dopar% sequentially: no parallel backend registered\n\n\nError in {: task 1 failed - \"API error number 126. Message: No data within specified period\"\n\n\nWe get an empty dataframe from mdba_gauge_getter\n\ndemo_levs_pre = gg.gauge_pull(r.gaugenum, start_time_user = r.weekbefore, end_time_user = r.daybefore)\n\nC:\\Users\\galen\\DOCUME~1\\Website\\GALEN_~1\\RpyEnvs\\PYTEST~1\\VENV~1\\Lib\\site-packages\\mdba_gauge_getter\\gauge_getter.py:82: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n  gauges['State'] = gauges['gauge_owner'].str.strip().str.split(' ', 1).str[0]\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 1 of 1\nERROR:mdba_gauge_getter.gauge_get:No valid data contained in response, skipping\n\ndemo_levs_pre\n\nEmpty DataFrame\nColumns: [DATASOURCEID, SITEID, SUBJECTID, DATETIME, VALUE, QUALITYCODE]\nIndex: []\n\n\nSo, that’s slightly different behavior, but neither is returning misleading data."
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html#all-dates-with-gauge",
    "href": "data_acquisition/gauge_data_pre_gauge.html#all-dates-with-gauge",
    "title": "Gauge data and gauged period",
    "section": "All dates with gauge",
    "text": "All dates with gauge\nNow {vicwater} gives a dataframe.\n\nget_ts_traces(state = 'NSW', \n                site_list = gaugenum, \n                var_list = '141',\n                start_time = gaugestart,\n                end_time = weeklater,\n                interval = 'day',\n                data_type = 'mean')\n\n# A tibble: 8 × 20\n  error_num compressed timezone site_sho…¹ longi…² site_…³ latit…⁴ org_n…⁵ value\n      <int> <chr>      <chr>    <chr>        <dbl> <chr>     <dbl> <chr>   <dbl>\n1         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  511.\n2         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  552.\n3         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  536.\n4         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  516.\n5         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  513.\n6         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  486.\n7         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  492.\n8         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  533.\n# … with 11 more variables: time <dttm>, quality_codes_id <int>, site <chr>,\n#   variable_short_name <chr>, precision <chr>, subdesc <chr>, variable <chr>,\n#   units <chr>, variable_name <chr>, quality_codes <chr>, data_type <chr>, and\n#   abbreviated variable names ¹​site_short_name, ²​longitude, ³​site_name,\n#   ⁴​latitude, ⁵​org_name\n\n\nAs does mdba_gauge_getter\n\ndemo_levs_exists = gg.gauge_pull(r.gaugenum, start_time_user = r.gaugestart, end_time_user = r.weeklater)\n\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 1 of 1\n\ndemo_levs_exists\n\n  DATASOURCEID  SITEID SUBJECTID    DATETIME    VALUE  QUALITYCODE\n0          NSW  410007     WATER  1979-01-10  510.939          255\n1          NSW  410007     WATER  1979-01-11  551.848          130\n2          NSW  410007     WATER  1979-01-12  535.988          130\n3          NSW  410007     WATER  1979-01-13  515.685          130\n4          NSW  410007     WATER  1979-01-14  512.529          130\n5          NSW  410007     WATER  1979-01-15  486.422          130\n6          NSW  410007     WATER  1979-01-16  491.710          130\n7          NSW  410007     WATER  1979-01-17  532.730          130\n\n\nThat again returns what it should. All the dates have data."
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html#dates-spanning-gauge-start",
    "href": "data_acquisition/gauge_data_pre_gauge.html#dates-spanning-gauge-start",
    "title": "Gauge data and gauged period",
    "section": "Dates spanning gauge start",
    "text": "Dates spanning gauge start\nNow {vicwater} gives a dataframe, but that initial period has value = 0, which is wrong. It should be NA, but the API returns 0 silently.\n\nget_ts_traces(state = 'NSW', \n                site_list = gaugenum, \n                var_list = '141',\n                start_time = weekbefore,\n                end_time = weeklater,\n                interval = 'day',\n                data_type = 'mean')\n\n# A tibble: 15 × 20\n   error_num compressed timezone site_sh…¹ longi…² site_…³ latit…⁴ org_n…⁵ value\n       <int> <chr>      <chr>    <chr>       <dbl> <chr>     <dbl> <chr>   <dbl>\n 1         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 2         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 3         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 4         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 5         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 6         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 7         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 8         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  511.\n 9         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  552.\n10         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  536.\n11         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  516.\n12         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  513.\n13         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  486.\n14         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  492.\n15         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  533.\n# … with 11 more variables: time <dttm>, quality_codes_id <int>, site <chr>,\n#   variable_short_name <chr>, precision <chr>, subdesc <chr>, variable <chr>,\n#   units <chr>, variable_name <chr>, quality_codes <chr>, data_type <chr>, and\n#   abbreviated variable names ¹​site_short_name, ²​longitude, ³​site_name,\n#   ⁴​latitude, ⁵​org_name\n\n\nThe same thing happens with mdba_gauge_getter\n\ndemo_levs_span = gg.gauge_pull(r.gaugenum, start_time_user = r.weekbefore, end_time_user = r.weeklater)\n\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 1 of 1\n\ndemo_levs_span\n\n   DATASOURCEID  SITEID SUBJECTID    DATETIME    VALUE  QUALITYCODE\n0           NSW  410007     WATER  1979-01-03    0.000          255\n1           NSW  410007     WATER  1979-01-04    0.000          255\n2           NSW  410007     WATER  1979-01-05    0.000          255\n3           NSW  410007     WATER  1979-01-06    0.000          255\n4           NSW  410007     WATER  1979-01-07    0.000          255\n5           NSW  410007     WATER  1979-01-08    0.000          255\n6           NSW  410007     WATER  1979-01-09    0.000          255\n7           NSW  410007     WATER  1979-01-10  510.939          255\n8           NSW  410007     WATER  1979-01-11  551.848          130\n9           NSW  410007     WATER  1979-01-12  535.988          130\n10          NSW  410007     WATER  1979-01-13  515.685          130\n11          NSW  410007     WATER  1979-01-14  512.529          130\n12          NSW  410007     WATER  1979-01-15  486.422          130\n13          NSW  410007     WATER  1979-01-16  491.710          130\n14          NSW  410007     WATER  1979-01-17  532.730          130\n\n\nSo, that’s not good. Especially the silent part"
  },
  {
    "objectID": "drones/overlaps.html",
    "href": "drones/overlaps.html",
    "title": "Overlaps",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "drones/overlaps.html#calculating-photo-separation-for-desired-overlaps",
    "href": "drones/overlaps.html#calculating-photo-separation-for-desired-overlaps",
    "title": "Overlaps",
    "section": "Calculating photo separation for desired overlaps",
    "text": "Calculating photo separation for desired overlaps\nThere are a lot of variables that go into this. My goal here is to start by making a plot of how this varies with height. And then sort out how to calculate photo frequency given speed (or vice versa). Though Litchi allows photos by distance interval, which is nice."
  },
  {
    "objectID": "drones/overlaps.html#gsd",
    "href": "drones/overlaps.html#gsd",
    "title": "Overlaps",
    "section": "GSD",
    "text": "GSD\nGround sampling distance. Lots of people determine heights to fly for a desired ground sampling distance (GSD) in cm/pixel. That’s useful to know, but we’re almost certainly always going to be flying low enough for really good GSD. Our main need here is that the overlap calculations rely on GSD. So, let’s find it. The equation is \\[GSD_w=\\frac{HS_w}{FI_w}\\]\nand\n\\[GSD_h=\\frac{HS_h}{FI_h}\\]\nwhere the subscripts h and w are the dimensions height (along path) and width (across path), H is flight altitude in meters, S is sensor width or height (ie physical size of the sensor in mm), F is focal length (true, in mm. not 35mm equivalent), and I is image width or height in pixels (be careful here- often the default photo settings are 16:9 but the sensor dims are given for 4:3, and the 16:9 crops to get there. Easiest to change to shoot in 4:3, and get better coverage to boot. This gives GSD in m/px. We typically want it in cm/px, so we adjust H in the calculations below.\n\nWhere to get those values?\n\nSensor size I\nDJI sensor sizes are given here https://www.djzphoto.com/blog/2018/12/5/dji-drone-quick-specs-amp-comparison-page. For the drones we have, the Mini 2 is 6.3w x 4.7h (called 1/2.3” sensor), and the Phantom 4 Pro V2 is 13.2w x 8.8h (called 1” sensor).\nParameterise for Mini 2\n\nsw <- 6.3\nsh <- 4.7\n\n\n\nImage size\nImage sizes depend on photo settings, and are given for the Mini 2 and Phantom 4 Pro V2 by DJI. You can also get them from the photo’s EXIF data (either in Properties, or more completely from https://jimpl.com/. It’s a good idea to check, because the set aspect ratio can change. Mini can be 16:9 or 4:3 (best), and Phantom can be 16:9, 4:3, or 3:2 (best).\nMini 2\n\n4:3: 4000×3000\n16:9: 4000×2250\n\nPhantom 4 Pro V2\n\n3:2 Aspect Ratio: 5472×3648\n4:3 Aspect Ratio: 4864×3648\n16:9 Aspect Ratio: 5472×3078\n\nParameterise for Mini 2 at 4:3\n\nIw <- 4000\nIh <- 3000\n\n\n\nFocal length\nI found the true focal length from the EXIF data, though it can be back-calculated from the 35mm format equivalent on the specs pages above.\nThe EXIF had Mini 2 at 4.5, Phantom at 8.8.\nTo back-calculate, the expression is \\(Focal*Scale=35mmequiv\\) where the scale factor is found in the EXIF, or is known and can be looked up for the sensor size. Both drones have a 24mm 35mm equivalent, but scale factors differ.\nParameterise for Mini 2\n\nFocal <- 4.5 \n\n\n\n\nCalculate GSD\nWe now have what we need to get the two GSDs. Let’s get one as a function of height, and another to solve for height given GSD\n\nget_gsd <- function(H_m, focal_mm, sensor_mm, image_px, h_units = 'm') {\n  # make cm\n  if (h_units == 'm') {H_m = H_m * 100} else if (h_units != 'cm') {stop(\"units not supported\")}\n  gsd <- (H_m * sensor_mm)/(focal_mm * image_px)\n}\n\nAnd typically, the recommendation is to use the maximum of the height and width GSDs, as that’s the worst resolution.\n\nworst_gsd <- function(H_m, focal_mm, sensor_w, sensor_h, image_w, image_h, h_units = 'm') {\n  gsd_w <- get_gsd(H_m, focal_mm, sensor_w, image_w)\n  gsd_h <- get_gsd(H_m, focal_mm, sensor_h, image_h)\n  \n  gsd <- pmax(gsd_w, gsd_h)\n}\n\nWhat is that for the mini 2 for a range of heights? I’ll look at all of them, even if that’s not typically what we’d do.\n\nheights <- seq(0, 100, by = 0.1)\nmini_gsd_w <- get_gsd(heights, Focal, sw, Iw)\nmini_gsd_h <- get_gsd(heights, Focal, sh, Ih)\n\nmini_gsd_worst <- worst_gsd(heights, Focal, sw, sh, Iw, Ih)\n\nPlot\n\nmini_gsd <- tibble(altitude_m = heights, width_gsd = mini_gsd_w, height_gsd = mini_gsd_w, worst_gsd = mini_gsd_worst)\n\n\nmini_gsd |> \n  tidyr::pivot_longer(cols = ends_with('gsd'), names_to = 'gsd_type', values_to = 'GSD_cmperpx') |> \nggplot(aes(x = altitude_m, y = GSD_cmperpx, color = gsd_type)) + geom_line()\n\n\n\n\nNot much difference there between h and w.\nDoes it match the Pix4d calculator?\nThat gives a GSD of 0.07 for a height of 2m, I get\n\nformat(mini_gsd_w[which(heights == 2)], scientific = FALSE)\n\n[1] \"0.07\""
  },
  {
    "objectID": "drones/overlaps.html#distance-and-overlap",
    "href": "drones/overlaps.html#distance-and-overlap",
    "title": "Overlaps",
    "section": "Distance and overlap",
    "text": "Distance and overlap\nWhat we really want is to get the flight distance we need to get a desired overlap. That will depend on height and GSD (from which we get the ground area covered per photo). And if we want photo timings, we need speed as well. Using the equations from pix4d, but rearranged so the order of operations makes sense to me.\nThe image size on the ground in meters (again using h and w for height (along path) and width (across path), given image height in px and GSD in cm/px is \\[D_h=(I_hGSD)/100\\]\nThen the flight distance needed for an overlap % O_p expressed in 0-1 is \\[d = D_h-O_pD_h = D_h(1-O_p)\\]\nThe pix4d then goes on to back that back out to the definition of D_h, \\[d=((I_hGSD)/100)(1-O_p)\\]\nIf we can’t set a travel distance d for the drone, we will need to adjust it’s velocity v in m/s and the photo interval t in seconds. In practice, well want to adjust them in tandem (and for a given height). To get the photo interval for a given velocity, it’s simply the desired distance divided by velocity, \\[t = d/v\\]\nand so the velocity for a given interval is\n\\[\nv=d/t\n\\]\nWe can obviously break this down into the equation for d, e.g. \\[t = D_h(1-O_p)/v\\].\n\nFunctions\n\nGiven GSD\nFirst, the ground distance in m\n\nground_dist <- function(image_px, gsd_cmpx) {\n  D <- image_px*gsd_cmpx/100\n}\n\nNext, the distance the drone should travel, given overlap\n\ndrone_dist <- function(ground_dist_m, overlap_prop) {\n  d <- ground_dist_m * (1-overlap_prop)\n}\n\nThe time interval, given velocity\n\nphoto_interval <- function(drone_dist_m, v_ms) {\n  t <- drone_dist_m/v_ms\n}\n\nThe velocity, given interval (not typical, but we might want it since we can only set intervals down to 2 seconds when hand-flying.\n\nvelocity <- function(drone_dist_m, p_s) {\n  v <- drone_dist_m/p_s\n}\n\n\n\nWrap those up\nDrone dist is just one level\n\ndrone_dist_from_gsd <- function(image_px, gsd_cmpx, overlap_prop) {\n  D <- ground_dist(image_px, gsd_cmpx)\n  dd <- drone_dist(D, overlap_prop)\n}\n\nPhoto interval and velocity need to depend on that\n\nphoto_interval_from_gsd <- function(image_px, gsd_cmpx, overlap_prop, v_ms) {\n  dd <- drone_dist_from_gsd(image_px, gsd_cmpx, overlap_prop)\n  t <- photo_interval(dd, v_ms)\n}\n\n\nvelocity_from_gsd <- function(image_px, gsd_cmpx, overlap_prop, p_s) {\n  dd <- drone_dist_from_gsd(image_px, gsd_cmpx, overlap_prop)\n  v <- velocity(dd, p_s)\n}"
  },
  {
    "objectID": "drones/overlaps.html#values-from-h-and-overlap",
    "href": "drones/overlaps.html#values-from-h-and-overlap",
    "title": "Overlaps",
    "section": "Values from H and overlap",
    "text": "Values from H and overlap\nDo this separately for h and w, I guess?\n\nDrone distance\nThis might be all we need for litchi, and regardless, it will tell us about how close flightpaths need to be.\n\ndrone_dist_H_o <- function(H_m, overlap_prop, \n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  gsd <- get_gsd(H_m, focal_mm, \n                   sensor_mm, \n                   image_px, \n                   h_units = 'm')\n  \n  gD <- ground_dist(image_px, gsd)\n  \n  dd <- drone_dist(gD, overlap_prop)\n  \n  return(dd)\n}\n\n\n\nVelocity and photo intervals\n\nv_H_o <- function(H_m, overlap_prop, p_s,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  dd <- drone_dist_H_o(H_m, overlap_prop,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm')\n  v <- dd/p_s # could use velocity(dd/ps), but not worth it here.\n  return(v)\n}\n\n\nt_H_o <- function(H_m, overlap_prop, v_ms,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  dd <- drone_dist_H_o(H_m, overlap_prop,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm')\n  t <- dd/v_ms # could use velocity(dd/ps), but not worth it here.\n  \n  return(t)\n}"
  },
  {
    "objectID": "drones/overlaps.html#plots",
    "href": "drones/overlaps.html#plots",
    "title": "Overlaps",
    "section": "Plots",
    "text": "Plots\n\nDrone distance\nHow does the drone distance depend on height and overlap?\nFor a fixed overlap of 80%, and the same heights sequences as above, for the h dimension (along flight path) and w (across),\n\ndd_h <- drone_dist_H_o(heights, 0.8,\n                       Focal, sh, Ih)\n\ndd_w <- drone_dist_H_o(heights, 0.8,\n                       Focal, sw, Iw)\n\nThose are fairly different, actually\n\ndd_hw <- tibble(altitude_m = heights, width_dd = dd_w, height_dd = dd_h) |> \n  tidyr::pivot_longer(cols = ends_with('dd'), names_to = 'dd_direction', values_to = 'drone_photo_distance')\n\n\nggplot(dd_hw, aes(x = altitude_m, y = drone_photo_distance, color = dd_direction)) + geom_line()\n\n\n\n\nCould use plotly here to have mouseover. Or use observable or shiny.\nWhat if we zoom in on the lower altitudes (< 10m)?\n\ndd_hw |> \n  dplyr::filter(altitude_m <= 10) |> \nggplot(aes(x = altitude_m, y = drone_photo_distance, color = dd_direction)) + geom_line()\n\n\n\n\nWe could make a heatmap with overlaps, but I’m not sure we really care that much? We’d really only be interested in maybe 75, 80, 85 or something, and this is for rule of thumb. Do that later.\nWhat are those at 2 and 4 m?\n\ndd_hw |> \n  dplyr::filter(altitude_m %in% c(2,4))\n\n# A tibble: 4 × 3\n  altitude_m dd_direction drone_photo_distance\n       <dbl> <chr>                       <dbl>\n1          2 width_dd                    0.56 \n2          2 height_dd                   0.418\n3          4 width_dd                    1.12 \n4          4 height_dd                   0.836\n\n\n\n\nVelocity from interval\nHere, let’s say we have a fixed overlap, and want to know the velocity we need to fly to get that at a given height and photo interval. This sounds contrived, but is pretty much exactly our situation when hand flying- the shortest interval we have is 2seconds, so how fast/slow do we need to fly to get 80% overlap at a range of heights?\nHere, we’ll focus on the h dimension. While we could fly sideways, we usually will fly with forward velocity.\nLet’s say 2 seconds, and then look at a heatmap.\n\nv_h <- v_H_o(heights, 0.8, 2,\n                       Focal, sh, Ih)\n\nv_tib <- tibble(altitude_m = heights, velocity_ms = v_h)\n\n\nggplot(v_tib, aes(x = altitude_m, y = velocity_ms)) + geom_line()\n\n\n\n\nAnd again zoom in\n\nv_tib |> \n  dplyr::filter(altitude_m <= 10) |> \nggplot(aes(x = altitude_m, y = velocity_ms)) + geom_line()\n\n\n\n\nSo, to get 80% overlap if we’re limited to intervals of 2 seconds, we’d need to fly at about 0.2m/s at 2m or 0.4 at 4m. Should do a tooltip kinda thing. But for now\n\nv_tib |> \n  dplyr::filter(altitude_m %in% c(2, 4)) \n\n# A tibble: 2 × 2\n  altitude_m velocity_ms\n       <dbl>       <dbl>\n1          2       0.209\n2          4       0.418\n\n\nHow about a heatmap of heights and intervals?\n\nintervals <- seq(0.1, 5, by = 0.1)\n\nvel_map <- tidyr::expand_grid(altitude_m = heights, photo_intervals = intervals) |> \n  mutate(velocity_ms = v_H_o(altitude_m, 0.8, photo_intervals,\n                       Focal, sh, Ih))\n\n\nvel_map |> \nggplot(aes(x = altitude_m, y = photo_intervals, fill = velocity_ms)) + geom_raster()\n\n\n\n\nThat’s a dumb scale. Obviously we can fly super fast at 100m and super fast photo intervals\n\nvel_map |> \n  dplyr::filter(altitude_m <= 10) |> \nggplot(aes(x = altitude_m, y = photo_intervals, fill = velocity_ms)) + geom_raster()\n\n\n\n\nStill not particularly useful. Moving on.\n\n\nInterval from velocity\nHow often do we need to take photos given a velocity and height? Let’s start by saying velocity of 1m/s (3.6km/h).\n\nt_h <- t_H_o(heights, 0.8, 1,\n                       Focal, sh, Ih)\n\nt_tib <- tibble(altitude_m = heights, photo_interval = t_h)\n\n\nt_tib |> \n  ggplot(aes(x = altitude_m, y = photo_interval)) + geom_line()\n\n\n\n\n\nt_tib |> \n  dplyr::filter(altitude_m <= 10) |> \n  ggplot(aes(x = altitude_m, y = photo_interval)) + geom_line()\n\n\n\n\nSo, at that velocity, the photo interval can’t be as long as 2 seconds without flying at 10m. Clearly that would change as we fly slower.\nMake a heatmap again, I guess, even if it wasn’t very useful before and we know this is just flipping axes and colors.\n\nspeeds <- seq(0.1, 5, by = 0.1)\n\ninterval_map <- tidyr::expand_grid(altitude_m = heights, velocity_m = speeds) |> \n  mutate(photo_interval = t_H_o(altitude_m, 0.8, velocity_m,\n                       Focal, sh, Ih))\n\n\ninterval_map |> \n  dplyr::filter(altitude_m <= 10) |> \nggplot(aes(x = altitude_m, y = velocity_m, fill = photo_interval)) + geom_raster()"
  },
  {
    "objectID": "drones/overlaps.html#next-steps",
    "href": "drones/overlaps.html#next-steps",
    "title": "Overlaps",
    "section": "Next steps",
    "text": "Next steps\nThe obvious thing to do here is to make an observable quarto where we can select drone type, desired overlap, things we want to set, and it makes the plot and returns values we want. For now though, these plots get us most of the way there.\nWhat do those distances look like for the phantom?"
  },
  {
    "objectID": "drones/overlaps_reactive.html",
    "href": "drones/overlaps_reactive.html",
    "title": "Drone flight calculations",
    "section": "",
    "text": "This is an interactive page based on more detailed exploration of drone overlaps.\nThis gives us the opportunity to plug in flight parameters and back-calculate others. For example, give us the size of the photos on the ground given height, the needed drone flight distance for a desired overlap at a given height, or the speed we need to fly at a given height to yield the right overlap given the photo interval.\nOverlap is fixed at 80% for the moment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport { aq, op } from '@uwdata/arquero'\n\ndroned_aq = aq.from(transpose(droned))\ndronev_aq = aq.from(transpose(dronev))\ndronei_aq = aq.from(transpose(dronei))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof dronetype = Inputs.checkbox(\n  [\"Mini 2\", \"Phantom 4 Pro V2\"],\n  {value: [\"Mini 2\"],\n    label: \"Drone\"\n  }\n)\n\nviewof aspect = Inputs.checkbox(\n  [\"3:2\", \"4:3\", \"16:9\"],\n  {value: [\"4:3\"],\n    label: \"Aspect Ratio\"\n  }\n)\n\nviewof overlap_p = Inputs.range(\n  [0.7, 0.95],\n  {value: 0.8, step: 0.05, label: \"Overlap:\"}\n)\n\nviewof height = Inputs.range(\n  [1, 50],\n  {value: 5, step: 0.1, label: \"Altitude (m):\"}\n)\n\nviewof velocity = Inputs.range(\n  [0.1, 5],\n  {value: 1, step: 0.1, label: \"Velocity (m/s):\"}\n)\n\nviewof interval = Inputs.range(\n  [0.1, 5],\n  {value: 2, step: 0.1, label: \"Photo interval (s):\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the selected values\nGround distances (photo footprints) and needed photo distances to achieve overlap are\n\ndist_h\n  .select('altitude_m', 'overlap', 'direction', 'photo_distance_m', 'ground_distance_m')\n  .view()\n\n\n\n\n\n\nFor the selected interval and height, the needed velocity is\n\nvel_h\n  .select('altitude_m', 'overlap', 'intervals', 'velocity_ms')\n  .view()\n\n\n\n\n\n\nFor the selected velocity, the needed photo interval is\n\ni_h\n  .select('altitude_m', 'overlap', 'velocity_ms', 'photo_interval')\n  .view()\n\n\n\n\n\n\n\n\n\n\n\ndistfilter = droned_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect\n})\n  .filter((d, $) => op.includes(d.drone, $.dr))\n  .filter((d, $) => op.equal(d.overlap, $.ov))\n  .filter((d, $) => op.includes(d.aspect, $.asp))\n\ndist_h = distfilter\n  .params({\n  h: height\n})\n  .filter((d, $) => op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvfilter = dronev_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect,\n  inter: interval\n})\n  .filter((d, $) => op.includes(d.drone, $.dr))\n  .filter((d, $) => op.equal(d.overlap, $.ov))\n  .filter((d, $) => op.includes(d.aspect, $.asp))\n  .filter((d, $) => op.equal(d.intervals, $.inter))\n\nvel_h = vfilter\n  .params({\n  h: height\n})\n  .filter((d, $) => op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nifilter = dronei_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect,\n  v: velocity\n})\n  .filter((d, $) => op.includes(d.drone, $.dr))\n  .filter((d, $) => op.equal(d.overlap, $.ov))\n  .filter((d, $) => op.includes(d.aspect, $.asp))\n  .filter((d, $) => op.equal(d.velocity_ms, $.v))\n\ni_h = ifilter\n  .params({\n  h: height\n})\n  .filter((d, $) => op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGround distancePhoto distanceVelocityPhoto interval\n\n\n\nPlot.plot({\n  grid: false,\n  color: {\n    legend: true\n  },\n  marks: [\n    Plot.line(distfilter, {x: \"altitude_m\", y: \"ground_distance_m\", stroke: \"direction\"}),\n    Plot.dot(dist_h, {x: \"altitude_m\", y: \"ground_distance_m\", stroke: \"drone\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  color: {\n    legend: true\n  },\n  marks: [\n    Plot.line(distfilter, {x: \"altitude_m\", y: \"photo_distance_m\", stroke: \"direction\"}),\n    Plot.dot(dist_h, {x: \"altitude_m\", y: \"photo_distance_m\", stroke: \"drone\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(vfilter, {x: \"altitude_m\", y: \"velocity_ms\"}),\n    Plot.dot(vel_h, {x: \"altitude_m\", y: \"velocity_ms\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(ifilter, {x: \"altitude_m\", y: \"photo_interval\"}),\n    Plot.dot(i_h, {x: \"altitude_m\", y: \"photo_interval\"})\n  ]\n})"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a placeholder. I am a community ecologist with a focus on aquatic ecology. My background is in theoretical community ecology, though I also do fieldwork in aquatic systems and develop management-focused models. I have a particular interest in scaling, probabalistic modeling, and climate impacts.\nPostdoctoral researcher in the QAEL Lab at Deakin University."
  },
  {
    "objectID": "observable/trying_observable.html",
    "href": "observable/trying_observable.html",
    "title": "Trying observablejs chunks",
    "section": "",
    "text": "Quarto has the option of using Observable JS for code chunks. This gives the ability to add interactivity in-browser, without needing to do server-side calcs, as we do with Shiny. The catch is with Shiny, I can serve R objects (e.g. ggplots) that I’m familiar with. Using observable means I need to figure out how that system works. I’m also a bit unclear how much processing can happen. My understanding is that any actual processing that happens needs to happen in the ojs, not R chunks, so we can’t interactively change a setting in the observable chunk and have that kick off some R. Though I might be wrong.\nI have quite a few use cases in mind if I can get this to work- serving maps, as an interface to {vicwater}, some drone settings, playing with population models, etc."
  },
  {
    "objectID": "observable/trying_observable.html#issues",
    "href": "observable/trying_observable.html#issues",
    "title": "Trying observablejs chunks",
    "section": "ISSUES",
    "text": "ISSUES\n\nInteractive notebooks\nThe ojs chunks dont work in interactive mode, and throw errors like “Error in ojs_define(iris = iris) : could not find function”ojs_define”“. So to work on anything past the first ojs chunk requires rendering. But that brings us to the next issue:\n\n\nNo output\nObservable chunks don’t have output unless you use quarto preview, not just quarto render. And the ‘Render’ button in Rstudio renders and previews, making this more confusing. JUST RUNNING quarto render doc.qmd at the terminal yields a document with code chunks but no output. This is expected behavior, but is super counter intuitive, espcially given the Render button’s name.\nUnfortunately, there is no per-document quarto preview at the terminal like there is for quarto render. So if you’re working in a quarto project (website, book, etc), you have to preview the whole thing just to check a document.\nThat means you’ll almost certainly want to turn caching on for the project (probably do anyway if it’s big), but if caching is on for the quarto project, it won’t render because the ojs_defined object can’t be cached. So the chunk with ojs_define needs to have #| cache: false added to it. Or perhaps just turn caching off in the yaml headers for pages using ojs. Depends on how much pre-processing happens in R, probably.\n\n\nChunk options\nPython and R both use #| option: value for setting chunk options. ojs cells use //| option: value.\n\n\nColumn names\nObservable uses object.thing notation like python, but it also uses the . to reference columns in arquero. That means Sepal_Length is confusing, because it gets referenced as d.Sepal_Length. So change the names.\n\nnames(iris) <- stringr::str_replace_all(names(iris), '\\\\.', '_')\n\n\n\nCode changes\nI’m running into issues where I change some R code, and it works when I run it interactively, but then when I go to render, the new R code just doesn’t happen. E.g. I’ll add code that makes a dataframe with more values, and I can see them interactively in R, but they don’t appear in the render. I think it has something to do with the cache not resetting with changes, but I’m not positive."
  },
  {
    "objectID": "observable/trying_observable.html#r-setup",
    "href": "observable/trying_observable.html#r-setup",
    "title": "Trying observablejs chunks",
    "section": "R setup",
    "text": "R setup\n\nlibrary(ggplot2)\n\nI know the cool thing to do is {palmerpenguins}, but I’m just going to use {iris}.\nI have a feeling ojs is likely just as happy plotting vectors, but I’ll tend to have dataframes from analyses, so let’s stick with that.\nTo start, can we reproduce a simple ggplot?\n\nggplot(iris, aes(x = Sepal_Length, y = Sepal_Width, color = Species)) + geom_point()\n\n\n\n\nLet’s try that without reactivity to start."
  },
  {
    "objectID": "observable/trying_observable.html#data-to-ojs",
    "href": "observable/trying_observable.html#data-to-ojs",
    "title": "Trying observablejs chunks",
    "section": "Data to ojs",
    "text": "Data to ojs\nIt seems like Arquero makes a lot of sense, since it’s dplyr-like. But the example (and all other examples I can find) use it to read data in from an external file (csv, json, etc). That’s almost never what I’m going to want to do. So, how do I get a dataframe into Arquero? I’m guessing I can’t just grab it. The data sources documentation says we need to use ojs_define in R to make things available. Let’s see if we can do that and then make it an arquero object?\nNOTE I’ve not seen this mentioned anywhere, but ojs_define cannot be found in an interactive session- it’s only available on render. So interactively it throws “Error in ojs_define(iris = iris) : could not find function”ojs_define”“. AND, if caching is on for the quarto project, it won’t render because the ojs_defined object can’t be cached.\n\nojs_define(iris = iris)\n\nCan I see that in ojs? I thought .view made tables? Maybe not if we haven’t imported arquero? But I also thought order didn’t matter for ojs.\n\niris.view()\n\n\n\n\n\n\nAnyway, we can see it as an object (once we preview instead of render). I’m still grumpy about that.\n\niris\n\n\n\n\n\n\nI think we usually need to transpose according to various stackexchanges.\n\ntiris = transpose(iris)\ntiris\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, the arquero docs seem to suggest I might be able to use from to make it arquero?\n\nimport { aq, op } from '@uwdata/arquero'\nirtab = aq.from(iris)\n\nirtab.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat does look like it’s the wrong dims, let’s used the transposed.\n\ntirtab = aq.from(tiris)\ntirtab.view()"
  },
  {
    "objectID": "observable/trying_observable.html#plots",
    "href": "observable/trying_observable.html#plots",
    "title": "Trying observablejs chunks",
    "section": "Plots",
    "text": "Plots\nTo make a first plot, do something I’m pretty sure should work, stolen directly from the penguins example that starts with a dataframe and just modifying the name and removing a facet level.\n\nPlot.rectY(tiris,\n  Plot.binX(\n    {y: \"count\"},\n    {x: \"Sepal_Width\", fill: \"Species\", thresholds: 10}\n  ))\n  .plot({\n    facet: {\n      data: tiris,\n      y: \"Species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\nScatter\nNow, can we make a scatter?\n\nPlot.dot(tiris, {x: \"Sepal_Length\", y: \"Sepal_Width\", fill: \"Species\"}).plot()\n\n\n\n\n\n\nThere’s lots of cleanup we could do to make that look different, but let’s go with that for now.\nCan I make a line? It’ll be jumbled, but whatever. Maybe I can sort it at least with arquero.\nRemember to use the arquero version of the data- this barfs with tiris.\n\ntirtab\n  .orderby('Sepal_Length')\n  .view()\n\n\n\n\n\n\nNow, how to plot that? Does the chunk above order tirtab permanently? Doesn’t seem to\n\ntirtab.view()\n\n\n\n\n\n\n\n\nLine\nLet’s try the line with the orderby\n\nPlot.line(tirtab.orderby('Sepal_Length'), {x: \"Sepal_Length\", y: \"Sepal_Width\", fill: \"Species\"}).plot()\n\n\n\n\n\n\nThat seems to have worked, but it sure is goofy looking. Oh. Is it because i’m using fill? Use stroke (not color- this isn’t ggplot).\n\nPlot.line(tirtab.orderby('Sepal_Length'), {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}).plot()\n\n\n\n\n\n\nWould be good to not do the data processing inside the plot call.\nI assume that’s as easy as\n\nirorder = tirtab.orderby('Sepal_Length')\n\nPlot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}).plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoint and line\nNeed to figure this out. Can I just do both? I think the answer might be to use a Plot.plot with mutliple marks?\nFirst, how does that syntax work? This should just recreate the above, right?\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"})\n  ]\n})\n\n\n\n\n\n\ncan we just add more Plot.marktypes?\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nCool. Would be nice if there was a ggplot-esque way to use the same x,y,color and just change the marks. Maybe there is? Look for that later.\nMoving toward reactivity, let’s say I only want dots where Sepal_Length > 5 and < 6\nI don’t quite seem to know the filter syntax. Not entirely sure what the d=> means. Seems to be an internal reference to the data, but that feels weird and extra. I can get it to work, but doing anything complicated will require more thinking I think. Note that almost all the examples I can find use op.operation and so confused me for a bit thinking I needed op. The op access mathematical operations like abs, round, etc, and here I just need a simple ><.\n\nsl_filter = irorder\n  .filter(d => (d.Sepal_Length < 6 & d.Sepal_Length > 5))\n  \nsl_filter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow use that in a plot\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(sl_filter, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nThat seems to work"
  },
  {
    "objectID": "observable/trying_observable.html#reactivity",
    "href": "observable/trying_observable.html#reactivity",
    "title": "Trying observablejs chunks",
    "section": "Reactivity",
    "text": "Reactivity\nThe thing here is to use viewof and Inputs.typeofinput. But what are those types? The observable docs seem to have a good overview.\nLet’s replicate the above, but also allow selecting the species. Basically following the quarto docs, but with a couple modifications. There’s got to be a way to obtain the ranges, species names, etc in code and not hardcode them in.\n\nviewof min_sl = Inputs.range(\n  [4, 8],\n  {value: 5, step: 0.1, label: \"Min Sepal Length:\"}\n)\n\nviewof max_sl = Inputs.range(\n  [4, 8],\n  {value: 6, step: 0.1, label: \"Max Sepal Length:\"}\n)\n\nviewof sp = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI like the tabset thing they do in the help, but to keep it simple just make the plot.\nI’m going to filter the data in its own chunk to try to aid figuring this out.\nThe .params here is needed to use the reactive values, and then gets referenced as $, while the data is d.\n\nspfilter = irorder\n  .params({\n  spf: sp\n})\n  .filter((d, $) => op.includes($.spf, d.Species))\n  \nspfilter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsizefilter = spfilter\n  .params({\n  minsl: min_sl,\n  maxsl: max_sl\n})\n  .filter((d, $) => d.Sepal_Length > $.minsl && d.Sepal_Length < $.maxsl)\n  \nsizefilter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd plot\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(sizefilter, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nThat seems to work. Can I package it up pretty like in the example?"
  },
  {
    "objectID": "observable/trying_observable.html#making-better-ux",
    "href": "observable/trying_observable.html#making-better-ux",
    "title": "Trying observablejs chunks",
    "section": "Making better UX",
    "text": "Making better UX\nLet’s build that same thing, but at least kill off displaying code. We need to use different names here because ojs is reactive and so you can’t define variables in two places. Maybe I’ll just use petals instead of sepals.\n\nviewof min_pl = Inputs.range(\n  [1, 7],\n  {value: 5, step: 0.1, label: \"Min Petal Length:\"}\n)\n\nviewof max_pl = Inputs.range(\n  [1, 7],\n  {value: 6, step: 0.1, label: \"Max Petal Length:\"}\n)\n\nviewof sp2 = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [\"versicolor\"],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspfilter2 = irorder\n  .orderby('Petal_Length')\n  .params({\n  spf: sp2\n})\n  .filter((d, $) => op.includes($.spf, d.Species))\n\npetfilter = spfilter2\n  .params({\n  minpl: min_pl,\n  maxpl: max_pl\n})\n  .filter((d, $) => d.Petal_Length > $.minpl && d.Petal_Length < $.maxpl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter2, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\nFancy layouts\nSee the quarto layouts docs for help here, I’ll only try a couple things.\n\nTabset\nMake a tabset with the plot and data\n\nPlotData\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter2, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\npetfilter.view()\n\n\n\n\n\n\n\n\n\n\n\nSidebar panel\nNeed to rename the inputs to avoid double-naming\n\n\nviewof min_pl3 = Inputs.range(\n  [1, 7],\n  {value: 5, step: 0.1, label: \"Min Petal Length:\"}\n)\n\nviewof max_pl3 = Inputs.range(\n  [1, 7],\n  {value: 6, step: 0.1, label: \"Max Petal Length:\"}\n)\n\nviewof sp3 = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [\"versicolor\"],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter3, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter3, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\nspfilter3 = irorder\n  .orderby('Petal_Length')\n  .params({\n  spf: sp3\n})\n  .filter((d, $) => op.includes($.spf, d.Species))\n\npetfilter3 = spfilter3\n  .params({\n  minpl: min_pl3,\n  maxpl: max_pl3\n})\n  .filter((d, $) => d.Petal_Length > $.minpl && d.Petal_Length < $.maxpl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI think I’ll stop there. There’s lots more I could do to get better at using observable, and lots more I could do to integrate that with Quarto layouts, but my goal here was to figure out how to get it to work, and those other things will make more sense with specific use cases or their own quartos or something."
  },
  {
    "objectID": "package/package_creation.html",
    "href": "package/package_creation.html",
    "title": "Creating a package",
    "section": "",
    "text": "I’ve always meant to build packages, but never quite have the time, and often end up with very convoluted projects that are not ideal for shoehorning into a typical package structure, particularly as a first try.\nIn part, I think, that is because my code is often a combination of package-type-things (functions, tests, other software flow) and analyses. It’s unclear what the best approach to this sort of flow is, where we absolutely want functions, but they are very specific to the analyses, of which there are many. Do the analyses go in the package? In two projects, but that’s a hassle? Anyway, that’s a topic for a longer post.\nHere, I have a self-contained, broadly usable bit of code I’m working on to extract information from the Victoria (Australia) waterdata network API. It’s more interesting than a Hello World type package, but also constrained in scope and the analyses can clearly go elsewhere.\nThis doc will be developed as I go, and so like most docs on this site isn’t a tutorial per se, but a sequence of steps, including pitfalls and recoveries (hopefully).\n\n\nFirst, created a repo in git.\nFor the main package development, I’m largely going to follow https://r-pkgs.org/, though I’m hoping I don’t have to read the whole thing (I know I should, but time is time).\nOpened a new Rstudio session (I use renv, but want to adjust some things globally- particularly {devtools}).\ninstall_packages(\"devtools\"), then devtools::dev_sitrep() and install any requested updates (in my case, {roxygen2} was out of date.\n\ndevtools::dev_sitrep()\n\n── R ───────────────────────────────────────────────────────────────────────────\n• version: 4.2.1\n• path: 'C:/Program Files/R/R-4.2.1/'\n\n\n• R is out of date (4.2.1 vs 4.2.2)\n\n\n── Rtools ──────────────────────────────────────────────────────────────────────\n• path: 'C:/rtools42/usr/bin/'\n── devtools ────────────────────────────────────────────────────────────────────\n• version: 2.4.5\n\n\n• devtools or its dependencies out of date:\n  'jsonlite', 'stringr', 'openssl', 'whisker', 'gert'\n  Update them with `devtools::update_packages(\"devtools\")`\n\n\n── dev package ─────────────────────────────────────────────────────────────────\n• package: <unset>\n• path: <unset>\n\n\nR is also out of date (at the time of writing). Fix it with rig, then re-run and update the packages.\n\ndevtools::update_packages('devtools')\n\nCheck the name I used works.\n\navailable::available('vicwater')\n\nWarning: package 'tidytext' was built under R version 4.2.2\n\n\n── vicwater ────────────────────────────────────────────────────────────────────\nName valid: ✔\nAvailable on CRAN: ✔ \nAvailable on Bioconductor: ✔\nAvailable on GitHub:  ✔ \nAbbreviations: http://www.abbreviations.com/vicwater\nWikipedia: https://en.wikipedia.org/wiki/vicwater\nWiktionary: https://en.wiktionary.org/wiki/vicwater\nSentiment:???\n\n\nLooks good.\nQuestion- I typically use Rprojects and renv to manage dependencies and sandbox projects. I also know that I can just devtools::create() (which I think just wraps usethis::create_package(). Can I start with the Rproj and then turn it into a package? Should I want to?\nAnswer- I just needed to read a bit further. Rstudio has devtools and Rprojects working together. So calling usethis::create_package() builds the project and puts all the scaffolding where it needs to be. I’ll need to cross the existing complex Rproj –> package bridge with another project later, but this is fairly straightforward here.\nSo, let’s create the package.\n\nusethis::create_package('~Galen/Documents/vicwater')\n\nAnd that worked with an existing directory. Was kind of worried about that. And it auto-opens a new Rstudio session.\nNow I’m mostly moving over there, but I ran usethis::use_mit_license() to set the license. Looks like description and namespace need work, but do that later.\nLet’s start building.\n\n\n\nI’ve been testing and poking at the API in some qmds here. I expect a lot of that ends up as vignettes in the package, and some is ready to become functions. I’ll likely maintain that flow- test in the qmd, make into functions there, repeat.\nI’m going to go write a function, and then figure out how to use it.\nSwitching to the native pipe |> to see how it goes and reduce dependencies.\n\n\nFor dependencies, I used usethis::use_package(), which installs and auto-populates the DESCRIPTION file. But I think I’m going to try using renv in here too, so I don’t always overwrite system-wide libraries. Hope it doesn’t screw anything up. Usual renv::init().\npackages that are nice to have (e.g. to allow parallelisation) are usethis::use_package('packagename', type = \"suggests\"). And if we want to import a function and not use package::function, use_import_from()- see below for the %dopar%.\n\n\n\nSo, I think usually the thing to do is run devtools::load_all() within the package project. I’m sure I’ll end up doing that. But it is also be possible to run it here, just passing the path, e.g. devtools::load_all(\"path/to/package/dir\"). That lets me work on test and development qmds and scripts here. For a bit. But why? For one, seems like vignettes have to use rmd at least at present. And it keeps all the trial and error out of that repo.\nI got hung up here for a while trying to pre-figure out how I’d install it once it was on github. Turns out it’s super straightforward (see below). It ends up just working as long as the thing on github has basic package structure.\n\n\n\nI’m using roxygen comments, as in the package dev book and roxygen docs for things like inheriting parameters and sections. Running devtools::document() builds the .rd files and means ?function works. There’s a lot of fancy stuff we could do there, but keeping it simple at first.\n\n\n\nI like having actual demonstrations of the code, rather than just function docs, so using usethis::use_vignette to start building some. They have to be in rmd, not qmd. But the visual editor still works, which is nice. Just going to have to re-remember rmd chunk headers.\nI can’t get df_print: paged to work. I think it might be a difference between html and html_vignette, but it is listed as an option in the help. For now using kable even though it’s huge for tables.\nI ended up using the main vignette as an example in the primary github readme. To do that, I did usethis::use_readme_rmd(). Would be good to sort out {pkgdown}, or maybe there’s a streamlined quarto version that builds a website?\n\n\n\nUsing usethis::use_testthat(3) and writing tests was fairly straighforward, but I think there will be a learning curve about what and how to test. I tend to look very granularly at ad-hoc tests, i.e. scanning for weird NA, types, etc. But testthat and the expect_* functions lend themselves to simpler checks.\nIt gets sort of cumbersome if a function takes a while and generates something complex. In that case, I built tests that run the function (and so are fragile to the function just erroring out), and then run multiple different expect_* tests against it to make sure the output is right. As an example,\n\ntest_that(\"derived variables work for ts\", {\n  s3 <- get_response(\"https://data.water.vic.gov.au/cgi/webservice.exe?\",\n                     paramlist = list(\"function\" = 'get_ts_traces',\n                                      \"version\" = \"2\",\n                                      \"params\" = list(\"site_list\" = '233217',\n                                                      \"start_time\" = 20200101,\n                                                      \"varfrom\" = \"100\",\n                                                      \"varto\" = \"140\",\n                                                      \"interval\" = \"day\",\n                                                      \"datasource\" = \"A\",\n                                                      \"end_time\" = 20200105,\n                                                      \"data_type\" = \"mean\",\n                                                      \"multiplier\" = 1)))\n  expect_equal(class(s3), 'list')\n  expect_equal(s3[[1]], 0)\n\n})\n\nAnd then, if I want to hit the function with edge cases, etc, I have to do that over and over. There’s likely a better way, but I’ll need to experiment.\n\n\n\n\nTrying to use %dopar%, but can’t get foreach::%dopar% to work, or with backticks. Putting it in a roxygen comment as @importFrom foreach %dopar% failed too. Seems to have worked to do usethis::use_import_from('foreach', '%dopar%'), which built some new files.\nHaving a hard time testing with doFuture, since it can’t find this package. pause that for a while\n\n\n\nOnce it’s pushed to github, it’s fairly straightforward to install- just\n\ndevtools::install_github(\"galenholt/vicwater\")\n\n\n\n\nIt ended up being pretty straightforward to use devtools::check() and using continuous integration with github to run the checks and put the little badges on, as described in the book.\nIt is easy to end up with funny missing pieces and issues if you forget to run devtools::check() and just push to github followed by devtools::install_github or even more likely if you just devtools::install_local from the directory with the code in it. In general, I think the github actions should take care of the check, but I never seem to get the emails that say it’s happened.\nI often forget these steps. But to actually make the package usable other than with load_all(), we seem to need to devtools::check() and if there’s an rmd readme, knit that.\nThe readme ends up being hard to build when it gets updated without reinstalling the package. I ended up in a weird loop once where I couldn’t build the package with a broken readme, but couldn’t update the readme without package updates. The solution is devtools::build_readme() to install a temp package and build the readme from that, and then devtools::check().\n\n\n\n\n\nI use {renv} for package management and reproducibility, which usually (in a non-package Rproject) puts symlinks to the package in a projdir/renv/library/R-4.x/CPUtype/ directory. But interestingly, in a package project, it puts the symlinked library/R4.x/... in a central location (in my case, ~/AppData/Local/R/cache/R/renv/library/PACKAGENAME-HASH/R4.x/….\n\n\n\nI need to make some standard figure functions as part of the package. To test them, I’ve found the {vdiffr} package. It saves a figure if one doesn’t exist and if one does exist, it checks against the saved version. It seems to work well, the only trick is to remember to usethis::use_package('vdiffr', 'Suggests'), or it won’t be available to use by devtools::check().\n\n\n\nWhen I devtools::check() on a package using dplyr, I get a million errors about ‘no visible binding for global variable ’variable_name’’. The issue is that R CMD CHECK is interpreting the bare variable names in mutate, summarise, etc as variables and can’t find them. The code runs fine, but it’s annoying.\nThe answer, unfortunately, is to use the .data[['variable_name']] or .data$variable_name convention everywhere and usethis::use_import_from('rlang', '.data'). That works to get rid of the errors, but now we’ve lost one of the really nice things about writing dplyr code- the simplicity of bare data variable names."
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html",
    "href": "parallelism/changing_batchtools_template.html",
    "title": "Changing batchtools template",
    "section": "",
    "text": "I got future.batchtools working, and now I have a bunch of follow-up tests.\nCan we call a different template, even if it’s not named batchtools.slurm.tmpl?\nIn my slurm testing repo, I have the default templates from future.batchtools and batchtools saved in /batchtools_templates. The one from future.batchtools is also saved as batchtools.slurm.tmpl in the outer directory (where it gets found by default and we know it works)."
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html#modify-path",
    "href": "parallelism/changing_batchtools_template.html#modify-path",
    "title": "Changing batchtools template",
    "section": "Modify path",
    "text": "Modify path\nFirst, I’ll just send a path, as in plan(batchtools_slurm, template = \"/path/to/batchtools.slurm.tmpl\") to the original template from future.batchtools that I copied to make /batchtools.slurm.tmpl.\nAnd then I’ll use the batchtools template and see how that works.\nTo do both of these, I’ll modify slurm_r_tests/testing_future_batchtools.R to take the path as an argument so I can pass it from the command line.\nI could loop over that like I did when testing single-node plans but I think I don’t for the moment.\nCheck that the default works (no path argument at command line, sbatch batchtools_R.sh testing_future_batchtools.R. That works.\nThe same template but with a different name and in a subdirectory works if we’re careful with the path-\nsbatch batchtools_R.sh testing_future_batchtools.R ./batchtools_templates/slur\nm.tmpl\nThe template that comes from batchtools doesn’t work.\nsbatch batchtools_R.sh testing_future_batchtools.R ./batchtools_templates/slur\nm-simple.tmpl\nWhy not? I think because it doesn’t actually specify any resources. It expects that to come in from elsewhere.\nSo, let’s figure out resource specification, and then try that one again."
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html#specifying-resources",
    "href": "parallelism/changing_batchtools_template.html#specifying-resources",
    "title": "Changing batchtools template",
    "section": "Specifying resources",
    "text": "Specifying resources\nRemember, this is per job.\nWhat I want to do is use tweak- e.g. tweak(batchtools_slurm, resources = list(ncpus = 10, nodes = 2)). Again, remembering this is what gets requested for every job-\n\nso does it ever make sense to request nodes > 1?\nShould I only request ncpus > 1 if I use list-plans to then go to multisession or multicore?\n\nBut first, does that let me use the slurm-simple template that didn’t request anything itself?\nI’ve done it by hardcoding some resource requests in tweak_resources.R in the test repo, but they could be build in a script if we wanted.\nDidn’t work. But I think the answer is in th %< resources$whatever >% - THOSE ARE THE NAMES. AND I CAN CHANGE WHAT THE LIST RETURNS- character instead of integer, for ex.\nSo the slurm-simple.tmpl from batchtools has\n#SBATCH --job-name=<%= job.name %>\n#SBATCH --output=<%= log.file %>\n#SBATCH --error=<%= log.file %>\n#SBATCH --time=<%= ceiling(resources$walltime / 60) %>\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=<%= resources$ncpus %>\n#SBATCH --mem-per-cpu=<%= resources$memory %>\nAnd so only lets us set those values (job-name, output, error, time, ntasks, cpus-per-task, and mem-per-cpu), and we have to do that in the right slots of resources and with the right type- e.g. resources$walltime sets --time, and has to be numeric. BUT, we could change that with a different template file that just uses the character vector “hh:mm:ss” (as I do with the non-template any_R.sh. Or passing things like 4GB instead of memory in mb.\nNow, if I run that with resources$ncpus = 12, I get the same output as before. But I think I’m using 100 cpus, but each one is also sitting on 11 others. I’m just not saving what I need to check. The ntasks instead of nodes is a bit confusing too- I thought tasks were threads on the cpu. Maybe that is the case- the hardcode nthreads = 1 here says don’t thread below the cpu level. And no node request I assume just defaults to 1.\nOR if we aren’t defining nodes, does slurm just auto-assign work to cpus across nodes? ie node-agnostic? And then we don’t have to worry about necessarily matching work to CPUs on nodes?\nI think the future.batchtools template batchtools_templates/slurm.tmpl is more flexible. Instead of individually filling parts of the slurm script as above, it just fills whatever options we want. It has the minimal set to get things to run hardcoded, but the section\n## Resources needed:\n<% if (length(resources) > 0) {\n  opts <- unlist(resources, use.names = TRUE)\n  opts <- sprintf(\"--%s=%s\", names(opts), opts)\n  opts <- paste(opts, collapse = \" \") %>\n#SBATCH <%= opts %>\n<% } %>\nJust writes in anything. So, can I get that to work in tweak_resources.R? It should be easier, but wasn’t working for me."
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html#tweak-has-to-match-the-template",
    "href": "parallelism/changing_batchtools_template.html#tweak-has-to-match-the-template",
    "title": "Changing batchtools template",
    "section": "Tweak has to match the template!",
    "text": "Tweak has to match the template!\nSo, that means the way tweak(batchtools_slurm, resources = …) works is template-specific. Some might not have parsing for what gets passed, sometimes it might be the wrong type, etc).\n\nExamples\nplan(tweak(batchtools_slurm,\n           template = \"./batchtools_templates/slurm-simple.tmpl\",\n           resources = list(ncpus = 12,\n                            memory = 1000,\n                            walltime=60*5)))\nFor the slurm-simple.tmpl, the SLURM --time is referenced to resources$walltime and gets divided by 60 so has to be a numeric in seconds.\nThe ncpus = 12 here gets 12 CPUs that all get assigned (kinda weirdly though, with --cpus-per-task), even though we only use one- see the top of the output\n## Nodes and pids\n# A tibble: 100 × 6\n# Groups:   all_job_nodes, node, pid, taskid [100]\n    all_job_nodes node             pid taskid cpus_avail n_reps\n    <chr>         <chr>          <int> <chr>  <chr>       <int>\n  1 gandalf-vm02  gandalf-vm02 3329413 0      12              6\n  2 gandalf-vm02  gandalf-vm02 3329479 0      12              7\n  3 gandalf-vm02  gandalf-vm02 3329545 0      12              7\n  4 gandalf-vm02  gandalf-vm02 3329611 0      12              6\n  5 gandalf-vm02  gandalf-vm02 3329679 0      12              7\n  6 gandalf-vm02  gandalf-vm02 3329747 0      12              6\n  7 gandalf-vm02  gandalf-vm02 3329811 0      12              6\nWhereas cpus_avail is 1 without that line-\n## Nodes and pids\n# A tibble: 100 × 6\n# Groups:   all_job_nodes, node, pid, taskid [100]\n    all_job_nodes node             pid taskid cpus_avail n_reps\n    <chr>         <chr>          <int> <chr>  <chr>       <int>\n  1 gandalf-vm01  gandalf-vm01   12812 0      1               6\n  2 gandalf-vm01  gandalf-vm01   12881 0      1               6\n  3 gandalf-vm01  gandalf-vm01   12966 0      1               6\n  4 gandalf-vm01  gandalf-vm01   13043 0      1               6\n  5 gandalf-vm01  gandalf-vm01   13117 0      1               6\n  6 gandalf-vm01  gandalf-vm01   13179 0      1               6\n  7 gandalf-vm01  gandalf-vm01   13261 0      1               6\nIt looks like a major catch here is I can’t use names in the resources list with dashes, e.g. ntasks-per-node. And so to set those i’ll have to translate, as in slurm-simple, I think. Unless batchtools auto-translates under the hood, but I think not.\nGoing to have to come back to this. It’d be nice if it were possible. How did that github issue do it? It used ncpus in slurm-simple.tmpl. So maybe I’ll just do that for now. Then I can get on with checking the use of chunks and arrays and nodes and nesting. And whether i can use the ncpus thing to get (and use) more cpus than exist on single nodes.\nDoes that mean I can auto-generate jobnames???? That would be great"
  },
  {
    "objectID": "parallelism/conditional_plans.html",
    "href": "parallelism/conditional_plans.html",
    "title": "Conditional futures",
    "section": "",
    "text": "library(future)\n\nIt’s likely that we want to write code that just runs either locally or on an HPC without having to change a bunch of things inside the relevant scripts. This is useful for local prototyping, as well as just sometimes needing to run things locally.\nThe portability of {futures} is a major benefit for this reason- futures should all work the same, no matter what plan is used to resolve them. So, we can write code that runs both locally and on the HPC by changing the plan. And we can automate this process by making the plan conditional.\nDifferent HPCs define themselves differently, but in the simple case where the code only runs on Linux on an HPC, something like\n\nif (grepl('^Windows', Sys.info()[\"sysname\"])) {\n  inpath <- file.path('path', 'to', 'local', 'inputs')\n  outpath <- file.path('path', 'to', 'local', 'outputs')\n  plan(multisession)\n}\n\nif (grepl('^Linux', Sys.info()[\"sysname\"])) {\n  inpath <- file.path('path', 'to', 'HPC', 'inputs')\n  outpath <- file.path('path', 'to', 'HPC', 'outputs')\n  plan(list(tweak(batchtools_slurm,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 5,\n                                 ntasks.per.node = 12, \n                                 mem = 1000)),\n          multicore))\n}\n\nObviously if you run on Linux that’s NOT an HPC, that second if needs to ask about something else. Sys.info()$user and login are often the same, but I’ve had success with something like\n\nif (grepl('^nameofcluster', Sys.info()[\"nodename\"]) | \n    grepl('^c', Sys.info()[\"nodename\"])) {\n  inpath <- file.path('path', 'to', 'HPC', 'inputs')\n  outpath <- file.path('path', 'to', 'HPC', 'outputs')\n  plan(list(tweak(batchtools_slurm,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 5,\n                                 ntasks.per.node = 12, \n                                 mem = 1000)),\n          multicore))\n}\n\nwhere that second grepl is because the working nodes have a different nodename than the login node, which tends to have the same name as the whole HPC. But I have encountered HPCs where there a many different formats for that nodename, so there’s just some trial and error involved, often involving some jobs like those in slurm_r_tests that query and return HPC resources used, including the names of the nodes."
  },
  {
    "objectID": "parallelism/globals_speed.html",
    "href": "parallelism/globals_speed.html",
    "title": "Foreach globals and speed",
    "section": "",
    "text": "I previously tested the impact of unused globals on speed, but only briefly. Here, I’ll be more systematic, because it gets tricky fast if we need to be super careful about what objects exist in the global environment.\nThere are a couple things to check here\nI’ll tackle these by"
  },
  {
    "objectID": "parallelism/globals_speed.html#nothing-exists",
    "href": "parallelism/globals_speed.html#nothing-exists",
    "title": "Foreach globals and speed",
    "section": "Nothing exists",
    "text": "Nothing exists\nWell, almost nothing. I’m going to set a couple scalars and define a function for furrr and future.apply . I’m not using any of the globals or export arguments in the functions.\n\nBare\n\nn_reps = 100\nsize <- 1000\n\nfn_to_call <- function(rep, size) {\n    a <- rnorm(size, mean = rep)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n\n\nBenchmark\n\nmicrobenchmark(\n  dofut0 = {foreach(i = 1:n_reps, \n                       .combine = cbind) %dorng% {\n    a <- rnorm(size, mean = i)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  furr0 = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE))},\n  \n  fuapply0 = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE)},\n  times = 10\n)\n\nUnit: milliseconds\n     expr      min       lq     mean   median       uq       max neval\n   dofut0 666.6571 694.0119 845.0106 718.1011 738.6922 2018.6689    10\n    furr0 696.5679 716.0437 918.4407 727.5611 745.7060 2626.3897    10\n fuapply0 672.0328 708.2940 728.8155 726.5226 762.0094  771.1695    10\n\n\nSo, doFuture and furrr are slower than future.apply, but not by a ton. The key thing here is this sets the baseline, so we can see if things slow down once we have big objects in memory.\n\n\n\nInside a function\nThese functions are from testing parallel speed, though they have different names here. I’ve added the ability to change the way they handle globals so I don’t have to write new functions for comparing that later, with the default set at the function default.\n\nforeach\n\nforeach_fun <- function(n_reps = 100, size = 1000, .export = NULL, .noexport = NULL) {\n  c_foreach <- foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .export = .export,\n                       .noexport = .noexport) %dorng% {\n    a <- rnorm(size, mean = i)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  return(c_foreach)\n}\n\n\n\nfurrrr\n\nfurrr_fun <- function(n_reps = 100, size = 1000, globals = TRUE) {\n  fn_to_call <- function(rep, size) {\n    a <- rnorm(size, mean = rep)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  \n  c_map <- future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, \n                                               globals = globals))\n  matrix(unlist(c_map), ncol = n_reps)\n}\n\n\n\nfuture.apply\n\nfuapply_fun <- function(n_reps = 100, size = 1000, future.globals = TRUE) {\n    fn_to_call <- function(rep, size) {\n    a <- rnorm(size, mean = rep)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n    }\n    \n  c_apply <- future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE,\n                           future.globals = future.globals)\n  \n    matrix(unlist(c_apply), ncol = n_reps)\n}\n\n\n\nBenchmark\n\nmicrobenchmark(\n  dofut_fun = foreach_fun(n_reps = 100, size = 1000),\n  fur_fun = furrr_fun(n_reps = 100, size = 1000),\n  app_fun = fuapply_fun(n_reps = 100, size = 1000),\n  times = 10\n)\n\nUnit: milliseconds\n      expr      min       lq     mean   median       uq      max neval\n dofut_fun 688.9580 715.9357 725.7951 725.2115 742.3267 745.3953    10\n   fur_fun 717.4536 727.3583 745.8955 740.4063 769.3066 784.2487    10\n   app_fun 720.7231 741.9849 757.3939 751.2876 762.0890 831.7847    10\n\n\nThis sets the other baseline before we have big objects in memory, so we can see if things respond differently when used inside a function’s environment vs directly in the global. Now all three functions are basically equivalent."
  },
  {
    "objectID": "parallelism/globals_speed.html#with-big-global",
    "href": "parallelism/globals_speed.html#with-big-global",
    "title": "Foreach globals and speed",
    "section": "With big global",
    "text": "With big global\nDefault future.globals.maxsize is 500MB. Should i increase that, or just try to hit it? I think just try to get just under it.\n\n# This is 1.6GB\n# big_obj <- matrix(rnorm(20000*10000), nrow = 10000)\n# 496 MB\nbig_obj <- matrix(rnorm(10000*6200), nrow = 10000)\n\nNow, same tests as before, and some that reference it but don’t use it.\nThe comparisons to make here are:\n\nMatched to above- does just having the object exist slow things down, even if not called?\nReferenced and not- does it only get passed in if asked for and slow things down?\n\nNot exactly sure how I’ll check that. Maybe instead of referencing it in the function (which is hard to do without using it, especially with furrr and future.apply), I’ll explicitly send it in with their globals arguments.\n\n\n\nBare\n\nBenchmark\nI’m going to run this for default (no global argument), explicitly sending them in, and explicitly excluding them.\n\nmicrobenchmark(\n  # default- same as above, but now big_obj exists, but is not used in the actual processing\n  dofut0 = {foreach(i = 1:n_reps, \n                       .combine = cbind) %dorng% {\n    a <- rnorm(size, mean = i)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  furr0 = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE))},\n  \n  fuapply0 = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE)},\n  \n  # Explicitly telling it not to send big global (I can't sort out getting .export to work)\n  dofut_no_g = {foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .noexport = \"big_obj\") %dorng% {\n    a <- rnorm(size, mean = i)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  \n  furr_no_g = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, \n                                               globals = FALSE))},\n  \n  fuapply_no_g = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE,\n                           future.globals = FALSE)},\n  \n  # Explicitly telling it to send the unused global\n  dofut_g = {foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .export = 'big_obj') %dorng% {\n    a <- rnorm(size, mean = i)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  \n  furr_g = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, \n                                               globals = 'big_obj'))},\n  \n  fuapply_g = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE,\n                           future.globals = 'big_obj')},\n  \n  times = 10\n)\n\nUnit: milliseconds\n         expr       min        lq      mean    median        uq       max neval\n       dofut0  622.3521  627.1805  684.9883  703.2614  712.3879  771.1737    10\n        furr0  617.5179  651.7789  691.6123  694.2537  735.1046  776.9627    10\n     fuapply0  634.4952  641.2787  682.2679  683.2625  710.3911  742.8530    10\n   dofut_no_g  608.3405  663.3900  682.8678  669.4480  717.3878  749.9284    10\n    furr_no_g  621.6507  643.7371  673.7533  667.9665  703.6524  724.5566    10\n fuapply_no_g  622.4711  652.5555  679.7137  669.7782  690.2530  762.7595    10\n      dofut_g 7463.9133 7746.4635 7956.0593 7915.0786 8241.5531 8473.0075    10\n       furr_g 7391.4087 7488.0329 7746.6185 7685.3700 7854.3536 8301.3615    10\n    fuapply_g 7419.2943 7690.8532 7948.7242 7942.7046 8197.5952 8526.3078    10\n\n\nNow there’s a big object sitting in global memory, but it does not slow down the default run relative to the enforced-non-pass version or the version from before it existed (above). It does show major slowdown when it is explicitly passed.\nUnused globals therefore are NOT passed by default, even when code is running straight in the global environment.\n\n\n\nInside functions\nThe functions have an option to change the way globals are handled.\n\nBenchmark\n\nmicrobenchmark(\n  # default\n  dofut_default = foreach_fun(n_reps = 100, size = 1000),\n  fur_default = furrr_fun(n_reps = 100, size = 1000),\n  app_default = fuapply_fun(n_reps = 100, size = 1000),\n  \n  # No globals\n  dofut_no_g = foreach_fun(n_reps = 100, size = 1000, .noexport = 'big_obj'),\n  fur_no_g = furrr_fun(n_reps = 100, size = 1000,\n                          globals = FALSE),\n  app_no_g = fuapply_fun(n_reps = 100, size = 1000,\n                            future.globals = FALSE),\n  \n  # Explicit globals\n  dofut_g = foreach_fun(n_reps = 100, size = 1000,\n                              .export = 'big_obj'),\n  fur_g = furrr_fun(n_reps = 100, size = 1000, \n                          globals = 'big_obj'),\n  app_g = fuapply_fun(n_reps = 100, size = 1000,\n                            future.globals = 'big_obj'),\n  \n  \n  times = 10\n)\n\nUnit: milliseconds\n          expr       min        lq      mean    median        uq       max\n dofut_default  629.1086  654.8935  664.8788  669.1030  676.3397  699.0022\n   fur_default  648.5858  669.6395  689.7485  682.5734  699.5598  746.6292\n   app_default  651.2509  655.6757  667.3066  664.9745  669.3691  713.9691\n    dofut_no_g  623.7273  641.6626  661.3558  668.6618  678.1437  684.8114\n      fur_no_g  632.2700  639.9049  648.2439  643.9862  652.9461  687.4514\n      app_no_g  630.7245  653.2999  672.8701  674.8392  693.8775  712.1876\n       dofut_g 7281.1734 7421.9364 7509.7354 7506.4282 7633.5690 7718.6878\n         fur_g 7242.9499 7329.6440 7432.2407 7441.0199 7537.4374 7638.8029\n         app_g 7250.6937 7398.2939 7575.0305 7490.0815 7632.4715 8491.8494\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nUsing functions yields the same result as before- the big objects sitting in the global environment do not get passed in and slow things down if they aren’t actually used in the functions (or explicitly sent in).\nUnused globals therefore are NOT passed by default into parallelised functions."
  },
  {
    "objectID": "parallelism/hpc_ephemera.html",
    "href": "parallelism/hpc_ephemera.html",
    "title": "HPC ephemera",
    "section": "",
    "text": "To see characteristics of the system (nodes, CPUs, memory, etc) and state:\nsinfo --Node --long"
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#common-workflow",
    "href": "parallelism/hpc_ephemera.html#common-workflow",
    "title": "HPC ephemera",
    "section": "Common workflow:",
    "text": "Common workflow:\ngit clone repo, cd into it\nset up environment with renv - usually just module load R then R for an interactive session\nwhen things break because the R version is too old, it sometimes works to force it to update using {rig}\ndo most dev on local computer, push\ngit pull to HPC- if on branch\ngit pull origin BRANCHNAME\nrun code with sbatch\nsomething breaks\nfix locally, push, pull, re-run\nuse scripts to copy data down"
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#transferring-files",
    "href": "parallelism/hpc_ephemera.html#transferring-files",
    "title": "HPC ephemera",
    "section": "Transferring files",
    "text": "Transferring files\n\nThe best solution- WinSCP\nUnless we want to batch-transfer a lot of stuff automatically, USE WINSCP- it’s way easier, and we can open docs with notepad++, etc.\n\n\nScripting\nI have better ones, but simply, if we are on a local terminal in a directory we want to put the file (or maybe we want it in a subdir)\nscp user@remote.address:~/path/to/file/filename.txt /subdir/filename.txt\nThat’s annoying because we need to start local, and so have an scp terminal running alongside the one that’s sshed. Otherwise we have to treat local as remote from inside the ssh session."
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#running-any-file",
    "href": "parallelism/hpc_ephemera.html#running-any-file",
    "title": "HPC ephemera",
    "section": "Running any file",
    "text": "Running any file\nThe how-tos for using SLURM all have a line that is Rscript filename.R to run that file. But that means we have to have a different shell script for each R script we want to run. THat’s really annoying, especially when prototyping or with a lot of similar R scripts. Instead, we want to build a shell script that takes the R script name as input in the sbatch call. E.g.\nsbatch shellname.sh\nWith the R script hardcoded in sh.\nInstead, we want\nsbatch shellname.sh rname.R\nthat can take an arbitrary R script.\nThis then will get to other arguments, but this is the first step and super useful.\nIt’s relatively easy- just use Rscript $1 in the shell script, and then the command above works."
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#arguments-in-sbatch",
    "href": "parallelism/hpc_ephemera.html#arguments-in-sbatch",
    "title": "HPC ephemera",
    "section": "Arguments in sbatch",
    "text": "Arguments in sbatch\nSometimes we want to pass arguments to R scripts in the sbatch call so we don’t need fully-new Rscripts just to change a parameter value. In that case, we can pass the args, and they’re available in R with commandArgs(). There are a couple things to be aware of to use this. Primarily, they boil down to The shell SLURM script must match the argument extraction in the R script- ie they need to know the argument order and meaning.\n\nSlots for them MUST be available in the slurm script. e.g.\nRscript $1 $2\nwill work to pass sbatch any_R.sh rscriptname.R argument1 in, with rscriptname.R being $1(the first argument to the slurm script) and argument1 being $2. (The first additional argument, intended for R). If you send sbatch any_R.sh rscriptname.R argument1 argument2 in to a slurm script as above, argument2 will just disappear into the ether.\nAlternatively, the slurm script itself can define arguments-\nRscript $1 \"argument1\" \"argument2\"\nmakes whatever $1 is coming in from command line, as well as \"argument1\" and \"argument2\" available in R via commandArgs()\nWe can use arbitrary numbers of arguments as well with $* , which accesses all the arguments.\nRscript $*\nNote that now we’ve dropped the $1- it’s included as the first item in the $* . This is a bit dangerous- we need to make sure we use the right order, but it is flexible.\nAccessing the arguments is tricky- they are numbered in order, but there are some initial invisible ones. It seems, but may not always be true, that the first four are set, with the fourth being --file=filename.R for the file called by Rscript, then 5 is --args and the subsequent args are 6-n.\nNumbers come through as characters\n${SLURM_ARRAY_TASK_ID} is a particularly useful variable to include, as it lets us manually divide tasks among a slurm array."
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#naming-jobs",
    "href": "parallelism/hpc_ephemera.html#naming-jobs",
    "title": "HPC ephemera",
    "section": "Naming jobs",
    "text": "Naming jobs\nAnd especially putting the name on the stdout and err. Creating a new dir for them would be even nicer, but I’ll leave that for later.\nthe produced stdout and err get hard to find after a lot of jobs. If we use %x in their names in the slurm script, it appends the jobname\nThe jobname by default is the name of the shell script, e.g. sbatch shellscript.sh has jobname “shellscript.sh”. That’s actually pretty useful if we have individual shell scripts. But if we’re using a script that takes R script names as arguments, it’s not. In that case, we can set the job with --job-name or -J flags, e.g. sbatch -J test_job shellscript.sh.\nNote that the jobname flag has to happen before the script, and does NOT affect the args returned by commandArgs() (thankfully)."
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#testing-future-plans",
    "href": "parallelism/hpc_ephemera.html#testing-future-plans",
    "title": "HPC ephemera",
    "section": "Testing future plans",
    "text": "Testing future plans\nplan(\"list\") tells us what the plan is. This is super helpful for checking what’s going on.\n\nlibrary(future)\nplan(multisession)\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(multisession)"
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html",
    "href": "parallelism/initial_future_batchtools.html",
    "title": "Using future.batchtools",
    "section": "",
    "text": "I need to sort out how to use futures for parallel processing on the HPC.There’s a few things I’ve tried previously that didn’t work, and a way I’ve cobbled together that’s not ideal.\nThere are some issues with my current approach"
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#what-do-i-want",
    "href": "parallelism/initial_future_batchtools.html#what-do-i-want",
    "title": "Using future.batchtools",
    "section": "What do I want?",
    "text": "What do I want?\nThere are some improvements I want to make that will make my code work better (and faster)\n\nMinor (if any) changes when running locally or on an HPC\nIn-code splitting of work into nodes to balance the work across them\nMake sure we’re using both nodes and cores within them\n\nThe main solution seems to be using future.batchtools, but I wasn’t able to get it working quickly.\n\nSome questions\nThere are a few big-picture things I have questions about that aren’t clear from reading the docs (in addition to just ‘how do we get a run to work’)\n\nHow do jobs actually start? I think, but am not 100% sure, that the R script essentailly builds slurm bash scripts and then calls sbatch. Is that what happens?\nDo we still use sbatch or other command-line bash at all? Or is everything managed in R? If so, how do we actually start the runs? Rscript? srun on an R control script?\n\nif Rscript, do we end up with that main R process running in the login node the whole time? What about if srun or…\nThis issue implies we need to leave R running. But can it run with Rscript? srun?sinteractive? sbatch any_R.sh analysis_script.R? With any_R.sh having low resources but maybe long walltime? It looks like srun barfs to the terminal and blocks, while sbatch outputs to file and is non-blocking. So that’s likely the way to go.\n\nDoes it make sense to manage nodes and cores separately, or do we just ask for a ton of cores and it auto-manages nodes to get them?\n\nI think {slurmR} with plan(cluster) does the latter, but not positive\nI’m not actually sure what future.batchtools does by default (will check), but I think a list-plan likely makes sense.\n\ndo we use SLURM job arrays? Or does it generate a bunch of batch scripts that get called as separate jobs instead of array jobs? Does it matter?\n\nwhat are chunks.\n\nIf we feed it a big set of iterations, does it send each one to its own node? Its own core? Is there any chunking?"
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#templates",
    "href": "parallelism/initial_future_batchtools.html#templates",
    "title": "Using future.batchtools",
    "section": "Templates",
    "text": "Templates\nI’m still a bit confused by the overall workflow, but it’s clear I need a template. There’s one in future.batchtools github, and a few at the batchtools github.\nThen I think in the plan call, we tweak that? Let’s just get it working. Trying first with the one that comes from future.batchtools. Though the one from batchtools looks like it has more capability for doing things like managing cores on nodes. Maybe try them both as we go?\nI have both of those. I kind of want to test both. The docs say the template should be either at ./batchtools.slurm.tmpl (associated with a particular working directory) or ~/.batchtools.slurm.tmpl (for all processes to find it). But I want to be able to test multiple templates. I should be able to use\nplan(batchtools_slurm, template = \"/path/to/batchtools.slurm.tmpl\")\nbut its a bit unclear whether the templates still need to be named batchtools.slurm.tmpl, or can have whatever filename we want, as long as I give the path. Guess I’ll test that. Try first with it as batchtools.slurm.tmpl in the repo directory first though."
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#try-a-simple-job",
    "href": "parallelism/initial_future_batchtools.html#try-a-simple-job",
    "title": "Using future.batchtools",
    "section": "Try a simple job",
    "text": "Try a simple job\nUse the future.batchtools template, and a foreach loop using %:%.\nStart with Rscript filename.R\nIt starts printing directly to terminal, which is annoying.\nOpening another terminal and typing squeue shows that I have 4 nodes- though actually that was just at that moment.\nIt doesn’t seem to produce stdout or stderr, which is going to make it tricky to see what happened. See below- this is true unless we run the master R session through sbatch.\nI can copy in from the terminal output:\n::: {#Simple output} Loading required package: foreach Loading required package: future Warning message: package ‘future’ was built under R version 4.0.5 Loading required package: parallelly Warning messages: 1: package ‘future.batchtools’ was built under R version 4.0.5 2: package ‘parallelly’ was built under R version 4.0.5\nPlan is: List of future strategies: 1. batchtools_slurm: - args: function (expr, envir = parent.frame(), substitute = TRUE, globals = TRUE, label = NULL, template = NULL, resources = list(), workers = NULL, registry = list(), …) - tweaked: FALSE - call: plan(batchtools_slurm)\n\navailable workers:\nlocalhost localhost localhost localhost localhost localhost localhost localhost\n\n\ntotal workers:\n8\n\n\nunique workers:\nlocalhost\n\n\navailable Cores:\n\nnon-slurm\n8\n\n\nslurm method\n1\n\n\n\nMain PID:\n3195570 There were 50 or more warnings (use warnings() to see the first 50)\n\n\nUnique processes\n100\nIDs of all cores used\n238059 1524768 1518289 1500375 1513942 3264114 1269189 1345061 2623254 238128 3264182 1269259 1345124 1524852 1518372 3264251 238197 1500450 1514026 1524930 1518447 238268 3264327 1500525 1514101 238335 1525010 1518527 1500596 1514176 3264408 238401 1525084 1518604 1500666 1514254 3264478 1518681 238466 1525164 1500739 1518756 1514328 238530 1500811 1525241 1514402 1518828 1500879 238594 1525316 1514479 3264562 1518901 1500952 238662 1525391 1518977 1514555 1501019 3264640 238727 1525467 1519049 1514630 238791 1501090 1525542 3264715 1519121 1501158 238857 1525621 1519194 1514708 1501225 3264796 238922 1525693 1519271 1514786 1501292 3264863 1525766 238989 1519345 1514861 1525839 1501361 239060 3264939 1519420 1514936 1501428 1525912 239129 3265003 1519494 1501496 1525987"
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#nodes-and-pids",
    "href": "parallelism/initial_future_batchtools.html#nodes-and-pids",
    "title": "Using future.batchtools",
    "section": "Nodes and pids",
    "text": "Nodes and pids\n           238059 238128 238197 238268 238335 238401 238466 238530 238594\nbilbo 6 6 6 6 6 6 7 6 7 frodo-vs01 0 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0 0\n           238662 238727 238791 238857 238922 238989 239060 239129 1269189\nbilbo 6 7 7 6 7 6 7 6 0 frodo-vs01 0 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 7 gandalf-vm04 0 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0 0\n           1269259 1345061 1345124 1500375 1500450 1500525 1500596 1500666\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 6 6 6 6 7 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 6 0 0 0 0 0 0 0 gandalf-vm04 0 6 6 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1500739 1500811 1500879 1500952 1501019 1501090 1501158 1501225\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 6 6 6 6 6 6 6 6 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1501292 1501361 1501428 1501496 1513942 1514026 1514101 1514176\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 7 6 7 6 0 0 0 0 frodo-vs04 0 0 0 0 6 7 6 6 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1514254 1514328 1514402 1514479 1514555 1514630 1514708 1514786\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 6 7 7 6 6 6 6 6 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1514861 1514936 1518289 1518372 1518447 1518527 1518604 1518681\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 7 7 6 6 6 6 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 6 6 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1518756 1518828 1518901 1518977 1519049 1519121 1519194 1519271\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 6 6 7 7 6 7 7 6 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1519345 1519420 1519494 1524768 1524852 1524930 1525010 1525084\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 6 6 6 7 6 frodo-vs02 7 6 7 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1525164 1525241 1525316 1525391 1525467 1525542 1525621 1525693\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 6 6 6 6 6 6 6 6 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1525766 1525839 1525912 1525987 2623254 3264114 3264182 3264251\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 6 6 6 6 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 6 7 6 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 6 0 0 0\n           3264327 3264408 3264478 3264562 3264640 3264715 3264796 3264863\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 7 7 6 6 6 6 6 6 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           3264939 3265003\nbilbo 0 0 frodo-vs01 0 0 frodo-vs02 0 0 frodo-vs03 0 0 frodo-vs04 0 0 gandalf-vm02 6 6 gandalf-vm03 0 0 gandalf-vm04 0 0 gandalf-vm05 0 0 :::\nThe formatting of the table is all boogered up, but that looks like I got 9 nodes, and used something like 9-14 PIDs each, each about 7-6 times. It’s a bit confusing, because sinfo --Node --long shows that the number of CPUs is different across those nodes. And that seems to be what happened here. I need to change the output so I can better see what I used. But, roughly, that looks about right- it send jobs to CPUs as needed.\n\n# number of iterations\n25*25\n\n[1] 625\n\n9*12*6\n\n[1] 648"
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#calling-and-output",
    "href": "parallelism/initial_future_batchtools.html#calling-and-output",
    "title": "Using future.batchtools",
    "section": "Calling and output",
    "text": "Calling and output\nWe seem to need to leave an R session running to operate the futures. We can probably do that interactively module load R, R, then run the code, or Rscript. But that’s all interactive. As is srun. And it all prints all printable output to the terminal, and doesn’t save it. I think an sbatch might be the way to go. Try that.\nThat does run, and spawn the others\n\nIt’s silly that we need another wrapper call, but maybe that’s ok. Gives us the option to do some auto-setup in an intermediate script.\nrepeatedly calling squeue shows something interesting- it looks like it’s creating separate jobs for each task, not doing anything node-aware (e.g. chunking jobs to a node, and then multisessioning). I guess that makes sense, based on the help and other testing, but we can almost certainly speed things up by chunking and list-planning.\nIf the passing-object overhead really becomes an issue, we might want to just use batchtools directly, which I think also could remove the need to leave a master R session running. But I think that gets rid of the major advantage of being able to use essentially the same code locally and on the HPC, just by changing the plan.\nThe output using sbatch batchtools_R.sh testing_future_batchtools.R (where batchtools_R.sh is a lightweight master job) is\nℹ Using R 4.0.3 (lockfile was generated with R 4.2.2)\n\n Plan is:\nList of future strategies:\n1. batchtools_slurm:\n   - args: function (expr, envir = parent.frame(), substitute = TRUE, globals = TRUE, label = NULL, template = NULL, resources = list(), workers = NULL, registry = list(), ...)\n   - tweaked: FALSE\n   - call: plan(batchtools_slurm)\n\n### available workers:\ngandalf-vm01\n\n\n### total workers:\n1\n\n### unique workers:\ngandalf-vm01\n\n### available Cores:\n\n#### non-slurm\n1\n\n#### slurm method\n1\n\n### Main PID:\n4135554\n\n### Unique processes\n100\n\nIDs of all cores used\n\n241089\n1528094\n1522003\n1503388\n1516888\n3269690\n1522077\n241160\n1528170\n1503459\n1516963\n3269759\n1522156\n1528243\n241230\n3269825\n1271878\n1528317\n1522235\n241297\n1503531\n1528392\n1517044\n1522308\n241367\n1503606\n1528471\n1522380\n1517120\n241436\n3269894\n1528548\n1522456\n1503681\n1517195\n241505\n1522531\n1528626\n3269975\n1271968\n1503749\n241573\n1522605\n3270042\n1528703\n1503817\n241641\n1522682\n1528778\n1517275\n3270121\n1272046\n241709\n1528853\n1522758\n1503892\n1517348\n241778\n1528932\n1522829\n1503961\n1517420\n3270199\n241843\n1529005\n1522901\n1504030\n241910\n1529082\n1517500\n3270277\n1522975\n1504098\n241978\n1529157\n1523051\n1504167\n1517581\n3270355\n1529231\n242042\n1523125\n1504238\n1529306\n1517659\n3270422\n242108\n1523197\n1529384\n1504307\n1517731\n3270488\n242173\n1523272\n1529459\n1504376\n1517812\n3270555\n242239\n1529532\n\n## Nodes and pids\n# A tibble: 100 × 3\n# Groups:   node [7]\n    node             pid n_reps\n    <chr>          <int>  <int>\n  1 bilbo         241089      6\n  2 bilbo         241160      6\n  3 bilbo         241230      7\n  4 bilbo         241297      6\n  5 bilbo         241367      6\n  6 bilbo         241436      6\n  7 bilbo         241505      6\n  8 bilbo         241573      6\n  9 bilbo         241641      7\n 10 bilbo         241709      6\n 11 bilbo         241778      7\n 12 bilbo         241843      6\n 13 bilbo         241910      6\n 14 bilbo         241978      7\n 15 bilbo         242042      6\n 16 bilbo         242108      6\n 17 bilbo         242173      6\n 18 bilbo         242239      6\n 19 frodo-vs01   1528094      6\n 20 frodo-vs01   1528170      6\n 21 frodo-vs01   1528243      6\n 22 frodo-vs01   1528317      6\n 23 frodo-vs01   1528392      6\n 24 frodo-vs01   1528471      7\n 25 frodo-vs01   1528548      6\n 26 frodo-vs01   1528626      6\n 27 frodo-vs01   1528703      6\n 28 frodo-vs01   1528778      6\n 29 frodo-vs01   1528853      7\n 30 frodo-vs01   1528932      6\n 31 frodo-vs01   1529005      6\n 32 frodo-vs01   1529082      6\n 33 frodo-vs01   1529157      6\n 34 frodo-vs01   1529231      6\n 35 frodo-vs01   1529306      6\n 36 frodo-vs01   1529384      6\n 37 frodo-vs01   1529459      6\n 38 frodo-vs01   1529532      6\n 39 frodo-vs02   1522003      7\n 40 frodo-vs02   1522077      7\n 41 frodo-vs02   1522156      6\n 42 frodo-vs02   1522235      7\n 43 frodo-vs02   1522308      6\n 44 frodo-vs02   1522380      6\n 45 frodo-vs02   1522456      6\n 46 frodo-vs02   1522531      6\n 47 frodo-vs02   1522605      7\n 48 frodo-vs02   1522682      6\n 49 frodo-vs02   1522758      6\n 50 frodo-vs02   1522829      6\n 51 frodo-vs02   1522901      7\n 52 frodo-vs02   1522975      6\n 53 frodo-vs02   1523051      6\n 54 frodo-vs02   1523125      7\n 55 frodo-vs02   1523197      6\n 56 frodo-vs02   1523272      7\n 57 frodo-vs03   1503388      6\n 58 frodo-vs03   1503459      6\n 59 frodo-vs03   1503531      6\n 60 frodo-vs03   1503606      6\n 61 frodo-vs03   1503681      6\n 62 frodo-vs03   1503749      6\n 63 frodo-vs03   1503817      6\n 64 frodo-vs03   1503892      6\n 65 frodo-vs03   1503961      6\n 66 frodo-vs03   1504030      6\n 67 frodo-vs03   1504098      6\n 68 frodo-vs03   1504167      6\n 69 frodo-vs03   1504238      6\n 70 frodo-vs03   1504307      7\n 71 frodo-vs03   1504376      6\n 72 frodo-vs04   1516888      6\n 73 frodo-vs04   1516963      7\n 74 frodo-vs04   1517044      7\n 75 frodo-vs04   1517120      6\n 76 frodo-vs04   1517195      7\n 77 frodo-vs04   1517275      7\n 78 frodo-vs04   1517348      6\n 79 frodo-vs04   1517420      7\n 80 frodo-vs04   1517500      7\n 81 frodo-vs04   1517581      7\n 82 frodo-vs04   1517659      6\n 83 frodo-vs04   1517731      6\n 84 frodo-vs04   1517812      6\n 85 gandalf-vm02 3269690      6\n 86 gandalf-vm02 3269759      6\n 87 gandalf-vm02 3269825      6\n 88 gandalf-vm02 3269894      7\n 89 gandalf-vm02 3269975      7\n 90 gandalf-vm02 3270042      6\n 91 gandalf-vm02 3270121      6\n 92 gandalf-vm02 3270199      6\n 93 gandalf-vm02 3270277      6\n 94 gandalf-vm02 3270355      6\n 95 gandalf-vm02 3270422      7\n 96 gandalf-vm02 3270488      6\n 97 gandalf-vm02 3270555      7\n 98 gandalf-vm03 1271878      6\n 99 gandalf-vm03 1271968      6\n100 gandalf-vm03 1272046      6\n\nTime taken for code: 185\nSo, we can see the lightweight wrapper uses 1 cpu on one node, but spawns 100 workers, each of which gets used 6-7 times."
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#todo",
    "href": "parallelism/initial_future_batchtools.html#todo",
    "title": "Using future.batchtools",
    "section": "TODO",
    "text": "TODO\nother templates\nmanaging resources\nlist-plans and nesting\nchunks.as.array\nauto-managing plan locally vs HPC\nconsequences of master R session dying (if Rscript, if sbatched)\netc"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html",
    "href": "parallelism/interrogating_parallel.html",
    "title": "Interrogating parallel functions",
    "section": "",
    "text": "In testing parallel functions, especially on new machines, or if we’re trying to get a granular understanding of what they’re doing, we will want to do more than benchmark. We might, for example, want to know what cores and processes they’r using to make sure they’re taking full advantage of resources, or to better understand how resources get divided up, or to better understand the differences between plans.\nI’ve done some basic speed testing of parallel functions as well as some testing of nested functions Here, I’ll build on that to better understand how the workers get divided up in those nested functions, and use that to jump off into testing different plans."
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#packages-and-setup",
    "href": "parallelism/interrogating_parallel.html#packages-and-setup",
    "title": "Interrogating parallel functions",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}, because I tend to like writing for loops (there’s a reason I’ll try to write up sometime later). I test other packages in the {future} family (furrr, future_apply) where I try to better understand when they do and don’t give speed advantages.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nlibrary(foreach)\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tibble)\nlibrary(patchwork)\n\nregisterDoFuture()\nplan(multisession)"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#workers-and-cores",
    "href": "parallelism/interrogating_parallel.html#workers-and-cores",
    "title": "Interrogating parallel functions",
    "section": "Workers and cores",
    "text": "Workers and cores\nWe get assigned workers and cores when we call plan . We can also get the outer process id\nnote on HPC, we need to use availableCores(methods = 'Slurm') .\n\navailableWorkers()\n\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n\navailableCores()\n\nsystem \n    20 \n\nSys.getpid()\n\n[1] 21416"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#the-setup--nested-functions",
    "href": "parallelism/interrogating_parallel.html#the-setup--nested-functions",
    "title": "Interrogating parallel functions",
    "section": "The setup- nested functions",
    "text": "The setup- nested functions\nI’ll use the inner and outer parallel functions similar to my tests of nested functions, but instead of doing anything, they’ll just track the processes.\nWhat do I want to interrogate? The process id, for one- I can use Sys.getpid(). I think I might just skip all the actual processing and just get the IDs\nWhat do I want to check? How the processes get divvied up. Are the inner loops getting different processes? Are the outer, and then the inner all use that one? Does it change through the outer loop? Does it depend on the number of iterations?\n\ninner_par <- function(outer_it, size) {\n  inner_out <- foreach(j = 1:size,\n                       .combine = bind_rows) %dorng% {\n    \n                         thisproc <- tibble(loop = \"inner\",\n                                            outer_iteration = outer_it,\n                                            inner_iteration = j, \n                                            pid = Sys.getpid())\n    # d <- rnorm(size, mean = j)\n    # \n    # f <- matrix(rnorm(size*size), nrow = size)\n    # \n    # g <- d %*% f\n    # \n    # mean(g)\n    \n  }\n}\n\nFor the outer loop, let’s check the PID both before and after the inner loop runs.\n\nouter_par <- function(outer_size, innerfun, inner_size) {\n  outer_out <- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %dorng% {\n                         \n                        outerpre <- tibble(loop = 'outer_pre',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out <- innerfun(outer_it = i, size = inner_size)\n                         \n                         outerpost <- tibble(loop = 'outer_post',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         bind_rows(outerpre, inner_out, outerpost)\n                         \n                         \n                       }\n  \n  return(outer_out)\n}"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#sequential-outer-loop",
    "href": "parallelism/interrogating_parallel.html#sequential-outer-loop",
    "title": "Interrogating parallel functions",
    "section": "sequential outer loop",
    "text": "sequential outer loop\nI assume if we make the outer loop sequential, the inner will then get PIDs\n\nouter_seq <- function(outer_size, innerfun, inner_size) {\n  outer_out <- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %do% {\n                         \n                        outerpre <- tibble(loop = 'outer_pre',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out <- innerfun(outer_it = i, size = inner_size)\n                         \n                         outerpost <- tibble(loop = 'outer_post',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         bind_rows(outerpre, inner_out, outerpost)\n                         \n                         \n                       }\n  \n  return(outer_out)\n}\n\n\ntestseq10 <- outer_seq(outer_size = 10, innerfun = inner_par, inner_size = 10)\n\n\nggplot(testseq10, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0)) +\n  scale_x_continuous(breaks = 0:10) +\n  scale_color_binned(breaks = 0:10)\n\n\n\n\nAt least that- if we have a sequential outer, it lets the inner parallelise."
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#nested-with",
    "href": "parallelism/interrogating_parallel.html#nested-with",
    "title": "Interrogating parallel functions",
    "section": "Nested with %:%",
    "text": "Nested with %:%\nThe ‘proper’ way to nest foreach loops is with %:%. That’s not always possible, but I think we can here, to check what they’re getting.\n\nouter_nest <- function(outer_size, innerfun, inner_size) {\n  outer_out <- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %:% \n    foreach(j = 1:inner_size,\n                       .combine = bind_rows) %dopar% {\n    \n                         thisproc <- tibble(loop = \"\",\n                                            outer_iteration = i,\n                                            inner_iteration = j, \n                                            pid = Sys.getpid())\n                       }\n  \n  return(outer_out)\n}\n\nnote- inner_par isn’t doing anything here, since it’s not a function anymore\n\ntestnest <- outer_nest(10, inner_par, 10)\n\n\nggplot(testnest, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0)) +\n  scale_x_continuous(breaks = 0:10) +\n  scale_color_binned(breaks = 0:10)\n\n\n\n\nNow, each outer loop is getting two PIDs to split up with its inner loop.\nSo that makes sense- this way foreach knows what’s coming and can split up workers. It looks like the outer loop is favored, though that shouldn’t matter when they’re specified this way.\nWe can check though\n\ntestnest50_10 <- outer_nest(50, inner_par, 10)\ntestnest10_50 <- outer_nest(10, inner_par, 50)\n\n\nplotnest50_10 <- ggplot(testnest50_10, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplotnest10_50 <- ggplot(testnest10_50, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplotnest50_10 + plotnest10_50\n\n\n\n\nThose look more similar than they are because of different axes. I think that is giving more PIDs to the outer when it has more iterations."
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#list-plans",
    "href": "parallelism/interrogating_parallel.html#list-plans",
    "title": "Interrogating parallel functions",
    "section": "List-plans",
    "text": "List-plans\n\nNaive- just defaults\nThere is information out there, largely related to using future.batchtools, that a list of plans lets us handle nested futures. Does that work with multisession?\nplan(\"list\") tells us what the plan is. This is super helpful for checking what’s going on.\n\nplan(list(multisession, multisession))\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(list(multisession, multisession))\n2. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(list(multisession, multisession))\n\n\nThen let’s use the same outer_par we tried earlier\nLet’s check an outer loop with 1, and inner with 10, and vice versa\n\ntest1_10_double <- outer_par(outer_size = 1, \n                             innerfun = inner_par, inner_size = 10)\ntest10_1_double <- outer_par(outer_size = 10, \n                             innerfun = inner_par, inner_size = 1)\n\n\nplot1_10_double <- ggplot(test1_10_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot10_1_double <- ggplot(test10_1_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot1_10_double + plot10_1_double\n\n\n\n\nThat didn’t work. BUT, plan(\"list\") shows that the first one uses workers = availableCores(). Does that eat all the cores?\n\n\nTweak outer plan\nIf we limit the outer plan, do the ‘leftover’ cores go to the inner?\n\nplan(list(tweak(multisession, workers = 2), multisession))\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = 2, envir = parent.frame())\n   - tweaked: TRUE\n   - call: plan(list(tweak(multisession, workers = 2), multisession))\n2. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(list(tweak(multisession, workers = 2), multisession))\n\n\nLet’s check an outer loop with 1, and inner with 10, and vice versa\n\ntest1_10_double <- outer_par(outer_size = 1, \n                             innerfun = inner_par, inner_size = 10)\ntest10_1_double <- outer_par(outer_size = 10, \n                             innerfun = inner_par, inner_size = 1)\n\n\nplot1_10_double <- ggplot(test1_10_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot10_1_double <- ggplot(test10_1_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot1_10_double + plot10_1_double\n\n\n\n\nThat restricted the outer, but didn’t give any to the inner. Can I get them there?\n\n\nTweak both plans\nNow, we’re explicitly telling each plan how many workers it gets.\n\nplan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 5)))\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = 2, envir = parent.frame())\n   - tweaked: TRUE\n   - call: plan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 5)))\n2. multisession:\n   - args: function (..., workers = 5, envir = parent.frame())\n   - tweaked: TRUE\n   - call: plan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 5)))\n\n\nLet’s check an outer loop with 1, and inner with 10, and vice versa\n\ntest1_10_double <- outer_par(outer_size = 1, \n                             innerfun = inner_par, inner_size = 10)\ntest10_1_double <- outer_par(outer_size = 10, \n                             innerfun = inner_par, inner_size = 1)\n\n\nplot1_10_double <- ggplot(test1_10_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot10_1_double <- ggplot(test10_1_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot1_10_double + plot10_1_double\n\n\n\n\nThat worked! So, if we explicitly give each plan workers, we can manage nestedness.\nTurn the plan back to multisession\n\nplan(multisession)\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(multisession)"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#inner-loop",
    "href": "parallelism/interrogating_parallel.html#inner-loop",
    "title": "Interrogating parallel functions",
    "section": "Inner loop",
    "text": "Inner loop\n\nParallel version\n\ninner_par <- function(in_vec, size, outer_it) {\n  inner_out <- foreach(j = in_vec,\n                       .combine = bind_rows) %dorng% {\n    d <- rnorm(size, mean = j)\n    \n    f <- matrix(rnorm(size*size), nrow = size)\n    \n    g <- d %*% f\n    \n    h <- mean(g)\n    \n    thisproc <- tibble(loop = \"inner\",\n                       outer_iteration = outer_it,\n                       inner_iteration = j, \n                       pid = Sys.getpid())\n    \n                       }\n  return(inner_out)\n}\n\n\n\nSequential version\n\ninner_seq <- function(in_vec, size, outer_it) {\n  inner_out <- foreach(j = in_vec,\n                       .combine = bind_rows) %do% {\n    d <- rnorm(size, mean = j)\n    \n    f <- matrix(rnorm(size*size), nrow = size)\n    \n    g <- d %*% f\n    \n    h <- mean(g)\n    \n    thisproc <- tibble(loop = \"inner\",\n                       outer_iteration = outer_it,\n                       inner_iteration = j, \n                       pid = Sys.getpid())\n    \n                       }\n  \n  return(inner_out)\n}\n\n\n\nUsing preallocated for\nThis is likely to be faster than the sequential. Preallocate both the vector and the new tibble output.\n\ninner_for <- function(in_vec, size, outer_it) {\n  inner_out <- vector(mode = 'numeric', length = size)\n  \n  thisproc <- tibble(loop = \"inner\",\n                       outer_iteration = outer_it,\n                       inner_iteration = 1:length(in_vec), \n                       pid = Sys.getpid())\n  \n  for(j in 1:length(in_vec)) {\n    d <- rnorm(size, mean = in_vec[j])\n    \n    f <- matrix(rnorm(size*size), nrow = size)\n    \n    g <- d %*% f\n    \n    inner_out[j] <- mean(g)\n    \n    thisproc$pid[j] <- Sys.getpid()\n    thisproc$inner_iteration[j] <- j\n    \n  }\n  \n  return(thisproc)\n}"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#outer-loop",
    "href": "parallelism/interrogating_parallel.html#outer-loop",
    "title": "Interrogating parallel functions",
    "section": "Outer loop",
    "text": "Outer loop\nI cant divide by inner_out now that it’s not a matrix, so just get a cv.\n\nparallel\n\nouter_par <- function(size, innerfun) {\n  outer_out <- foreach(i = 1:size,\n                       .combine = bind_rows) %dorng% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a <- rnorm(size, mean = i)\n                         \n                         b <- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec <- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out <- innerfun(in_vec = cvec, \n                                               size = size, \n                                               outer_it = i)\n                         \n                         h <- sd(cvec)/mean(cvec)\n                         \n                         inner_out\n                         \n                       }\n  \n  return(outer_out)\n}\n\n\n\nsequential\n\nouter_seq <- function(size, innerfun) {\n  outer_out <- foreach(i = 1:size,\n                       .combine = bind_rows) %do% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a <- rnorm(size, mean = i)\n                         \n                         b <- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec <- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out <- innerfun(in_vec = cvec, \n                                               size = size, outer_it = i)\n                         \n                         h <- sd(cvec)/mean(cvec)\n                         \n                         inner_out\n                       }\n  \n  return(outer_out)\n}\n\n\n\nUn-preallocated for\nBecause this would need to replace chnks in the tibble, it’s hard to preallocate. Just don’t bother- the point isn’t speed, it’s testing pids.\n\nouter_for <- function(size, innerfun) {\n  outer_out <- matrix(nrow = size, ncol = size)\n  \n  thisproc <- tibble(loop = \"inner\",\n                       outer_iteration = 1,\n                       inner_iteration = 1, \n                       pid = Sys.getpid(),\n                     .rows = 0)\n  \n  for(i in 1:size) {\n    \n    # Do a matrix mult on a vector specified with i\n    a <- rnorm(size, mean = i)\n    \n    b <- matrix(rnorm(size*size), nrow = size)\n    \n    cvec <- a %*% b\n    \n    # Now iterate over the values in c to do somethign else\n    inner_out <- innerfun(in_vec = cvec, size = size, outer_it = i)\n    \n    outer_out[, i] <- sd(cvec)/mean(cvec)\n    \n    thisproc <- bind_rows(thisproc, inner_out)\n    \n    \n  }\n  outer_out <- c(outer_out) \n  \n  \n  return(thisproc)\n}"
  },
  {
    "objectID": "parallelism/master_persistence.html",
    "href": "parallelism/master_persistence.html",
    "title": "Control process dying or timing out",
    "section": "",
    "text": "The way future.batchtools works there’s a controlling R process that starts the futures and waits for them to return. In typical use, the value() of futures are intended to be used, whereas in some cases, particularly when I use them on the cluster, I’m using them to fire off a bunch of HPC jobs that save output, and don’t care if they return. This is a very similar issue to the discussion here.\nThere are a couple issues this use-case brings up\n\nWhat happens if that master R script that creates the futures dies?\n\nDo we need to tie up a core just to run that? (YES)\nDoes it have to stay in use for the entire period all futures take to finish? (NO- see next)\nWhat if it dies once the jobs have started running vs before they’ve started\n\ne.g. does it create a queue that just runs no matter what? (NO)\nonce a job starts, does it finish no matter what? (YES, unless it times out)\n\n\nDoes it matter if the master runs through sbatch, sinteractive, or just Rscript on the login node? (Probably not, but sbatch is likely safest/most robust).\nShould I actually be using batchtools::submitJobs() instead, as suggested in that github thread? I really like the automatic control of jobs and globals etc in future.batchtools, and the ability to be portable to local computers. (Quite possibly, but that’s for another day, I think. And we’d potentially lose a lot of the advantages of future)."
  },
  {
    "objectID": "parallelism/master_persistence.html#conclusions",
    "href": "parallelism/master_persistence.html#conclusions",
    "title": "Control process dying or timing out",
    "section": "Conclusions",
    "text": "Conclusions\nThe master needs to run until all jobs have started, but not necessarily until all jobs have finished (unless it actually does something with the output).\nThat will be hard to manage, since the time it takes to start jobs depends on SLURM queues.\nThere’s no reason to use srun or sinteractive- they still need to ask for resources, and die when disconnect, so even more finicky.\nUsing Rscript on the login node is I guess a potential workaround, but I think it also dies when we disconnect and potentially times out anyway. It’s still tying up a node, just in a different place. I think that’d make people even grumpier. It’s also hard to test with short-ish runs."
  },
  {
    "objectID": "parallelism/nested_parallel.html",
    "href": "parallelism/nested_parallel.html",
    "title": "Nested parallelism",
    "section": "",
    "text": "I tend to run a lot of simulation code that consists of some core functions that are then run for a large number of different parameter values. This bit is entirely independent and clearly parallelisable. There are also typically large calculations inside the simulation functions that can be parallelised. What’s not clear to me is whether I should write them all with parallel-friendly code (foreach %dopar%, furrr, etc), or just one or the other. While my particular situation gives me the option to choose, it’s likely not uncommon to call parallelisable functions from a package in code that is itself parallelised, and so is useful to know how this works.\nI already did some tests of what sort of work makes most sense to be parallelised so I’ll try to follow those ideas as I do these tests- assuming that the internal parallel code actually makes sense to be parallel, and wouldn’t just be faster sequential anyway. To test this, I’ll attempt to build an example that is non-trivial, but still try to stay minimally complex to avoid getting into writing a complex population dynamics model."
  },
  {
    "objectID": "parallelism/nested_parallel.html#packages-and-setup",
    "href": "parallelism/nested_parallel.html#packages-and-setup",
    "title": "Nested parallelism",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}, because I tend to like writing for loops (there’s a reason I’ll try to write up sometime later). I test other packages in the {future} family (furrr, future_apply) where I try to better understand when they do and don’t give speed advantages.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nlibrary(foreach)\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nlibrary(ggplot2)\n\nregisterDoFuture()\nplan(multisession)"
  },
  {
    "objectID": "parallelism/nested_parallel.html#built-in-nesting",
    "href": "parallelism/nested_parallel.html#built-in-nesting",
    "title": "Nested parallelism",
    "section": "Built-in nesting",
    "text": "Built-in nesting\nThe foreach package provides built-in nesting, with constructions using %:%. This is designed for loops that can be written tightly together (no processing between them). For example, we might write a nested loop over two sets of parameters mean and sd and calculate the realised coefficient of variation and return it as a matrix.\n\nrealised_cv <- foreach(i = 1:10, .combine = cbind) %:%\n  foreach(j = seq(from = 0, to = 1, by = 0.1), .combine = rbind) %dopar% {\n    a <- rnorm(1000, mean = 1, sd = j)\n    sd(a)/mean(a)\n  }\nrealised_cv\n\n                [,1]      [,2]      [,3]       [,4]       [,5]       [,6]\nresult.1  0.00000000 0.0000000 0.0000000 0.00000000 0.00000000 0.00000000\nresult.2  0.09891223 0.1008605 0.0945898 0.09907191 0.09714435 0.09994607\nresult.3  0.19514135 0.2004283 0.1978253 0.19998854 0.19953238 0.19845697\nresult.4  0.30272246 0.2979960 0.2879971 0.28725665 0.29370995 0.31131028\nresult.5  0.40790566 0.3990116 0.4000965 0.39641709 0.39208482 0.40541046\nresult.6  0.49720731 0.4974663 0.5049968 0.50798889 0.48008738 0.49096348\nresult.7  0.64000011 0.5884977 0.6004626 0.65640455 0.59444297 0.60885005\nresult.8  0.72563956 0.7155720 0.7112370 0.72276167 0.74244327 0.70035968\nresult.9  0.81651582 0.8431277 0.7823390 0.78515947 0.79606197 0.79760404\nresult.10 0.85954100 0.9478188 0.8785366 0.89448581 0.91753339 0.86162369\nresult.11 0.93010890 1.0626908 0.9827518 0.99434673 0.98176869 1.01819481\n               [,7]       [,8]       [,9]      [,10]\nresult.1  0.0000000 0.00000000 0.00000000 0.00000000\nresult.2  0.1005070 0.09780583 0.09893618 0.09814039\nresult.3  0.2078426 0.20334995 0.20473360 0.19951013\nresult.4  0.3012958 0.29666971 0.30354469 0.29686108\nresult.5  0.3995659 0.38186272 0.39905156 0.39615277\nresult.6  0.4946903 0.50200764 0.48092292 0.48294714\nresult.7  0.5863672 0.62224528 0.61382700 0.61524278\nresult.8  0.7167759 0.68353520 0.72894997 0.69285072\nresult.9  0.8086785 0.80463310 0.78535213 0.79137067\nresult.10 0.9121453 0.88604900 0.89767558 0.92785435\nresult.11 1.0691658 0.95093915 1.05077995 0.99766564\n\n\nThat can be super useful, but isn’t the goal here- I’m interested in the situation where we have\nforloop () {\n  lots of processing\n  \n  forloop2(outcomes of the processing) {\n    more processing\n  }\n  \n  more processing\n}"
  },
  {
    "objectID": "parallelism/nested_parallel.html#inner-loop",
    "href": "parallelism/nested_parallel.html#inner-loop",
    "title": "Nested parallelism",
    "section": "Inner loop",
    "text": "Inner loop\n\nParallel version\n\ninner_par <- function(in_vec, size) {\n  inner_out <- foreach(j = in_vec,\n                       .combine = c) %dorng% {\n    d <- rnorm(size, mean = j)\n    \n    f <- matrix(rnorm(size*size), nrow = size)\n    \n    g <- d %*% f\n    \n    mean(g)\n    \n  }\n}\n\n\n\nSequential version\n\ninner_seq <- function(in_vec, size) {\n  inner_out <- foreach(j = in_vec,\n                       .combine = c) %do% {\n    d <- rnorm(size, mean = j)\n    \n    f <- matrix(rnorm(size*size), nrow = size)\n    \n    g <- d %*% f\n    \n    mean(g)\n    \n  }\n}\n\n\n\nUsing preallocated for\nThis is likely to be faster than the sequential\n\ninner_for <- function(in_vec, size) {\n  inner_out <- vector(mode = 'numeric', length = size)\n  \n  for(j in 1:length(in_vec)) {\n    d <- rnorm(size, mean = in_vec[j])\n    \n    f <- matrix(rnorm(size*size), nrow = size)\n    \n    g <- d %*% f\n    \n    inner_out[j] <- mean(g)\n    \n  }\n  return(inner_out)\n}"
  },
  {
    "objectID": "parallelism/nested_parallel.html#outer-loop",
    "href": "parallelism/nested_parallel.html#outer-loop",
    "title": "Nested parallelism",
    "section": "Outer loop",
    "text": "Outer loop\n\nparallel\n\nouter_par <- function(size, innerfun) {\n  outer_out <- foreach(i = 1:size,\n                       .combine = c) %dorng% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a <- rnorm(size, mean = i)\n                         \n                         b <- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec <- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out <- innerfun(in_vec = cvec, size = size)\n                         \n                         h <- sd(cvec)/inner_out\n                         \n                         \n                       }\n  \n  return(outer_out)\n}\n\n\n\nsequential\n\nouter_seq <- function(size, innerfun) {\n  outer_out <- foreach(i = 1:size,\n                       .combine = c) %do% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a <- rnorm(size, mean = i)\n                         \n                         b <- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec <- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out <- innerfun(in_vec = cvec, size = size)\n                         \n                         h <- sd(cvec)/inner_out\n                         \n                         \n                       }\n  \n  return(outer_out)\n}\n\n\n\nPreallocated for\n\nouter_for <- function(size, innerfun) {\n  outer_out <- matrix(nrow = size, ncol = size)\n  for(i in 1:size) {\n    \n    # Do a matrix mult on a vector specified with i\n    a <- rnorm(size, mean = i)\n    \n    b <- matrix(rnorm(size*size), nrow = size)\n    \n    cvec <- a %*% b\n    \n    # Now iterate over the values in c to do somethign else\n    inner_out <- innerfun(in_vec = cvec, size = size)\n    \n    outer_out[, i] <- sd(cvec)/inner_out\n    \n  }\n  outer_out <- c(outer_out) \n  \n  return(outer_out)\n}"
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html",
    "href": "parallelism/nested_parallel_hpc.html",
    "title": "Nested futures with batchtools",
    "section": "",
    "text": "Sometimes it makes sense to just send all parallel jobs to single cpus each. But there are other situations, like when we have some high-level parallelization over parameters, followed by additional parallelization over something else, where we can take advantage of the structure of HPC clusters to send biggish jobs to a node, and then further parallelize within it onto CPUS. Which is faster is going to be highly context-dependent, I think, but let’s figure out how and then set up some scripts that can be modified for speed testing.\nI’ll use the parallel inner and outer functions developed in testing nested paralellism, but modified as in most of the slurm_r_tests repo to just return PIDs and other diagnostics so we can see what resources get used.\nThe approach here is learning a lot from this github issue."
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html#creating-a-nested-plan",
    "href": "parallelism/nested_parallel_hpc.html#creating-a-nested-plan",
    "title": "Nested futures with batchtools",
    "section": "Creating a nested plan",
    "text": "Creating a nested plan\n\nUnnested template\nIf I was using a plan like this to assign jobs to CPUS\n\nplan(tweak(batchtools_slurm,\n           template = \"batchtools.slurm.tmpl\",\n           resources = list(time = 5,\n                            ntasks.per.node = 1, \n                            mem = 1000,\n                            job.name = 'NewName')))\n\n\n\nNested with list()\nI now need to do two things- wrap it in a list with a second plan type, and ask for more tasks per node. (It should also work to do this with a focus on tasks and cpus per tasks), but that should work similarly.\nSo, we can wrap that in a list\n\nplan(list(tweak(batchtools_slurm,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 5,\n                                 ntasks.per.node = 12, \n                                 mem = 1000,\n                                 job.name = 'NewName')),\n          multicore))"
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html#nested-functions-to-return-resource-use",
    "href": "parallelism/nested_parallel_hpc.html#nested-functions-to-return-resource-use",
    "title": "Nested futures with batchtools",
    "section": "Nested functions to return resource use",
    "text": "Nested functions to return resource use\nAnd set up nested functions that return the outer and inner PIDs, as well as some other stuff:\n\ninner_par <- function(inner_size, outer_it, outer_pid) {\n  inner_out <- foreach(j = 1:inner_size,\n                       .combine = bind_rows) %dorng% {\n                         thisproc <- tibble(all_job_nodes = paste(Sys.getenv(\"SLURM_JOB_NODELIST\"),\n                                                                  collapse = \",\"),\n                                            node = Sys.getenv(\"SLURMD_NODENAME\"),\n                                            loop = \"inner\",\n                                            outer_iteration = outer_it,\n                                            outer_pid = outer_pid,\n                                            inner_iteration = j, \n                                            inner_pid = Sys.getpid(),\n                                            taskid = Sys.getenv(\"SLURM_LOCALID\"),\n                                            cpus_avail = Sys.getenv(\"SLURM_JOB_CPUS_PER_NODE\"))\n                         \n                       }\n  return(inner_out)\n}\n\n# The outer loop calls the inner one\nouter_par <- function(outer_size, inner_size) {\n  outer_out <- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %dorng% {\n                         \n                         # do some stupid work so this isn't trivially nested\n                         a <- 1\n                         b <- 1\n                         d <- a+b\n                         # Now iterate over the values in c to do somethign else\n                         inner_out <- inner_par(inner_size = inner_size,\n                                                outer_it = i, outer_pid = Sys.getpid())\n                         \n                         inner_out\n                       }\n  \n  return(outer_out)\n}\n\nAnd it WORKS!--\nI did this in nested_plan.R in the slurm_r_tests repo, and running it for 25 loops in each of the nested functions shows that we’re getting and using 12 inner PIDS per outer PID\n## Nodes and pids simple\n# A tibble: 25 × 2\n   outer_pid n_inner\n       <int>   <int>\n 1    549832      12\n 2    549960      12\n 3    550097      12\n 4    550238      12\n 5   1507878      12\n 6   1508006      12\n 7   1508130      12\n 8   1508254      12\n 9   1508380      12\n10   1508502      12\n11   1508624      12\n12   3494670      12\n13   3494799      12\n14   3494921      12\n15   3495044      12\n16   3495168      12\n17   3495292      12\n18   3495415      12\n19   4189058      12\n20   4189200      12\n21   4189329      12\n22   4189455      12\n23   4189580      12\n24   4189702      12\n25   4189829      12\nFor more detail, run the code in the slurm_r_tests repo. But it shows that we get 25 nodes and 12 cores each (because of ntasks.per.node = 12 in resources), each of which gets used 2-3 times, which makes sense for a 25 outer x 25 inner looping."
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html#controlling-chunks-and-workers",
    "href": "parallelism/nested_parallel_hpc.html#controlling-chunks-and-workers",
    "title": "Nested futures with batchtools",
    "section": "Controlling chunks and workers",
    "text": "Controlling chunks and workers\nI’m not going to bother with trying to figure out array jobs (sounds like not easy or supported, and the batchtools framework basically works by spitting out sbatch calls instead of arrays. That’s not ideal, but it’s how it works at least for now. .\nIt probably is worth addressing chunking. One reason is just to not overload the cluster- if I ask for a million batch futures, that’s not going to make anyone happy, and it’s not efficient anyway. It looks like we can set this with a workers argument (which has default 100). That means however many futures we ask for, they get chunked into workers chunks, and that many jobs get submitted. So, we also might want to modify this if we have a job that naturally wants to be nested, and we know we want a node per outer loop, then we could say workers = OUTER_LOOP_LENGTH.\nAs a test, we can modify the plan above to have workers = 5 , which should only grab 5 nodes, and send 5 of the 25 outer loops to each of them. I’m going to up the time.\n\nplan(list(tweak(batchtools_slurm,\n                workers = 5,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 15,\n                                 ntasks.per.node = 12, \n                                 mem = 1000,\n                                 job.name = 'NewName')),\n          multicore))\n\nAnd that does what it’s supposed to- keep everything on 5 nodes\n## Nodes and pids simple\n# A tibble: 5 × 2\n  outer_pid n_inner\n      <int>   <int>\n1    552072      60\n2   1510060      60\n3   3496848      60\n4   4191978      60\n5   4192353      60"
  },
  {
    "objectID": "parallelism/parallel_speed.html",
    "href": "parallelism/parallel_speed.html",
    "title": "Investigating parallel speedups",
    "section": "",
    "text": "I tend to run a lot of code that can be parallelised, but it’s not always clear when it’s worth it and how best to structure the paralellisation. Should it be at the outermost layer, where I’m typically looping over parameters, some intermediate layer where I might be looping over indices or iterators, or to handle large datasets?\nFor reference, I often have population dynamics models with many species and locations. At each timestep I need to make a lot of calculations on the species, including some large matrix multiplications to get dispersal. These could be parallelised over species. And after simulations are complete, I calculate a lot of covariances over species, space, and time that can be parallelised over pairwise combinations. Both of these cases operate on large arrays, and so would feed large amounts of data to parallelised functions, which would then do some limited processing on it (e.g. calculate covariances and clean them up for return). At the other extreme, each of these situations is governed by an initial set of parameters, giving, for example, environmental conditions, species growth rates, etc. These are often just vectors, and so parallelising over them would feed the parallel function small amounts of data and kick off large amounts of work.\nTo test parallel performance under these different situations, I’ll attempt to build an example that is non-trivial, but still try to stay minimally complex to avoid getting into writing a complex population dynamics model."
  },
  {
    "objectID": "parallelism/parallel_speed.html#packages-and-setup",
    "href": "parallelism/parallel_speed.html#packages-and-setup",
    "title": "Investigating parallel speedups",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}, because I tend to like writing for loops (there’s a reason- I’ll try to write up sometime later). I’ll also test {furrr} and {future.apply} to see if they differ in any appreciable way.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nlibrary(foreach)\nlibrary(furrr)\nlibrary(future.apply)\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nlibrary(listenv)\n\nJust set up a typical doFuture situation with plan(multisession). Sorting out plans is a topic for another day.\n\nregisterDoFuture()\nplan(multisession)"
  },
  {
    "objectID": "parallelism/parallel_speed.html#foreach",
    "href": "parallelism/parallel_speed.html#foreach",
    "title": "Investigating parallel speedups",
    "section": "foreach",
    "text": "foreach\n\nmult_foreach <- function(a, b) {\n  c_foreach <- foreach(i = 1:ncol(a), .combine = rbind) %dopar% {\n    a[,i] %*% b\n  }\n  return(t(c_foreach))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#furrr",
    "href": "parallelism/parallel_speed.html#furrr",
    "title": "Investigating parallel speedups",
    "section": "furrr",
    "text": "furrr\npurrr (and so furrr) don’t seem to work on matrices. So, I guess have a silly pre-step to make it a list. I’m going to do that outside the function, simply because if we went this way, we’d set the data up to work.\n\nmult_furrr <- function(a_list, b) {\n  c_map <- future_map(a_list, \\(x) x %*% b)\n  matrix(unlist(c_map), ncol = 2)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#future.apply",
    "href": "parallelism/parallel_speed.html#future.apply",
    "title": "Investigating parallel speedups",
    "section": "future.apply",
    "text": "future.apply\n\nmult_apply <- function(a, b) {\n  future_apply(a, MARGIN = 2, FUN = function(x) x %*% b)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#simple-for",
    "href": "parallelism/parallel_speed.html#simple-for",
    "title": "Investigating parallel speedups",
    "section": "simple for",
    "text": "simple for\nPreallocate, because I’m not a heathen\n\nmult_for <- function(a, b) {\n  \n  c_for <- a\n  for(i in 1:ncol(a)) {\n    c_for[,i] <- a[,i] %*% b\n  }\n  return(c_for)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#future-for",
    "href": "parallelism/parallel_speed.html#future-for",
    "title": "Investigating parallel speedups",
    "section": "future for",
    "text": "future for\nWe can write a usual for loop if we use futures directly. the futures themselves have to go in a list, because they are futures, not values, and so can’t go straight into a matrix. That list can be preallocated.\nThere are two ways to do this- explicit and implicit- see the future docs.\n\nExplicit futures\n\nmult_for_future_e <- function(a, b) {\n  \n  c_for <- vector(mode = 'list', length = ncol(a))\n  \n  for(i in 1:ncol(a)) {\n    c_for[[i]] <- future({a[,i] %*% b})\n  }\n  # get values and make a matrix\n  v_for <- lapply(c_for, FUN = value)\n  \n  return(matrix(unlist(v_for), ncol = ncol(a)))\n}\n\n\n\nImplicit futures\nusing listenv\n\nmult_for_future_i <- function(a, b) {\n  \n  c_for <- listenv()\n  \n  for(i in 1:ncol(a)) {\n    c_for[[i]] %<-% {a[,i] %*% b}\n  }\n  # get values and make a matrix\n  v_for <- as.list(c_for)\n  \n  return(matrix(unlist(v_for), ncol = ncol(a)))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#linear-algebra",
    "href": "parallelism/parallel_speed.html#linear-algebra",
    "title": "Investigating parallel speedups",
    "section": "linear algebra",
    "text": "linear algebra\n\nmult_linear <- function(a,b) {\n  t(a %*% b)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#preallocate-the-foreach",
    "href": "parallelism/parallel_speed.html#preallocate-the-foreach",
    "title": "Investigating parallel speedups",
    "section": "preallocate the foreach",
    "text": "preallocate the foreach\nI typically don’t do this, since my understanding of foreach is that it builds them with the .combine, so preallocating doesn’t do anything. But maybe?\n\nmult_foreach_pre <- function(a, b) {\n  c_foreach <- t(a)\n  c_foreach <- foreach(i = 1:ncol(a), .combine = rbind) %dopar% {\n    a[,i] %*% b\n  }\n  return(t(c_foreach))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#return-the-foreach-as-a-list",
    "href": "parallelism/parallel_speed.html#return-the-foreach-as-a-list",
    "title": "Investigating parallel speedups",
    "section": "Return the foreach as a list",
    "text": "Return the foreach as a list\nIt is possible that using .combine is forcing slower behaviour for the foreach, and it’s optimized for a list?\n\nmult_foreach_list <- function(a, b) {\n  c_foreach <- foreach(i = 1:ncol(a)) %dopar% {\n    a[,i] %*% b\n  }\n  \n  # Do the binding in one step\n  return(matrix(unlist(c_foreach), ncol = 2))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#dont-preallocate-the-for",
    "href": "parallelism/parallel_speed.html#dont-preallocate-the-for",
    "title": "Investigating parallel speedups",
    "section": "Don’t preallocate the for",
    "text": "Don’t preallocate the for\nHow bad is this- I almost always DO preallocate (it’s faster, and the loop starts at 1, and it’s just cleaner), but it’s possible not to.\n\nmult_for_build <- function(a, b) {\n  \n  c_for <- a[,1] %*% b\n  \n  for(i in 2:ncol(a)) {\n    ctemp <- a[,i] %*% b\n    c_for <- rbind(c_for, ctemp)\n  }\n  return(t(c_for))\n}\n\nI’ll include the furrr and future.apply here too, I guess as reference.\n\nmicrobenchmark(\n  futurefurrr = mult_furrr(a100_l, b),\n  futureapply = mult_apply(a100, b),\n  futureforeach = mult_foreach(a100, b),\n  preallocate_foreach = mult_foreach_pre(a100, b),\n  foreach_list = mult_foreach_list(a100, b),\n  bare_for = mult_for(a100, b),\n  unallocate_for = mult_for_build(a100, b),\n  bare_linear = mult_linear(t(a100), b),\n  times = 10\n)\n\nUnit: milliseconds\n                expr      min       lq     mean    median       uq      max\n         futurefurrr 443.6282 486.7204 514.9555 499.42955 508.2764 724.0422\n         futureapply 587.7048 634.2925 673.7851 647.22095 740.1291 767.1812\n       futureforeach 244.3625 253.6442 321.1252 256.70470 287.4974 804.0015\n preallocate_foreach 242.1381 251.5804 278.5400 277.79885 282.3750 374.8727\n        foreach_list 248.4650 272.0348 290.8289 299.22400 306.9559 343.0977\n            bare_for  88.4069  89.8161 100.2756  93.38755 103.5877 133.0030\n      unallocate_for 106.7770 111.9356 119.9050 119.36350 129.2688 134.1177\n         bare_linear  35.4130  38.0087  42.1330  39.99825  43.7534  58.0615\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nThese results are interesting, and the actual slowdown (not just lack of speedup) is worrying for how I do some things. I think the overhead of shifting the data around is absolutely killing these parallel processes. And I definitely have code that does this sort of thing.\nI have two more questions now (plus one for later):\n\nDoes the foreach loop work as fast as a for if I use %do% instead of %dopar%? Or is the overhead still there?\n\nAnd same with all the futures if I set plan(sequential)?\n\nIf I don’t send prebuilt data, but just some parameters and build the data internally, how do they compare?\n\nSometimes this flow makes sense, and sometimes it doesn’t– e.g. if I’m simulating populations, the outer set of parallelisation just sends parameters. But the internal set often needs to work on things like population matrices. And so maybe that internal loop just shouldn’t be parallel.\n\nHow do other plan options affect these answers? I think this deserves its own page, and could get very complicated once I get into future.callr, future.batchtools, cluster, etc. And multicore might avoid the passing and use pointers, but i can’t test on windows?"
  },
  {
    "objectID": "parallelism/parallel_speed.html#foreach-do",
    "href": "parallelism/parallel_speed.html#foreach-do",
    "title": "Investigating parallel speedups",
    "section": "foreach %do%",
    "text": "foreach %do%\nLet’s try shifting to %do% for the foreach\n\nmult_foreach_do <- function(a, b) {\n  c_foreach <- foreach(i = 1:ncol(a), .combine = rbind) %do% {\n    a[,i] %*% b\n  }\n  return(t(c_foreach))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#list-foreach-do",
    "href": "parallelism/parallel_speed.html#list-foreach-do",
    "title": "Investigating parallel speedups",
    "section": "list foreach %do%",
    "text": "list foreach %do%\n\nmult_foreach_list_do <- function(a, b) {\n  c_foreach <- foreach(i = 1:ncol(a)) %do% {\n    a[,i] %*% b\n  }\n  \n  # Do the binding in one step\n  return(matrix(unlist(c_foreach), ncol = 2))\n}\n\nTest against parallel foreach, bare for, and the unallocated for (since that’s kind of what the foreach is doing- building up an object). Since that’s sometimes nice behaviour and leads to cleaner code than a for loop, I’d like to see how they compare.\n\nmicrobenchmark(\n  futureforeach = mult_foreach(a100, b),\n  foreachdo = mult_foreach_do(a100, b),\n  foreachlistdo = mult_foreach_list_do(a100, b),\n  bare_for = mult_for(a100, b),\n  unallocate_for = mult_for_build(a100, b),\n  bare_linear = mult_linear(t(a100), b),\n  times = 10\n)\n\nUnit: milliseconds\n           expr      min       lq      mean   median       uq      max neval\n  futureforeach 235.1256 243.4750 264.39874 258.7047 280.2599 321.3101    10\n      foreachdo  94.8164  97.6870 100.78289 100.6124 102.1715 107.7091    10\n  foreachlistdo  96.9755  98.4346 100.76391 101.1035 103.3189 105.4224    10\n       bare_for  86.3208  87.9507  88.75037  88.3432  89.2126  92.0338    10\n unallocate_for 104.1609 108.6303 110.51359 109.9748 112.3416 116.5181    10\n    bare_linear  35.3966  36.1527  37.14453  36.3332  37.2698  42.9894    10\n\n\nInteresting. Nearly identical to the unallocated for and quite a bit faster than the parallel version, which seems to hint that it’s the data transfer that’s killing things. And backs up my assumption of what’s going on under the hood in terms of constructing the object as in an unallocated for loop. There’s no appreciable difference in using the foreach with a list and then combining vs combining as we go with .combine."
  },
  {
    "objectID": "parallelism/parallel_speed.html#plansequential",
    "href": "parallelism/parallel_speed.html#plansequential",
    "title": "Investigating parallel speedups",
    "section": "plan(sequential)",
    "text": "plan(sequential)\nHow do the parallel versions work with plan(sequential)? Do they all get a speedup from avoiding data transfer? This is the same benchmark test as above, but now run sequentially.\n\nplan(sequential)\n\nmicrobenchmark(\n  futurefurrr = mult_furrr(a100_l, b),\n  futureapply = mult_apply(a100, b),\n  futureforeach = mult_foreach(a100, b),\n  bare_for = mult_for(a100, b),\n  unallocate_for = mult_for_build(a100, b),\n  bare_linear = mult_linear(t(a100), b),\n  times = 10\n)\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:listenv':\n\n    map\n\n\nThe following objects are masked from 'package:foreach':\n\n    accumulate, when\n\n\nUnit: milliseconds\n           expr      min       lq      mean   median       uq      max neval\n    futurefurrr 110.2440 114.4588 119.01770 118.1613 121.6648 136.4901    10\n    futureapply 327.7384 328.5536 332.10152 329.8668 336.5330 341.8481    10\n  futureforeach 106.6627 107.0847 110.10954 109.0071 109.9754 120.1479    10\n       bare_for  86.4083  87.0670  88.07185  87.3572  88.7584  91.1030    10\n unallocate_for 109.3728 109.9509 113.80280 113.8166 116.2439 121.0449    10\n    bare_linear  35.3039  35.7138  37.46994  36.4234  38.2815  43.2047    10\n\n\nInteresting. foreach and furrr both sped up about as expected (furrr just seems a bit slower in general), but future.apply had much less of a speedup. It must not fall back to a simpler function, and still tries to use the parallel data shuffling? It is still much faster than with plan(multisession) (was 590 microseconds), so something is happening, but it’s not getting down to the speeds of the other futures. And the simple for is still fastest (other than just using linear algebra, obviously).\n\nThe message so far\nTest the parallel implementation at different points in the code- if there’s no way to avoid data passing, a simple for (or other sequential function like apply could be fastest."
  },
  {
    "objectID": "parallelism/parallel_speed.html#future-for-1",
    "href": "parallelism/parallel_speed.html#future-for-1",
    "title": "Investigating parallel speedups",
    "section": "future for",
    "text": "future for\nWe can write a usual for loop if we use futures directly. the futures themselves have to go in a list, because they are futures, not values, and so can’t go straight into a matrix. That list can be preallocated.\nThere are two ways to do this- explicit and implicit- see the future docs.\n\nExplicit futures\n\nmult_for_future_internal_e <- function(n_reps = 100, size = 1000) {\n  \n  c_for <- vector(mode = 'list', length = n_reps)\n  \n  for(i in 1:n_reps) {\n    c_for[[i]] <- future({a <- rnorm(size, mean = i)\n        b <- matrix(rnorm(size * size), nrow = size)\n        a %*% b}, seed = TRUE)\n  }\n  # get values and make a matrix\n  v_for <- lapply(c_for, FUN = value)\n  \n  return(matrix(unlist(v_for), ncol = n_reps))\n}\n\n\n\nImplicit futures\nusing listenv\nThat’s a funny way to set the seed. Good to know.\n\nmult_for_future_internal_i <- function(n_reps = 100, size = 1000) {\n  \n  c_for <- listenv()\n  \n  for(i in 1:n_reps) {\n    c_for[[i]] %<-% {a <- rnorm(size, mean = i)\n        b <- matrix(rnorm(size * size), nrow = size)\n        a %*% b} %seed% TRUE\n  }\n  # get values and make a matrix\n  v_for <- as.list(c_for)\n  \n  return(matrix(unlist(v_for), ncol = n_reps))\n}\n\nThere’s no reason to have a linear algebra version here, since we’re by definition not operating on existing matrices.\nTry that without adjusting what globals are passed to the futures\n\nmicrobenchmark(\n  futurefurrr = mult_furrr_internal(n_reps = 100, size = 1000),\n  futureapply = mult_apply_internal(n_reps = 100, size = 1000),\n  futureforeach = mult_foreach_internal(n_reps = 100, size = 1000),\n  futurefor_e = mult_for_future_internal_e(n_reps = 100, size = 1000),\n  futurefor_i = mult_for_future_internal_i(n_reps = 100, size = 1000),\n  bare_for = mult_for_internal(n_reps = 100, size = 1000),\n  times = 10\n)\n\nUnit: milliseconds\n          expr       min        lq      mean    median        uq       max\n   futurefurrr  638.0028  658.8124  868.5756  663.1727  673.2477 2710.5420\n   futureapply  647.0352  653.3856  779.9339  660.3188  669.0498 1852.4155\n futureforeach  637.7363  644.3180  670.8410  655.9570  677.6647  776.2754\n   futurefor_e 2639.2926 2669.8748 2688.2644 2682.2723 2702.9564 2751.7045\n   futurefor_i 2658.7272 2668.5496 2721.6388 2720.5006 2738.6566 2825.9734\n      bare_for 3137.0728 3160.3969 3193.4383 3182.6047 3211.6341 3323.5784\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nNow the parallelisation is helping quite a bit, even the for loops with futures, though they’re still much slower than the other future methods. However, this is much slower than creating the matrices as we actually should do for this particular calculation, and is even slower than passing those matrices in. So, if the operations need to happen this way anyway (parallel lots of stuff over parameters), this makes lots of sense and speeds up. If we CAN pre-generate matrices, linear algebra is fastest (unsurprisingly), and the loops are next, even if we have to eat pass-in cost. Though in that case sequential is probably better.\nI think the slower bare futures are likely because there’s no chunking being done, and so data is copied to each iteration, rather than to chunks of iterations (which is built into doFuture and I think furrr and future.apply. I could try to manually chunk to test that, but I think I’m just going to skip it."
  },
  {
    "objectID": "parallelism/parallel_speed.html#globals",
    "href": "parallelism/parallel_speed.html#globals",
    "title": "Investigating parallel speedups",
    "section": "Globals",
    "text": "Globals\nOne of the nice things about future compared to some other parallel backends is that it does pass the global environment, so i don’t have to manage what it gets- it works as it does interactively. BUT, if data passing is what’s killing the speed, maybe I do need to manage what gets passed. In theory, the test above should get even faster if we don’t pass it the global environment, but only the arguments.\nSo, a new version of the above, explicitly limiting passing.\n\nforeach\n\nmult_foreach_internal_g <- function(n_reps = 100, size = 1000) {\n  c_foreach <- foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .export = NULL) %dorng% {\n    a <- rnorm(size, mean = i)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  return(c_foreach)\n}\n\n\n\nfurrrr\n\nmult_furrr_internal_g <- function(n_reps = 100, size = 1000) {\n  fn_to_call <- function(rep, size) {\n    a <- rnorm(size, mean = rep)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  \n  c_map <- future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, globals = NULL))\n  matrix(unlist(c_map), ncol = n_reps)\n}\n\n\n\nfuture.apply\n\nmult_apply_internal_g <- function(n_reps = 100, size = 1000) {\n    fn_to_call <- function(rep, size) {\n    a <- rnorm(size, mean = rep)\n    b <- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n    }\n    \n  c_apply <- future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE, future.globals = FALSE)\n  \n    matrix(unlist(c_apply), ncol = n_reps)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#benchmark-1",
    "href": "parallelism/parallel_speed.html#benchmark-1",
    "title": "Investigating parallel speedups",
    "section": "Benchmark",
    "text": "Benchmark\n\nmicrobenchmark(\n  futurefurrr = mult_furrr_internal(n_reps = 100, size = 1000),\n  futureapply = mult_apply_internal(n_reps = 100, size = 1000),\n  futureforeach = mult_foreach_internal(n_reps = 100, size = 1000),\n  futurefurrr_g = mult_furrr_internal_g(n_reps = 100, size = 1000),\n  futureapply_g = mult_apply_internal_g(n_reps = 100, size = 1000),\n  futureforeach_g = mult_foreach_internal_g(n_reps = 100, size = 1000),\n  bare_for = mult_for_internal(n_reps = 100, size = 1000),\n  times = 10\n)\n\nUnit: milliseconds\n            expr       min        lq      mean    median        uq       max\n     futurefurrr  650.9615  655.6274  667.7335  665.4363  672.8008  695.4913\n     futureapply  660.9141  681.4908  691.7151  685.7910  705.6650  735.2728\n   futureforeach  643.2652  665.4859  671.3939  670.2467  672.7690  700.9367\n   futurefurrr_g  670.0894  676.9485  690.0467  684.3684  695.3350  723.5180\n   futureapply_g  658.1669  669.3657  675.3438  671.9081  682.6484  695.2184\n futureforeach_g  641.1861  654.8838  679.7260  666.7028  705.6335  748.3529\n        bare_for 3191.5845 3219.5277 3261.9446 3256.3190 3309.1928 3349.3968\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nSo, that’s not hauling around a ton of extra stuff. I think, based on the doFuture vignette, that future will send the globals, but only those that are needed. So it’s not that they’re in functions, it’s that future can tell it doesn’t need to pass extra variables. This means we don’t really get performance gains because future already had our back."
  },
  {
    "objectID": "parallelism/plans_and_hpc.html",
    "href": "parallelism/plans_and_hpc.html",
    "title": "Testing plans and systems",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tibble)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nregisterDoFuture()\nI want to test how tasks get split up in different plans (e.g. multisession, multicore, cluster, future.batchtools, future.callr). In a local case on Windows, I think it’s fairly clear we want plan(multisession) and on unix/mac, plan(multicore). But when we have access to something bigger, like an HPC, I want to know how the plans work where there are multiple nodes, each with several cores. Can we access more than one node? How? What’s the difference between cluster, batchtools and callr?\nI’ve done some haphazard poking at this, and at that time, if we just had a slurm script that requested multiple nodes, but then called a single R script, I couldn’t use workers on more than one node. My workaround was to use array jobs to do the node-parallelisation, and then {future} for the cores. But this splits the parallelisation into some being handled by shell scripts and slurm, and some handled by R and futures. And that makes it harder to control and load-balance. And in fact, I had an extra level, where I had shell scripts that started a set of SLURM array jobs, which then split into cores. This approach worked, but it’s very hardcodey, even when we auto-generate the batch and SLURM scripts. It tends to not be well-balanced without a lot of manual intervention every time anything changes. And it’s not portable- we have to restructure the code to run locally vs on the HPC.\nWhat would be nice is to auto-break-up the set of work into evenly-sized chunks, and then fire off the SLURM commands to start an appropriate number of nodes with some number of cores. And have that also work locally, where it would just parallelise over those things. Can I figure that out?\nI’m not sure how I’m going to test this in a quarto doc, since I need to run things on the HPC and return to stdout. Might have to copy-paste and treat this like onenote or something. But I should be able to set up the desired structure here, anyway.\nI think I’ll start similarly to some of my local parallel testing, where I just returned process IDs to see what resources are being used.\nAnd I think I’ll start by requesting resources with sbatch and slurm scripts that call Rscript and see what that gets me and whether we can access all the resources, and then move on to trying to get R to generate the SLURM calls, likely with future.batchtools."
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#setting-up-tests",
    "href": "parallelism/plans_and_hpc.html#setting-up-tests",
    "title": "Testing plans and systems",
    "section": "Setting up tests",
    "text": "Setting up tests\nI want to be able to see if I’m getting nodes and cores\nAs a first pass, I don’t particularly care about speed- deal with that after I figure out how it’s actually working."
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#structure",
    "href": "parallelism/plans_and_hpc.html#structure",
    "title": "Testing plans and systems",
    "section": "Structure",
    "text": "Structure\nI’ll write a slurm script to start an sbatch job that requests > 1 node and all cores on those nodes. That will call a test R script that calls a plan and runs some foreach loops that return PIDs, as I did previously. These loops need to iterate over at least nodes X cores so we can see what gets used.\nThe easiest thing to do will be to just use print statements to print to stdout, I think, though I guess I could save rds files or something.\nCan I automate? Or can I call all the plans one after each other in the same script? Probably yes to both. What’s the best way to get the code over to the HPC? I usually use git/github to sync changes, but I don’t really want to git this whole website over there. It’s a hassle, but I might set up a template slurm testing repo and use that.\n\nThe R code\nBasically, I want to call plan(PLAN_NAME), and then run a loop, checking PID as before. That loop will be like the simple nested loop with %:% I built before at least to start. I might do more complex nesting and try plan(list(PLAN1, PLAN2) later. Why am I looping at all? In case there’s some built-in capacity to throw one set of loops on nodes and the other on cores. This is more likely to come in later, but might as well set it up now.\nThat nested function is\n\nnest_test <- function(outer_size, inner_size, planname) {\n  outer_out <- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %:% \n    foreach(j = 1:inner_size,\n                       .combine = bind_rows) %dopar% {\n    \n                         thisproc <- tibble(plan = planname,\n                                            outer_iteration = i,\n                                            inner_iteration = j, \n                                            pid = Sys.getpid())\n                       }\n  \n  return(outer_out)\n}\n\nAnd I also want to know availableWorkers() and availableCores(). Also try availableCores(methods = 'Slurm') to see how that differs and which matches the resources we actually use.\nCan I make the stdout auto-generate something in markdown I can copy-paste in here? That’d be nice.\nI think as a first pass, I’ll try this for the single-machine plans- sequential, multicore, and multiprocess. I have a feeling I’ll encounter more difficulty with the cluster, callr, and batchtools, so get the basics figured out first.\nSo, what should that R script look like? I think I’ll use a for over the plans.\nThe test HPC I’ll use has 20 nodes with 12 cores. I’ll request 2 nodes and 24 cores for testing so I can see if it uses multiple nodes. That means I’ll need at least 24 loops, and ideally more. Might as well go 50- this won’t actually take any time.\nI can run this locally too, so I guess do that?\n\nplannames <- c('sequential', 'multisession', 'multicore')\n\n# The loopings\nnest_test <- function(outer_size, inner_size, planname) {\n  outer_out <- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %:% \n    foreach(j = 1:inner_size,\n            .combine = bind_rows) %dopar% {\n              \n              thisproc <- tibble(plan = planname,\n                                 outer_iteration = i,\n                                 inner_iteration = j, \n                                 pid = Sys.getpid())\n            }\n  \n  return(outer_out)\n}\n\n\nfor (planname in plannames) {\n  print(paste0(\"# \", planname))\n  plan(planname)\n  \n  print('## available Workers:')\n  print(availableWorkers())\n  \n  print('## available Cores:')\n  print(\"### non-slurm\")\n  print(availableCores())\n  print(\"### slurm method\")\n  print(availableCores(methods = 'Slurm'))\n  \n  # base R process id\n  print('## Main PID:')\n  print(Sys.getpid())\n  \n  looptib <- nest_test(25, 25, planname)\n  \n  print('## Unique processes')\n  print(length(unique(looptib$pid)))\n  print(\"This should be the IDs of all cores used\")\n  print(unique(looptib$pid))\n  \n  print('## Full loop data')\n  print(looptib)\n  \n  \n}\n\n[1] \"# sequential\"\n[1] \"## available Workers:\"\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n[1] \"## available Cores:\"\n[1] \"### non-slurm\"\nsystem \n    20 \n[1] \"### slurm method\"\ncurrent \n      1 \n[1] \"## Main PID:\"\n[1] 24368\n[1] \"## Unique processes\"\n[1] 1\n[1] \"This should be the IDs of all cores used\"\n[1] 24368\n[1] \"## Full loop data\"\n# A tibble: 625 × 4\n   plan       outer_iteration inner_iteration   pid\n   <chr>                <int>           <int> <int>\n 1 sequential               1               1 24368\n 2 sequential               1               2 24368\n 3 sequential               1               3 24368\n 4 sequential               1               4 24368\n 5 sequential               1               5 24368\n 6 sequential               1               6 24368\n 7 sequential               1               7 24368\n 8 sequential               1               8 24368\n 9 sequential               1               9 24368\n10 sequential               1              10 24368\n# … with 615 more rows\n[1] \"# multisession\"\n[1] \"## available Workers:\"\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n[1] \"## available Cores:\"\n[1] \"### non-slurm\"\nsystem \n    20 \n[1] \"### slurm method\"\ncurrent \n      1 \n[1] \"## Main PID:\"\n[1] 24368\n[1] \"## Unique processes\"\n[1] 20\n[1] \"This should be the IDs of all cores used\"\n [1] 26432 22348 26260 24924 21440 25436 11104  4544 18052 12440 18076 22024\n[13] 25728 20768  7728 12632  6628 25368 23212 24668\n[1] \"## Full loop data\"\n# A tibble: 625 × 4\n   plan         outer_iteration inner_iteration   pid\n   <chr>                  <int>           <int> <int>\n 1 multisession               1               1 26432\n 2 multisession               1               2 26432\n 3 multisession               1               3 26432\n 4 multisession               1               4 26432\n 5 multisession               1               5 26432\n 6 multisession               1               6 26432\n 7 multisession               1               7 26432\n 8 multisession               1               8 26432\n 9 multisession               1               9 26432\n10 multisession               1              10 26432\n# … with 615 more rows\n[1] \"# multicore\"\n[1] \"## available Workers:\"\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n[1] \"## available Cores:\"\n[1] \"### non-slurm\"\nsystem \n    20 \n[1] \"### slurm method\"\ncurrent \n      1 \n[1] \"## Main PID:\"\n[1] 24368\n[1] \"## Unique processes\"\n[1] 1\n[1] \"This should be the IDs of all cores used\"\n[1] 24368\n[1] \"## Full loop data\"\n# A tibble: 625 × 4\n   plan      outer_iteration inner_iteration   pid\n   <chr>               <int>           <int> <int>\n 1 multicore               1               1 24368\n 2 multicore               1               2 24368\n 3 multicore               1               3 24368\n 4 multicore               1               4 24368\n 5 multicore               1               5 24368\n 6 multicore               1               6 24368\n 7 multicore               1               7 24368\n 8 multicore               1               8 24368\n 9 multicore               1               9 24368\n10 multicore               1              10 24368\n# … with 615 more rows\n\n\n\n\nThe slurm script\nwe need a batch script that calls the R script, requests resources (here, 2 nodes with 12 cores each so we can see node utilisation- or not).\n#!/bin/bash\n\n# # Resources on test system: 20 nodes, each with 12 cores. 70GB RAM\n\n#SBATCH --time=0:05:00 # request time (walltime, not compute time)\n#SBATCH --mem=8GB # request memory. 8 should be more than enough to test\n#SBATCH --nodes=2 # number of nodes. Need > 1 to test utilisation\n#SBATCH --ntasks-per-node=12 # Cores per node\n\n#SBATCH -o node_core_%A_%a.out # Standard output\n#SBATCH -e node_core_%A_%a.err # Standard error\n\n# timing\nbegin=`date +%s`\n\nmodule load R\n\nRscript testing_plans.R\n\n\nend=`date +%s`\nelapsed=`expr $end - $begin`\n\necho Time taken for code: $elapsed\nThat’s then called with\nsbatch filename.sh\nFrom within the directory."
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#sequential",
    "href": "parallelism/plans_and_hpc.html#sequential",
    "title": "Testing plans and systems",
    "section": "sequential",
    "text": "sequential\n\navailable workers:\ngandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04\n\n\ntotal workers:\n24\n\n\nunique workers:\ngandalf-vm03 gandalf-vm04\n\n\navailable Cores:\n\nnon-slurm\n12\n\n\nslurm method\n12\n\n\n\nMain PID:\n1224603\n\n\nUnique processes\n1\nIDs of all cores used\n1224603"
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#multisession",
    "href": "parallelism/plans_and_hpc.html#multisession",
    "title": "Testing plans and systems",
    "section": "multisession",
    "text": "multisession\nℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2)\n\navailable workers:\ngandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04\n\n\ntotal workers:\n24\n\n\nunique workers:\ngandalf-vm03 gandalf-vm04\n\n\navailable Cores:\n\nnon-slurm\n12\n\n\nslurm method\n12\n\n\n\nMain PID:\n1224603\n\n\nUnique processes\n12\nIDs of all cores used\n1224723 1224717 1224715 1224716 1224718 1224725 1224724 1224721 1224720 1224722 1224719 1224726"
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#multicore",
    "href": "parallelism/plans_and_hpc.html#multicore",
    "title": "Testing plans and systems",
    "section": "multicore",
    "text": "multicore\n\navailable workers:\ngandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04\n\n\ntotal workers:\n24\n\n\nunique workers:\ngandalf-vm03 gandalf-vm04\n\n\navailable Cores:\n\nnon-slurm\n12\n\n\nslurm method\n12\n\n\n\nMain PID:\n1224603\n\n\nUnique processes\n12\nIDs of all cores used\n1225561 1225564 1225567 1225570 1225573 1225576 1225581 1225586 1225591 1225596 1225601 1225606\nTime taken for code: 15 :::\nSo, in all cases, the workers were seen across all nodes ({parallely} says as much in the help- this uses scontrol to find workers), while the cores are local. And only 12 cores ever end up getting used here, even with multisession and multicore."
  },
  {
    "objectID": "parallelism/template_modification.html",
    "href": "parallelism/template_modification.html",
    "title": "Template modification",
    "section": "",
    "text": "I don’t want to spend a ton of time on this, but if I want to add the ability to adjust any of the SLURM variables, including those that have dashes ‘-’ in their names, and I don’t want to use the format of the resources list enforced by the {batchtools} template slurm-simple.tmpl, I’ll need to modify the template.\nTo do that, it’s a bit helpful to know how {brew} works, though it’s mostly self-explanatory from reading the templates. I’m primarily confused why I can’t pass names with backticks, when somethign like\nWorks, since the future.batchtools template just uses unlist and names. e.g. the relevant bit is\nIt must be enforced by the batchtools wrapper for brew, because this works\nyeilding\nDo I want to dig into why batchtools enforces syntactic names? Not really.\nDo I want to bypass batchtools slurm generation? Not really.\nSo, can I modify the slurm script to do what I want? I think probably."
  },
  {
    "objectID": "parallelism/template_modification.html#what-do-i-want",
    "href": "parallelism/template_modification.html#what-do-i-want",
    "title": "Template modification",
    "section": "What do I want?",
    "text": "What do I want?\n\nTo pass arbitrary slurm arguments, similar to how future.batchtools ’s slurm.tmpl works, but including dash names. I really don’t like that batchtools takes over some of those arguments and calls them different things.\nTo be able to pass the options in the formats slurm allows (e.g. --time in seconds or as 00:05:00, --mem as an integer of megabytes, or as \"1GB\")\n\nI think what might work is to use dots in the names of resources and then translate to dashes?\ne.g. insert one small gsub line in the future.batchtools script:\n\nresources_dots <- list('name.with.ticks' = 17, namenoticks = 'a')\nopts <- unlist(resources_dots, use.names = TRUE)\nopts <- sprintf(\"--%s=%s\", names(opts), opts)\nopts <- gsub('\\\\.', '-', opts)\nopts <- paste(opts, collapse = \" \")\nopts\n\n[1] \"--name-with-ticks=17 --namenoticks=a\"\n\n\nI generally like to structure SLURM scripts with\n#SBATCH --option1=value1\n#SBATCH --option2=value2\nAnd this template structures them\n#SBATCH --option1=value1 --option2=value2\nI could almost certainly write a thing that put carriage returns and #SBATCH in front of each, but we never see these templates, so I think I’ll skip that.\nI’ve changed it in batchtools.slurm.tmpl, and now the question is whether it runs afoul of some other batchtools error-catcher."
  },
  {
    "objectID": "parallelism/template_modification.html#outcome",
    "href": "parallelism/template_modification.html#outcome",
    "title": "Template modification",
    "section": "Outcome",
    "text": "Outcome\nIt seems to work\nI can also change --job-name now with resources$job.name, but the catch is I don’t know if there’s a way to change it on a per-job basis, since I have to specify it in plan with the resources list, and the iteration won’t be known until later. I would have thought that’s where the default job.name was coming from in slurm.tmpl and slurm-simple.tmpl, but they seem to just call themselves ‘doFuture’."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html",
    "href": "pix4d/read_pix4d_outputs.html",
    "title": "Pix4d outputs",
    "section": "",
    "text": "Trying to read in pix4d outputs- specifically point clouds, orthophotos, dsm_xyz, and dtm.\nI’m going to assume I need stars and sf.\nTypically the projdir would have a better name (likely similar/same as the project name)"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#raster-dsm",
    "href": "pix4d/read_pix4d_outputs.html#raster-dsm",
    "title": "Pix4d outputs",
    "section": "Raster DSM",
    "text": "Raster DSM\nThe settings for this seem to be in the DSM and orthomosaic tab and resolution matches the ortho, as far as I can tell.\n\ndsm_rpath <- file.path(projdir, projname, '3_dsm_ortho', '1_dsm')\n\nread it in?\n\ndsm_r <- read_stars(file.path(dsm_rpath, paste0(projname, '_dsm.tif')))\n\n\ndsm_r\n\nstars object with 2 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                           Min.   1st Qu.    Median      Mean   3rd Qu.\n4m_upandback_dsm.tif  -24.73554 -24.70843 -24.68753 -24.68809 -24.67387\n                           Max.  NA's\n4m_upandback_dsm.tif  -24.62663 96962\ndimension(s):\n  from   to  offset    delta                refsys point x/y\nx    1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny    1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\n\n\nReally not super clear why useRaster = TRUE isn’t working for these, but whatever.\n\nplot(dsm_r, useRaster = FALSE)\n\ndownsample set to 1\n\n\n\n\n\nThose are funny z-values. What is the reference- clearly not elevation.\nHow many pixels? 1.0983136^{7}. 11 million is a lot."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#grid-dsm",
    "href": "pix4d/read_pix4d_outputs.html#grid-dsm",
    "title": "Pix4d outputs",
    "section": "Grid DSM",
    "text": "Grid DSM\nThis is an xyz point file.The settings are in the Additional Outputs tab, and we can set the grid spacing there. This was done with a spacing of 100cm. That implies these are centers, and this is a much coarser grid than in the dsm raster, which is at GSD scale.\n\ndsm_xyzpath <- file.path(projdir, projname, '3_dsm_ortho', '1_dsm')\n\n\ndsm_xyz <- readr::read_csv(file.path(dsm_xyzpath, paste0(projname, '_dsm_1cm.xyz')),\n                                 col_names = c('x', 'y', 'z', 'r', 'g', 'b'))\n\nRows: 1490037 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): x, y, z, r, g, b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nassume the last 3 cols are rgb, use my function\n\ndsm_xyz <- dsm_xyz %>% \n  mutate(hexcol = rgb2hex(r,g,b))\n\nIgnore spatial for a minute, can I just slam it in ggplot? using z as fill first. If we want the rgb, it’ll either be an orthomosiac or we’ll need to do something 3d with color overlay\n\ndsm_xyzgg <- ggplot(dsm_xyz, aes(x = x, y = y, fill = z)) + geom_raster() + coord_equal()\ndsm_xyzgg\n\n\n\n\nThat was really weirdly fast.\nCan I use the rgb to see the orthomosiac?\n\ndsm_xyzorthogg <- ggplot(dsm_xyz, aes(x = x, y = y, fill = hexcol)) + geom_raster() + coord_equal() + scale_fill_identity()\ndsm_xyzorthogg\n\n\n\n\nWould be cool to do a 3d map with color, maybe {rayshader}? Would really be nice to be able to rotate it etc.\nThat has color and height- might be good to feed to a ML algorithm to ID rocks using both sets of info and their relationships to each other.\nThis is supposedly a 100cm grid, but it has 1.5 million pixels, which is a lot less than the dsm raster at GSD resolution, but is nowhere near the difference I’d expect. It’s approximately 10x less, but GSD is < 1cm. And even at 1cm, I’d expect 10,000x fewer pixels. And those pictures are clearly not 1m pixels. What IS the spacing? Iterates fastest on x\n\ndsm_xyz[2, 'x'] - dsm_xyz[1, 'x']\n\n     x\n1 0.01\n\n# dsm_xyz[2, 'y'] - dsm_xyz[1, 'y']\n\nSo, what is 0.01? That’s a cm, isn’t it?\nAssuming the same crs as the raster dsm, the LENGTHUNIT is 1 meter. So, 0.01 would be 1cm, and that makes more sense.\n\nst_crs(dsm_r)\n\nCoordinate Reference System:\n  User input: WGS 84 / UTM zone 55S \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 55S\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 55S\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",147,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",10000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 144°E and 150°E, southern hemisphere between 80°S and equator, onshore and offshore. Australia. Papua New Guinea.\"],\n        BBOX[-80,144,0,150]],\n    ID[\"EPSG\",32755]]\n\n\nIt’s not exactly a translation from the raster, because the raster has all the NA off the edges, and a step of\n\ndiff(st_get_dimension_values(dsm_r, 'x')[1:2])\n\n[1] 0.00556\n\n\nSo, the raster should have about 4x, but also includes the NAs.\n\nsum(is.na(dsm_r[[1]]))\n\n[1] 5896746\n\n\nThat’s not entirely it. Still about 266k more pixels in the rnn than expected once we drop the NA and adjust by pixel size.\n\n#non-na in raster\nrnn  <- (2416*4546)-sum(is.na(dsm_r[[1]]))\n#pixel difference- how many dsm_r pixels per dsm_xyz pixels?\npixdif <- (0.01/diff(st_get_dimension_values(dsm_r, 'x')[1:2]))^2\n\nexpected_rnn <- nrow(dsm_xyz)*pixdif\n\nexpected_rnn-rnn\n\n[1] -266385.2\n\n\n\nMake geographic\n\nsf\nCan I make it geographic (sf)? Assume the same crs as the dsm_r\n\ndsm_xyzsf <- st_as_sf(dsm_xyz, coords = c('x', 'y'), crs = st_crs(dsm_r))\n\nuse the sf plot method? bad idea. takes forever. The points (next) are points, but it might make more sense to make these stars anyway- it is gridded.\n\nplot(dsm_xyzsf[,'z'])\n\n\n\n\n\n\nstars\nTry stars- first, confirm it is gridded- what’s the step?\nNeed to do a bit of cleanup- the x’s have negatives when they turnover y’s, and the y’s have a ton of zeros because they iterate slower.\n\nrawx <- diff(pull(dsm_xyz[, 'x']))\ncleanx <- rawx[rawx > 0]\n# rounding error\ndifclean <- abs(cleanx-cleanx[1])\n\n\n# huh- some are still out. by a lot. It seems systematic and strange\nall(difclean < 1e-8)\n\n[1] FALSE\n\nwhich(difclean > 1e-8)\n\n  [1]  191679  192281  192883  343775  344443  345109  375364  378772  378773\n [10]  380145  381514  382200  443113  443822  483744  484458  485170  485881\n [19]  486590  487296  527316  528041  530943  764429  765245  766058  766870\n [28]  767680  768486  773330  774145  774961  775777  822605  823429  824254\n [37]  825076  825896  877470  878310  879147  879982  884988  999784 1000608\n [46] 1001431 1002252 1002256 1003070 1003881 1004687 1070957 1071726 1136369\n [55] 1137116 1137859 1143776 1144521 1145270 1146022 1146781 1162799 1163554\n [64] 1205080 1205808 1206534 1207256 1207976 1212992 1333223 1384324 1385469\n [73] 1389993 1390552 1429954 1430434 1457519 1457948 1458373 1458795 1459216\n [82] 1459636 1460053 1460467 1460877 1461283 1461685 1462082 1462475 1462863\n [91] 1463247 1463626 1464003 1464376 1464744 1465107 1465466 1465821 1466172\n[100] 1466520 1466861 1467199 1467534 1467865 1468192 1468513 1468831 1469145\n[109] 1469455 1469760 1470062 1470360 1470655 1470946 1471233 1471516 1471796\n[118] 1472073 1472346 1472616 1472881 1473144 1473402 1473656 1473905 1474150\n[127] 1474392 1474629 1474862 1475092 1475316 1475537 1475754 1475966 1476174\n\ncleanx[which(difclean > 1e-8)]\n\n  [1] 0.06 0.04 0.03 0.02 0.05 0.09 0.04 0.02 0.02 0.02 0.02 0.02 0.02 0.04 0.02\n [16] 0.06 0.08 0.10 0.13 0.15 0.03 0.04 0.04 0.02 0.04 0.07 0.08 0.11 0.14 0.03\n [31] 0.05 0.07 0.09 0.02 0.04 0.05 0.08 0.11 0.02 0.03 0.07 0.10 0.02 0.02 0.03\n [46] 0.05 0.06 0.03 0.14 0.15 0.20 0.03 0.06 0.02 0.03 0.06 0.29 0.25 0.22 0.17\n [61] 0.10 0.03 0.05 0.03 0.04 0.07 0.09 0.10 0.02 0.03 0.02 0.02 0.03 0.05 0.02\n [76] 0.04 0.02 0.05 0.08 0.10 0.10 0.11 0.11 0.14 0.17 0.21 0.24 0.27 0.30 0.33\n [91] 0.38 0.39 0.42 0.45 0.49 0.53 0.55 0.58 0.61 0.65 0.69 0.70 0.73 0.76 0.80\n[106] 0.84 0.86 0.90 0.93 0.97 1.00 1.01 1.05 1.09 1.12 1.15 1.17 1.21 1.25 1.28\n[121] 1.31 1.32 1.37 1.40 1.43 1.46 1.48 1.52 1.55 1.59 1.62 1.64 1.68 1.71 1.74\n\n\nIf I plot that does it become clear? Because I took diffs and cut out the big steps, I can’t just attach it. Will need to modify in place.\n\ndsm_xyz <- dsm_xyz |> \n  mutate(xdifs = x - lag(x, 1),\n         ydifs = y - lag(y, 1)) |> \n  mutate(xdifs = ifelse(ydifs == 0, xdifs, NA),\n         ydifs = ifelse(ydifs == 0, NA, ydifs))\n\nThe steps are where there are weird things happening along angled edges.\n\ndsm_xyz_x <- ggplot(dsm_xyz, aes(x = x, y = y, fill = xdifs)) + geom_raster() + coord_equal() + scale_fill_gradient(low = 'white', high = 'red')\ndsm_xyz_x\n\n\n\n\nCan we see it better with size?\n\ndsm_xyz_xp <- ggplot(dsm_xyz, aes(x = x, y = y, color = xdifs, size = xdifs)) + geom_point() + coord_equal() + scale_color_gradient(low = 'white', high = 'red')\ndsm_xyz_xp\n\nWarning: Removed 2370 rows containing missing values (`geom_point()`).\n\n\n\n\n\nYeah, it’s also centered on corners.\nCheck y in the same way\n\ndsm_xyz_yp <- ggplot(dsm_xyz, aes(x = x, y = y, color = ydifs, size = ydifs)) + geom_point() + coord_equal() + scale_color_gradient(low = 'white', high = 'red')\ndsm_xyz_yp\n\nWarning: Removed 1487668 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe ydifs are all 0.01, and we can see the rows stepover on the left edge.\nI think it’s safe to ignore the few discontinuities.\nNow, actually make it stars\n\ndsm_xyzstars <- st_as_stars(dsm_xyz, crs = st_crs(dtm))\n\n\nplot(dsm_xyzstars['z'])\n\n\n\n\n\nplot(dsm_xyzstars['hexcol'])\n\n\n\n\nggplot version- orthomosaic\n\ndsm_xyzstarsgg <- ggplot() + \n  geom_stars(data = dsm_xyzstars, aes(fill = hexcol)) +\n  scale_fill_identity() +\n  theme(legend.position = 'none') + \n  coord_equal()\ndsm_xyzstarsgg\n\n\n\n\nggplot version- elevation\n\ndsm_xyzstarsgg_z <- ggplot() + \n  geom_stars(data = dsm_xyzstars, aes(fill = z)) +\n  colorspace::scale_fill_continuous_sequential(palette = 'Terrain 2') +\n  theme(legend.position = 'none') + \n  coord_equal()\ndsm_xyzstarsgg_z\n\n\n\n\nIs it faster to just use geom_raster? Would need to have a non-spatial df as we did above. This just errors when I try it.\n\ndsm_xyzstarsgg_z_raster <- ggplot() + \n  geom_raster(data = dsm_xyzstars, aes(fill = z)) +\n  colorspace::scale_fill_continuous_sequential(palette = 'Terrain 2') +\n  theme(legend.position = 'none') + \n  coord_equal()\ndsm_xyzstarsgg_z_raster\n\nWhat about geom_sf on the point version? I plotted it above, but haven’t tried ggplot. This is more specifically what the point cloud is, below. Change everything to color from fill.\n\ndsm_xyzsfgg_z <- ggplot() + \n  geom_sf(data = dsm_xyzsf, aes(color = z)) +\n  colorspace::scale_color_continuous_sequential(palette = 'Terrain 2') +\n  theme(legend.position = 'none') + \n  coord_sf()\ndsm_xyzsfgg_z"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#check-pixels-dsm-ortho",
    "href": "pix4d/read_pix4d_outputs.html#check-pixels-dsm-ortho",
    "title": "Pix4d outputs",
    "section": "Check pixels dsm & ortho",
    "text": "Check pixels dsm & ortho\n\nst_dimensions(dsm_r)\n\n  from   to  offset    delta                refsys point x/y\nx    1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny    1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\n\nst_dimensions(ortho)\n\n     from   to  offset    delta                refsys point x/y\nx       1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny       1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\nband    1    4      NA       NA                    NA    NA    \n\n\nConfirm they match\n\nall(st_get_dimension_values(dsm_r, 'x') == st_get_dimension_values(ortho, 'x'))\n\n[1] TRUE\n\nall(st_get_dimension_values(dsm_r, 'y') == st_get_dimension_values(ortho, 'y'))\n\n[1] TRUE\n\n\nCan we combine those into one stars? the catch is that the rgb vals aren’t on the same scale as the z. For the ortho, we have a pixel value for each of 4 bands (r,g,b, alpha). But z isn’t a band. So how would we do it? Make the bands attributes, probably. Or just call z a band?\nTo make the bands attributes and add z, this works. Will need to better define what’s needed later before deciding whether to then merge this back or what. And maybe we should be using hex (e.g. merging colmos and dsm_r).\n\northosplit <- split(ortho, 'band') |> \n  setNames(c('r', 'g','b', 'alpha'))\n\ndsm_r <- setNames(dsm_r, 'z')\n\northodsm <- c(orthosplit, dsm_r)\n\nMerging into a dim- takes a while. and it’s unclear what to call this dimension or its values- ‘band’ isn’t right, and the values aren’t 0-255 for the z. I think don’t do this unless we’re really sure we want to.\n\northodsm_dim <- orthodsm |> \n  merge()\n\nDoes it actually make more sense to add a z dimension? Maybe? we can’t just merge the dsm_r, because then it has no attributes. Can we control the merge above? Maybe, but it’ll be a bit tricky. ignore until we have a clearer definition of what we need. The few obvious things I’ve tried have failed.\nWhatever we do about the combination, the pixels match for the DSM raster and the orthophoto"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#check-pixels-dtm-ortho",
    "href": "pix4d/read_pix4d_outputs.html#check-pixels-dtm-ortho",
    "title": "Pix4d outputs",
    "section": "Check pixels dtm & ortho",
    "text": "Check pixels dtm & ortho\nThese are unlikely to match-the dtm’s settings are done elsewhere and unless we were really particular to set them the same as the ortho, (which would require downgrading the ortho), they won’t match. AND, it’s unnecessary, since there’s a raster dsm that does match.\nLooking at the from-to, they clearly don’t match, and looking at dimensions confirms it\n\nst_dimensions(ortho)\n\n     from   to  offset    delta                refsys point x/y\nx       1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny       1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\nband    1    4      NA       NA                    NA    NA    \n\nst_dimensions(dtm)\n\n  from   to  offset    delta                refsys point x/y\nx    1 1932  263755  0.00695 WGS 84 / UTM zone 55S FALSE [x]\ny    1 3636 5774223 -0.00695 WGS 84 / UTM zone 55S FALSE [y]\n\n\nBut, the DTM is some sort of smoothed thing, and is set in the Additional Outputs tab. We could make the ortho matched, but it’d be contrived (and pointless, given the existence of the raster dsm."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#convert-dsm_xyz-to-image-with-z",
    "href": "pix4d/read_pix4d_outputs.html#convert-dsm_xyz-to-image-with-z",
    "title": "Pix4d outputs",
    "section": "Convert dsm_xyz to image with z?",
    "text": "Convert dsm_xyz to image with z?\nAgain, I don’t really see the point of this turning the dsm_xyz into an image (raster) is already done in the raster dsm + ortho. We can merge if we want. Just not sure exactly how we’d want to present that."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#can-i-get-rayshader-to-work",
    "href": "pix4d/read_pix4d_outputs.html#can-i-get-rayshader-to-work",
    "title": "Pix4d outputs",
    "section": "Can I get rayshader to work?",
    "text": "Can I get rayshader to work?\nWould be cool to do height with z and color with hexcol to actually map the stream in 3d with photo overlay. Pix4d does it, but would be nice to do here too."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#why-does-the-point-cloud-not-include-the-water",
    "href": "pix4d/read_pix4d_outputs.html#why-does-the-point-cloud-not-include-the-water",
    "title": "Pix4d outputs",
    "section": "Why does the point cloud not include the water?",
    "text": "Why does the point cloud not include the water?\nDoes that make our lives easier in some ways?"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#where-is-this-stuff-in-space-is-it-in-the-right-place",
    "href": "pix4d/read_pix4d_outputs.html#where-is-this-stuff-in-space-is-it-in-the-right-place",
    "title": "Pix4d outputs",
    "section": "Where is this stuff in space? is it in the right place?",
    "text": "Where is this stuff in space? is it in the right place?"
  },
  {
    "objectID": "plotting/faded_colors.html",
    "href": "plotting/faded_colors.html",
    "title": "Faded colors",
    "section": "",
    "text": "There are a number of reasons we might want bivariate color axes in plots. The particular use I’m looking for now is to use a faded color to indicate less certainty in a result. Other uses will be developed later or elsewhere, but should build on this fairly straightforwardly.\nI’m doing this with colorspace because it’s hue-chroma-luminance approach makes it at least appear logical to shift along those dimensions. We might want hue (or luminance) to show one thing, and intensity to show another. Though we will play around with how that looks in practice. The specific use motivatiung this is to show the predicted amount of something with hue, and certainty with chroma or luminance (in particular, we have a model that makes predictions more accurately in some places than others). But there are many other potential uses.\nIn the HCL exploration file, I figure out HOW to generate faded colors and find some palettes that might work. Here, I’m going to sort out how to go from there to using them in plots, including creating legends."
  },
  {
    "objectID": "plotting/faded_colors.html#plot-the-bivariate-colors",
    "href": "plotting/faded_colors.html#plot-the-bivariate-colors",
    "title": "Faded colors",
    "section": "Plot the bivariate colors",
    "text": "Plot the bivariate colors\nBefore trying to plot with the colors, first I want to actually plot them themselves. One reason is to test how they are being created and specified, and the other is potentially to use the plot as a legend.\nWhy? The legend() part of ggplot may not handle the bivariate nature of the colors well, so need to basically homebrew one. This is the most flexible option- make the plot, then shrink and pretend it’s a legend. But, could also make a legend in vector form, then stack. Just not sure how well that’ll work. The shrunk plot would work better for continuous variables, the legend probably works better to use other parts of ggplot and not always have to screw around with grobs or ggarrange or patchwork or cowplot. I’ll try them all, I guess.\nFirst, make a matrix of colors. Take the base palette, fade it and save the color values for the whole thing. The for loop is lame, should be a function, but I’m just looking right now.\n\nbaseramp <- sequential_hcl(8, 'ag_Sunset')\n\nfadesteps <- seq(0,1, by = 0.25)\n\ncolormat <- matrix(rep(baseramp, length(fadesteps)), nrow = 5, byrow = TRUE)\n\nfor(i in 1:length(fadesteps)) {\n  colormat[i, ] <- lighten(colormat[i, ], amount = fadesteps[i]) %>%\n    desaturate(amount = fadesteps[i])\n}\n\nOption 1 is to make that into a plot that we can then smash on top\n\n# Make a tibble from the matrix to feed to ggplot\ncoltib <- as_tibble(colormat, rownames = 'row') %>%\n  pivot_longer(cols = starts_with('V'), names_to = 'column')\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n# coltib\n\nggplot(coltib, aes(y = row, x = column, fill = value)) + \n  geom_tile() + scale_fill_identity()\n\n\n\n\nThat’s upside-down with how I tend to think about it. How about flipping the construction?\n\nfadesteps <- rev(seq(0,1, by = 0.25))\ncolormat <- matrix(rep(baseramp, length(fadesteps)), nrow = 5, byrow = TRUE)\n\nfor(i in 1:length(fadesteps)) {\n  colormat[i, ] <- lighten(colormat[i, ], amount = fadesteps[i]) %>%\n    desaturate(amount = fadesteps[i])\n}\n\ncoltib <- as_tibble(colormat, rownames = 'row') %>%\n  pivot_longer(cols = starts_with('V'), names_to = 'column')\n\n\nggplot(coltib, aes(y = row, x = column, fill = value)) +\n  geom_tile() + scale_fill_identity()"
  },
  {
    "objectID": "plotting/faded_colors.html#programmatic-color-setting",
    "href": "plotting/faded_colors.html#programmatic-color-setting",
    "title": "Faded colors",
    "section": "Programmatic color setting",
    "text": "Programmatic color setting\nCreate a function basically following the above. But allow it to take palettes by name or raw hue values if they are obtained elsewhere (like from a manually specified hue ramp). hex color vals and pal names are both characters, but hex always starts with ‘#’, so should be able to auto-detect. It can take a number of fades, or a vector of specific fade levels, and returns the matrix of colors.\n\ncol2dmat <- function(pal, n1, n2 = 2, dropwhite = TRUE, fadevals = NULL) {\n  # pal can be either a palette name or a vector of hex colors (or single hex color)\n  # dropwhite is there to by default knock off the bottom row that's all white\n  # fadevals is a way to bypass the n2 and specify specific fade levels (ie if nonlinear)\n\n  if (all(str_detect(pal, '#'))) {\n    baseramp <- pal\n  } else {\n    baseramp <- sequential_hcl(n1, pal)\n  }\n\n  if (is.null(fadevals)) {\n    if (dropwhite) {n2 = n2+1}\n\n    fadesteps <- rev(seq(0,1, length.out = n2))\n\n    if (dropwhite) {fadesteps <- fadesteps[2:length(fadesteps)]}\n\n  }\n\n  if (!is.null(fadevals)) {\n    fadesteps <- sort(fadevals, decreasing = TRUE)\n  }\n\n  colormat <- matrix(rep(baseramp, length(fadesteps)), nrow = length(fadesteps), byrow = TRUE)\n\n\n  for(i in 1:length(fadesteps)) {\n    colormat[i, ] <- lighten(colormat[i, ], amount = fadesteps[i]) %>%\n      desaturate(amount = fadesteps[i])\n  }\n\n  return(colormat)\n}\n\nCreate another function that plots a matrix of colors. Typically that matrix comes out of col2dmat. Why not make one big function? because we will often want to access the color values themselves, and not always just plot them.\n\nplot2dcols <- function(colmat) {\n  coltib <- as_tibble(colmat, rownames = 'row') %>%\n    pivot_longer(cols = starts_with('V'), names_to = 'column') %>%\n    mutate(row = as.numeric(row), column = as.numeric(str_remove(column, 'V')))\n\n  colplot <- ggplot(coltib, aes(y = row, x = column, fill = value)) +\n    geom_tile() + scale_fill_identity()\n\n  return(colplot)\n}\n\nTest that works with a given number of fades\n\nnewcolors <- col2dmat('ag_Sunset', n1 = 8, n2 = 4)\nplot2dcols(newcolors)\n\n\n\n\nTest with set fade levels. REMEMBER FADE is FADE, not intensity. ie 0 is darkest.\n\nnewcolsuneven <- col2dmat('ag_Sunset', n1 = 8, fadevals = c(0, 0.33, 0.8))\nplot2dcols(newcolsuneven)\n\n\n\n\nTest with non-built in palettes- ie setting hue manually. This could be particularly useful if we want quantitative hues. This tests the ability to auto-detect a vector of colors.\nUse the manual-set colors from hcl exploration for testing.\n\nhclmat <- cbind(50, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 50, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg <- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\nWorks!\n\npgmat <- col2dmat(hex(pg), n2 = 4)\nplot2dcols(pgmat)"
  },
  {
    "objectID": "plotting/faded_colors.html#plotting-the-data",
    "href": "plotting/faded_colors.html#plotting-the-data",
    "title": "Faded colors",
    "section": "Plotting the data",
    "text": "Plotting the data\nAbove, we were trying to plot the colors. Now, we want to assign those colors to data so we can plot the data with the appropriate color.\n\nSingle datapoint\nThe above is fine for looking at a color matrix, but in general, we’ll have a dataframe with a value for each dimension, and need to assign it a single color. Step one is figuring out how to do that assignment.\nCan I take a ‘datapoint’ with arbitrary values on both axes and choose its color?\nCan we do that for both color bins or continuous color?\nWe’ll need to relativise the data, since neither hue or fade are defined on the real line, but by their endpoints.\nLet’s fake some data. Don’t use round numbers (e.g. 0, 100) to avoid making stupid mistakes relating to relativising the scale. We need to know the endpoints of the data to match the endpoints of the hue and fade, and then a datapoint somewhere in the middle to create.\n\n# what is the range of the data?\n  # don't use round numbers (e.g. 0, 100)\nmax1 <- 750\nmin1 <- 150\n\nmax2 <- 67\nmin2 <- -55\n\n\n# get color for a single value pair\nval1 <- 455\nval2 <- 8\n\njust use a simple linear transform to get position on the min-max axes. Could use logit or something for either, but keeping it simple. The value above the min divided by the range gives where the data point is on a 0-1 scale from min to max. In reality, we will have two vectors (well, cols in a dataframe), and this is actually easier to do in that case because we can just get the min and max directly.\n\nvalpos1 <- (val1-min1)/(max1-min1)\nvalpos2 <- (val2-min2)/(max2-min2)\n\nThat’s easy to vectorize, which is basically how we’ll do it with a dataframe.\nFor now, can we just get individual colors to assign to a value pair?\nNeed to specify the min and max hue- these are the hue endpoints, not data endpoints.\n\nminhue <- 130\nmaxhue <- 275\n\nfind the hue value at the same relative position as the datapoint\n\nmatchH1 <- (maxhue-minhue)*valpos1 + minhue\n\nUsing the manual colors\n\nsinglehclmat1 <- cbind(50, max_chroma(h = matchH1, l = 50, floor = TRUE),\n                matchH1)\n\npgsingle1 <- polarLUV(singlehclmat1)\nswatchplot(hex(pgsingle1))\n\n\n\n\nalso need the other axis. That’s also just on 0-1 (well, 1-0, since it’s fade, not intensity) and so would be done the same way.\n\nsinglecol <- col2dmat(hex(pgsingle1), fadevals = (1-valpos2))\nswatchplot(singlecol)\n\n\n\n\nIt’s clear we can write all this as functions, and that we’ll need to. So…\n\n\nProgramatically finding colors\nEarlier, we made col2dmat, which found colors and faded them. We want to do something similar here, but the goal isn’t quite the same- we don’t really care about the full matrix, but about a single point. We could modify col2dmat, but probably easier (and fewer horrible logicals) to just write purpose-built functions.\nNeed new functions to 1) find the hue, 2) adjust the fade\n\nFind the hue\nTakes either a number of bins or Inf for continuous.\n\nhuefinder <- function(hueval, minhue, maxhue, n = Inf, palname = NULL) {\n\n  # If continuous, use the value\n  # If binned, find the value of the bin the value is in\n  if (is.infinite(n)) {\n    matchH <- (maxhue-minhue)*hueval + minhue\n  } else if (!is.infinite(n)) {\n\n    nvec <- seq(from = 0, to = 1, length.out = n)\n\n    # The nvecs need to choose the COLOR, but the last one gets dropped in\n    # findInterval, so need an n+1\n    whichbin <- findInterval(hueval,\n                             seq(from = 0, to = 1, length.out = n+1),\n                             rightmost.closed = TRUE)\n\n    # Don't build if using named palette because won't have min and max\n    if (is.null(palname)) {\n      binhue <- nvec[whichbin]\n      matchH <- (maxhue-minhue)*binhue + minhue\n    }\n\n  }\n\n  if (is.null(palname)) {\n    h <- cbind(50, max_chroma(h = matchH, l = 50, floor = TRUE),\n               matchH)\n    h <- hex(polarLUV(h))\n  } else {\n    h <- sequential_hcl(n, palname)[whichbin]\n  }\n\n  return(h)\n}\n\n\n\nFind the fade\nThis takes the just found hue as basehue, and fades it. Again, n specifies either a number of fade bins or if infinite it is continuous and so just fades by whatever the value is.\n\nfadefinder <- function(fadeval, basehue, n = Inf) {\n\n  # If n is infinite, just use fadeval. Otherwise, bin, dropping the all-white level\n  if (is.infinite(n)) {\n    fadeval <- fadeval\n  } else {\n    # The +1 drops the white level\n    fadevec <- seq(from = 0, to = 1, length.out = n + 1)\n\n    # Rightmost closed fixes an issue right at 1\n    fadeval <- fadevec[findInterval(fadeval, fadevec, rightmost.closed = TRUE) + 1]\n  }\n\n  fadedcol <- lighten(basehue, amount = 1-fadeval) %>%\n    desaturate(amount = 1-fadeval)\n}\n\n\n\nHue and fade\nThis is meant to use in a mutate to take two columns of data and find the appropriate color. Should use … to pass, but whatever\n\ncolfinder <- function(hueval, fadeval, minhue, maxhue, nhue = Inf, nfade = Inf, palname = NULL) {\n  thishue <- huefinder(hueval, minhue, maxhue, nhue, palname)\n  thiscolor <- fadefinder(fadeval, thishue, nfade)\n}\n\nQuick tests\n\nfunhue <- huefinder(valpos1, minhue = minhue, maxhue = maxhue)\nfunfaded <- fadefinder(valpos2, funhue)\nswatchplot(funfaded)\n\n\n\n\nshould be the same as\n\nfunboth <- colfinder(valpos1, valpos2, minhue, maxhue)\nswatchplot(funboth)\n\n\n\n\n\n\n\nCalculating for dataframes\nVectorizing the relativization calculations is straightforward.\n\nvec1 <- c(150, 588, 750, 455, 234)\n\n# get it for each value in vectorized way\n(vec1 - min(vec1))/(max(vec1)-min(vec1))\n\n[1] 0.0000000 0.7300000 1.0000000 0.5083333 0.1400000\n\n\nMaking a function to get the relative position. We can use this in the mutate once we move on to dataframes.\n\nrelpos <- function(vec) {\n  (vec - min(vec))/(max(vec)-min(vec))\n}\n\nNow, let’s make a dataframe of fake data, with one column that should map to hue and the other mapping to fade. This just puts points all across the space of both variables so we can make sure everything is getting assigned correctly. Then, we’ll use the functions we just created to do a few different things:\n\ncustom hue ramps and built-in palettes\nbinned hue and fade\ncontinuous hue and binned fade\nboth continuous\n\nThe ‘continuous’ examples using inbuilt palettes are only pseudo-continuous by using large numbers of bins because that’s easier for the moment given the way sequential_hcl() works. There’s probably a way around it, but for the moment I’ll ignore it.\n\ncolortibble <- tibble(rvec1 = runif(10000, min = -20, max = 50),\n       rvec2 = runif(10000, min = 53, max = 99)) %>%\n  mutate(rel1 = relpos(rvec1),\n         rel2 = relpos(rvec2)) %>%\n  mutate(colorval = colfinder(rel1, rel2, minhue, maxhue),\n         binval = colfinder(rel1, rel2, minhue, maxhue, nhue = 8, nfade = 4),\n         # need to bypass some args\n         binsun = colfinder(rel1, rel2, nhue = 8, nfade = 4, palname = 'ag_Sunset',\n                            minhue = NULL, maxhue = NULL),\n         pseudoconsun = colfinder(rel1, rel2, nhue = 1000, nfade = 4, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL),\n         pseudoconsun2 = colfinder(rel1, rel2, nhue = 1000, nfade = Inf, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL))\n\nContinuous in both dimensions, using custom hue ramp\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = colorval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nBinned both dims, custom ramp\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = binval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nInbuilt palette, binned both dims.\nThere is a spot in this ag_Sunset palette that matches the ggplot default grey background and so hard to see, but I’ll ignore that for the moment since it doesn’t affect the main thing we’re doing. THese aren’t production plots.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = binsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nPseudo-continuous, binned fades.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = pseudoconsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nPseudo-continuous both dimensions.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = pseudoconsun2)) +\n  geom_point() +\n  scale_color_identity()"
  },
  {
    "objectID": "plotting/faded_colors.html#plotting-data",
    "href": "plotting/faded_colors.html#plotting-data",
    "title": "Faded colors",
    "section": "Plotting data",
    "text": "Plotting data\nNow, let’s see how that might look for some real data. I’ll use some with point data (iris) and then move on to maps, since that’s originally what this was developed for. It should easily extend to anything we can aes() on, e.g. barplot fills, etc.\n\nScatterplot\nTo keep it simple, let’s use iris\nIt won’t span the full space because of the relationship, but that’s OK, I think. We did that above. Here’s iris- now let’s color this plot.\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width)) + geom_point()\n\n\n\n\n\nFade defined by an axis\nThis is how we did it above when plotting the colors to make sure they were working.\nRelativize the x and y to define colors.\n\ncoloriris <- iris %>%\n  mutate(rel1 = relpos(Sepal.Length),\n         rel2 = relpos(Petal.Width)) %>%\n  mutate(colorval = colfinder(rel1, rel2, minhue, maxhue),\n         binval = colfinder(rel1, rel2, minhue, maxhue, nhue = 8, nfade = 4),\n         # need to bypass some args\n         binsun = colfinder(rel1, rel2, nhue = 8, nfade = 4, palname = 'ag_Sunset',\n                            minhue = NULL, maxhue = NULL),\n         pseudoconsun = colfinder(rel1, rel2, nhue = 1000, nfade = 4, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL),\n         pseudoconsun2 = colfinder(rel1, rel2, nhue = 1000, nfade = Inf, palname = 'ag_Sunset',\n                                   minhue = NULL, maxhue = NULL))\n\nMake some plots to see the colors and fades correspond to the axis values in binned and unbinned ways.\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = colorval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = pseudoconsun2)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = binsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\nFade as a new aesthetic\nTo actually match what I want to use this for, it’s more like we’d say versicolor is less certain. IE Species defines the fade. This is like fade is an aesthetic in ggplot, but we’re sort of manually doing it.\nLet’s set hue by sepal length, and fade by species\n\nuncertainVers <- iris %>%\n  mutate(rel1 = relpos(Sepal.Length),\n         faded = ifelse(Species == 'versicolor', 0.50, 1)) %>%\n  mutate(binhue = huefinder(rel1, n = 8, palname = 'ag_Sunset'),\n         conhue = huefinder(rel1, n = 1000, palname = 'ag_Sunset'),\n         binfade = fadefinder(faded, binhue),\n         confade = fadefinder(faded, conhue))\n\nNow, versicolor should be faded relative to the others\n\nggplot(uncertainVers, aes(x = Sepal.Length, y = Petal.Width, color = binfade)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\nggplot(uncertainVers, aes(x = Sepal.Length, y = Petal.Width, color = confade)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nThat seems to be working, both binned and continous on the hue scale.\n\n\n\nMaps\nWhat I really want this for is a map, with each polygon having a value of the variable of interest mapped to hue, and a ‘certainty’ determining the fade. Though that axis could really be any other value. Can I mock that up?\nRead a map in of catchments in Australia.\n\nallbasins <- read_sf(file.path('data', '42343_shp', 'rbasin_polygon.shp'))\n\nIgnoring fade for the minute, what should we color by? Probably should be random, really, for the demo.\nColoring by centroid will just put a cross-country fade on:\n\nggplot(allbasins, aes(fill = CENTROID_X)) + geom_sf() + scale_fill_continuous_sequential('ag_Sunset')\n\n\n\n\nLet’s make a column representing the value we want to plot for each basin, just chosen at random\n\nallbasins <- allbasins %>%\n  mutate(fakevals = runif(nrow(allbasins))) %>%\n  mutate(rel1 = relpos(fakevals)) %>%\n  mutate(binhue = huefinder(rel1, n = 8, palname = 'ag_Sunset'),\n         conhue = huefinder(rel1, n = 1000, palname = 'ag_Sunset'))\n\nI can use the values directly here with scale_fill_XX if I don’t care about fade\n\nggplot(allbasins, aes(fill = fakevals)) + geom_sf() + scale_fill_continuous_sequential('ag_Sunset')\n\n\n\n\nbut the hues for the faded should match the set hues. Now, I need to use scale_fill_identity(). Works for binned and pseudo-continuous. I’ll save the binned to compare later with the faded version.\n\nhuesonly <- ggplot(allbasins, aes(fill = binhue)) +\n  geom_sf() +\n  scale_fill_identity()\nhuesonly\n\n\n\nggplot(allbasins, aes(fill = conhue)) +\n  geom_sf() +\n  scale_fill_identity()\n\n\n\n\nNow, fade some out (with relatively low probability)\n\nallbasins <- allbasins %>%\n  mutate(faded = sample(x = c(1, 0.5),\n                           size = nrow(allbasins),\n                           replace = TRUE,\n                           prob = c(0.8, 0.2))) %>%\n  mutate(binfade = fadefinder(faded, binhue),\n         confade = fadefinder(faded, conhue))\n\nBinned and continuous. Again, save the binned for comparison\n\nhuefade <- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity()\nhuefade\n\n\n\nggplot(allbasins, aes(fill = confade)) +\n  geom_sf() +\n  scale_fill_identity()\n\n\n\n\nplot the raw and faded next to each other using patchwork. We can now see that some of the catchments are faded versions of the original hue.\n\nhuesonly + huefade\n\n\n\n\n\nLegends\nWe need legends. Could be done by playing with the actual ggplot legend or making mini plot and gluing on.\nQuick attempt at guide fails, because the colors are mixed up because of the RGB sorting.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend') +\n  guides(fill = guide_legend(ncol = 2))\n\n\n\n\nCan I change the order by basing it on the hues and then the fades? Does ‘breaks’ work? Yeah, sort of. And need to sort them in the right way.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = unique(allbasins$binhue))\n\n\n\n\nI think that will basically work, but I’ll need to edit a bit There’s probably a way to write the functions better to just do this all in the mutates, but for now, I can create a tibble of breaks and labels using summarise.\n\nbreaksnlabels <- allbasins %>%\n  st_drop_geometry() %>%\n  group_by(binhue) %>%\n  summarize(minbin = min(fakevals),\n            maxbin = max(fakevals),\n            fromto = paste0(as.character(round(minbin, 2)),\n                            ' to ',\n                            as.character(round(maxbin, 2)))) %>%\n  ungroup() %>%\n  arrange(minbin)\n\nWorks for the unfaded\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = breaksnlabels$binhue,\n                      labels = breaksnlabels$fromto)\n\n\n\n\nI could now ALSO fade those, but I might be able to do it as one summarise using the faded column\n\nfadebreaks <- allbasins %>%\n  st_drop_geometry() %>%\n  # needs to capture the color boundaries, whether or not faded\n  group_by(binhue) %>%\n  mutate(minbin = min(fakevals),\n            maxbin = max(fakevals),\n            fromto = paste0(as.character(round(minbin, 2)),\n                            ' to ',\n                            as.character(round(maxbin, 2)))) %>%\n  ungroup() %>%\n  group_by(binfade, faded) %>%\n  summarize(minbin = first(minbin),\n            maxbin = first(maxbin),\n            fromto = first(fromto)) %>%\n  ungroup() %>%\n  arrange(minbin, desc(faded))\n\n`summarise()` has grouped output by 'binfade'. You can override using the\n`.groups` argument.\n\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fadebreaks$binfade,\n                      labels = fadebreaks$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\nPlot tweaking\nThat’s close. Can I make the labels better? Ideally, drop from the faded, and make them at 45 or something. and fix up the size.\nFirst, drop the labels on the faded, since they are the same as the base hue.\n\nfb2 <- fadebreaks %>%\n  mutate(fromto = ifelse(faded == 0.5, '', fromto))\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom')\n\n\n\n\nFixing up the sizes and angles. The size doesn’t do what I want (square), because the text is too big.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n      legend.background = element_blank(),\n      legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n      legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nCan I fake it on the row labels by inserting line breaks? The number of lines is really unstable across device sizes or saving the figure, so the number of line breaks will have to be adjusted every time this gets saved etc. But it might kind of work.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value\\n\\n\\n\\nCertain\\n\\n\\nUncertain', title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nCan I bold that title?\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = expression(atop(bold('Value'),atop('Certain','Uncertain'))),\n                             title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nThat doesn’t work very well. Does ggtext do it? Allows markdown syntax and HTML (hence the  instead of ). It works, but still, the number of breaks will depend on the size of the figure device or file\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = '**Value**<br><br><br><br>Certain<br><br>Uncertain',\n                             title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'), # This should make them square, but isn't because the angled value labels don't allow it.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nIf we want square legend boxes and readable text for the value labels, might have to go vertical and that means re-doing the breaks and labels dataframe\n\nfbv <- fadebreaks %>%\n  mutate(fromto = ifelse(faded == 1, '', fromto)) %>%\n  arrange(desc(faded), minbin)\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  guides(fill = guide_legend(title = '**Value**<br><br>Certain Uncertain',\n                             title.position = 'top',\n                             ncol = 2, label.position = 'right')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'right',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'))\n\n\n\n\nThat works pretty well. If we wanted multiple levels of uncertainty (fades), a similar thing would work with just having more columns. That basically works. If I want to label the fades more robustly, I think I’ll likely need to resort to grobs, in which case I probably might as well do the figure as legend method.\n\n\nMini-figure legends\nSometimes we want to create a legend and then add it back into a figure (maybe if it’s shared, or we want a standard legend across a group of figures). Here, we might want to create a different legend for the certian and uncertain, glue them together, and then glue them back on the main figure.\nto show how this might make sense, let’s make three plots- one with just the certain, one with uncertain, and one with no legend, and then glue together Making this as vertical, but easy enough to swap\nMake the map alone\n\njustmap <- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  theme(legend.position = 'none')\n\n# used later- continuous specification of color and fade\njustmapcon <- ggplot(allbasins, aes(fill = confade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  theme(legend.position = 'none')\n\nGet the indices for the two fades\n\ncerts <- which(fbv$faded == 1)\nuncerts <- which(fbv$faded == 0.5)\n\nMake the legend for the unfaded\n\ncertleg <- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade[certs],\n                      labels = fbv$fromto[certs]) +\n  guides(fill = guide_legend(title = 'Certain',\n                             title.position = 'top',\n                             ncol = 1, label.position = 'right')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'right',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'))\n\n# I don't actually want the plot, just the legend, so\n certleg <- ggpubr::get_legend(certleg)\n\nAnd the faded\n\n uncertleg <- ggplot(allbasins, aes(fill = binfade)) +\n   geom_sf() +\n   scale_fill_identity(guide = 'legend',\n                       breaks = fbv$binfade[uncerts],\n                       labels = fbv$fromto[uncerts]) +\n   guides(fill = guide_legend(title = 'Uncertain',\n                              title.position = 'top',\n                              ncol = 1, label.position = 'right')) +\n   theme(legend.title = ggtext::element_markdown(),\n         legend.position = 'right',\n         legend.background = element_blank(),\n         legend.key.size = unit(0.5, 'cm'))\n\n # I don't actually want the plot, just the legend, so\n uncertleg <- ggpubr::get_legend(uncertleg)\n\nGlue those legends\n\nbothleg <- ggpubr::ggarrange(certleg, uncertleg)\n\nand glue on the plot\n\n plotpluslegs <- ggpubr::ggarrange(justmap, bothleg, widths = c(8,2))\n plotpluslegs\n\n\n\n\nThat’s not really any better than what I had before. It is useful to have this level of control sometimes though. In particular, we might want to use a PLOT as a legend, either binned or not.\nTo use a plot as a legend\nHere, binned is obviously the way to go, especially for the two fade levels, but let’s demo both.\nabove, we defined a function col2dmat that makes a plot of the color matrix. Let’s use that to demo a few options. First create the figures that will be the legends.\nBinned both dims, two fades, but just low-high labels\n\nbinnedplotmat <- col2dmat('ag_Sunset', n1 = 8, fadevals = c(0, 0.5))\n bin2legqual <- plot2dcols(binnedplotmat) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = c(1, 2), labels = c('Uncertain', 'Certain')) +\n   # Vague levels\n   scale_x_continuous(breaks = c(1, 8), labels = c('Low', 'High')) +\n   theme(axis.text = element_text())\n bin2legqual\n\n\n\n\nBinned both dims, but now the hue values are quantitatively labeled\n\nnamedlabs <- filter(fb2, fromto != '') %>% select(fromto) %>% pull()\n bin2legquant <- plot2dcols(binnedplotmat) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = c(1, 2), labels = c('Uncertain', 'Certain')) +\n   # Vague levels\n   scale_x_continuous(breaks = 1:8, labels = namedlabs) +\n   theme(axis.text.y = element_text(),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n   ggtitle('Value')\n bin2legquant\n\n\n\n\nA few levels of fade. Very similar to above\n\nmat4fade <- col2dmat('ag_Sunset', n1 = 8, n2 = 4)\n\n fadevals <- rev(seq(0,1, length.out = 4+1))[1:4]\n bin4leg <- plot2dcols(mat4fade) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = 1:4, labels = rev(fadevals), name = 'Certainty') +\n   scale_x_continuous(breaks = 1:8, labels = namedlabs, name = 'Value') +\n   theme(axis.text.y = element_text(),\n         axis.title.y = element_text(angle = 90),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n         axis.title.x = element_text())\n bin4leg\n\n\n\n\npseudo-continuous. put the x-axis on top, because that’s what we’d expect for a legend, really. Labels can take a lambda function of the breaks, allowing us to use auto-chosen breaks. But probably better to reference the values they correspond to. It’s just that for this silly demo they are 0-1. Let’s pretend for the minute that they’re logged just for fun and to demo how to do it.\n\nmatcfade <- col2dmat('ag_Sunset', n1 = 100, n2 = 100)\n conleg <- plot2dcols(matcfade) +\n   theme_void() +\n   scale_y_continuous(name = 'Certainty %') +\n   #\n   scale_x_continuous(labels = ~round(log(.), 2), name = 'Value', position = 'top') +\n   theme(axis.text.y = element_text(),\n         axis.title.y = element_text(angle = 90),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n         axis.title.x = element_text())\n conleg\n\n\n\n\nNow, attach those to the map as legends.\nI’ll use patchwork for most of them, but ggpubr::ggarrange would work too, just with different tweaking. The way patchwork does insets and sizes is working better for me right now, so that’s what I’ll use.\nTaking the grey background off because it’s distracting with inset legends.\nTwo-level binned legend with high-low\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element(bin2legqual, left = 0.1, bottom = 0.1, right = 0.5, top = 0.2)\n\n\n\n\nSame, but quantitative legend labels. Text is a bit absurd.\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((bin2legquant + theme(axis.text = element_text(size = 8),\n                                       title = element_text(size = 8))),\n                 left = 0.1, bottom = 0, right = 0.5, top = 0.25)\n\n\n\n\nA 4-fade example with quantitative fades as well. That’s not our immediate need, but good to be able to do. maybe fade according to standard error or something.\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((bin4leg + theme(axis.text = element_text(size = 8),\n                                       title = element_text(size = 8))),\n                 left = 0.1, bottom = 0, right = 0.5, top = 0.25)\n\n\n\n\nContinuous values in both dimensions. Here, we use a map where colors and fades are both defined continuously.\n\n(justmapcon + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((conleg + coord_fixed() +\n                    theme(axis.text = element_text(size = 8),\n                          title = element_text(size = 8))),\n                 left = 0.1, bottom = 0.05, right = 0.5, top = 0.25)\n\n\n\n\nCan I put the legend off to the side just by specifying bigger coords? sort of- it goes but gets lost\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((conleg + coord_fixed() +\n                    theme(axis.text = element_text(size = 8),\n                          title = element_text(size = 8))),\n                 left = 1, bottom = 0.4, right = 1.5, top = 0.75)\n\n\n\n\nWorks with making a small plot with spacers and then glueing that onto the big plot\n\nguidespot <- plot_spacer() /\n   (conleg + coord_fixed() +\n   theme(axis.text = element_text(size = 8),\n         title = element_text(size = 8))) /\n   plot_spacer()\n\n (justmap + theme_bw() + theme(legend.position = 'none')) +\n   guidespot +\n   plot_layout(widths = c(9, 1))\n\n\n\n\nDoes that work with the simpler ones? Yeah, although the 2-fades makes more sense horizontal, so do that\n\n# I can't fiugre out why this creates a dataframe. results = 'hide' doesn't hide it, wrapping with invisible(), etc. I give up. Giving it its own code block\nguidespot2 <- plot_spacer() |\n   (bin2legquant + theme(axis.text = element_text(size = 8),\n                         title = element_text(size = 8))) |\n   plot_spacer()\n\n\n (justmap + theme_bw() + theme(legend.position = 'none')) /\n   guidespot2 +\n   plot_layout(heights = c(9, 1))\n\n\n\n\nA very similar approach would work for ggpubr::ggarrange\nThere’s quite a lot more that could be done here, but this gets me what I need for now."
  },
  {
    "objectID": "plotting/faded_colors.html#notes",
    "href": "plotting/faded_colors.html#notes",
    "title": "Faded colors",
    "section": "Notes",
    "text": "Notes\nif this were truly bivariate (ie two variables of interest), could rotate 45 degrees to equally weight (and likely use different color ramps). But it’s not- it’s certainty along one axis, so leaving horiz and having a lightness axis fits what we’re doing here better."
  },
  {
    "objectID": "plotting/fonts.html",
    "href": "plotting/fonts.html",
    "title": "Fonts",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\nUsing knitr::inline_expr(r message = FALSE, warning = FALSE) hopefully stops it printing all the package messages\n\nlibrary(tidyverse) # Overkill, but easier than picking and choosing\n\nThese are mostly little plot tweaks and small things that I find and forget all the time.\n\nAccessing fonts\nIn the past, I’ve used extrafonts to use fonts within figures, but it’s failing for me (‘No FontName, skipping’ error as in https://github.com/wch/extrafont/issues/88).\nTry sysfonts. Actually, showtext on top of sysfonts. First, look at how it finds the fonts.\n\nlibrary(showtext)\n\nWarning: package 'showtext' was built under R version 4.2.2\n\n\nLoading required package: sysfonts\n\n\nWarning: package 'sysfonts' was built under R version 4.2.2\n\n\nLoading required package: showtextdb\n\n\nWarning: package 'showtextdb' was built under R version 4.2.2\n\nfontsIhave <- font_files()\nfontsIhave\n\n\n\n  \n\n\nstr(fontsIhave)\n\n'data.frame':   349 obs. of  6 variables:\n $ path   : chr  \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" ...\n $ file   : chr  \"AGENCYB.TTF\" \"AGENCYR.TTF\" \"ALGER.TTF\" \"ANTQUAB.TTF\" ...\n $ family : chr  \"Agency FB\" \"Agency FB\" \"Algerian\" \"Book Antiqua\" ...\n $ face   : chr  \"Bold\" \"Regular\" \"Regular\" \"Bold\" ...\n $ version: chr  \"Version 1.01\" \"Version 1.01\" \"Version 1.57\" \"Version 2.35\" ...\n $ ps_name: chr  \"AgencyFB-Bold\" \"AgencyFB-Reg\" \"Algerian\" \"BookAntiqua-Bold\" ...\n\n\nI should be able to use font_add\nFirst, what fonts are CURRENTLY available in R?\n\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\nfont_families()\n\n[1] \"sans\"         \"serif\"        \"mono\"         \"wqy-microhei\"\n\n\nTest with one of the\n\nfont_add('Bookman Old Style', regular = 'BOOKOS.TTF', \n         italic = 'BOOKOSI.TTF', \n         bold = 'BOOKOSB.TTF', \n         bolditalic = 'BOOKOSBI.TTF')\n\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\nfont_families()\n\n[1] \"sans\"              \"serif\"             \"mono\"             \n[4] \"wqy-microhei\"      \"Bookman Old Style\"\n\n\nI’m not quite understanding how this object is organised. What is that last line? are the $xxxx the defaults?\nTo test, let’s make a plot and try to change font.\nThe help (https://cran.rstudio.com/web/packages/showtext/vignettes/introduction.html) says we need to tell R to use showtext for text.\n\nshowtext_auto()\n\n\nbaseiris <- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\nbaseiris + theme(axis.title = element_text(family = 'Bookman Old Style'),\n                 legend.title = element_text(family = 'Bookman Old Style', face = 'bold.italic'))\n\n\n\n\nThat seems to work. Can I feed in all the fonts on my system automaticallly? Is that a bad idea? Might be if it takes a while and we only want one.\nFirst, though, can I give it a font name as a character and it load all of the faces automatically?\nNote: some of the fonts I have have weird faces. For now, just fail on those and stick with the ones supported by showtext. That should be fine.\n\nunique(fontsIhave$face)\n\n [1] \"Bold\"            \"Regular\"         \"Bold Italic\"     \"Italic\"         \n [5] \"Demibold\"        \"Demibold Italic\" \"Demibold Roman\"  \"Bold Oblique\"   \n [9] \"Oblique\"         \"Light\"          \n\n\nThis is a) a useful thing to simplify adding single fonts, and b) precursor to loading them all.\n\n# chosen more or less at random for testing\nfamilyname <- 'Candara'\n\n# I could use dplyr but this seems better to just use base logical indexing.\n# fontsIhave %>%\n#   filter(family == familyname & face == 'Regular') %>%\n#   select(file) %>%\n#   pull()\n\n# Could do all the indexing in the function call to font_add(), but it just gets ridiculous\nregfile <- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Regular'), 'file']\n\nitalfile <- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Italic'), 'file']\n\nboldfile <- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Bold'), 'file']\n\nbifile <- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Bold Italic'), 'file']\n\n\n# NEED TO TEST WHEN THE FACE DOESN'T EXIST AND THROW NULL\n  # If not there, the value will be character(0). testing for that and returning NULL (which is what the function needs) is a bit tricky:\nnoface <- function(x) {ifelse(rlang::is_empty(x), return(NULL), return(x))}\n\nfont_add(familyname, \n         regular = noface(regfile), \n         italic = noface(italfile), \n         bold = noface(boldfile), \n         bolditalic = noface(bifile))\n\nTest that with a figure\n\nbaseiris + theme(axis.title.x = element_text(family = familyname, face = 'italic'),\n                 axis.title.y = element_text(family = familyname, face = 'bold'),\n                 legend.text = element_text(family = familyname),\n                 legend.title = element_text(family = familyname, face = 'bold.italic'))\n\n\n\n\nHow bad an idea is it to just read them ALL in at once?\nEasy enough to feed the font_add above in a loop. Probably vectorizable too, but why bother?\nWrite it as a function, then it will work for all fonts or a subset if that’s a bad idea. Either feed it a dataframe of fonts or just use font_files() directly. It can also take NULL for fontvec, in which case it loads all the fonts.\n\nloadfonts <- function(fontvec = NULL, fontframe = NULL) {\n  \n  # Get all fonts if no fontframe\n  if (is.null(fontframe)) {\n    fontframe <- font_files()\n  }\n  \n  # Load all fonts if no fontvec\n  if (is.null(fontvec)) {\n    fontvec <- unique(fontframe$family)\n  }\n  \n  # Loop over the font families\n  for (i in 1:length(fontvec)) {\n    familyname <- fontvec[i]\n    regfile <- fontframe[which(fontframe$family == familyname &\n                   fontframe$face == 'Regular'), 'file']\n\n    italfile <- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Italic'), 'file']\n    \n    boldfile <- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Bold'), 'file']\n    \n    bifile <- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Bold Italic'), 'file']\n    \n## TODO: THROW A TRYCATCH ON HERE TO BYPASS AND ALERT FOR FAILURES\n    # For example, Berlin Sans FB Demi has no 'regular' and so fails. let's just skip those, this isn't supposed to be the most robust thing ever that handles all cases flawlessly.\n    try(font_add(fontvec[i], \n         regular = noface(regfile), \n         italic = noface(italfile), \n         bold = noface(boldfile), \n         bolditalic = noface(bifile)))\n    \n    # To avoid unforeseen carryover through the loop\n    rm(familyname, regfile, italfile, boldfile, bifile)\n  }\n  \n}\n\nLet’s try just reading everything in. Use try in the function above because there are failures for a few reasons, and instead of dealing with them I just want to quickly read in what’s easy to read in. I don’t have a ton of interest here in figuring out corner cases for weird fonts.\nTest the function with a vector of fontnames first, because can’t do that after try with everything\n\nloadfonts(fontvec = c('Consolas', 'Comic Sans MS', 'Tahoma'))\nfont_families()\n\n[1] \"sans\"              \"serif\"             \"mono\"             \n[4] \"wqy-microhei\"      \"Bookman Old Style\" \"Candara\"          \n[7] \"Consolas\"          \"Comic Sans MS\"     \"Tahoma\"           \n\n\nNow, go for it with everything. There are a million errors, so I’ve turned error reporting off for this code chunk.\n\nsystem.time(loadfonts())\n\nThat was pretty quick. What do I have?\n\nfont_families()\n\n  [1] \"sans\"                            \"serif\"                          \n  [3] \"mono\"                            \"wqy-microhei\"                   \n  [5] \"Bookman Old Style\"               \"Candara\"                        \n  [7] \"Consolas\"                        \"Comic Sans MS\"                  \n  [9] \"Tahoma\"                          \"Agency FB\"                      \n [11] \"Algerian\"                        \"Book Antiqua\"                   \n [13] \"Arial\"                           \"Arial Narrow\"                   \n [15] \"Arial Black\"                     \"Arial Rounded MT Bold\"          \n [17] \"Bahnschrift\"                     \"Baskerville Old Face\"           \n [19] \"Bauhaus 93\"                      \"Bell MT\"                        \n [21] \"Bernard MT Condensed\"            \"Bodoni MT\"                      \n [23] \"Bodoni MT Black\"                 \"Bodoni MT Condensed\"            \n [25] \"Bodoni MT Poster Compressed\"     \"Bradley Hand ITC\"               \n [27] \"Britannic Bold\"                  \"Berlin Sans FB\"                 \n [29] \"Broadway\"                        \"Bookshelf Symbol 7\"             \n [31] \"Calibri\"                         \"Calibri Light\"                  \n [33] \"Californian FB\"                  \"Calisto MT\"                     \n [35] \"Cambria\"                         \"Candara Light\"                  \n [37] \"Cascadia Code\"                   \"Cascadia Mono\"                  \n [39] \"Castellar\"                       \"Century Schoolbook\"             \n [41] \"Centaur\"                         \"Century\"                        \n [43] \"Chiller\"                         \"Colonna MT\"                     \n [45] \"Constantia\"                      \"Cooper Black\"                   \n [47] \"Copperplate Gothic Bold\"         \"Copperplate Gothic Light\"       \n [49] \"Corbel\"                          \"Corbel Light\"                   \n [51] \"Courier New\"                     \"Curlz MT\"                       \n [53] \"Dubai\"                           \"Dubai Light\"                    \n [55] \"Dubai Medium\"                    \"Ebrima\"                         \n [57] \"Elephant\"                        \"Engravers MT\"                   \n [59] \"Eras Bold ITC\"                   \"Eras Demi ITC\"                  \n [61] \"Eras Light ITC\"                  \"Eras Medium ITC\"                \n [63] \"Felix Titling\"                   \"Forte\"                          \n [65] \"Franklin Gothic Book\"            \"Franklin Gothic Demi\"           \n [67] \"Franklin Gothic Demi Cond\"       \"Franklin Gothic Heavy\"          \n [69] \"Franklin Gothic Medium\"          \"Franklin Gothic Medium Cond\"    \n [71] \"Freestyle Script\"                \"French Script MT\"               \n [73] \"Footlight MT Light\"              \"Gabriola\"                       \n [75] \"Gadugi\"                          \"Garamond\"                       \n [77] \"Georgia\"                         \"Gigi\"                           \n [79] \"Gill Sans MT\"                    \"Gill Sans MT Condensed\"         \n [81] \"Gill Sans Ultra Bold Condensed\"  \"Gill Sans Ultra Bold\"           \n [83] \"Gloucester MT Extra Condensed\"   \"Gill Sans MT Ext Condensed Bold\"\n [85] \"Century Gothic\"                  \"Goudy Old Style\"                \n [87] \"Goudy Stout\"                     \"Harrington\"                     \n [89] \"Haettenschweiler\"                \"Microsoft Himalaya\"             \n [91] \"HoloLens MDL2 Assets\"            \"HP Simplified\"                  \n [93] \"HP Simplified Light\"             \"HP Simplified Hans Light\"       \n [95] \"HP Simplified Hans\"              \"HP Simplified Jpan Light\"       \n [97] \"HP Simplified Jpan\"              \"High Tower Text\"                \n [99] \"Impact\"                          \"Imprint MT Shadow\"              \n[101] \"Informal Roman\"                  \"Ink Free\"                       \n[103] \"Blackadder ITC\"                  \"Edwardian Script ITC\"           \n[105] \"Kristen ITC\"                     \"Javanese Text\"                  \n[107] \"Jokerman\"                        \"Juice ITC\"                      \n[109] \"Kunstler Script\"                 \"Lucida Sans Unicode\"            \n[111] \"Wide Latin\"                      \"Lucida Bright\"                  \n[113] \"Leelawadee UI\"                   \"Leelawadee UI Semilight\"        \n[115] \"Lucida Fax\"                      \"Lucida Sans\"                    \n[117] \"Lucida Sans Typewriter\"          \"Lucida Console\"                 \n[119] \"Maiandra GD\"                     \"Malgun Gothic\"                  \n[121] \"Malgun Gothic Semilight\"         \"Marlett\"                        \n[123] \"Matura MT Script Capitals\"       \"Microsoft Sans Serif\"           \n[125] \"MingLiU-ExtB\"                    \"Mistral\"                        \n[127] \"Myanmar Text\"                    \"Modern No. 20\"                  \n[129] \"Mongolian Baiti\"                 \"MS Gothic\"                      \n[131] \"Microsoft JhengHei\"              \"Microsoft JhengHei Light\"       \n[133] \"Microsoft YaHei\"                 \"Microsoft YaHei Light\"          \n[135] \"Microsoft Yi Baiti\"              \"Monotype Corsiva\"               \n[137] \"MT Extra\"                        \"MV Boli\"                        \n[139] \"Niagara Engraved\"                \"Niagara Solid\"                  \n[141] \"Nirmala UI\"                      \"Nirmala UI Semilight\"           \n[143] \"Microsoft New Tai Lue\"           \"OCR A Extended\"                 \n[145] \"Old English Text MT\"             \"Onyx\"                           \n[147] \"MS Outlook\"                      \"Palatino Linotype\"              \n[149] \"Palace Script MT\"                \"Papyrus\"                        \n[151] \"Parchment\"                       \"Perpetua\"                       \n[153] \"Microsoft PhagsPa\"               \"Playbill\"                       \n[155] \"Poor Richard\"                    \"Pristina\"                       \n[157] \"Rage Italic\"                     \"Ravie\"                          \n[159] \"MS Reference Sans Serif\"         \"MS Reference Specialty\"         \n[161] \"Rockwell Condensed\"              \"Rockwell\"                       \n[163] \"Rockwell Extra Bold\"             \"Sans Serif Collection\"          \n[165] \"Script MT Bold\"                  \"Segoe MDL2 Assets\"              \n[167] \"Segoe Fluent Icons\"              \"Segoe Print\"                    \n[169] \"Segoe Script\"                    \"Segoe UI\"                       \n[171] \"Segoe UI Light\"                  \"Segoe UI Semilight\"             \n[173] \"Segoe UI Black\"                  \"Segoe UI Emoji\"                 \n[175] \"Segoe UI Historic\"               \"Segoe UI Semibold\"              \n[177] \"Segoe UI Symbol\"                 \"Segoe UI Variable\"              \n[179] \"Showcard Gothic\"                 \"SimSun\"                         \n[181] \"SimSun-ExtB\"                     \"Sitka Text\"                     \n[183] \"Snap ITC\"                        \"Stencil\"                        \n[185] \"Sylfaen\"                         \"Symbol\"                         \n[187] \"Microsoft Tai Le\"                \"Tw Cen MT\"                      \n[189] \"Tw Cen MT Condensed\"             \"Tw Cen MT Condensed Extra Bold\" \n[191] \"Tempus Sans ITC\"                 \"Times New Roman\"                \n[193] \"Trebuchet MS\"                    \"Verdana\"                        \n[195] \"Viner Hand ITC\"                  \"Vladimir Script\"                \n[197] \"Webdings\"                        \"Wingdings\"                      \n[199] \"Wingdings 2\"                     \"Wingdings 3\"                    \n[201] \"Yu Gothic\"                       \"Yu Gothic Light\"                \n[203] \"Yu Gothic Medium\"                \"ZWAdobeF\"                       \n\n\nI’m sure if there were something that got bypassed that I really needed I could get it directly with font_add(), but this is sure quick to get them all. Test a couple of the new ones.\n\nbaseiris + theme(axis.title.x = element_text(family = 'Poor Richard', face = 'italic'),\n                 axis.title.y = element_text(family = 'Stencil', face = 'bold'),\n                 legend.text = element_text(family = 'Papyrus'),\n                 legend.title = element_text(family = 'Onyx', face = 'bold.italic'))\n\n\n\n\nI have also put loadfonts in the functions folder so I can use it elsewhere."
  },
  {
    "objectID": "plotting/ggplot_themes.html",
    "href": "plotting/ggplot_themes.html",
    "title": "Custom ggplot themes",
    "section": "",
    "text": "I often want to consistently theme my ggplots across projects. I’ve developed some custom themes, but they’re usually ad-hoc, and don’t work particularly well in packages, because the simple way to do it isn’t a function.\nFor example, we might have a theme that’s good for publication, as in Saving and theming plots, where we specify size, backgrounds, and text. We load new fonts first.\nHere, the theme is essentially hardcoded, and we can add it to a figure."
  },
  {
    "objectID": "plotting/ggplot_themes.html#can-we-pass-other-arguments",
    "href": "plotting/ggplot_themes.html#can-we-pass-other-arguments",
    "title": "Custom ggplot themes",
    "section": "Can we pass other arguments?",
    "text": "Can we pass other arguments?\nIf the goal is to enforce a style, we might not want to allow passing other arguments to theme, but can we with …?\n\ntheme_pub_dots <- function(base_size = 10, font, ...) {\n  if (!(font %in% sysfonts::font_families())) {\n      loadfonts(font)\n  }\n\n  ggplot2::theme_bw(base_size = base_size) +\n    theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=font), \n        ...)\n}\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) + \n  geom_point() + \n  facet_wrap(~Species) +\n  theme_pub_dots(base_size = 8, font = 'Arial', legend.position = 'none')"
  },
  {
    "objectID": "plotting/hcl_exploration.html",
    "href": "plotting/hcl_exploration.html",
    "title": "hcl exploration",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\nUsing knitr::inline_expr(r message = FALSE, warning = FALSE) hopefully stops it printing all the package messages\nFinding colors to use for a given plot can be a pain. I’m trying to find some good color ramps for a project, and also sort out manipulating those colors to allow fading. This is me playing around to try to understand how to do those manipulations and looking at the various potential color palettes.\nColorspace (https://colorspace.r-forge.r-project.org/index.html) is a particularly useful package (though it is not the only color package I use).\nColorspace uses a hue-chroma-luminance specification for colors that is really powerful. It also has built-in palettes. For some other work, I was interested in exploring moving along those dimensions and generating color palettes for plotting.\nPreviously (for the project that gave rise to looking at fading colors), I was using purples and emerald, so let’s start there. But for simplicity switch to greens so constant hue.\nI actually like those single-hue fades a lot for showing more or less of something. But it SHOULD be possible to do a hue shift from green to purple for one axis? will that make sense?"
  },
  {
    "objectID": "plotting/hcl_exploration.html#hue-sequences",
    "href": "plotting/hcl_exploration.html#hue-sequences",
    "title": "hcl exploration",
    "section": "Hue sequences",
    "text": "Hue sequences\nI’d like to be able to specify the endpoints of a hue sequence and just shift along that axis. I’ll try it out with the purple and green above.\nFirst, I want to try to get the hue values (and the L and C as well) to make the endpoints. I can’t find a straightforward extraction in colorspace to get the HCLs though. So, since I know the endpoints are coming from those palettes above, I want their values. Make the palette, turn it into RGB, then turn the RGB into polarLUV to get the three axis values. Here, rows are the 8 fades in the palettes above.\n\nrgbpurps <- hex2RGB(sequential_hcl(8, 'Purples'))\n\nluvpurps <- as(rgbpurps, 'polarLUV')\nluvpurps\n\n            L         C        H\n[1,] 19.88570 55.128356 274.8415\n[2,] 34.37280 69.304529 274.3131\n[3,] 47.99202 56.799744 273.3506\n[4,] 60.90031 43.200021 272.3221\n[5,] 72.77975 31.772302 271.4182\n[6,] 83.46538 21.076091 271.6285\n[7,] 92.78865 10.863733 268.7090\n[8,] 98.79258  2.985742 276.3941\n\n\nThat’s sure roundabout, going palette that’s polarLUV under the hood but returns in hex to rgb and then back to polarLUV. Seems to work though.\n\nswatchplot(hex(luvpurps))\n\n\n\n\n\nrgbgrns <- hex2RGB(sequential_hcl(8, 'Greens'))\n\nluvgrns <- as(rgbgrns, 'polarLUV')\nluvgrns\n\n            L         C        H\n[1,] 25.06952 33.792199 132.8916\n[2,] 40.15678 49.456834 132.0640\n[3,] 54.06676 63.854764 129.4059\n[4,] 66.47833 62.340742 126.5380\n[5,] 77.49000 47.581607 123.8001\n[6,] 86.86700 33.248323 120.6451\n[7,] 93.95644 19.112933 117.4570\n[8,] 98.08100  5.367478 116.7639\n\n\nI can swatchplot them up together.\n\nswatchplot(hex(luvpurps), hex(luvgrns))\n\n\n\n\nNow, the goal is actually to identify those dark colors and transition between them. Now, can I get from purple to green? The L and C are quite different, unfortunately. Pick something the middle?\nHardcode numbers for now, though ideally we’ll get to a function that takes a start and end value.\n\npg <- polarLUV(L = 20, C = 40, H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\nWarning in max(nchar(rnam) - 1L): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\nThat fails. So, now we learned the ranges of the other axes matter. Likely chroma?\n\n# Fails\nmax_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20)\n\n[1] 29.55000 19.90429 16.62571 16.05429 17.74000 23.18000 41.07429 66.11000\n\n\nCan I just use the minimum max_chroma? Not really…\n\n# Guessing I can't just go with 16, but let's try\npg <- polarLUV(L = 20, C = 16, H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\n\n\n\nIf I try to fix how dark that is with chroma, it doesn’t work very well and I still lose one.\n\npg <- polarLUV(L = 20, \n               C = max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20),\n               H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\n\n\n\nUsing a matrix isn’t the answer- same thing, though a floor argument puts the missing color back\n\nhclmat <- cbind(20, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20, floor = TRUE),\n      seq(from = 130, to = 275, length.out = 8))\n\npg <- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\nGuessing I don’t want to just turn up luminance, but let’s see what that does to get a better sense how this all works.\n\nhclmat <- cbind(80, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 80, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg <- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\nLower luminance does work OK, but it’s still ‘darker’ in the middle and the shift to blue on the right is abrupt. The darker middle is likely why a lot of the colorspace palettes have triangular luminance. I don’t particularly want to get so fine-tuned here. I was looking for a way to programatically define these sequences, and getting into tweaking luminance in a nonlinear and nonmonotonic way could get very bespoke very quickly. Likely better to just use the built-in palettes where someone who understands color theory has already done that.\n\nhclmat <- cbind(50, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 50, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg <- polarLUV(hclmat)\nswatchplot(hex(pg))"
  },
  {
    "objectID": "plotting/hcl_exploration.html#fading",
    "href": "plotting/hcl_exploration.html#fading",
    "title": "hcl exploration",
    "section": "Fading",
    "text": "Fading\nI also want to make faded versions of palettes, and control levels of fade. The particular use I have in mind is to illustrate levels of uncertainty, but it could be any bivariate outcomes.\nI originally thought that I would need to manually adjust the chroma and luminance manually. But the exploration above suggests they interact and so it’s unlikely to just shift one or the other. Still, colorspace provides lighten, darken (which both shift luminance), and desaturate, which shifts chroma. I should be able to play with these to see how they work using either a homebrew base palette as above or the inbuilt ones.\nIn either case, we need the hex values\n\nhexcols <- hex(pg)\n\nLighten (increase luminance)\n\nswatchplot('orig' = hexcols,\n           '25' = lighten(hexcols, amount = 0.25),\n           '50' = lighten(hexcols, amount = 0.5),\n           '75' = lighten(hexcols, amount = 0.75),\n           '100' = lighten(hexcols, amount = 1))\n\n\n\n\nDarken (decrease luminance)\n\nswatchplot('orig' = hexcols,\n           '25' = darken(hexcols, amount = 0.25),\n           '50' = darken(hexcols, amount = 0.5),\n           '75' = darken(hexcols, amount = 0.75),\n           '100' = darken(hexcols, amount = 1))\n\n\n\n\nDesaturate (adjust chroma)\n\nswatchplot('orig' = hexcols,\n           '25' = desaturate(hexcols, amount = 0.25),\n           '50' = desaturate(hexcols, amount = 0.5),\n           '75' = desaturate(hexcols, amount = 0.75),\n           '100' = desaturate(hexcols, amount = 1))\n\n\n\n\nFor my particular use, I like desaturating better, in that it implies less information. But it also makes the values look more similar across the range, and we don’t want that. That gets captured better by lightening.\nAs a bit of an aside, the ends of the lightened versions are effectively ‘Purples’ and ‘Greens’, reading down instead of across. What does it look like if I desaturate those built-in palettes?\n\npurp8 <- sequential_hcl(8, 'Purples')\nswatchplot('orig' = purp8,\n           '25' = desaturate(purp8, amount = 0.25),\n           '50' = desaturate(purp8, amount = 0.5),\n           '75' = desaturate(purp8, amount = 0.75),\n           '100' = desaturate(purp8, amount = 1))\n\n\n\n\nIt does remove color, but it perceptually darkens as well, which is NOT what I want.\nWhat about choosing a pre-built set of colors and lightening/darkening? Start with viridis, we know it has good properties in greyscale, etc.\n\nvir8 <- sequential_hcl(8, 'Viridis')\nswatchplot('orig' = vir8,\n                 '25' = lighten(vir8, amount = 0.25),\n                 '50' = lighten(vir8, amount = 0.5),\n                 '75' = lighten(vir8, amount = 0.75),\n                 '100' = lighten(vir8, amount = 1))\n\n\n\n\nThat actually works pretty well, even though the original had a luminance ramp on it already (https://colorspace.r-forge.r-project.org/articles/approximations.html), this just shifts it each time, I think. We can compare using specplot.\n\nspecplot(vir8, lighten(vir8, amount = 0.75))\n\n\n\n\nWhat does a desaturated viridis look like?\n\nswatchplot('orig' = vir8,\n           '25' = desaturate(vir8, amount = 0.25),\n           '50' = desaturate(vir8, amount = 0.5),\n           '75' = desaturate(vir8, amount = 0.75),\n           '100' = desaturate(vir8, amount = 1))\n\n\n\n\nAgain, makes them more similar, though the underlying luminance ramp helps. I don’t like that the first level still ends up darker though.\n\nInteracting chroma and luminance\nSo, changing luminance makes colors brighter or darker, while adjusting chroma removes color but tends to make them darker. Neither is exactly what I want- a color ramp that look the same, just “faded”. Is the answer to control this interaction? Does a simultaneous lighten and desaturate give me what I want by avoiding the perceptual darkening from the desaturation?\n\nswatchplot('orig' = vir8,\n           '25' = desaturate(vir8, amount = 0.25) %>%\n             lighten(amount = 0.25),\n           '50' = desaturate(vir8, amount = 0.5) %>%\n             lighten(amount = 0.5),\n           '75' = desaturate(vir8, amount = 0.75) %>%\n             lighten(amount = 0.75),\n           '100' = desaturate(vir8, amount = 1) %>%\n             lighten(amount = 1))\n\n\n\n\nThat works really well, actually. Does the order of operations matter? No:\n\nswatchplot('orig' = vir8,\n           '25' = lighten(vir8, amount = 0.25) %>%\n             desaturate(amount = 0.25),\n           '50' = lighten(vir8, amount = 0.5) %>%\n             desaturate(amount = 0.5),\n           '75' = lighten(vir8, amount = 0.75) %>%\n             desaturate(amount = 0.75),\n           '100' = lighten(vir8, amount = 1) %>%\n             desaturate(amount = 1))\n\n\n\n\nDid I just get lucky with viridis, or does it work with other palettes too? how about my ramp that I made from green to purple? Seems to:\n\nswatchplot('orig' = hexcols,\n           '25' = lighten(hexcols, amount = 0.25) %>%\n             desaturate(amount = 0.25),\n           '50' = lighten(hexcols, amount = 0.5) %>%\n             desaturate(amount = 0.5),\n           '75' = lighten(hexcols, amount = 0.75) %>%\n             desaturate(amount = 0.75),\n           '100' = lighten(hexcols, amount = 1) %>%\n             desaturate(amount = 1))\n\n\n\n\nDoes the lighten and desat work for the single-hue scales? Seems like it shouldn’t because they’re already changing along those axes.\n\nswatchplot('orig' = purp8,\n           '25' = lighten(purp8, amount = 0.25) %>%\n             desaturate(amount = 0.25),\n           '50' = lighten(purp8, amount = 0.5) %>%\n             desaturate(amount = 0.5),\n           '75' = lighten(purp8, amount = 0.75) %>%\n             desaturate(amount = 0.75),\n           '100' = lighten(purp8, amount = 1) %>%\n             desaturate(amount = 1))\n\n\n\n\nNot really. It basically does what it should, but the light end is just always light and so doesn’t contain info in the faded dimension and very similar colors appear in both dimensions- values at row n and col m are frequently very similar to row n + 1 and col m - 1.\nI suppose that might be OK for particular situations, but still not ideal. Might work ok though if we limited that lower end? ie don’t let it fall all the way to white in the original? Getting pretty hacky at that point and the diagonals are still too similar.\n\nswatchplot('orig' = purp8[1:6],\n           '25' = lighten(purp8[1:6], amount = 0.25) %>%\n             desaturate(amount = 0.25),\n           '50' = lighten(purp8[1:6], amount = 0.5) %>%\n             desaturate(amount = 0.5),\n           '75' = lighten(purp8[1:6], amount = 0.75) %>%\n             desaturate(amount = 0.75),\n           '100' = lighten(purp8[1:6], amount = 1) %>%\n             desaturate(amount = 1))\n\n\n\n\n\nTesting with other palettes\nViridis and the one I made are both fine, but look at a couple other palettes too. This is not comprehensive, mostly looking at those that have greens and purples for the use I have in mind.\nWrite a little function to do the fade and make this less cut and paste\n\npalcheck <- function(palname, n = 8) {\n pal8 <- sequential_hcl(n, palname)\n \n swatchplot('orig' = pal8,\n            '25' = lighten(pal8, amount = 0.25) %>%\n              desaturate(amount = 0.25),\n            '50' = lighten(pal8, amount = 0.5) %>%\n              desaturate(amount = 0.5),\n            '75' = lighten(pal8, amount = 0.75) %>%\n              desaturate(amount = 0.75),\n            '100' = lighten(pal8, amount = 1) %>%\n              desaturate(amount = 1))\n \n}\n\nPlasma\n\npalcheck('Plasma')\n\n\n\n\nGreen-based\nag_GrnYl is OK, but does get a bit of the diagonal issue\n\npalcheck('ag_GrnYl')\n\n\n\n\nditto Emrld, but might work?\n\npalcheck('Emrld')\n\n\n\n\nTerrains might be OK? 2 is less gaudy\n\npalcheck('Terrain')\n\n\n\npalcheck('Terrain2')\n\n\n\n\nmints and TealGrn fail diagonal test\n\npalcheck('Dark Mint')\n\n\n\npalcheck('Mint')\n\n\n\npalcheck('TealGrn')\n\n\n\n\nYlGn is pretty good, actually.\n\npalcheck('YlGn')\n\n\n\n\nFor the specific use, keep in mind that it will be two levels of fade, and so I can do something like orig and 75% and it’ll be pretty different. But here I’m trying to be fairly general.\nMeh\n\npalcheck('BluGrn')\n\n\n\n\nas expected, batlow and Hawaii are extreme, though might be OK?\n\npalcheck('Batlow')\n\n\n\npalcheck('Hawaii')\n\n\n\n\nPurple-based\nsingle hue doesn’t work\n\npalcheck('Purples')\n\n\n\npalcheck('Purples 3')\n\n\n\n\nthese are all maybes with tricky diagonals\n\npalcheck('Purple-Blu')\n\n\n\npalcheck('Purple-Ora')\n\n\n\npalcheck('Purp')\n\n\n\npalcheck('PurpOr')\n\n\n\npalcheck('Sunset')\n\n\n\npalcheck('Magenta')\n\n\n\npalcheck('SunsetDark')\n\n\n\n\npretty good, but have a fair amount of green in, so could be confusing\n\npalcheck('Purple-Yellow')\n\n\n\npalcheck('Viridis')\n\n\n\npalcheck('Mako')\n\n\n\n\nPlasma pretty good\n\npalcheck('Plasma')\n\n\n\n\nInferno might actually be pretty good if I cut off the first one\n\npalcheck('Inferno')\n\n\n\n\nag_Sunset is better on the diagonals than similar hue sequences\n\npalcheck('ag_Sunset')\n\n\n\n\nGood, but would need to cut the last one; too white. It is less gaudy/ more obviously a hue ramp than ag sunset. Diagonals are tricky too\n\npalcheck('RdPu')\n\n\n\n\nPretty good, but blue could be an issue getting confused with water for this project.\n\npalcheck('BuPu')\n\n\n\n\n\n\n\nContinuous hue from specified palettes\nIf I want to map values to colors continously, that gets tricky using the specified palettes because sequential_hcl takes an n argument.\nCan I get the endpoints and make my own (as I did above with green and purple?)\ndoes the one I’m using use a linear hue scale\n\nspecplot(sequential_hcl(8, 'ag_Sunset'))\n\n\n\n\nIt does, but doesn’t use linear chroma. and it has luminance shift too.\nCan I extract the hue from the ends? The same way I did right at the beginning for the greens and purples.\n\nspecplot(sequential_hcl(2, 'ag_Sunset'))\n\n\n\nrgbsun <- hex2RGB(sequential_hcl(8, 'ag_Sunset'))\n\nluvsun <- as(rgbsun, 'polarLUV')\nluvsun\n\n            L         C          H\n[1,] 25.00933  69.80052 274.922758\n[2,] 33.57582  78.49556 296.995075\n[3,] 42.09671  87.16488 318.944488\n[4,] 50.70304  96.81962 341.141446\n[5,] 59.33484 102.07413   3.730076\n[6,] 67.89723  89.83472  25.626328\n[7,] 76.47041  74.80664  47.677726\n[8,] 84.95182  45.16493  69.593540\n\n\nThis generates the wrong thing (roughly, viridis) because the hue crosses 0\n\nsunmat <- cbind(seq(from = 85, to = 25, length.out = 8), \n                max_chroma(h = seq(from = 69, to = 275, length.out = 8), \n                           l = seq(from = 85, to = 25, length.out = 8), \n                           floor = TRUE),\n                seq(from = 69, to = 275, length.out = 8))\n\npgsun <- polarLUV(sunmat)\nswatchplot(hex(pgsun))\n\n\n\n\nCan I fix the zero-crossing? I’m sure there’s a polar coord package, but for now, add a 360 and take it off\n\nhvec <- seq(from = luvsun@coords[1, 3], to = 360+luvsun@coords[8,3], length.out = 8)\nhvec[hvec > 360] <- hvec[hvec>360]-360\n\nlvec <- seq(from = luvsun@coords[1, 1], to = luvsun@coords[8, 1], length.out = 8)\n\nThe max_chroma is intense, but not sure how else to choose the chromas if we’re trying to build a continuous ramp. Could just use n = 1000 or something to get pseudo-continuous\n\nsunmat <- cbind(lvec, \n                max_chroma(h = hvec, \n                           l = lvec, \n                           floor = TRUE),\n                hvec)\n\npgsun <- polarLUV(sunmat)\nswatchplot(hex(pgsun))\n\n\n\n\nSo, one option is just treating the built-in palettes as their endmembers like that and then doing it as I did before. But it does lose the actual built-in palettes, especially chroma or nonlinearity. Likely better to just use a large n for now and call it good."
  },
  {
    "objectID": "plotting/math_in_ggplot.html#using-latex2exp",
    "href": "plotting/math_in_ggplot.html#using-latex2exp",
    "title": "Math and greek in legends",
    "section": "Using latex2exp",
    "text": "Using latex2exp\n\nlibrary(ggplot2) \nlibrary(latex2exp)\n\nLet’s just try to do some things with that. Make the usual iris plot.\n\ntestplot <- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\ntestplot\n\n\n\n\nNow let’s add some math. I’d typically use labs for the x,y, and color, so try that. Spacing not great, but it works.\nBasically following the manual, it’s pretty self-explanatory.\nThe r says to use raw strings so don’t have to escape slashes.\n\ntestplot <- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho})'),\n       color = TeX(r'($\\left{ \\int_0^\\inf \\exp{\\eta x} dx \\right})'))\n\ntestplot\n\n\n\n\nCan I use amsmath in latex? Maybe, but not for linebreaks- this errors.\n\ntestplot <- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho}$)'),\n       color = TeX(r'($\\begin{split}\\left{ \\int_0^\\inf \\\\ \\exp{\\eta x} dx \\right}\\end{split}$)'))\n\ntestplot"
  },
  {
    "objectID": "plotting/rgb_to_hex.html",
    "href": "plotting/rgb_to_hex.html",
    "title": "RGB to hex",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(colorspace)\nlibrary(ggplot2)\n\nLet’s say we have a dataframe with R, G, B values, but we want the hex. Why? Maybe we want to use scale_color_identity to plot the values in some other columns. Let’s say x and y.\n\nset.seed(17)\nrgbtib <- tidyr::expand_grid(x = 1:10, y = 1:10) %>% \n  mutate(R = sample(0:255, 100),\n                         G = sample(0:255, 100),\n                         B = sample(0:255, 100))\nrgbtib\n\n# A tibble: 100 × 5\n       x     y     R     G     B\n   <int> <int> <int> <int> <int>\n 1     1     1   177   166    44\n 2     1     2   231    80    48\n 3     1     3   224   188   136\n 4     1     4   221   168   181\n 5     1     5   107    82    93\n 6     1     6   238   111   179\n 7     1     7   246   242   124\n 8     1     8   230    57   213\n 9     1     9   172   151   130\n10     1    10   166    93    84\n# … with 90 more rows\n\n\nWe can use {colorspace}, but it’s convoluted- have to make an RGB object first, and then convert to hex. And the RGB need to be on 0-1, not 0-255.\nWriting that out doesn’t work because the colorspace RGB object can’t get stuffed in the dataframe. But this is the idea\n\nrgbtib_writeout <- rgbtib %>% \n  # Convert to 0-1\n  mutate(across(all_of(c('R', 'G', 'B')), ~./255)) %>%\n  # Create the rgb object\n  mutate(rgbobj = colorspace::RGB(R, G, B)) %>% \n  # Get the hex values\n  mutate(hexval = colorspace::hex(rgbobj))\n\nSo, make a function. Have a maxval the user can pass (don’t assume it’s 1 or 255).\n\nrgb2hex <- function(R, G, B, maxval = 255) {\n  rgbobj <- colorspace::RGB(R/maxval, G/maxval, B/maxval)\n  hexval <- colorspace::hex(rgbobj)\n  return(hexval)\n}\n\nTest that\n\nrgb2hex(177, 41, 147)\n\n[1] \"#D970C8\"\n\n\napparently quarto doesn’t do the cool printing of color thing in output, just input.\n\n\"#D970C8\"\n\n[1] \"#D970C8\"\n\n\nNow, use that in the mutate\n\nrgbtib <- rgbtib %>% \n  mutate(hexvals = rgb2hex(R, G, B))\nrgbtib\n\n# A tibble: 100 × 6\n       x     y     R     G     B hexvals\n   <int> <int> <int> <int> <int> <chr>  \n 1     1     1   177   166    44 #D9D373\n 2     1     2   231    80    48 #F49878\n 3     1     3   224   188   136 #F1DFC1\n 4     1     4   221   168   181 #EFD4DB\n 5     1     5   107    82    93 #AD9AA3\n 6     1     6   238   111   179 #F7B0DA\n 7     1     7   246   242   124 #FBF9B9\n 8     1     8   230    57   213 #F482EC\n 9     1     9   172   151   130 #D6CABD\n10     1    10   166    93    84 #D3A39B\n# … with 90 more rows\n\n\nPlot to show it works\n\nggplot(rgbtib, aes(x = x, y = y, fill = hexvals)) + geom_tile() + theme(legend.position = 'none')"
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html",
    "href": "plotting/setting_colors_for_groups.html",
    "title": "Setting colors for groups",
    "section": "",
    "text": "I often want the same colors to map to the same levels across many plots within a project, even if those levels aren’t included in a plot. E.g. if we plot the iris dataset with and without ‘setosa’, the colors change.\n\nlibrary(ggplot2)\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_brewer(palette = 'Dark2')\n\n\n\n\nvs\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_brewer(palette = 'Dark2')\n\n\n\n\nNow, virginica and versicolor have changed colors, and that’s confusing."
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#assign-colors",
    "href": "plotting/setting_colors_for_groups.html#assign-colors",
    "title": "Setting colors for groups",
    "section": "Assign colors",
    "text": "Assign colors\nNow use setNames to match\n\nsppal <- setNames(iriscols, unique(iris$Species))"
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#use-manual-scale-and-check",
    "href": "plotting/setting_colors_for_groups.html#use-manual-scale-and-check",
    "title": "Setting colors for groups",
    "section": "Use manual scale and check",
    "text": "Use manual scale and check\nand re-do the above two plots to demonstrate that dropping levels keeps colors the same\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_manual(values = sppal)\n\n\n\n\nand drop setosa\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_manual(values = sppal)\n\n\n\n\nNow they match."
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#creating-a-scale_-function",
    "href": "plotting/setting_colors_for_groups.html#creating-a-scale_-function",
    "title": "Setting colors for groups",
    "section": "Creating a scale_ function",
    "text": "Creating a scale_ function\nWe should also probably provide a function to create the standard color-level matching. And maybe that’s the solution to the above. If a palette isn’t passed in, create one in-function.\nFirst, can I make a silly wrapper that just takes the values? It’s no different than scale_color_manual at this point\n\nscale_color_custom <- function(pal) {\n  scale_color_manual(values = pal)\n}\n\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = sppal)\n\n\n\n\nNow, can we catch times when that’s not a named vector and if not pass something else? Do we want to? It starts getting infinite what we want to handle (e.g. palette names? from which packages?)\nI suppose as long as it’s a vector, use it but warn. I think otherwise just fail- it’s too hard to catch everything. Something like\n\nscale_color_custom <- function(pal) {\n  if (!is.character(pal)) {\n    stop(\"pal needs to be a character vector, ideally named\")\n  }\n  if (is.null(names(pal))) {\n    warning(\"unnamed vector, colors may not be consistent between plots.\")\n  }\n  \n  scale_color_manual(values = pal)\n}"
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#palette-generation",
    "href": "plotting/setting_colors_for_groups.html#palette-generation",
    "title": "Setting colors for groups",
    "section": "Palette generation",
    "text": "Palette generation\nHow about a palette generating function?\nThis will be easiest if I enforce a package, though I’m sure it can be made more flexible. For the sake of this quick demo, let’s just use {paletteer}, since it has access to lots of options. And we at least are limited to discrete scales, since we’re level-matching.\nOne thing I often want to do is set specific colors to specific levels (e.g. a reference). So make that possible.\n\nmake_pal <- function(levels, palette, refvals = NULL, refcols = NULL) {\n  if (is.factor(levels)) {levels <- as.character(levels)}\n  nonrefs <- levels[!(levels %in% refvals)]\n  cols <- paletteer::paletteer_d(palette, length(nonrefs))\n  \n  namedcols <- setNames(c(refcols, cols), c(refvals, nonrefs))\n}"
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#workflow",
    "href": "plotting/setting_colors_for_groups.html#workflow",
    "title": "Setting colors for groups",
    "section": "Workflow",
    "text": "Workflow\nFirst, we set colors with make_pal\n\nircols <- make_pal(unique(iris$Species), palette = 'calecopal::kelp2')\n\nThen we plot with scale_color_custom\nFirst with all three species\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircols)\n\n\n\n\nAnd removing setosa\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircols)\n\n\n\n\n\nReference level\nLet’s make setosa a reference level that stands out like purple.\n\nircolsS <- make_pal(unique(iris$Species), palette = 'calecopal::kelp2', refvals = 'setosa', refcols = 'purple')\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS)\n\n\n\n\nNote that now the colors of the others have shifted, because the reference-levelling happened at the start. If a reference is always a reference, that might be fine. But it might be the case that we only sometimes want to call attention to a particular level, and so we want to assign colors including it, so we can switch between an accentuated and unaccentuated palette. Let’s put another argument in make_pal (which requires some reframing of how we assign things). We also might want to return both the unreferenced and the referenced palettes at the same time, rather than calling the function twice.\n\nmake_pal <- function(levels, palette, refvals = NULL, refcols = NULL, includeRef = FALSE, returnUnref = FALSE) {\n  \n  if (returnUnref) {\n    if (!includeRef) {\n      stop(\"does not make sense to return a reffed and unreffed palette that don't match\")\n    }\n    }\n  \n  if (is.factor(levels)) {levels <- as.character(levels)}\n  \n  if (!includeRef) {levels <- levels[!(levels %in% refvals)]}\n  \n  # nonrefs <- levels[!(levels %in% refvals)]\n  cols <- paletteer::paletteer_d(palette, length(levels))\n  \n  if (returnUnref & includeRef) {unref <- setNames(cols, levels)}\n  \n  # delete the reference levels out of the vectors\n  whichlevs <- which(!(levels %in% refvals))\n  nonrefs <- levels[whichlevs]\n  nonrefcols <- cols[whichlevs]\n  \n  namedcols <- setNames(c(refcols, nonrefcols), c(refvals, nonrefs))\n  \n  if (returnUnref & includeRef) {\n    return(list(refcols = namedcols, unrefcols = unref))\n    } else {\n      return(namedcols)\n  }\n\n}\n\nNow test that- setting includeref = TRUE should keep these the same as earlier plots except for setosa (i.e. the light blue that would go to setosa just drops out\n\nircolsS_include <- make_pal(unique(iris$Species), palette = 'calecopal::kelp2', refvals = 'setosa', refcols = 'purple', includeRef = TRUE)\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS_include)\n\n\n\n\nAnd if I want to get both palettes (reffed and unreffed) so I can accentuate sometimes,\n\nircolsS_both <- make_pal(unique(iris$Species), palette = 'calecopal::kelp2', refvals = 'setosa', refcols = 'purple', includeRef = TRUE, returnUnref = TRUE)\n\nWith setosa as a reference- should match above\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS_both$refcols)\n\n\n\n\nWithout setosa as a reference, others retain same colors.\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS_both$unrefcols)"
  },
  {
    "objectID": "plotting/tweaks_tricks.html#theming",
    "href": "plotting/tweaks_tricks.html#theming",
    "title": "Theming and saving",
    "section": "Theming",
    "text": "Theming\nI tend to establish a theme to set the basic plot look, including font sizes. I start with theme_bw() because the default ggplot grey background doesn’t look good in pubs. I used to set the sizes separately for each sort of text (commented out), but that is typically easier to just use the base_size argument and let ggplot handle the relative adjustments.\nCan also set theme differently for presentations, including doing things like setting font to match a ppt theme.\nDeveloping themes is now done more extensively in ggplot themes, where I develop them as more flexible functions.\nTypically, very few fonts are loaded into R and available for use. See fonts.Rmd for figuring out how to work with that. The short answer is that we use showtext to load what we need (if anything). If this step is skipped, will default to the default font and throw a warning about “fontfamily not found” because we haven’t loaded the selected font yet.\nWe could load fonts by hand Using functions from showtext and sysfonts, and specify the text = element_text(family=\"Ink Free\") with a manual character vector. It’s way easier to automate though, and saves issues of loading the wrong font.\nFirst, load the function I wrote that simplifies finding the files and their names to load them. And tell R to use showtext to render fonts.\n\nprint(file.path('functions', 'loadfonts.R'))\n\n[1] \"functions/loadfonts.R\"\n\nsource(file.path('functions', 'loadfonts.R'))\nshowtext::showtext_auto()\n\nThen, load the font(s) we want\n\ntalkfont <- 'Ink Free'\npubfont <- 'Cambria'\nloadfonts(fontvec = c(talkfont, pubfont))\n# loadfonts()\n\nNote that we could also just loadfonts() with no arguments to read in ALL available fonts\nPass talkfont and pubfont to the themes.\n\ntalktheme <- theme_bw(base_size = 18) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=talkfont)) # Replace with fontname used in PPT\n\n# \n        # axis.text = element_text(size = 18),\n        # axis.title = element_text(size = 24),\n        # strip.text = element_text(size = 24),\n        # plot.title = element_text(size = 24))\n\npubtheme <- theme_bw(base_size = 10) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=pubfont))\n\nAs an example, let’s make a simple plot with iris, and then look at the themed versions.\n\nbaseiris <- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\nbaseiris\n\n\n\n\nNow, what does a publication version look like?\n\nbaseiris + pubtheme\n\n\n\n\nNote that further theme changes can happen later on, e.g. Note that it’s easy to get in trouble with the internal legend positions when it comes time to save- as the dimensions change on export vs whatever arbitrary size you have the Rstudio plot pane, what looks good will changes as well.\n\nbaseiris + pubtheme +\n  theme(legend.title = element_text(face = 'bold'),\n        legend.position = c(0.8,0.2))\n\n\n\n\nFor talks, we use talktheme. Terrible font, but easy to see that it’s been shifted from default.\n\nbaseiris + talktheme\n\n\n\n\nWe can update parts of the theme including the font while keeping the rest. Though if we haven’t loaded all fonts, will need to load the new ones now.\n\n# load new font\nloadfonts(fontvec = 'Elephant')\n\ntalktheme <- talktheme + \n  theme(text = element_text(family = 'Elephant')) # Replace with fontname used in PPT\n\nAnd to show that it worked, plot again.\n\nbaseiris + talktheme"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Placeholder, research goes here in sections, needs pics and short blurbs"
  },
  {
    "objectID": "research.html#cv-google-scholar-etc",
    "href": "research.html#cv-google-scholar-etc",
    "title": "Research",
    "section": "CV, google scholar, etc",
    "text": "CV, google scholar, etc"
  },
  {
    "objectID": "RpyEnvs/end_run_renv.html",
    "href": "RpyEnvs/end_run_renv.html",
    "title": "End-running renv",
    "section": "",
    "text": "I use {renv} to manage packages, but it is not uncommon for renv::restore to fail if enough time has passed. Maybe a package no longer exists. Maybe the package versions don’t work with the current R version (most common). We can use rig to downgrade R for that project, but lately that is failing with new versions of Rstudio. And sometimes we just want to install all the packages in a project, but all-new versions. If everything worked right, renv::update() should do that, but not if the packages haven’t been successfully restore()d.\nWe can stil use renv to solve this. Get the dependencies, make them a character vector, and install them. That’s way less work than my old method of running renv::status(), seeing what it said was missing, and installing, especially since that method included dependencies, and so had a lot of needless installs that would have happened with the outer package.\n\ndeps <- renv::dependencies()\nchardeps <- unique(deps$Package)\n\n# I really don't want to run this here, and only partially trust eval: false\n# renv::install(chardeps)\n\nSome end up failing anyway. To find out where those are used, use\n\ndeps[which(deps$Package == 'FAILEDPACKAGENAME'), ]\n\nThe failures are annoying because they don’t just fail that one, they fail everything and then we have to start over. One dumb solution that fixes this would be to loop over each package in turn, and save its name if it fails. I have code to do that, but will need to find it."
  },
  {
    "objectID": "RpyEnvs/managingprivate.html",
    "href": "RpyEnvs/managingprivate.html",
    "title": "Private data and website",
    "section": "",
    "text": "I’m getting set up to use github pages to host a website. But some content I (might) host needs to be private. A clear option is to simply mock-up data matching that private data, and that’s the way I’ll go. But because a large part of the content here will be sorting through issues, the initial sort-through will likely depend on figuring out what it is about the private data that needs to be mocked-up, and some portion of the testing will depend on that data. And I want all of that to be version controlled, but not shared publicly. In short, I need a private development location, and then go through, make a clean version based on mocked-up data, and publish that. So, how?\nThe first thing that came to mind is to just have a local-only branch that I keep private. I could have a private/ folder, where I do dev, with that folder ignored in the master .gitignore. And have a different .gitignore in another branch so development would be tracked in that other branch. Then, as things were ready to make public, I could just drop them over. However, because github requires the whole github pages repo to be public, I would never be able to push this branch. Sure, people would be unlikely to poke around in it, but it would all be there. And if I ever forgot the process, I would expose things. And I don’t want to lose a cloud-hosted version- only storing locally isn’t so great, even if it is backed up or dropboxed.\nI’m now leaning towards having a second, private repo for development, and then drag and drop into the github pages repo once the doc I’m working on is clean. That’s basically the same idea as the internal private/ folder, but as a whole different repo, and so could actually be held as a private repo on github. There are two main catches that I can see with this approach at the outset-\n\nThe actual development history of files won’t be available on the public repo. That’s kind of the point, but it is a bit annoying\nKeeping the two repos synced will be a pain. If I make a small change to a file that’s already public in the public repo, I’d need to get it back into the other. The obvious solution is to do everything in the private, and then move things over. But if I make a small change to a file that’s already moved to the public repo in the private repo, I need to make sure it moves.\n\nI think I’ve dealt with this issue before, but can’t remember the details. I had a repo as a fork of another that was upstream or something. Will need to sort that out. It’s essentially a repo-diff, but needing to check what should be diff (still private), vs. what shouldn’t be a diff (a change that needs to move over).\n\n\nIs there a better solution that allows building from somewhere other than github pages, and so could use a private repo? Netlify would work. And might be better anyway. But if the whole point is to make messy code public, then we want it on a public repo, right? And just hold the private stuff back/ do it elsewhere."
  },
  {
    "objectID": "RpyEnvs/python_nameerror.html",
    "href": "RpyEnvs/python_nameerror.html",
    "title": "Python NameErrors",
    "section": "",
    "text": "Issue\nI keep getting NameErrors and weird behavior with mixed R and python chunks. Usually it seems to be that python can’t find objects if there’s an R chunk in between. It only happens when I render in quarto- running this interactively in Rstudio always works fine.\nI’ve set the RETICULATE_PYTHON and QUARTO_PYTHON environment variables, and put engine: knitr in the yaml, and it doesn’t seem to have helped.\nThe issue seems to be worst when R is somehow involved with the objects.\nThis doc is to try to systematically and simply reproduce the issue. I hope that works.\nIt was working, but the more often I rendered, the more intermittent the problem became. I’ve noted something about the frequency of errors for the chunks- some error more than others.\n\n```{r}\nlibrary(reticulate)\n```\n\nAn R chunk to make sure knitr kicks off\n\n```{r}\na <- 1\n```\n\nDefine a bunch of python variables to do different things with.\n\n```{python}\nb = 2\nc = 3\nd = 4\ne = 5\nf = 6\ng = 7\n```\n\nCan we see those immediately?\n\n```{python}\nb\n```\n\n2\n\n\nWhat if there’s an R chunk in the middle?\n\n```{r}\nrdummy <- 1\n```\n\nCan we still see the python? Yes\n\n```{python}\nc\n```\n\n3\n\n\nWhat if the R touches the python?\n\n```{r}\nrb <- py$d + 1\nrb\n```\n\n[1] 5\n\n\nThis now fails almost every time\n\n```{python}\nd\n```\n\n4\n\n\nAre the other variables unscathed? No, this also fails consistently (but not 100% of the time).\n\n```{python}\nc\n```\n\n3\n\n\nWhat about those that haven’t been used since declared? Fails again most of the time.\n\n```{python}\ne\n```\n\n5\n\n\nCan we declare more python?\n\n```{python}\nf = 6\n```\n\nagain, R in the middle\n\n```{r}\nrtest <- 5\n```\n\nDoes the new python persist?\n\n```{python}\nf\n```\n\n6\n\n\nWhat if there’s a python chunk that accesses R?\n\n```{python}\ng = r.a + 1\ng\n```\n\n2.0\n\n\nCan we access the previous python variables? Sometimes. I intermittently get an error here, but sometimes it runs.\n\n```{python}\nf\n```\n\n6\n\n\nCan we access that new python variable? Also only sometimes.\n\n```{python}\ng\n```\n\n2.0\n\n\nWhat if we declare python in two chunks? Do they all get annihilated after crossing the language boundary, or only on the basis of their chunk declaration?\n\n```{python}\ng = 7\nh = 8\n```\n\n\n```{python}\nj = 9\n```\n\nContaminate with R\n\n```{r}\nrgh <- py$g + 9\nrgh\n```\n\n[1] 16\n\n\nCan we see the python that was defined with g? This works sometimes. So whatever is going on is unstable. It’s strange that this and the following two work sometimes- they seem like the same thing that consistently fails above (though that’s now working too).\n\n```{python}\nh\n```\n\n8\n\n\nI wasn’t expecting that to work (and it only does sometimes). Can we see g itself? Sometimes\n\n```{python}\ng\n```\n\n7\n\n\nCan we get to the python that was defined in a different chunk? Sometimes.\n\n```{python}\nj\n```\n\n9\n\n\nIs the issue that we didn’t ask for the touched variable first? Do the same thing, but this time ask for the contaminated variable\n\n```{python}\nk = 10\nl = 11\n```\n\n\n```{python}\nm = 12\n```\n\nAccess in R\n\n```{r}\nr_m <- py$k + 9\n```\n\nIs that variable there in python?\n\n```{python}\nk\n```\n\n10\n\n\nIs the other one that’s defined with it?\n\n```{python}\nl\n```\n\n11\n\n\nHow about the one in the other code chunk?\n\n```{python}\nm\n```\n\n12\n\n\nI’m not really sure what to do with this. Anything defined before any interaction across languages (either py$pyvar or r.rvar) would die in python when I rendered this the first few times, but now it’s all working after about 10 update renders. I’m throwing execute: error: true in all the R-py yaml headers, but that doesn’t actually help when I actually want them to run and there are errors intermittently."
  },
  {
    "objectID": "RpyEnvs/python_setup.html",
    "href": "RpyEnvs/python_setup.html",
    "title": "Python setup",
    "section": "",
    "text": "I’m working on a Python project, and trying to figure out how to set up and get started. I’m used to R, where most simply, all I have to do is download R, Rstudio, and then start coding. R doesn’t need any environment manager to get going, but I do tend to use renv to manage packages, but that’s pretty lightweight and straightforward. And I can start coding without it.\nPython, on the other hand, is more opaque. In part it’s because I’m new to it, but a bit of googling suggests I’m not the only one. It’s likely also because there’s no one dedicated IDE/workflow that almost everyone uses, a la Rstudio (maybe that will change with the Rstudio–> Posit move?).\nSo, I’m going to work out getting setup to code in Python (I sorta did it before, but I’m trying a new way with fewer black boxes). And using this as a place to write notes/what I did as I go. That means this might end up being less a tutorial and more a series of pitfalls, but we’ll see how it goes.\n\n\nI’m trying to get set up to manage Python versions themselves with pyenv and packages with poetry. As far as I can tell, poetry does approximately similar things to renv (but more complicated because python). And I haven’t used something similar to pyenv to manage R versions themselves, but I am about to have to figure that out too because a lot of packages broke when I moved to R 4.2. Will probably try rig, and write another one of these. I’m assuming I’ll code primarily in VS Code, unless Posit suddenly runs python like R (without reticulate). Even then, remote work will all use VS Code for the time being. I’m loosely following https://briansunter.com/blog/python-setup-pyenv-poetry and https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/, and doing all the actual setup in VS Code, not Rstudio.\nRealising after I wrote this that I probably could have actually done all of this inside quarto- I think I can run powershell/system code in code blocks?"
  },
  {
    "objectID": "RpyEnvs/python_setup.html#getting-started--systemwide-installations",
    "href": "RpyEnvs/python_setup.html#getting-started--systemwide-installations",
    "title": "Python setup",
    "section": "Getting started- systemwide installations",
    "text": "Getting started- systemwide installations\nBoth those websites are working on Unix and Mac, so while step 1 is install pyenv, we aren’t going to apt-get or brew install. In fact, the pyenvgithub says we need to use a windows fork. Things already getting nonstandard. Should I just run everything in Windows Subsystem for Linux? Maybe, but I’d like to just use windows if possible, as much as I like WSL.\n\npyenv\nGuess I’ll just start at Quick start. Going to use powershell directly rather than inside vs code here, because vs code likes to open in recent directories instead of globally, and I think I want pyenv system-wide.\nStep 1- install in powershell with Invoke-WebRequest -UseBasicParsing -Uri \"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\" -OutFile \"./install-pyenv-win.ps1\"; &\"./install-pyenv-win.ps1\"\nCan’t run scripts (new computer). Sends me to https:/go.microsoft.com/fwlink/?LinkID=135170.\n\nSetting the policy to just the running process. Will probably regret that when I next try to run a script, but for now I don’t really want universal unrestricted powershell scripts.\n\nPowershell script permissions\nAside- it starts to get really annoying because pyenv runs scripts, so will need to fix. Get an error when I try to change Scope to CurrentUser because of a group policy. Setting it to Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope LocalMachine seems to work, despite apparently being a larger set than my User.\nIt says it wasn’t installed successfully, but when I try again it says it’s there. I guess push on?\n\nStep 2 was just to shut down and reopen powershell\nStep 3- Run pyenv --version. If you haven’t changed policy to something larger than Process, this will fail because of the ExecutionPolicy. Guess I need to turn it back on. I kept doing one-offs for a while until I got annoyed and then set it to LocalMachine (see above).\nStep 4, pyenv install -l lists a million python versions. Seems like a good thing. I’m going to need older versions in projects, but for now, let’s install the latest.\nStep 5- the install of python. There’s a 3.12.a1, which I’m assuming means alpha, so I’ll go with pyenv install 3.11.0, which is the most recent without the .a1.\nThe install seems to have worked.\nStep 6- set global pyenv global 3.11.0\nStep 7- check it worked pyenv version \nStep 8- check python with python -c \"import sys; print(sys.executable)\"\nworks, gives me the path, \\.pyenv\\pyenv-win\\versions\\3.11.0\\python.exe\nNow it says to validate by closing and reopening- do that. Now pyenv --version gives the version, while pyenv alone tells us the commands. They’re also all at the github page I’m following.\nThen try in a vs code terminal, also works. Note that using the git bash terminal instead of powershell bypasses the script permissions issue if it hasn’t been set larger than Process.\nNow, we have Python 3.11 as the global python version, but should be able to install other versions and use them in local projects. Assuming I’ll get there once I set up poetry.\n\n\n\npoetry\nOnce again, the websites I’m following have commands for mac/unix, so back to the main poetry page to sort this out on windows.\nAgain, powershell command on windows. Could I use the git bash in vs? Maybe? Just stick with powershell. (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -.\nThe instructions say that last bit should be py instead of python if python not installed from Microsoft Store, but had to use python anyway.\nseems to have worked- \nThe instructions then have an advanced section I’m skipping, but that install message above seems to match step 3, where we add Poetry to the PATH in order to run Poetry by just running poetry and not the full path.\nI could change it with powershell, but the instructions I found involved a bunch of regex. Instead, search for “Advanced System Settings”, then in the bottom right, click Environment Variables, then in the System Variables box, click on Path, then Edit button, then New. That creates a blank line, paste in the path from poetry install, or use the one from their website, %APPDATA%\\Python\\Scripts. OK out of all the system settings boxes.\nShut down powershell, then fire it back up and run poetry --version. If the path setting failed, it won’t be able to find poetry, if it worked, it’ll give the version number.\nThat worked for me once, but now isn’t working the next day. Actually, it works in powershell, but not vs code powershell. It wasn’t unpacking the %APPDATA% correctly - running (Get-ChildItem env:path).Value lists %APPDATA% instead of the expanded directory. I stuffed the full path as in the screenshot above in the PATH, restarted VS Code and it works. Interestingly though, I left the %APPDATA% version there too, and it’s now unpacked when I run (Get-ChildItem env:path).Value."
  },
  {
    "objectID": "RpyEnvs/python_setup.html#setting-up-a-project",
    "href": "RpyEnvs/python_setup.html#setting-up-a-project",
    "title": "Python setup",
    "section": "Setting up a project",
    "text": "Setting up a project\nLet’s first say we’re going to use a different-than-standard version of python, so install that with pyenv. For this test, let’s just use 3.8.9. Not really any particular reason. So, run pyenv install 3.8.9\nNow, pyenv versions (NO FLAGS- the “--” flag will give the version of pyenv) shwos two versions with an asterisk by global.\n\n\nCreate the project\nJust need a directory and cd inside it, I think. I’ll make it inside the directory with this qmd. mkdir pytesting, cd pytesting. Actually, this yields too much nesting. poetry builds a directory for the project, and another directory in that, so this just yields annoying levels of nesting. Call poetry new (see below) from the directory you want to contain the main project directory.\n\n\nSet the python version\nI think just pyenv local 3.8.9. That creates a .python-version file in the directory, which seems to be the idea.\n\n\nSet poetry\nAm I going to completely screw up my R project having this inside it? Guess we’ll find out.\npoetry new pytesting then creates another directory and returns “Created package pytesting in pytesting”. It builds the outer directory, so don’t make one first or the nesting gets silly.\nThat directory seems to be establishing a standard package structure and the lockfiles etc. Opening the .toml looks like it didn’t pick up the python version though- it’s using 3.11. Hmmm. Tried killing and restarting powershell and it’s still doing that. Not sure why it’s not picking up the local python.\nIf I move up a directory, the pyenv versions returns back to 3.11. So pyenv seems to be working, but poetry’s not picking it up. I guess I can change in manually, but that’s annoying.\nSeems to be a long-running known issue- recent posts here https://github.com/python-poetry/poetry/issues/651. Ignore for now, maybe fix manually if it becomes an issue. I tried the solution in the last post (poetry config virtualenvs.prefer-active-python true), and it didn’t fix it. Tried completely starting over a few times. No luck. Worry about that later. I guess that means the pyenv stuff might be useless for the moment- will need to use the version poetry thinks it has in the directory. It does look like can reinstall poetry, but that seems like a pain. (No one else seems to have issues with the config above).\nSo, I’ve just set the pyenv to the global for now, and moving ahead with poetry for the project. I guess I could change pyenv global each time I switch projects as an annoying workaround if it becomes an issue. An answer might be poetry env use , see https://python-poetry.org/docs/managing-environments/.\n\n\nUsing poetry for dependencies\nMuch like renv can install all dependencies from info in renv.lock, we could build the project with dependencies from pyproject.toml. Or, also like renv, as we’re developing a project, we can add iteratively. Let’s do that, since that’s what we’re doing.\nThere seems to be an intermediate step here though, running poetry install to initialise a virtual environment from the .toml. I assume this would install whatever’s in the toml, but we don’t have anything at present. That apparently creates a virtual environment somewhere globally (.cache/, according to https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/).\nAnd now I have poetry.lock . According to the docs, this takes precedence over the .toml, though I doubt that’s true for python version itself. This is what gets committed to share the project.\nTyping poetry shell at the command line activates the environment. But how do we activate it for a VS code session?\nSeems to just be active once we open a .py file in that directory (e.g. if we open the file, then a powershell at that location, it appears with pyenv shell already going.\nTo test adding a dependency, i’ll try numpy. First, I can run simply python code- a = 1 etc. But import numpy as np fails (as expected)- ModuleNotFoundError: No module named ‘numpy’. So, click back to the powershell terminal, and try poetry add numpy. It resolves dependencies and writes a lock file.\nAnd yet, if I try import numpy as np, same error. The poetry show command lists it as installed.\nSo, it’s because VScode doesn’t know where to find the venvs. On a one-off basis, can use poetry env info --path to get the path, then in VS code select interpreter (ctrl-shift-p for the search thingy), then paste in that path. But that’s annoying.\nI’m trying getting to the search thing, then User settings, then adding C:\\Users\\galen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ to the venv Folders. That seems to work, but it may just be remembering from last time.\nThe most robust option might be to not keep the venvs in .cache but instead local to the project, as described here. Looks like that’s done with poetry config virtualenvs.in-project true at the very outset, and then rebuilding the project (and it should persist for future projects).\nAnd that’s what we’re doing in the project I’m doing this for. So let’s do that. First, run poetry env list to get the name, then poetry env remove NAME to delete. But it failed, seemingly partway. So also go to AppData\\Local\\pypoetry\\Cache\\virtualenvs and delete the folder. Restart vs and now poetry env list doesn’t return anything.\nTo set the config, poetry config virtualenvs.in-project true . I believe that’s global (I ran it outside the project).\nNow rebuild from .toml in the project with poetry install. The .venv is there now, but VS still can’t find it.\nOK, shut everything down, and instead of opening VS again as usual, I opened it and then opened a new window, and now it works. It was somehow setting the root based on where VS happened to open, rather than based on where files were. That probably makes sense for a git repo with just python, but broke here. I assume we need to add the venv directory to the gitignore.\nJust did poetry add pandas and it works and I immediately have access to it in a python script.\n\n\nComplications with add\nTo add a specific version of a package, use\npoetry add packagename==2.0.5\nThere are a number of other ways to specify version ranges, installing from git, etc.\n\n\nVS code note\nSometimes VS seems to find the poetry venv and use it, and other times (I think if it’s not at the head dir of the workspace?) it needs to be pointed at the python.exe. To do that, open the command palette, (ctrl-shift-p), select python interpreter, then .venv\\scripts\\python.exe wherever that venv is."
  },
  {
    "objectID": "RpyEnvs/python_updated_functions.html",
    "href": "RpyEnvs/python_updated_functions.html",
    "title": "Updating function defs",
    "section": "",
    "text": "As I develop, I often try a function, tweak it, try again, etc. In R, I can just run the function definition to have access, or source(filewithfunction.R). In python, I could tweak the function, but just trying to use them elsewhere (e.g. in a .qmd) wasn’t working, even if I re-ran import filename. Clearly, there are differences between import in python and source in R. After poking around a bit, it looks like python caches on first import, and so subsequent ones don’t refresh.\nWhat does seem to work is to run importlib.reload(filename). Obviously we wouldn’t put that in a script, but when using an interactive session, it’s really helpful. Not sure why this requires a whole separate package, but it works. See stackoverflow. It appears to be typical to just restart, but that is really prohibitive if the testing involves processing data that took a long time to create."
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html",
    "href": "RpyEnvs/py_r_dates.html",
    "title": "Complex passing py-R",
    "section": "",
    "text": "```{r setup}\n#| warning: false\n#| message: false\n\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\nOld way of giving path to venv, new way is to set it in .Rprofile"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#general-problem",
    "href": "RpyEnvs/py_r_dates.html#general-problem",
    "title": "Complex passing py-R",
    "section": "General problem",
    "text": "General problem\nThe real issue here is when we bring a df into R from python and it has a list column with ‘environment’ in it. Then the conversion hasn’t really happened, and doing that conversion post-hoc has to convert every single environment, which is all rows. And that takes forever. It’s not necessarily dates (and as we can see below, sometimes they do work just fine). So, if that happens, rather than taking an eternity to purrr or otherwise go through the column to translate, just put it in something easier in py, bring it over, and put it back.\nI do wish I had a better idea about why it happened. I’m sure as it happens more I’ll start to figure it out.\nIt’s possible this has been fixed in more recent {reticulate}, and that’s why I can’t reproduce? https://github.com/rstudio/reticulate/pull/1266. No, I’m still getting the error where I was before. Still struggling to replicate it, but it does happen in the original case."
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#a-demonstration-sort-of",
    "href": "RpyEnvs/py_r_dates.html#a-demonstration-sort-of",
    "title": "Complex passing py-R",
    "section": "A demonstration (sort of)",
    "text": "A demonstration (sort of)\nSay we have a pandas dataframe with a few columns of simple types (numeric, character) and 1000 rows\n\n```{python}\nimport pandas as pd\nimport random\nimport string\n\nrandnums = [random.gauss(0, 1) for _ in range(1000)]\nallchars = list(string.ascii_lowercase) + list(string.ascii_uppercase)\n\nrandchars = [random.choice(allchars) for _ in range(1000)]\n\nsimpledf = pd.DataFrame({'rand_nums': randnums, 'rand_chars': randchars}, columns=['rand_nums', 'rand_chars'], index=range(1000))\n\nsimpledf\n```\n\n     rand_nums rand_chars\n0    -1.435625          U\n1     0.350753          e\n2     1.459661          w\n3    -0.880471          u\n4    -0.760653          p\n..         ...        ...\n995   0.485717          E\n996  -0.237177          p\n997   1.019685          U\n998   1.094408          C\n999  -1.525452          M\n\n[1000 rows x 2 columns]\n\n\nNow, we can get that into R without too much fuss using py$.\n\n```{r}\nrsimple <- py$simpledf\nrsimple\n```\n\n         rand_nums rand_chars\n1    -1.4356250829          U\n2     0.3507532402          e\n3     1.4596605257          w\n4    -0.8804708136          u\n5    -0.7606531555          p\n6    -1.2408422400          a\n7     0.8086022926          w\n8     0.1553681838          I\n9     0.1499931469          N\n10   -1.6905648266          U\n11    0.2735097750          J\n12    0.7656201820          t\n13   -0.1118194501          M\n14    0.0195489805          Z\n15    1.0395564063          d\n16   -0.1745102607          V\n17   -0.0554451868          l\n18    2.5258639534          u\n19   -0.1096351552          G\n20    0.2815596223          I\n21   -0.2717254117          p\n22   -0.6813295226          S\n23   -1.1741510134          x\n24    0.6772420918          Q\n25    1.5003135109          m\n26    0.1937154814          f\n27   -0.3254001300          h\n28    2.1922539812          g\n29    1.1573719445          J\n30   -0.0007215734          v\n31   -0.4449339143          V\n32   -0.6919343708          X\n33    0.3405342337          q\n34    1.7239006058          W\n35   -0.0064842740          m\n36   -0.4052370276          p\n37    0.2411737758          G\n38    0.4890874441          c\n39   -0.0497768017          c\n40    0.3144387082          n\n41   -0.5720470392          m\n42    1.3898167198          W\n43    0.1206725257          J\n44    0.8840267607          Q\n45   -1.5968558247          T\n46   -0.2623930356          c\n47   -0.3184930484          U\n48    0.5905771314          W\n49   -1.1009629286          P\n50    0.2663333987          z\n51    1.6098848034          Q\n52    0.1156195038          v\n53   -0.1574073941          v\n54   -0.0220035782          a\n55    0.4692603842          v\n56   -2.1948482826          L\n57    0.7017765392          d\n58    0.3143708586          D\n59    0.5112804985          Z\n60   -1.1066206206          m\n61   -1.9502416341          w\n62   -0.0130923968          T\n63    0.3933354221          M\n64   -1.6776265650          A\n65    0.7595630454          n\n66   -0.4379569391          W\n67    0.4705634414          p\n68    0.6172425521          e\n69   -0.7822788402          D\n70   -0.0734679537          B\n71    0.2588013336          K\n72   -1.8202384489          l\n73   -0.0657629065          C\n74   -2.5026896570          q\n75    1.9706540706          y\n76    0.0761237398          a\n77   -0.5691208781          Q\n78    0.5843364893          S\n79   -0.5815034410          J\n80    1.8895772379          f\n81   -0.2758890229          Y\n82   -0.2267390165          J\n83    0.9147952331          u\n84    0.2913826871          O\n85    0.9273477394          J\n86   -1.1968127602          c\n87    0.5527709545          Q\n88   -0.2768357343          L\n89   -0.7163124052          b\n90   -0.1545520263          G\n91   -0.9328695604          b\n92   -1.1274812211          z\n93   -1.3402513131          P\n94    1.0497130347          u\n95   -0.8909050504          V\n96    0.6315742888          a\n97   -0.1505737723          Q\n98    1.0482120825          O\n99   -0.2441229867          f\n100  -0.7436569070          Y\n101   0.8585831633          Y\n102   0.2174317439          J\n103  -2.1324529452          E\n104  -0.6294762732          D\n105  -1.5313542737          Y\n106   0.8950773893          J\n107  -0.9150658817          A\n108   1.7269690792          n\n109   1.2138556701          I\n110  -0.2263987893          d\n111   0.3058133383          X\n112  -1.9207386398          w\n113   0.4476041786          m\n114   0.5174985196          y\n115  -1.6798013843          a\n116  -0.6543114023          l\n117  -0.3858112352          y\n118   0.1964507535          t\n119   0.9569313976          z\n120   0.5350902274          q\n121   0.6451306060          T\n122  -0.4024269994          C\n123   0.9285307222          q\n124  -0.4165565632          l\n125  -0.9688939726          T\n126   0.2783331164          h\n127   1.8851084142          U\n128   0.5594758648          M\n129  -1.7098245945          c\n130  -0.4873313623          o\n131   0.1763417295          Y\n132   0.2993006790          w\n133  -2.1734429180          o\n134  -0.6241570945          l\n135  -0.1514943669          E\n136   1.1687435451          E\n137   0.2579055153          H\n138   1.2060193818          w\n139   1.0556381249          E\n140  -0.5344922474          k\n141   1.5665250820          K\n142  -0.7679826974          W\n143  -1.5371772325          r\n144  -0.9831036374          G\n145   0.5888212113          p\n146   0.7912402080          d\n147  -1.3955327843          N\n148  -0.5898197310          I\n149   0.4744448312          W\n150   0.9603159543          G\n151   2.6400880715          Z\n152  -0.9741516568          j\n153  -1.4112700642          J\n154  -0.7592268061          h\n155  -0.5696867131          Z\n156   0.1077654988          N\n157   0.2584803264          J\n158  -0.5042422113          K\n159   0.9500776329          I\n160  -0.1630936146          S\n161  -0.5806396607          A\n162  -1.0818705328          H\n163  -0.4924106609          j\n164  -2.0783763152          T\n165  -0.2960155852          v\n166   1.4480343617          n\n167   0.0124443684          z\n168  -1.3783466819          O\n169   0.9854687706          P\n170   0.0743033311          y\n171   0.8935141623          j\n172   0.9373337643          b\n173   0.8400493292          W\n174   1.1587772141          o\n175   0.4541633273          u\n176  -0.9076626722          o\n177  -0.0492898176          Z\n178  -0.6963647511          A\n179  -1.2348769677          i\n180   0.9441149440          R\n181  -1.3722730385          A\n182   1.0466268517          p\n183   0.9331375194          T\n184  -0.9846034420          A\n185  -0.5257966923          z\n186   1.0656305613          i\n187  -1.8448519003          o\n188   0.3906398956          b\n189  -0.7187897316          H\n190   1.7997578131          y\n191  -0.3585941310          i\n192  -1.6334092433          u\n193   0.5164272434          c\n194   0.1100665212          r\n195   1.0007529214          R\n196   0.9594131524          Z\n197  -0.6135962902          w\n198   1.5363903478          m\n199  -0.8191980108          z\n200   0.1921218219          s\n201  -0.6613979592          M\n202  -0.3993682663          z\n203  -0.8706114641          e\n204  -0.9649416441          T\n205   0.1691579534          j\n206   0.7859223252          B\n207   0.5452355720          m\n208   1.1415107984          a\n209  -0.0341986138          m\n210  -0.2736673388          s\n211   1.9545059960          o\n212   0.5166064879          p\n213  -1.2943858809          m\n214  -0.0860733171          c\n215   1.1033975021          V\n216  -0.3235088778          P\n217  -0.4308106016          S\n218   0.5587407714          W\n219   1.3721918439          g\n220   0.5293768430          Q\n221   0.0922332647          b\n222  -0.4510724456          S\n223   0.1751050964          v\n224  -1.5745327122          P\n225  -0.4686987790          k\n226  -0.0314020510          a\n227   0.1175743374          W\n228   0.4499665765          u\n229  -0.5197055690          h\n230  -1.1473497325          g\n231   0.0814614482          l\n232  -0.4677117340          L\n233  -1.0519902963          x\n234   1.6068351716          C\n235  -0.8488315795          d\n236   0.3508745290          j\n237  -1.6927363301          A\n238   0.9013266352          B\n239  -0.6471005275          w\n240   1.3371939534          O\n241   0.1646968304          E\n242   1.5944426768          f\n243  -0.1227216065          n\n244  -0.3611438571          x\n245   1.4321273859          g\n246   0.7598238254          a\n247   2.0796874924          j\n248   0.9923193860          J\n249   0.2693997486          X\n250   1.4039850626          V\n251  -0.9389568383          z\n252   0.4740103282          z\n253  -1.3101674090          x\n254  -0.1034074744          E\n255   0.4483205899          Y\n256  -0.6589250104          T\n257  -0.8429945222          J\n258  -1.1663515480          z\n259   1.9660000937          p\n260  -0.3433573563          s\n261   0.1148442635          q\n262  -0.5249875766          Z\n263   0.1676782899          n\n264  -1.0136286479          J\n265  -0.7333549673          j\n266   0.8995466753          P\n267   0.9695350837          Y\n268   0.9218253542          K\n269  -0.4414647146          Z\n270   0.6155998382          Y\n271  -1.7022643915          B\n272   1.4796970719          a\n273  -0.3965082786          X\n274   0.3768646438          e\n275   0.2447098868          B\n276   0.4360737055          A\n277  -0.9974945815          F\n278  -0.1484822796          d\n279  -1.4219579349          v\n280   0.1011745791          e\n281   0.8820198202          p\n282   1.2388310771          X\n283   0.7298684372          f\n284  -0.1346159341          l\n285   0.5706964135          u\n286   0.6212584662          Q\n287   0.3538824295          z\n288  -0.8331870919          H\n289  -0.1833103747          c\n290  -0.3476566622          S\n291   0.1821118186          C\n292  -1.5433383481          B\n293  -1.1503580712          Z\n294  -0.8614605342          i\n295   0.1128122810          t\n296  -0.0985476579          s\n297   0.4965837881          L\n298   2.1192126466          B\n299  -0.5390286611          i\n300   0.4939689152          x\n301  -0.9398666483          p\n302   1.2112709605          b\n303  -0.9908630925          U\n304   1.0170897273          r\n305  -0.5123478466          j\n306  -0.3169409330          Q\n307  -0.0496000290          U\n308   1.5109416952          t\n309   1.1367608816          N\n310   0.1629503225          e\n311   0.7811006908          z\n312  -1.1362155603          L\n313  -1.4870577788          q\n314  -1.8360952594          T\n315  -2.1588950838          K\n316  -0.4326770520          t\n317   0.7413403754          S\n318  -0.4247797000          X\n319   0.3798902630          m\n320   0.2838859199          K\n321  -1.4545669610          c\n322  -1.0195341893          S\n323  -1.6534753902          p\n324   0.0749989437          H\n325  -0.9867258338          t\n326  -0.4742745878          D\n327   1.2229742048          M\n328  -2.5855681339          t\n329  -1.6556717684          S\n330  -1.1646784061          v\n331   0.5203800497          D\n332   0.5579600179          T\n333   0.3906490013          f\n334  -1.0328063769          x\n335   1.0510518969          h\n336   0.9208684701          W\n337   0.4724651764          g\n338   1.0679444841          d\n339  -0.7056842927          t\n340  -0.1534711408          K\n341   0.3107689054          V\n342   0.7546560251          W\n343  -1.5540519197          N\n344   2.4087434653          z\n345   0.7456152114          Z\n346   1.3882134808          p\n347   0.3063834615          l\n348  -0.2253811679          d\n349   0.6709227969          f\n350   0.6163450537          E\n351   1.0458290120          q\n352  -0.6950405211          Q\n353   0.8089349122          G\n354   1.5097777051          N\n355   0.2409425751          z\n356   0.2326360981          D\n357   0.0179152979          p\n358  -0.3539070323          T\n359   0.1628877731          S\n360  -0.3378879952          X\n361  -1.0754435743          c\n362  -1.0773125573          j\n363  -0.6769689006          Q\n364  -0.6280201939          H\n365   0.8391456664          W\n366   0.4610277130          u\n367  -1.0067995950          d\n368  -0.0689080065          U\n369   1.2135150596          W\n370   0.0094342522          h\n371  -0.8918038513          y\n372  -0.9509841561          L\n373  -0.3823363638          L\n374   0.9561179356          I\n375   0.4782514730          L\n376  -0.8988786030          K\n377  -1.0871152944          z\n378   0.9368828239          A\n379  -2.0280363617          T\n380   0.6950611166          t\n381  -0.0778011249          o\n382   0.4850504467          x\n383  -0.1302079696          K\n384  -0.6757692734          h\n385  -1.0987856964          n\n386   0.0631926768          q\n387   0.7862546296          j\n388  -1.1601220549          p\n389  -1.6780539594          E\n390  -0.0420252980          O\n391  -0.8648674438          X\n392  -0.6161887965          v\n393  -1.2515025173          o\n394   0.2614429133          u\n395  -0.3355556764          L\n396  -0.4623944123          a\n397   0.1756127177          P\n398  -0.5110623986          W\n399  -2.4653633726          k\n400   1.8300360514          X\n401   1.2671240937          z\n402  -2.2638218529          T\n403  -0.4130177377          D\n404  -1.5986772412          F\n405  -0.9050381725          o\n406  -0.2207035830          l\n407   0.6872274530          z\n408   0.1474403991          o\n409   0.6622302218          c\n410  -0.3503004232          l\n411   0.4563815819          L\n412   0.5701071199          C\n413   0.0906618075          W\n414   1.1819219448          E\n415   0.4643936424          d\n416  -0.4826000752          j\n417   0.6778095083          W\n418   0.3964221019          H\n419  -0.6242097970          l\n420  -1.8358501504          c\n421  -0.2029451445          B\n422  -1.4362061020          q\n423   1.1718644480          k\n424  -0.9584450813          a\n425   0.1429398426          o\n426  -0.1582334564          A\n427   0.1017999980          U\n428   0.2995843707          n\n429  -0.8841513168          b\n430  -0.3601259299          O\n431  -1.0549965952          U\n432   0.3215733096          O\n433  -0.0753970105          y\n434   1.7370206652          h\n435   0.2147937492          S\n436  -0.3126901358          Z\n437   0.2484921691          p\n438  -0.2256731246          Z\n439  -0.1607114628          E\n440  -1.1739196544          k\n441   0.4915594650          l\n442  -0.3356931812          K\n443  -0.2034798788          p\n444  -1.0406683943          m\n445   0.7687679265          P\n446   0.1908507424          o\n447  -0.3661570524          b\n448   0.0431516062          U\n449  -0.5057734164          d\n450  -0.5616106457          K\n451  -0.3748887233          l\n452   1.3901152999          Q\n453  -0.3955657705          x\n454  -0.1297649382          j\n455   0.3468275607          H\n456  -1.0361510068          u\n457  -0.2491759501          P\n458  -1.1182386653          x\n459   0.0838799558          v\n460   0.3027978827          t\n461  -0.7197029213          j\n462   0.1816761240          r\n463  -0.1718854601          h\n464   0.7719029424          j\n465  -0.9477488874          p\n466   0.7173881083          D\n467  -1.7842144866          U\n468  -0.8781683693          u\n469  -0.1864393658          o\n470  -1.8302325975          w\n471  -0.2130204962          Z\n472  -0.1042004829          o\n473   0.3361470549          X\n474  -0.3565092879          Q\n475  -0.3805926689          N\n476   0.2535289877          S\n477   0.0670465047          A\n478  -0.2399207624          R\n479   0.1202291279          N\n480  -1.3423960921          m\n481  -1.9833013984          w\n482  -0.0452814876          f\n483   0.3224375385          d\n484   0.4351273110          k\n485  -1.0831513852          m\n486  -0.7251690123          K\n487   0.9957466415          Z\n488   0.2255315070          N\n489   0.2131987886          B\n490  -0.3769222145          m\n491   1.1495878927          P\n492   0.0497512253          U\n493  -0.4726971962          p\n494  -0.4938457998          f\n495  -1.1348367422          R\n496   0.2258044721          V\n497  -0.2224941741          m\n498  -0.7362201810          Z\n499   0.7199027875          T\n500   2.4773327897          Y\n501  -0.0347875918          w\n502   1.6032305068          k\n503   0.8043905486          I\n504  -1.2078634918          r\n505   0.1682168493          Q\n506  -0.3653803913          V\n507   1.6328767060          M\n508  -0.2464734566          a\n509   0.5052421295          E\n510  -1.9298476370          W\n511   1.0811950547          a\n512  -0.8551932728          l\n513  -1.2044521477          X\n514  -0.0659345853          B\n515  -1.9768769662          n\n516  -0.1430651396          O\n517  -0.5339352321          o\n518   0.7579213368          x\n519   0.5025175301          S\n520   1.2866448311          t\n521  -0.4127577384          K\n522   0.5947399825          O\n523  -0.0675055148          d\n524   2.1361242061          d\n525   0.6240153408          R\n526  -0.1176965132          v\n527  -0.2366361715          g\n528   0.3510820282          U\n529   0.4011780678          Z\n530   0.9993635667          n\n531  -0.6983236597          t\n532  -0.9957448381          L\n533   1.1252645724          Z\n534   0.6181532731          i\n535  -1.1917226797          w\n536  -0.9919948974          Y\n537  -0.2939287824          F\n538  -0.6528311633          s\n539   2.1459081731          e\n540   2.2434053423          C\n541   0.4085845431          b\n542   0.2609584344          O\n543  -0.7441409126          I\n544  -2.4852425042          X\n545   0.2907412532          J\n546  -1.5814775669          f\n547   0.8511448089          f\n548   0.0112779500          O\n549   0.9491043611          N\n550  -0.1126119305          V\n551  -0.7018560318          p\n552   1.2813835016          H\n553  -0.6879069731          h\n554  -0.5398284709          D\n555   0.6377341696          c\n556   1.5764889807          G\n557  -0.0363569839          I\n558  -0.0343756704          a\n559   1.0499723926          r\n560  -0.6640957865          L\n561  -1.2544309373          U\n562  -0.8508860329          o\n563  -1.1284958312          H\n564   1.2427636510          D\n565  -1.5351781274          V\n566   1.6405571142          N\n567  -0.0461611056          Y\n568   1.8776265491          V\n569   1.1385854881          Y\n570   0.9208311341          m\n571   0.8681816436          n\n572  -0.1472896631          L\n573   0.4536482734          r\n574  -0.3272810523          x\n575   0.3064916544          j\n576   0.2614519362          r\n577  -0.8630663069          Z\n578  -1.0655756794          N\n579   1.4327592285          F\n580  -0.3160375981          k\n581  -0.8802892399          f\n582   1.9316729877          G\n583  -0.1763009717          P\n584   1.0921020778          v\n585  -2.1345867632          v\n586   0.5431402746          T\n587  -0.8041737472          C\n588  -1.0574402043          z\n589   0.3675547284          W\n590   0.8715992940          E\n591   1.5273779412          M\n592  -0.9326235932          r\n593   0.4706451519          o\n594   0.1338730339          l\n595  -0.1177191566          B\n596  -1.6273225103          l\n597  -0.7543744425          s\n598  -0.5449306407          T\n599   0.7479379378          Y\n600   1.8482179900          g\n601  -1.1928893492          S\n602  -1.5691268049          y\n603  -0.3418812491          D\n604   1.0241253681          c\n605   0.2425019010          t\n606   2.1736949337          Y\n607   0.0431818921          y\n608  -1.0681295830          c\n609   0.5478019800          f\n610   0.4336793012          B\n611   0.9955053439          B\n612  -0.8192227453          u\n613   1.5524170984          Y\n614   1.5550608816          X\n615  -0.6538236589          U\n616   1.0270757733          v\n617  -1.4964887235          O\n618   0.6572280476          n\n619   1.5704374588          E\n620  -0.8529470310          w\n621   0.2923396089          m\n622  -1.3231860831          d\n623  -0.2959259351          z\n624   0.5128084589          L\n625   1.9487665318          b\n626   1.3832244461          s\n627   0.7132032233          Z\n628   0.7645145552          o\n629   1.4480694368          l\n630  -0.3508976214          Y\n631   0.4424745736          D\n632   0.0203015891          t\n633  -0.0553896461          a\n634   1.0007560633          y\n635  -1.7453024865          k\n636   0.6697785697          n\n637  -0.5467022986          m\n638   0.0032872397          R\n639   0.2466541497          C\n640  -1.5805592606          o\n641  -0.1628804257          t\n642   0.3694406318          t\n643  -0.7741426332          Q\n644   0.8306277427          n\n645   1.4438993600          A\n646  -0.9006198323          u\n647   0.0551616211          X\n648  -0.5802660414          A\n649   1.3339850366          H\n650  -1.1218159915          l\n651   1.3874433646          Z\n652   1.7740418043          w\n653  -0.3582909380          X\n654   0.4326761979          N\n655   1.8599260911          g\n656  -0.0515589281          g\n657  -0.1511704069          K\n658   0.3654940739          I\n659   0.0557857387          z\n660  -0.0502144173          Z\n661   0.0842921238          s\n662   0.4217485205          I\n663   0.5147079796          m\n664  -0.6725174764          z\n665   0.5500913935          A\n666   1.1357859013          E\n667  -0.7984130899          l\n668  -0.7343596558          W\n669  -1.3233067165          g\n670  -1.2316997547          m\n671   0.3999450357          R\n672   0.8553828521          M\n673  -1.6335708704          i\n674   1.9483301680          y\n675  -0.2870438394          K\n676  -0.2566898911          y\n677  -0.1474485335          U\n678   1.3271215612          S\n679  -0.8775754623          u\n680  -1.2155830393          U\n681  -0.3742011371          x\n682  -2.6046994551          G\n683  -0.0899833919          T\n684   1.2835171321          V\n685  -0.3292862529          G\n686   1.1517375302          k\n687  -1.3594397705          c\n688  -0.1602953491          f\n689   1.8073505470          p\n690  -1.4322742841          b\n691  -1.6973213261          a\n692  -0.6134905020          j\n693  -0.0363189681          f\n694   0.6621925827          y\n695   0.3289825598          F\n696  -0.8761095232          O\n697  -0.9252975332          Q\n698  -0.5176435351          J\n699  -1.0810739797          e\n700  -0.5888644456          U\n701   0.8421566796          w\n702   1.4033958540          f\n703   0.2411266087          Y\n704   1.5716191408          s\n705  -0.7147494606          m\n706  -0.5004655310          B\n707   0.8180332524          n\n708   0.4936324665          w\n709  -0.3363503058          h\n710   0.8956559281          D\n711   1.3828609744          z\n712  -1.0550426109          F\n713  -0.5347013594          L\n714  -2.0748510390          B\n715   0.7746284322          o\n716  -1.3735907689          R\n717  -0.0433717851          B\n718   0.6912673950          J\n719   1.9875243231          J\n720   2.8204671588          q\n721   0.3989149781          C\n722   0.4718595017          n\n723  -2.2086231807          W\n724   0.7256647729          p\n725  -0.6583667501          J\n726  -0.9675114467          c\n727  -0.6556954460          W\n728  -1.7068210667          V\n729  -0.8773345211          z\n730  -0.8186130547          T\n731   0.4052639884          l\n732  -0.1938228269          t\n733  -0.2826271572          O\n734   0.5910284230          P\n735  -0.7635431664          h\n736  -0.3030584913          X\n737   0.1754024233          Y\n738   0.0978343197          w\n739   0.4100248534          M\n740  -0.4225321916          D\n741   1.3220268905          f\n742   0.3976846405          c\n743   2.1278484651          r\n744  -0.4867650066          V\n745   0.0722010189          R\n746   1.9933897199          t\n747   1.3187658050          S\n748  -0.3802935934          F\n749  -0.4976430128          L\n750   2.1900243405          i\n751   0.6629678409          X\n752  -0.1803383095          o\n753   0.0204933652          z\n754  -2.1891889427          m\n755   1.9048633909          u\n756   0.2938377722          d\n757   0.1994997837          K\n758   0.2222239450          E\n759  -1.6525599612          w\n760   0.4384764245          H\n761  -1.0286405211          K\n762  -0.0296379505          h\n763   0.4701457916          N\n764  -0.1823601942          K\n765   1.6154218555          H\n766  -0.5450943771          g\n767   0.3097726597          z\n768  -0.0867932970          V\n769  -0.6806204747          x\n770  -0.3355014464          r\n771  -1.1927102668          Q\n772   1.1238276485          A\n773  -0.9660698463          q\n774   0.6473753255          G\n775  -0.6053154715          O\n776   0.0045197549          l\n777  -0.3501868355          j\n778   1.6011580826          H\n779  -0.8178530753          z\n780  -0.3879562040          K\n781   0.4709296364          E\n782   1.0002000181          G\n783  -0.3817134311          o\n784  -2.1674858772          a\n785  -1.3112834406          R\n786  -0.7580238175          z\n787   0.6833386923          X\n788   0.1322410330          X\n789  -0.9621452009          W\n790  -0.5595904750          k\n791   3.0578217912          k\n792  -0.6154739539          g\n793   1.6556115369          R\n794   2.2258373143          P\n795   0.2277487813          y\n796  -0.8286467582          x\n797   0.7158227141          m\n798  -0.9130298867          Z\n799   0.9293039234          s\n800  -0.2277858542          v\n801   1.5296430623          f\n802   0.6914462416          s\n803  -0.2046371192          M\n804   0.2332155662          z\n805   1.1283378241          j\n806  -0.3242708910          i\n807  -1.0138268307          l\n808  -0.5306936141          Q\n809  -0.5655371410          o\n810  -0.6043960277          F\n811  -1.4745518785          Y\n812   1.4755494220          v\n813   0.3559831243          H\n814   1.2143564027          v\n815   0.2595468206          x\n816   0.1048913752          l\n817  -2.2896758228          j\n818  -0.2395335866          f\n819  -0.7680140155          g\n820  -0.1350853924          q\n821   1.6345477170          i\n822  -2.2050813285          s\n823   0.2422273456          d\n824  -0.0430926724          k\n825   1.0375124386          M\n826   0.7836401506          N\n827  -0.9371478937          n\n828   1.6934176466          d\n829  -0.3028726917          i\n830   0.7263125001          f\n831   1.9958764851          b\n832  -0.7165897118          w\n833   0.0290052389          M\n834  -0.4649404607          J\n835   0.0194393456          v\n836  -0.4933723973          M\n837   2.6708817212          J\n838   0.2259238232          V\n839   0.9048908525          c\n840  -0.2022703883          J\n841   0.0625252408          G\n842   1.2306453822          l\n843  -0.2789331401          k\n844  -0.5231596137          v\n845  -1.0790768050          P\n846  -0.2360754836          l\n847  -1.4590440646          m\n848  -1.8257195965          U\n849   0.1257243001          G\n850  -0.2377404895          Z\n851   1.5569695911          I\n852  -2.1104718809          y\n853   1.0167074861          P\n854  -0.0312058985          V\n855   0.6917138377          z\n856   1.3053289112          w\n857  -0.5567598211          O\n858  -0.8863754897          G\n859  -2.0390878771          a\n860  -0.5575362616          M\n861  -1.3020316258          m\n862   0.7165802028          K\n863   0.3394295153          r\n864  -0.2002441120          h\n865  -0.5948763088          T\n866   0.3490135895          k\n867  -1.3994828976          n\n868  -0.4499683720          W\n869  -0.5712692443          c\n870  -1.0725852952          S\n871   0.1027209914          m\n872   0.6762481615          B\n873   1.7224167978          R\n874   1.5573622091          x\n875   1.0600675122          R\n876  -0.1386146968          v\n877  -0.9823891185          q\n878   0.5442199737          i\n879   0.7451978018          q\n880  -1.7618681436          P\n881   0.1218761799          e\n882  -0.3191614840          V\n883   2.5570525775          u\n884   0.4453312361          r\n885   0.4596987497          B\n886  -1.6793739145          s\n887   0.6694351160          t\n888  -1.6186995845          g\n889  -0.6517881144          C\n890   1.2151781540          P\n891   0.8251184813          I\n892  -1.1935697693          B\n893   0.4672807201          x\n894  -0.5007378477          W\n895   0.4095609602          j\n896   0.7323162222          M\n897  -1.0174594458          t\n898  -0.1296055638          b\n899   1.2862551180          D\n900  -0.1908550020          A\n901   0.9376658293          L\n902  -1.5571643408          U\n903   0.4181011101          m\n904   0.3284572498          x\n905  -0.5959016193          H\n906   0.1422047525          u\n907   0.1115222790          y\n908  -1.0054665629          i\n909  -1.5386254214          N\n910  -1.0294034984          t\n911  -0.5703681321          o\n912   0.1264461308          m\n913   0.3595942219          h\n914  -0.0460875825          w\n915  -0.6413127889          j\n916  -0.0692236714          h\n917  -0.2700500300          I\n918  -1.4601067883          A\n919  -0.4568255813          r\n920   0.6550714209          S\n921  -0.1869571432          t\n922  -0.0807894839          z\n923  -1.4176962902          x\n924  -0.3173178517          y\n925   0.1575146217          e\n926  -2.1369432019          E\n927   0.6528115331          v\n928  -2.0608392945          V\n929  -0.0869133045          I\n930   0.6653643633          I\n931   0.2230423898          R\n932  -0.2204405248          n\n933   1.6365320502          Z\n934   1.3114448457          q\n935   0.8474996999          O\n936   0.3226254339          m\n937   1.0237374383          D\n938  -2.1620718248          E\n939   0.1243898434          m\n940  -2.2005427786          W\n941   0.8433656779          g\n942   0.7158068384          O\n943   1.0535220331          k\n944  -0.8527234159          T\n945   0.7870504848          m\n946   0.0934380270          q\n947  -0.3628072450          B\n948  -0.3309138033          H\n949   1.1371379319          g\n950   0.2866303292          L\n951  -0.3149581319          S\n952  -0.2192921616          j\n953   0.8448657678          r\n954   0.0280955747          r\n955  -0.0878613950          X\n956   0.0587217855          P\n957   0.3899827194          e\n958   0.9712651483          t\n959  -0.0349161389          M\n960   0.2044860319          F\n961  -0.1828614216          i\n962   0.8386614472          G\n963  -0.9562269589          g\n964   0.5364526538          r\n965   2.8003546980          g\n966   2.7849531652          e\n967  -1.0016161232          C\n968  -2.2279832640          a\n969  -0.6641445078          R\n970   2.8482495152          p\n971  -0.4452859008          h\n972   0.8324226326          h\n973   1.8693815386          N\n974   0.6081408181          J\n975  -0.1499750710          P\n976   0.4443449714          U\n977   0.0082651354          G\n978   0.7993040354          H\n979  -1.4548646703          c\n980  -1.3535649026          I\n981   2.2544517991          V\n982   0.0999063445          a\n983   1.0053994577          s\n984  -0.8113950478          o\n985   0.0914670016          z\n986  -0.8424450842          o\n987  -0.3490376723          C\n988  -0.5986849840          K\n989  -0.5653782157          v\n990   0.4517825416          v\n991   1.3376457275          V\n992   0.4849415228          M\n993  -1.0763858087          n\n994  -0.4583067943          a\n995   0.3608778480          A\n996   0.4857165248          E\n997  -0.2371774650          p\n998   1.0196845951          U\n999   1.0944084292          C\n1000 -1.5254522908          M\n\n\nQuick and easy. I think py_to_r is supposed to do some of this, but I can never get it to work. I think maybe it would make more sense in a script where we’re moving back and forth than here where we have separate code chunks?"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#now-with-time",
    "href": "RpyEnvs/py_r_dates.html#now-with-time",
    "title": "Complex passing py-R",
    "section": "Now, with time",
    "text": "Now, with time\nLet’s add a column of dates to simpledf. First, create the dates, then add to simpledf.\n\n```{python}\nimport datetime\n\ndates = []\n\nd1 = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n\n# Because i starts at 0, the first loop is the start date\nfor i in range(1000):\n    # Add i days to the start date\n    day_new = d1 + datetime.timedelta(days=i)\n    # Append the current date string to the list of dates\n    dates.append(day_new)\n\ntimedf = simpledf.assign(date = dates)\n\n# Try another way too\ntimedf['date2'] = pd.to_datetime(timedf['date'])\n```\n\nNow, when we bring it into R, it just works? That’s not at all what happened to me when I had the original issue.\n\n```{r}\ntimedfR <- py$timedf\n# tibble::as_tibble(timedfR)\n```"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#for-future-reference",
    "href": "RpyEnvs/py_r_dates.html#for-future-reference",
    "title": "Complex passing py-R",
    "section": "For future reference",
    "text": "For future reference\nSo, I can’t seem to replicate the issue. Previously, the datetime col came in as a list-col into a tibble, and was wrapped in a python environment. It was possible to parse it with purrr (or lapply, but purrr was much faster (and weirdly, furrr was slower). Not running, because this isn’t in a python env, and so this doesn’t actually work.\n\n```{r}\n#| eval: false\ndemodates <- timedfR$date\nrdates <- purrr::map(demodates, py_to_r) %>%\n  tibble(.name_repair = ~'Date') %>%\n  unnest(cols = Date)\n```\n\nBut what was much faster was to convert the column to strings in python with\n\n```{python}\ntimedf['date_str'] = timedf['date'].astype(str)\ntimedf.info()\n```\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   rand_nums   1000 non-null   float64       \n 1   rand_chars  1000 non-null   object        \n 2   date        1000 non-null   datetime64[ns]\n 3   date2       1000 non-null   datetime64[ns]\n 4   date_str    1000 non-null   object        \ndtypes: datetime64[ns](2), float64(1), object(2)\nmemory usage: 39.2+ KB\n\n\nAnd then lubridate back to dates in R\n\n```{r}\ntimedfRstr <- py$timedf\ntimedfRstr <- dplyr::select(timedfRstr, -date) |> \n  dplyr::mutate(date = lubridate::ymd(date_str))\nstr(timedfRstr)\n```\n\n'data.frame':   1000 obs. of  5 variables:\n $ rand_nums : num  -1.436 0.351 1.46 -0.88 -0.761 ...\n $ rand_chars: chr  \"U\" \"e\" \"w\" \"u\" ...\n $ date2     : POSIXct, format: \"2000-01-01 11:00:00\" \"2000-01-02 11:00:00\" ...\n $ date_str  : chr  \"2000-01-01\" \"2000-01-02\" \"2000-01-03\" \"2000-01-04\" ...\n $ date      : Date, format: \"2000-01-01\" \"2000-01-02\" ...\n - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=1000, step=1)"
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html",
    "href": "RpyEnvs/py_r_project_overview.html",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "I’m working on a project that needs both Python and R, and they need to talk to each other. Most of the work is in R, but the first bit is Python, and more importantly, it has to call a python package (more in future, likely). I’m packaging everything up so the user doesn’t need to do everything through the repo, but can just library(package) and be off and running, but that’s complicated by the fact that any actual use of this project needs both languages.\nIt seems like there are a couple options to how I deal with the two languages.\n\nHave a python package and an R package, and make the user install both and manage the bilinguilism in their own scripts using the packages.\n\nSeems most cumbersome for user, but the users are fairly involved in the project, so could work\n\nSkip creating my own python package entirely, and just have the bits of python wrapper in inst/python\n\nLikely how I’ll start to figure out how to include python in package\n\nAnd see below- I think it’ll be easier to rapidly iterate the py code.\n\nAs python side grows, this will get very cumbersome\nIs it slower than the first option?\n\nMake my own python package, and wrap that in inst/python\n\nI think this is actually the best option, since I won’t have to maintain two copies of py code\nWill also allow a user to just use option 1 (or for the whole project to move that way in future, or shifting functions over to python).\nIt will make the R package more cumbersome than option 1, but I think it’s the best tradeoff.\nIt will also slow down initial dev- as I change the python side, I’ll have to rebuild the py package and re-install it into my environment. There’s no obvious analogue to devtools::load_all that can reach across and do all that. Whereas I think if I have my py code in inst it’ll get refreshed when I load_all.\n\n\nAll of these will follow {reticulate} docs, but those are fairly sketchy as to how to actually write code that works.\n\nhttps://rstudio.github.io/reticulate/articles/package.html (seems to conflict a bit with the next)\nhttps://rstudio.github.io/reticulate/articles/python_dependencies.html\nhttps://rstudio.github.io/reticulate/articles/calling_python.html\n\n\n\nhttps://stackoverflow.com/questions/72185273/reticulate-fails-automatic-configuration-in-r-package\nhttps://github.com/rstudio/reticulate/issues/997\nWill need to test how this works both when we already have a venv and when we don’t.\n\n\n\nas I’m developing\nThis works from console, after I put the file in the inst/python\nreticulate::source_python(file.path('inst', 'python', 'controller_functions.py'))\ntested with\nscene_namer(file.path('inst', 'extdata', 'testsmall'))\nAt first it didn’t work to have the Config/retictulate in description and the source_python in .onLoad. I didn’t get anything. Then I fixed it by adding it to the package’s global environment with the argument envir = globalenv(), and it started working. But only when I already have a python environment. If I try to get the Config/reticulate to build a python environment with the necessary dependencies in a bare project, it won’t even install the package.\nI think the issue there is that the way the config/reticulate and onload are set up, it doesn’t try to install dependencies until Python is used, and for some reason running that file in onLoad doesn’t trigger it.\nSo, somehow I need to be able to install the package without needing the py dependencies, and then install them the first time someone types library(packagename) or otherwise tries to use it.\nThat’s a whole new thing to figure out, I think.\nIf we assume the user has the python dependencies installed and accessible to reticulate, then the Config/reticulate works with the following .onLoad.\n.onLoad <- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n\n  reticulate::source_python(system.file(\"python/py_functions.py\", package = 'packagename'), envir = globalenv())\n\n}\nSee some initial work I’ve done sorting this out.\n\n\n\nThe above method works to expose python functions, but the specific ones I have take dicts and lists as arguments. How do I pass those from R? We can’t just assign them to a variable in R, because those formats don’t work- e.g. we cannot create the lists and dicts in R to pass.\n\noutputType = ['summary', 'all']\n\nallowance ={'minThreshold': MINT, 'maxThreshold': MAXT, 'duration': DUR, 'drawdown': DRAW}\n\nI have sorted out how to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. See my testing of type-passing (well, if I could get it to render in quarto).\n\n\n\nIs there a way to auto-build the Config/reticulate part of DESCRIPTION? Maybe from pyproject.toml? Similar to the way usethis::use_package installs automatically add them to renv? That’s the other way though, so maybe it’s moot. Still, would be nice to automate."
  },
  {
    "objectID": "RpyEnvs/RandPython.html",
    "href": "RpyEnvs/RandPython.html",
    "title": "Using R and python together",
    "section": "",
    "text": "```{r setup}\n#| warning: false\n#| message: false\n\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#the-issue",
    "href": "RpyEnvs/RandPython.html#the-issue",
    "title": "Using R and python together",
    "section": "The issue",
    "text": "The issue\nI have a project primarily in R, but needs some python. For the big python work, I’ll have a directory with a poetry environment and python code. But I’ve run into the issue that I want to run just one or two lines of python from R. The specific case is that I have python code for extracting river gauge data, and I’ve filtered some river gauges in R for something else, and rather than do the finding of the gauges again in python, I’d rather just do the extraction in R. I think that means I have to sort out {reticulate}, but also how to point reticulate at my python environment. The situation I have is a poetry project inside a directory with an Rproj (which probably needs to be split up, but it’s what I have now).\nMy python_setup sets up a very similar situation, so let’s see if I can use it.\nI’m going to try to remember to always put\nexecute:\n  echo: fenced\nin the yaml headers of mixed r-python notebooks, so it’s clear which chunks are which language (though we can usually tell by the code).\n\nAdditional issues\nThis all worked, but then stopped- python chunks separated by R chunks couldn’t share objects. This particular notebook still ran, but others would run fine interactively but when rendered would throw errors about “name ‘pyobjname’ is not defined”. I tried making sure jupyter was in the python env and set engine: knitr in yaml headers, since that’s what the help suggested. And I set the QUARTO_PYTHON environment variable in an _environments file, since that helped me previously. It’s unclear why it worked previously."
  },
  {
    "objectID": "RpyEnvs/RandPython.html#set-up-reticulate-from-r",
    "href": "RpyEnvs/RandPython.html#set-up-reticulate-from-r",
    "title": "Using R and python together",
    "section": "Set up reticulate from R",
    "text": "Set up reticulate from R\nPoint reticulate at the venv. See stackoverflow. This seems to not be necessary if the .venv is in the outer project directory. Or if we’ve set the RETICULATE_PYTHON environment variable elsewhere (like .Rprofile).\nreticulate::use_virtualenv(file.path('RpyEnvs', 'pytesting', 'Scripts', '.venv'), required = TRUE)\nIf this is more than a one-off and you’re using an R project, it’s usually better to set the RETICULATE_PYTHON environment variable in .Rprofile. Here, that means adding this line in .Rprofile . This has the added bonus of stopping Rstudio/R throwing warnings about conda at startup when it detects {reticulate} being used in the project.\n```{r}\nSys.setenv(RETICULATE_PYTHON = file.path('RpyEnvs', 'pytesting', '.venv'))\n```\nSee Quarto notes for some similar issues for different python-related env variables.\nLoad the library. Interestingly, the python code chunks will run without loading the library, but I can’t access their values using py$pythonobject unless I load it.\n\n```{r}\nlibrary(reticulate)\n```"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#r",
    "href": "RpyEnvs/RandPython.html#r",
    "title": "Using R and python together",
    "section": "R",
    "text": "R\nFirst, let’s create some things in R.\n\n```{r}\na <- 1\nb <- 2\n```"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#python",
    "href": "RpyEnvs/RandPython.html#python",
    "title": "Using R and python together",
    "section": "Python",
    "text": "Python\nDoes not just inherit the values from R, but runs.\n\n```{python}\na = 1\nb = 2\na+b\n```\n\n3\n\n\nDo I have access to packages? Yes.\n\n```{python}\nimport numpy as np\n\nx = np.arange(15, dtype=np.int64).reshape(3, 5)\nx[1:, ::2] = -99\nx\n```\n\narray([[  0,   1,   2,   3,   4],\n       [-99,   6, -99,   8, -99],\n       [-99,  11, -99,  13, -99]], dtype=int64)\n\n\nDoes access to python objects persist? Yes\nThough in lot of other docs, this has proved to be super unstable, and fails intermittently\n\n```{python}\nx.max(axis=1)\n```\n\narray([ 4,  8, 13], dtype=int64)"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#moving-data-back-and-forth",
    "href": "RpyEnvs/RandPython.html#moving-data-back-and-forth",
    "title": "Using R and python together",
    "section": "Moving data back and forth",
    "text": "Moving data back and forth\n\nPython to R\nCan I access objects with R? Yes, but not quite directly. Have to use the py$pythonObject notation. But only if I’ve loaded library(reticulate) or specified with reticulate::py. That’s a pain, so probably almost always better to load the library. Even though the python chunks run fine without explictly loading it, I can’t seem to access py without loading it.\n\n```{r}\n# x\nreticulate::py$x\n```\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    2    3    4\n[2,]  -99    6  -99    8  -99\n[3,]  -99   11  -99   13  -99\n\n\n\n\nR to python\nSimilar to python objects being in py, R objects are in r, and are accessed with . instead of $.\n\n```{r}\nc <- 17\n```\n\nInterestingly, the r. notation to get R into python does not need reticulate:: on it. Which I guess makes some sense- this block is actually running in python and python doesn’t know what reticulate is. But it does know what r. is, somehow. Pretty cool.\n\n```{python}\nr.c + b\n```\n\n19.0\n\n\n\n\nPython-R-python and NameErrors\nElsewhere I’m running into a new issue of getting “NameError: name ‘pyobjectname’ is not defined” when I try to access an object defined in a previous python chunk in a later python chunk. It seems to be worse when there’s an R chunk in the middle. It doesn’t seem to be happening here, since the preceding chunk could get at b, which was defined way earlier.\nDoes the issue happen when R has touched it? YES. This all runs fine interactively, but when I try to render, I get “Error in py_call_impl(callable, dots$args, dots$keywords) : NameError: name ‘b’ is not defined” in the python chunk.\n\n```{python}\nb + a\n```\n\n3\n\n\n\n```{r}\nrb <- py$b + c\nrb\n```\n\n[1] 19\n\n\n\n```{python}\nb + a\n```\n\n3"
  },
  {
    "objectID": "RpyEnvs/rig.html",
    "href": "RpyEnvs/rig.html",
    "title": "Managing R versions",
    "section": "",
    "text": "I’ve update to R 4.2, but have projects that were built with 4.0 and even 3.x. Most new versions of packages for R 4.x don’t work in 3.x, and 4.2 seems to have broken quite a bit compared to 4.0. So I can’t just renv::restore(), and would need to update packages one at a time, but I know doing that will break things, and I don’t have time to do a full update of the project.\nI use renv to manage the packages, but not currently anything to switch/manage R versions itself. In python, there’s pyenv to manage python versions. I’ve run across rig (https://github.com/r-lib/rig)."
  },
  {
    "objectID": "RpyEnvs/rig.html#install",
    "href": "RpyEnvs/rig.html#install",
    "title": "Managing R versions",
    "section": "Install",
    "text": "Install\nclick on windows installer. Restart terminal. Type rig list to see what R is available."
  },
  {
    "objectID": "RpyEnvs/rig.html#using-it",
    "href": "RpyEnvs/rig.html#using-it",
    "title": "Managing R versions",
    "section": "Using it",
    "text": "Using it\nGo to the project I want to run, and figure out what version of R it was using. The one I’m testing on uses 4.0.2, which still is different enough from 4.2 that a lot of the packages fail when I renv::restore() in a session with 4.2. To avoid any issues, I will downgrade to that to run the project because I really just need things to work. Then, once I know it works, I can start updating and testing.\nSo, try rig add 4.0.2. Seems to have worked. Set the default to current, though.\nrig default 4.2.1.\n\nChoosing for a project\nWhat if I just open the project file by double clicking? There’s no obvious way to change the R version just by opening Rstudio- it uses the default.\nI think there’s probably a way to use the CLI to change the R version and then double click, but what seems to be easiest is cd path/to/repo and then rig rstudio renv.lock to open with the version in the lockfile.\nNote: this does not work with the new version of Rstudio (2022.12.0 Build 353 and others): https://github.com/r-lib/rig/issues/134 and https://github.com/rstudio/rstudio/issues/12545\nAnd do I keep using other R versions elsewhere? Seem to. For now, this should do what I need."
  },
  {
    "objectID": "RpyEnvs/rig.html#installing-rtools",
    "href": "RpyEnvs/rig.html#installing-rtools",
    "title": "Managing R versions",
    "section": "Installing rtools",
    "text": "Installing rtools\nWe need rtools to install packages with compiled components. R 4.2 has updated to Rtools 42 (from 40), and so using previous versions of R need older Rtools. The telltale is when trying to install a package, we get errors about ‘make’ not being found. The rig documents imply that rig system update-rtools40 should work, but I get “Error: the system cannot find the path specified”. I’m not sure what path that is, so hard to fix. So, I seem to be OK until I need something that needs ‘make’, and then I’m out of luck.\n\nThe solution\nTo install Rtools40, needed for R 4.0- 4.1, run rig add rtools40. Seems to be all it took, now I can compile. I assume there’s a similar command for even older Rtools if need to downgrade to R 3.x, but haven’t tried."
  },
  {
    "objectID": "RpyEnvs/R_py_package.html",
    "href": "RpyEnvs/R_py_package.html",
    "title": "Wrapping python in R package",
    "section": "",
    "text": "I have a package that is mostly R, but needs to call some python functions. Those functions are themselves fairly lightweight wrappers of functions from another python package. So, the basic structure is that I have a package with the usual R package structure, and a python script in inst/python/pyscript.py that contains some function definitions.\nThere are two main issues I need to address here\nSolving these was surprisingly quick for dev, using devtools::load_all-"
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#works-with-load_all",
    "href": "RpyEnvs/R_py_package.html#works-with-load_all",
    "title": "Wrapping python in R package",
    "section": "Works with load_all",
    "text": "Works with load_all\nI began dev with the following setup, and it just worked with load_all\n\nEnvironment\nThe python environment I’m using is in a separate directory in the same repo, so I can point to it by going up and over with ../pydirectory\nIn .Rprofile, set Sys.setenv(RETICULATE_PYTHON = '../pydir/.venv/Scripts/python.exe')\nI also followed instructions from reticulate to handle dependencies in the .onLoad function and DESCRIPTION file, but am pretty sure that doesn’t come into play when I use load_all.\n\n\nExposing functions\nThe reticulate::source_python('pyfile.py') function makes functions in pyfile.py directly callable. So I naively created R/loadpys.R that contains only this line, since everything in R gets read on package load: reticulate::source_python(system.file(\"python/pyfile.py\", package = 'packagename')) .\n\n\nBreaking issues\nWhen I try to actually install the package (or check, build, etc), I first got errors about not being able to find the python environment at ../pydir/.venv/Scripts/python.exe .\nWhen the package is built, I do not have access to the python functions. Presumably this is because load_all puts everything into memory, including private functions, but actually installing only gets the exported ones."
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#fixing-environment-path",
    "href": "RpyEnvs/R_py_package.html#fixing-environment-path",
    "title": "Wrapping python in R package",
    "section": "Fixing environment path",
    "text": "Fixing environment path\nRunning devtools::check seems to have worked to just replace the relative path to the environment with a full path, e.g.\nSys.setenv(RETICULATE_PYTHON = 'C:/Users/USER/Documents/REPO_DIR/pydir/.venv/Scripts/python.exe').\nIt’s not clear to me how this will work for installing the package elsewhere (e.g. by a user, not from within the repo during dev). Obviously they’ll need their own python env, and so will need to set that up. Will come back to that once I can actually get the package built with the functions."
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#exporting-python-functions",
    "href": "RpyEnvs/R_py_package.html#exporting-python-functions",
    "title": "Wrapping python in R package",
    "section": "Exporting python functions",
    "text": "Exporting python functions\nThere are brief instructions at the reticulate vignettes for wrapping python modules. Presumably this would work, but it’s unclear how to call them, whether they would be exported, and whether they’d need to be called python-style with module.function, or just the bare names. It also conflicts with the advice in the vignette about package dependencies which seems to be more recent. It’s not completely clear what still applies- clearly the ways of specifying environments has changed, but has the way to access functions changed as well? And how do we make them available to a user (and not just internally)?\n\nWrite exported R wrappers\nIn loadpys.R, instead of just having reticulate::source_python(system.file(\"python/pyfile.py\", package = 'packagename')) call, write an R wrapper for the functions we want to export. NOTE that we have to put the source_python inside the R function (it does not work to have it at the head of the file).\npyfun_R <- function(...) {\n  reticulate::source_python(system.file(\"python/pyfile.py\", \n                                        package = 'packagename'))\n  pyfun(...)\n}\nThat seems to work if we make sure there’s a @export in the roxygen documentation. But it’s not exactly pretty.\nNote- it also works if we bring over the full arguments instead of dots, which would be better for documenting, probably. Guess it depends if we want to read R or python documentation.\nNote- the following error happens when the source_python isn’t in the function definition.\nError in `(function (command = NULL, args = character(), error_on_status = TRUE, …`:"
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#can-i-use-onload",
    "href": "RpyEnvs/R_py_package.html#can-i-use-onload",
    "title": "Wrapping python in R package",
    "section": "Can I use onLoad?",
    "text": "Can I use onLoad?\nThe instructions from reticulate describe bringing in a module. I think that is ultimately how we should do things, but unclear how difficult.\n\nBack to load_all\nTo test, let’s just see what we get when we devtools::load_all()\n\n\nImport module\nThe instructions above reference importing a module that is installed. We should be able to do that by making our python code a small package. But in the meantime, let’s use reticulate::import_from_path.\nUsing load_all, py_functions <<- reticulate::import_from_path(\"pyfile\", path = \"inst/python\", delay_load = TRUE) works. I get an object py_functions and can access the individual functions with py_functions$func1 etc. Note- it also works to pass the path to the python directory py_functions <<- reticulate::import_from_path(\"pyfile\", path = \"inst/python\", delay_load = TRUE).\n\n\nRunning\nThat runs after a load_all by calling pyfile$pyfun(args). But does it still need to be exported in an R wrapper to actually use as a package? YES- building the package can’t find the function.\nAnd if we do it this way, are the python functions just floating around in the user’s global environment? That would be bad. NO- they don’t seem to be.\nIt does seem to fail when I try to run the vignette in a different directory after devtools::install_github(). So that implies the method really isn’t working right. Why does it work for load_all?\n\n\nAnother issue\nThe approach above seems to work, I can devtools::install()` and run the package, using the exported R functions.\nThe catch is that devtools::check() hangs on building a vignette that uses those functions (even though I can run that same vignette after devtools::install().\nIs it that the python env gets called in one of them but not the other? setting RETICULATE_PYTHON in the vignette doesn’t help.\nDoes it actually not work, and I’m just carrying around info to make it work somewhere? Nope, it doesn’t work, after using devtools::install_local to install into a separate R session."
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#solution-for-now",
    "href": "RpyEnvs/R_py_package.html#solution-for-now",
    "title": "Wrapping python in R package",
    "section": "Solution (for now)",
    "text": "Solution (for now)\nUse reticulate::source_python, but with envir = globalenv(), which seems to accomplish what the <<- was doing with reticulate::import in .onLoad. And in fact we can do that in .onLoad. so my .onLoadnow looks like\n.onLoad <- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n\n  reticulate::source_python(system.file(\"python/controller_functions.py\", \n                                        package = 'werptoolkitr'), \n                            envir = globalenv())\n\n}\nThat doesn’t allow the user to library(package) and then set up their python- they’ll need to have python sorted ahead of time. Using the delay_load = TRUEin reticulate::importwould be better, but it doesn’t seem to work without a full py package.\n\nHow to test (relatively) quickly in a clean session-\n\nStart a new Rproject\nRun devtools::install_local(\"path/to/package\", force = TRUE) .\n\nWithout the force = TRUE this almost never rebuilds even with changes.\n\nThen library(packagename)\nSys.setenv(RETICULATE_PYTHON = 'path/to/venv') (otherwise it will try to install conda and barf)\n\nI would have thought I’d have to do this before the library, but it seems to work.\nBetter is to have a path to venv in .Rprofile, anyway\nNOTE if the .venv is in the outer directory, I’m getting weird errors when I try to Sys.setenv or otherwise set the path to the venv (it’s either prepending ~/virtualenvs or C::/ unless I pass a full fixed path. Seems to work in that case to just not set the environment variable though, and {reticulate} sorts it out correctly.\n\nTry to use the functions and see if they break."
  },
  {
    "objectID": "RpyEnvs/R_py_package.html#passing-types",
    "href": "RpyEnvs/R_py_package.html#passing-types",
    "title": "Wrapping python in R package",
    "section": "Passing types",
    "text": "Passing types\nWe might need to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. See my testing of type-passing."
  },
  {
    "objectID": "RpyEnvs/R_py_shared_projects.html",
    "href": "RpyEnvs/R_py_shared_projects.html",
    "title": "R and python envs in same project",
    "section": "",
    "text": "Both renv and poetry want to set up project structures and work within them. And I want to do both in the same git repo because I’m using both for the same project. And have access to both everywhere within the project (ie I want to be able to use reticulate, but more importantly I want the different parts of the project to be able to have both py and R components.\nI’ve done a bit getting them to work in the same script, and setting up clean python environments with poetry. With the shared scripts, I did it with a subdirectory, and that sort of worked for testing, but won’t work for a project where they’re both used a fair amount and in both places.\n\n\nCan I just initiate them both in the base git directory? I know i can with renv, but does poetry let us stick a project in an existing repo? When I was sorting out poetry, I always made a new dir with poetry new dirname.\nIt looks like py code should be in the inner directory of the poetry structure. Let’s assume that. Which roughly matches R structure, where we’ll have code in an R/ dir if it’s a package or in some other dir structure. IE, if we can just get the environment management into the outer dir of the repo, and then all other code inside. I’m not sure though that I’ll want to split py from R at present. Think about that.\nSo, really, the question is whether I can poetry new and poetry install in a dir that already exists.\nMaybe poetry init instead of poetry new? Asks a bunch of questions.\nIt creates a pyproject.toml file, and then poetry install creates the poetry.lockand .venv, but the rest of the structure’s not there (tests dir, second level of the project dir). Will it work? Probably. Do we want that structure? Probably.\n\n\n\nSo, maybe better to poetry new somewhere else and drag over, then poetry install. Does that work? I copied over everything inside the outer dir, since I want the whole project to share the outer dir. It makes the lock and venv, but I get ‘dirname does not contain any element’. I’m guessing because I made a poetry env with a different name. Try using the same name, then again copying over the internals.\nThat seems to work. Now to build the env so everything actually works. But that’s about project details, so I’ll leave this here.\nThat seemed to have made vs code happy- it can find a venv in the workspace and use it. It didn’t do that automatically when the venv was in a subdir. (I had to command palette- select python interpreter)."
  },
  {
    "objectID": "RpyEnvs/R_py_type_passing.html",
    "href": "RpyEnvs/R_py_type_passing.html",
    "title": "R-py type passing",
    "section": "",
    "text": "This runs fine interactively, but when I render, everything fails. The python chunks lose what was in the previous ones, and the references to py$ in R chunks all fail. For now I’m going to just skip errors and have this not render correctly, which is unsatisfying. Not much more I can do though until I sort out the NameError issue, which is tricky because it doesn’t replicate consistently."
  },
  {
    "objectID": "RpyEnvs/R_py_type_passing.html#passing-types",
    "href": "RpyEnvs/R_py_type_passing.html#passing-types",
    "title": "R-py type passing",
    "section": "Passing types",
    "text": "Passing types\nI have a python function that takes dicts and lists as arguments. How do I pass those from R? We can’t just assign them to a variable in R, because those formats don’t work- e.g. we cannot create the lists and dicts in R to pass.\n\n```{r}\n#| eval: false\noutputType = ['summary', 'all']\n\nallowance ={'minThreshold': MINT, 'maxThreshold': MAXT, 'duration': DUR, 'drawdown': DRAW}\n```\n\nSo, let’s write a function to tell me the type of what I’m passing and try a few things.\n\n```{python}\ndef test_type(testarg):\n  \n  return(type(testarg))\n```\n\nIs a named list a dict or a list? what about just a c()? Is that a list?\n\n```{r}\nrlist <- list(dict1 = 100, dict2 = 'testing')\nrc <- c(100, 50)\n```\n\nThe named list is a dict. Look at it in python and R.\n\n```{python}\ntest_type(r.rlist)\n```\n\n<class 'dict'>\n\n\n\n```{r}\npy$test_type(rlist)\n```\n\n<class 'dict'>\n\n\nThe c() is a list- but see below- this fails if it’s length-one\n\n```{r}\npy$test_type(rc)\n```\n\n<class 'list'>\n\n\nAn unnamed list is a list\n\n```{r}\nrlistu = list(100, 'testunname')\npy$test_type(rlistu)\n```\n\n<class 'list'>\n\n\nThat is useful, since creating a length-one list doesn’t work with single values or c() wrapping single values\n\n```{r}\nrone = 100\npy$test_type(rone)\n```\n\n<class 'float'>\n\n\n\n```{r}\nronec = c('testingc')\npy$test_type(ronec)\n```\n\n<class 'str'>\n\n\nWe do get a length-one list with an unnamed list of length 1.\n\n```{r}\nronel = list(100)\npy$test_type(ronel)\n```\n\n<class 'list'>"
  },
  {
    "objectID": "setup/rstudio_themes.html",
    "href": "setup/rstudio_themes.html",
    "title": "Editing Rstudio themes",
    "section": "",
    "text": "I really don’t like how faint the selection colours are for all of the dark themes. When I do a ‘find’, I don’t want to hunt around for dark grey on black. So to fix that, I need to edit the theme.\n\n\nI went to this massive list of themes, clicked ‘gallery’, chose the one I wanted (just a simple edit of Tomorrow Night, will save a more complex hunt for another day). Then the pane in the middle lets us change all the colours. I clicked the ‘General’ tab, and changed the lineHighlight and selection to a nice blue. I also changed the comment colour to green, not sure if I like that or not.\n\nThen, go to the ‘Info’ tab and change the name before downloading. Otherwise Rstudio won’t find it under a new name.\nDownload. This saves as a .tmTheme file, which I think might just be able to be used directly (see new Posit documentation, but I was looking at something old and so used rstudioapi::convertTheme('setup/Tomorrow Night HL.tmTheme', outputLocation = 'setup') to create a .rstheme file.\nThen global options, add, and select the theme. I had to restart Rstudio a couple times for it to take. The edited theme is available in the git for this website."
  },
  {
    "objectID": "setup/R_in_VS.html",
    "href": "setup/R_in_VS.html",
    "title": "R in VS code",
    "section": "",
    "text": "I typically use Rstudio, and am very used to it. But I need to use VScode for working on Azure, and am trying to sort that out. There are also idiosyncracies with using Azure, but I’ll try to hold those for somewhere else and just keep this about VS."
  },
  {
    "objectID": "setup/R_in_VS.html#the-basics",
    "href": "setup/R_in_VS.html#the-basics",
    "title": "R in VS code",
    "section": "The basics",
    "text": "The basics\nThe VS code documentation gives a pretty good overview of the basics- install VScode, install languageserver, and install the R extension. That gets us up and running. Though it is worth noting that if you tend to work in renv for everything, it’s probably better to install languageserver globally (ie in a non-renv-managed session).\nNow, supposedly that provides linting, debugging, code completion, help, etc. And the add-ons (radian and httpgd look good too in terms of nicer terminal and visualisations). The question now is, HOW do we actually use all that functionality. I’m so used to Rstudio, it’ll take some playing. I’ll try to write down here what I try and how to get it to work.\nI tried to install.packages('languageserver') globally, but it gets grumpy sometimes and can’t find it inside a renv- managed repo because the .libPaths don’t have wherever the global package directory is. It seems to have worked on Windows, but not Azure/Unix.\n\nRadian\nI tried installing radian as the terminal. Assuming I don’t want it to mess up the project-level python environments, I installed it globally through the git-bash in windows terminal. It works when I type radian into VS bash, but does not run when I try to actually run something from a .R file. In the command pallette -> settings -> extensions -> R, there’s an option for Rterm:Windows that says it can be the path to radian. I tried where radian in git-bash (which radian on unix), and it gave me two paths in py-env/shims. The one with the .bat works on windows (C:\\Users\\galen\\.pyenv\\pyenv-win\\shims\\radian.bat). On Unix, it’s just a standard usr/bin/…. I’ve turned radian back off, though, because it throws a really annoying amount of weird errors that don’t actually stop the code from running, and that don’t appear in the base R terminal. Things like ‘unexpected & in }’, when there are no ‘&’ symbols in the code (and it still completes (usually). I think it works in .R scripts, but not in quarto. It’s something about bracketed paste not working right in notebooks I think. Working on sorting that out.\n\n\nlinting\nThis is something that (weirdly, I think) isn’t included in Rstudio. It supposedly is in VScode, but I’m not seeing obvious signs of it.\nInteresting. I don’t know what changed, but it has suddenly started linting. Maybe I turned something on in the settings->extensions->R section?\nAnd now all the blue lines are super annoying. Would be nice to at least have a turn off for comments setting. Or a good way to wrap comments a la Rstudio ctrl-shift /.\nGuess I need to figure out how to step through lintr and fix issues."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html",
    "href": "simmodelling/twoDautocorr.html",
    "title": "2d autocorrelation",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#motivation",
    "href": "simmodelling/twoDautocorr.html#motivation",
    "title": "2d autocorrelation",
    "section": "Motivation",
    "text": "Motivation\nI often need to simulate processes that are autocorrelated in two dimensions. Sometimes that’s time and 1d space, sometimes 2d space. Clearly 3d is likely needed as well, and I’ll update this with that once I get to it.\nThis is code that builds on work I’ve done in a couple projects, both across matlab and R. I’m doing it here in R because that’s the most up to date and open-source, but the matlab translation is straightforward.\nWe want to be able to generate a set of values with given statistical properties- means, standard deviations, and correlations in both dimensions. For the moment, I’m developing this with a gaussian random variable, but extensions to other random variables that are transforms from gaussian are relatively straightforward by backcalculating the needed \\(\\mu\\) and \\(\\sigma\\). Care must be taken if the correlations need to also be defined on the final scale.\n\nFuture/elsewhere\nI’ve done the back-calculations for the lognormal to allow setting desired correlations, means, and variances on the lognormal scale, and will add it in here later as an example. Likewise, we might want to set the correlation length \\(\\tau\\) rather than the correlation \\(\\rho\\), and in that case we need to back-calculate \\(\\rho\\) from the desired \\(\\tau\\). I’ve done that as well and will add it in. Finally, I have written up the math to obtain the equations used in this function, and will add that later as well."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#process",
    "href": "simmodelling/twoDautocorr.html#process",
    "title": "2d autocorrelation",
    "section": "Process",
    "text": "Process\nThe goal is a U matrix that is 2d AC, on the normal scale\n\nSet up autocorrelation in the y dimension in U with a usual \\(y+1 = y*\\rho + a\\) formulation, where \\(a\\) is uncorrelated errors\nSet up autocorrelation in the x dimension\n\n\n\nthe errors here (\\(\\varepsilon\\) matrix) need to be correlated in the y dimension\nthese errors are thus generated by an AC process and so need their own set of errors (which are uncorrelated) for that AC\n\nVariances are set for all error matrices (\\(a\\), \\(\\varepsilon\\), and sub-errors (\\(z\\) matrix)) according to the relationships between normVar (the desired \\(\\sigma^2\\) of the final distribution) and the \\(\\rho_y\\) and \\(\\rho_x\\) (the desired correlations in both dimensions)."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#function",
    "href": "simmodelling/twoDautocorr.html#function",
    "title": "2d autocorrelation",
    "section": "Function",
    "text": "Function\nI usually do a bunch of demos, but here I’ve developed this and just want it available more easily. So I’ll lead with the function and then demonstrate it and a few extensions.\n\nac2d <- function(n_x, n_y, \n                 rho_y = 0, rho_x = 0, \n                 normVar = 1,\n                 printStats = FALSE,\n                 returnStats = FALSE) {\n  # n_x = number of sites along the x-dimension\n  # n_y = number of sites along the y-dimension\n  # rho_y = desired autocorr in the x direction\n  # rho_x = desired autocorr in the y direction\n  # normVar = desired variance of the underlying normal distribution\n  \n  # The goal is a U matrix that is 2d AC, on the normal scale\n  \n  # make the U matrix as rnorms to initialise\n  U <- matrix(rnorm(n_x*n_y)*sqrt(normVar), nrow = n_y)\n  \n  # Set up the errors for the y process alone\n  # generate the errors - set the SD of these (hence the sqrt around the\n  # variance)\n  a <- rnorm(n_y) * sqrt((normVar*(1-rho_y^2)))\n  \n  # Make the y ac for the U matrix\n  for (i in 1:(n_y-1)) {\n    U[i+1, ] <- (rho_y * U[i, ]) + a[i]\n  }\n  \n  # Set up for the x-autocorr, which needs to have errors autocorred in the y-dimension\n  \n  # first, generate a z error matrix- these are the errors for epsilon, which\n  # are in turn the errors for U(t,x).\n  # What should var(z) be theoretically?\n  varZ <- normVar*(1-rho_y^2)*(1-rho_x^2)\n  \n  # Make z, adjusting its standard deviation\n  # should have 'y' rows\n  z <- matrix(rnorm(n_x*n_y), nrow = n_y) * \n    (sqrt(normVar * (1-rho_y^2) * (1-rho_x^2)))\n  \n  # now let's generate an epsilon matrix\n  # These are the errors for x part of the 2d ac process. These errors are\n  # themselves autocorrelated in the y dimension.\n  vareps <- normVar * (1-rho_x^2)\n  eps <- matrix(rnorm(n_x*n_y), nrow = n_y) * sqrt(vareps)\n  \n  # Now, generate the eps matrix y-autocorrelated (that is, going down rows within each column)\n  # eps is already created, so just write into the rows\n  for (i in 1:(n_y-1)) {\n    eps[i+1, ] <- (rho_y * eps[i, ]) + z[i, ]\n  }\n  \n  # Now, make the U matrix x-autocorrelated\n  for (t in 1:(n_x-1)) {\n    U[ ,t+1] <- (rho_x * U[ ,t]) + eps[ ,t]\n    \n  }\n  \n  # Check the stats if asked\n  if (printStats | returnStats) {\n    # calc stats in both dimensions\n    acstats <- ac2dstats(U)\n    \n    if (printStats) {\n      print(paste0('Mean of all points is ', round(mean(c(U)), 3)))\n      print(paste0('Var of all points is ', round(var(c(U)), 3)))\n      print(paste0('Mean y AC is ', round(mean(acstats$ac_y), 3)))\n      print(paste0('Mean x AC is ', round(mean(acstats$ac_x), 3)))\n    }\n  }\n  \n  # usually don't want a list with the stats, and can always get later if needed, I suppose\n  if (returnStats) {\n    return(lst(U, acstats))\n  } else {\n    return(U)\n  }\n  \n}\n\nThat potentially calls another function to get the stats, which is here.\n\n# 2d ac stats function, useful for calling elsewhere\nac2dstats <- function(acmatrix) {\n  # Calculate the autocorrs in both dimensions\n  \n  # Conditionals on 0 variance are because ar throws an error if there's no variance. Could have set up a try, but this is clearer\n  # Using 1 as the ac in that case because with no variance each value is the same as previous and so perfectly correlated. NA would be another option.\n  \n  # Get the ac in x-dimension: do this for each y (row)\n  ac_x <- vector(mode = 'numeric', length = nrow(acmatrix)-1)\n  for (i in 1:(nrow(acmatrix)-1)) {\n    if (sd(acmatrix[i, ]) == 0) {\n      ac_x <- 1\n    } else {\n      ac_x[i] <- acf(acmatrix[i, ], lag.max = 1, type = 'correlation', plot = FALSE, demean = TRUE)$acf[2]\n    }\n    \n  } \n  \n  # Get the ac acorss the stream: do this for each x (column)\n  ac_y <- vector(mode = 'numeric', length = ncol(acmatrix)-1)\n  for (i in 1:(ncol(acmatrix)-1)) {\n    \n    if (sd(acmatrix[,i]) == 0) {\n      ac_y[i] <- 1\n    } else {\n      ac_y[i] <- acf(acmatrix[ ,i], lag.max = 1, type = 'correlation', plot = FALSE, demean = TRUE)$acf[2]\n    }\n    \n  } \n  \n  return(lst(ac_y, ac_x))\n}"
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#testing",
    "href": "simmodelling/twoDautocorr.html#testing",
    "title": "2d autocorrelation",
    "section": "Testing",
    "text": "Testing\nA couple edge cases to make sure it doesn’t break. 0 and 1 correlations.\n\nacmatrix_0_1 <- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0, rho_y = 1,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is 0.028\"\n[1] \"Var of all points is 0.976\"\n[1] \"Mean y AC is 1\"\n[1] \"Mean x AC is 0.043\"\n\n\n0 variance, but try to set autocorrelations- forces all points equal, which is right.\n\nacmatrix_0_1 <- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 0, printStats = TRUE)\n\n[1] \"Mean of all points is 0\"\n[1] \"Var of all points is 0\"\n[1] \"Mean y AC is 1\"\n[1] \"Mean x AC is 1\""
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#demonstration",
    "href": "simmodelling/twoDautocorr.html#demonstration",
    "title": "2d autocorrelation",
    "section": "Demonstration",
    "text": "Demonstration\nHow do we use that? Let’s say we want to create an environment that is 1000 x 500 sites, with \\(\\rho_y = 0.9\\) and \\(\\rho_x = 0.7\\), with the whole environment having a variance of 1 (for simplicity).\nSetting printstats = TRUE prints out the statistics and confirms the final matrix has been created with the desired correlations.\n\nacmatrix_7_9 <- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is 0.021\"\n[1] \"Var of all points is 0.993\"\n[1] \"Mean y AC is 0.89\"\n[1] \"Mean x AC is 0.698\"\n\n\nWe can plot that up, easiest is to use ggplot because that’s what I’m used to. First, make it a tibble\n\nactib_7_9 <- tibble::as_tibble(acmatrix_7_9) %>%\n  mutate(y = row_number()) %>%\n  pivot_longer(cols = starts_with('V')) %>%\n  mutate(x = as.numeric(str_remove(name, 'V'))) %>%\n  select(-name)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\nPlot it two different ways. It’s a monster though, so cut it to just a 100x100 block.\nFirst, a contour\n\nggplot(filter(actib_7_9, x > 100 & x <= 200 & y > 300 & y < 400), aes(x = x, y = y, z = value)) +\n  geom_contour_filled()\n\n\n\n\nAnd a tiled version, which is more precisely the data.\n\nggplot(filter(actib_7_9, x > 100 & x <= 200 & y > 300 & y < 400), aes(x = x, y = y, fill = value)) + \n  geom_tile() +\n  viridis::scale_fill_viridis(option = 'viridis')"
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#extensions",
    "href": "simmodelling/twoDautocorr.html#extensions",
    "title": "2d autocorrelation",
    "section": "Extensions",
    "text": "Extensions\n\n2 species\nA crude step toward 3d autocorr is to say we want 2d autocorr for two species (or really, just a second set of 2d autocorrelated values) with known correlation to the first set. I’ve done that, but it’s very task-specific and so not including here until I generalise a bit better.\n\n\nCross-correlation\nBy definition, the 2d autocorrelated matrices here have embedded nonzero cross-correlations at different lags (see analytical work for what they are once I put it in here). As a quick example, we can use ccf to get the cross correlation between two adjacent vectors along the x-dimension (columns), or the same along the y-dimension (rows).\nColumns\n\nccf(x = acmatrix_7_9[,100], y = acmatrix_7_9[,101], lag.max = 10, type = 'correlation')\n\n\n\n\nRows\n\nccf(x = acmatrix_7_9[100,], y = acmatrix_7_9[101,], lag.max = 10, type = 'correlation')"
  },
  {
    "objectID": "small_helpers/json_api_construction.html",
    "href": "small_helpers/json_api_construction.html",
    "title": "JSON API coding",
    "section": "",
    "text": "I’m working on an API that uses JSON in the body, but getting it to come out right with square brackets, curly brackets, commas, etc where they’re supposed to be has been trial and error. I’m going to put what I’ve figured out here. I’m using examples from the {vicwater} package, but the main point is to show how to get different sorts of output.\nUsing the httr2 examples with req_dry_run to see what the request looks like and check the formats.\n\nlibrary(httr2)\nreq <- request(\"http://httpbin.org/post\")\n\n\n\nTo pass simple one to one key-value pairs, wrapped in {} use a list.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\n\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 40\n\n{\"function\":\"get_db_info\",\"version\":\"3\"}\n\n\n\n\n\nTo pass nested key-value pairs, use nested lists\n\nparams <- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '123abc',\n                               \"datasource\" = \"A\"))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 95\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"123abc\",\"datasource\":\"A\"}}\n\n\n\n\n\nThese cannot be created with c(), because that does something else (square brackets- see below).\n\nparams <- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '233217, 405328, 405331'))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 100\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331\"}}\n\n\n\n\n\nTo get square brackets, we need a vector. So, typically c() the bits together in the call (or previously).\n\nparams <- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = c('A', 'TELEM')))\n\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}\n\n\n\n\n\nTo get patterns like [['a', 'b'],['c', 'd']], use a matrix (and I think maybe a df). Which makes sense if we think of that as a group of vectors. The pattern is [[row1], [row2], [row_n]].\n\ntopleft <- c('-35', '148')\nbottomright <- c('-36', '149')\n\nrectbox <- rbind(topleft, bottomright)\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"148\"],[\"-36\",\"149\"]]}}}\n\n\n\n\nGives some horrible combination of curly and square braces including column and row names.\n\nrectdf <- data.frame(rectbox)\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 208\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"X1\":\"-35\",\"X2\":\"148\",\"_row\":\"topleft\"},{\"X1\":\"-36\",\"X2\":\"149\",\"_row\":\"bottomright\"}]}}}\n\n\nTibbles aren’t really any different, but the names are a bit cleaner\n\nrectdf <- tibble::as_tibble(rectbox)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %>%\n  req_body_json(params) %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}\n\n\n\n\n\n\nThere are arguments to toJSON that alter how matrices and dfs get parsed. Matrices are by default row-wise, but we can change to cols (e.g. [['col1'], ['col2']] with matrix = 'columnmajor'.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %>%\n  req_body_json(params, matrix = 'columnmajor') %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"-36\"],[\"148\",\"149\"]]}}}\n\n\nSimilarly, we can alter how dfs work, which might actually be fairly useful in the way it handles named columns especially. The default (above) is dataframe = 'rows' , which is kind of a mess (or at least not how my brain parses what a dataframe means). But dataframe = 'columns' ends up with named vectors. I don’t currently need that, but it sure makes more sense.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %>%\n  req_body_json(params, dataframe = 'columns') %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 160\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":{\"V1\":[\"-35\",\"-36\"],\"V2\":[\"148\",\"149\"]}}}}\n\n\nUsing dataframe = 'values' is again a confusing list.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %>%\n  req_body_json(params, datafraem = 'values') %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}\n\n\n\n\n\nTo get square brackets around multiple sets of curlies, e.g. `[{‘key’: ‘value’}, {‘key2’: ‘value2’}], use a list of lists.\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \n                               \"complex_filter\" = list(list('fieldname' = 'stntype', \n                                                       'value' = 'HYD'),\n                                                   list('combine' = 'OR',\n                                                        'fieldname' = 'stntype',\n                                                        'value' = 'VIR'))))\nreq %>%\n  req_body_json(params, datafraem = 'values') %>%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 203\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"complex_filter\":[{\"fieldname\":\"stntype\",\"value\":\"HYD\"},{\"combine\":\"OR\",\"fieldname\":\"stntype\",\"value\":\"VIR\"}]}}"
  },
  {
    "objectID": "small_helpers/list_names.html",
    "href": "small_helpers/list_names.html",
    "title": "List names as variables",
    "section": "",
    "text": "I often end up wanting to build a list with named items, and the names come in as variables.\nFor example, instead of the typical\nI might have the names defined elsewhere. This is particularly common inside functions.\nWe can’t pass those in as usual- this uses name# as the name, not the value of the variable."
  },
  {
    "objectID": "small_helpers/list_names.html#various-solutions",
    "href": "small_helpers/list_names.html#various-solutions",
    "title": "List names as variables",
    "section": "Various solutions",
    "text": "Various solutions\nWe can use setNames\n\nsetNames(list('first', 'second',c(3,4)), c(name1, name2, name3))\n\n$a\n[1] \"first\"\n\n$b\n[1] \"second\"\n\n$d\n[1] 3 4\n\n\nwe can do it in two steps with names , which I think is what setNames wraps, and is just extra verbose and requires carrying data copies around.\n\nbarelist <- list('first', 'second',c(3,4))\nnames(barelist) <- c(name1, name2, name3)\nbarelist\n\n$a\n[1] \"first\"\n\n$b\n[1] \"second\"\n\n$d\n[1] 3 4\n\n\nCan we unquote/eval?\nI can almost never get !! or !!! to work. this doesn’t, as usual.\n\nlist(!!name1 = 'first', !!name2 = 'second', !!name3 = c(3,4))\n\nNor this\n\nlist(eval(name1) = 'first', eval(name2) = 'second', eval(name3) = c(3,4))\n\nNor this, despite the eval working\n\nrlang::eval_bare(name1)\n\n[1] \"a\"\n\n\n\nlist(rlang::eval_bare(name1) = 'first', rlang::eval_bare(name2) = 'second', rlang::eval_bare(name3) = c(3,4))\n\nHow about tibble::lst ? I often use it for lists of variables because it self-names them, so maybe it’s the answer here. Yep. That’s just cleaner.\n\ntibble::lst(name1 = 'first', name2 = 'second', name3 = c(3,4))\n\n$name1\n[1] \"first\"\n\n$name2\n[1] \"second\"\n\n$name3\n[1] 3 4\n\n\nAnd, that self-naming I was describing, which solves a different problem- having to write list(name = name, age = age).\n\nname <- c('David', 'Susan')\nage <- c(1,2)\n\ntibble::lst(name, age)\n\n$name\n[1] \"David\" \"Susan\"\n\n$age\n[1] 1 2"
  },
  {
    "objectID": "small_helpers/quarto_notes.html",
    "href": "small_helpers/quarto_notes.html",
    "title": "Quarto notes",
    "section": "",
    "text": "If there’s not a Quarto project (which is not the same as an Rproject), i.e. the .qmds are standalone, then the above rmarkdown method works to set the root to the Rproject. But if there is a Quarto project (I’ve moved to almost always doing this), then we can set the Quarto root directory in the _quarto.yml for the project. There are actually two useful settings we can make there- a project-wide output directory, and what to use to execute. There are (to my knowledge) two options for execute-dir- execute-dir: file and execute-dir: project, which set the root for rendering as the file location or the project. It almost always makes most sense to use project, because then everything uses the same reference for relative paths. For this website, that yaml is\n\nproject:\n  type: website\n  output-dir: docs\n  execute-dir: project\n\n\nThis is fairly specific to websites (and I guess books?). Sorted out in the website-specific page. Takehome is links need to be relative to the file. And so those in different directories often have to use ../other_dir/other_file.qmd to get up and over.\n\n\n\nThe only exception that I’ve run into so far is a weird situation where I have a Quarto project with an Rproject in a subdirectory because I want the Quarto to have access to several different code projects. The catch is that if I use paths relative to the R project and ask Quarto to render, all the paths are wrong. They are also wrong if I Run or run the code cells interactively. If I add the Rmarkdown setup chunk above knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()), then Run and interactive stuff works, but when I try to quarto render, it can’t find the Rproject because it’s looking at the same level and up, and the Rproj is in a directory down.\nSo, the workaround I’ve come up with is to set execute-dir: file, so the dir is set to the file dir, which is inside the Rproject. Then, use an Rmarkdown-style setup chunk with knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) to reset the directory to the Rproject root dir.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nThen the code both Runs and renders, and we don’t have to keep track of paths relative to two different nested projects manually. This approach works if the internal projects are self-contained (and we often want them to be). But if we need to render something across several, we’ll need to do something different.\nOther options that might work but I haven’t tried are parameterised notebooks (with conditional params?) Or using freeze and virtual environments (see docs)\nWhat I really want is a way to set the execute-dir on a file-by-file basis in the header yaml, but that doesn’t seem to work."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#headers-in-vs-code-vs-rstudio",
    "href": "small_helpers/quarto_notes.html#headers-in-vs-code-vs-rstudio",
    "title": "Quarto notes",
    "section": "Headers in VS code vs Rstudio",
    "text": "Headers in VS code vs Rstudio\nI had a few notebooks that ran fine in Rstudio, but wouldn’t render in VS (or from command line). I got the very cryptic error YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key with a line and col number that didn’t seem to correspond to anything with YAML in the project- not the file header, not the _quarto.yml, not _quarto.yaml.local, and nothing particularly useful showed up on google.\nThe solution seems to be to add an explicit line for editor: visual in the file header, e.g.\ntitle: Test\nauthor: Galen Holt\nformat:\n  html:\n    df-print: paged\neditor: visual\nIt doesn’t seem to matter if the title and author are wrapped in double-quotes (which was one suggestion online).\nI think what’s happening is that Rstudio’s visual editor is adding some hidden formatting syntax that gets exposed and looks like YAML to VS and the quarto CLI."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#python-paths",
    "href": "small_helpers/quarto_notes.html#python-paths",
    "title": "Quarto notes",
    "section": "Python paths",
    "text": "Python paths\nIf we use {reticulate}, R wants to know where the python lives, and gets grumpy if it can’t find it. Sometimes it silently points somewhere we don’t want, and other times we see errors like\nError creating conda environment 'C:/Users/galen/Documents/Website/galen_website/renv/python/r-reticulate' [exit code 1]\n\nRprofile\nThe main solution is to edit .Rprofile to set the RETICULATE_PYTHON environment variable,\nSys.setenv(RETICULATE_PYTHON = '../werptoolkitpy/.venv/Scripts/python.exe')\nNote that this can either be a full path, or relative to the R project directory- in the situation above, I have the R project nested in a Quarto project that also contains a py project, so we need to go up and over to get to the .venv.\nThis is needed to get any of the reticulate code to work (though you can set it on a file-by-file basis.\nAdditional issues can come up with Quarto though.\n\n\nQuarto environment\nWhen we run Quarto inside Rstudio in parallel with an R project, everything works fine. But, if Quarto is run through VScode, for some reason it doesn’t hit the .Rprofile and so doesn’t run the line we just added, and we’re back to using the wrong python and errors about conda. To make it more complex, Quarto has its own python environment variable QUARTO_PYTHON. This is all more complicated when the Quarto project directory doesn’t match the R project directory.\nThe solution is to set up a _environment file in the Quarto project directory to set those variables according to the docs (and maybe PY_PYTHON too, just to be safe- I can’t find the docs to know how these differ). For my case, my _environment file looks like\nRETICULATE_PYTHON='../werptoolkitpy/.venv/Scripts/python.exe'\nQUARTO_PYTHON='werptoolkitpy/.venv/Scripts/python.exe'\nPY_PYTHON='werptoolkitpy/.venv/Scripts/python.exe'\nHere, again, I have Rproject nested in Quarto proj, but they access this file differently so the paths differ even though they point to the same place. RETICULATE_PYTHON needs to get up and out of the R subdir and over to the py side of things, while the Quarto project is the outer dir and so QUARTO_PYTHON can go straight in to the py subdir.\nAs far as I can tell, this does NOT supersede the necessity of setting .Rprofile, which is needed for the R code to run. This stuff in _environment is in addition so Quarto (especially in VS) can access the right info."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#table-printing",
    "href": "small_helpers/quarto_notes.html#table-printing",
    "title": "Quarto notes",
    "section": "Table printing",
    "text": "Table printing\n\nStyle\nUse the yaml header to declare style, one of paged, kable, tibble, and default as in documentation. In a twist, default seems to be the way to do some customisation using S3, see https://debruine.github.io/quarto_demo/table.html, though I haven’t played with that.\nSo, typically the yaml header would be something like\nformat:\n  html:\n    df-print: paged\nCan I change that for a single chunk? Work on that later. Putting it in as #| df-print: option isn’t recognized.\n\n\nRow number\nThe different df-print options have different defaults of how much they print (and with paged it doesn’t matter so much). As far as I can tell, tibble prints the whole thing, and kable prints 10 rows. Sometimes we want to control that though - maybe we have a df with 13 rows, and we want to just print the whole thing.\nThe default is\n\niris\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\n\nTo print more rows, we can use the rows.print option.\n\niris\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica"
  },
  {
    "objectID": "small_helpers/quarto_notes.html#website",
    "href": "small_helpers/quarto_notes.html#website",
    "title": "Quarto notes",
    "section": "Website",
    "text": "Website\nStarting and updating quarto websites has a few tricks, written up in more detail at those links."
  },
  {
    "objectID": "small_helpers/smallpieces.html",
    "href": "small_helpers/smallpieces.html",
    "title": "Small pieces",
    "section": "",
    "text": "This is mostly quick little code snippets to copy-paste and avoid re-writing. load tidyverse and get going.\n\nlibrary(tidyverse)\n\n\n\nWe often want to set the root directory not to the file but to the project. In Rmarkdown, we use the following in the setup chunk. Quarto typically uses a different method, but see the Quarto notes for some exceptions. Converting from Rmarkdown to quarto with knitr::convert_chunk_header kills this block, and it’s annoying to always have the header. In both Rmarkdown and Quarto, this has to be in a setup chunk.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nI thought it’d be easiest to set in the global options, but that doesn’t seem to persist to render or knit.\n\n\n\n\n\nnewdir <- file.path('output', 'testdir')\nif (!dir.exists(newdir)) {dir.create(newdir, recursive = TRUE)}\n\n\n\n\nWindows paths come in with \\, which R treats as an escape character. We can use file.path to just avoid them, or replace them with / or \\\\. But sometimes we just want to paste a path in quickly and be done with it. As of R 4.0, we can do that with r. It requires the parentheses to be in a funny place- inside the quotes.\n\npastepath <- r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\"\npastepath\n\n[1] \"C:\\\\Users\\\\username\\\\path\\\\to\\\\somewhere.csv\"\n\n\nAnd we can feed that straight into functions that need paths, eg.\n\nreadr::read_csv(r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\")\n\n\n\n\nFunctions like duplicated give the second (and greater) values that match. e.g.\n\nx <- c(1,2,1,3,4,2)\nduplicated(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nBut we often want to grab all values that are repeated- ie if everything matches in one column what’s going on in the others. do do that we can use group_by and filter to get those with > 1 row.\nIE, let’s compare cars with duplicated mpg values\n\nmtcars %>%\n  dplyr::group_by(mpg) %>%\n  dplyr::filter(n() > 1) %>%\n  dplyr::arrange(mpg) # makes the comparisons easier\n\n\n\n  \n\n\n\nWhy is that useful? We can see not only that these aren’t fully duplicated rows (which we also could have done with duplicated on the whole table), but also actually look at what differs easily.\n\n\n\nSometimes with long csvs, readr’s guess of col type based on the first thousand rows is wrong. But only for some cols. If we want to not have to specify all of them, we can use .default and only specify the offending col.\nFirst, save dummy data\n\ndumtib <- tibble(c1 = 1:3000, c2 = rep(letters, length.out = 3000), c3 = c(c1[1:2000], c2[2001:3000]))\n\nwrite_csv(dumtib, file = file.path(newdir, 'colspectest.csv'))\n\nIf we read in without the cols, it assumes c3 is numeric and we get errors. But it doesn’t. why not? It keeps getting me elsewhere, but now I can’t create the problem. FIgure this out later, I guess\n\nfilein <- read_csv(file.path(newdir, 'colspectest.csv'), guess_max = 100)\n\nRows: 3000 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): c2, c3\ndbl (1): c1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTell it the third col is character.\n\nfilein <- readr::read_csv(file.path(newdir, 'colspectest.csv'), col_types = cols(.default = \"?\", c3 = \"c\"))\n\n\n\n\nYes, we should be building as a library in this case, but it’s often easier at least initially to not deal with the overhead. If, for example, all functions are in the ‘functions’ directory,\n\n# Location-setting header\n# source everything in the functions folder. This really is turning into a package\nfunfiles <- list.files('functions')\nfor (s in 1:length(funfiles)) {\n  source(file.path('functions', funfiles[s])) \n}\n\n\n\n\nRender in quarto defaults to making dfs text, and so often we can’t see all the columns (or rows), or access them. setting the df-print option to paged allows them to work. The header should look like this (commented out because this isn’t a header)\n\n# title: \"TITLE\"\n# author: \"AUTHOR\"\n# format:\n#   html:\n#     df-print: paged\n\n\n\n\nconvert_chunk_headers is the main thing, but I want to apply it to a full directory. Let’s get the dir for here.\n\nallrmd <- list.files(rprojroot::find_rstudio_root_file(), pattern = '.Rmd', recursive = TRUE, full.names = TRUE)\n\nallrmd <- allrmd[!stringr::str_detect(allrmd, 'renv')]\n\nallqmd <- stringr::str_replace(allrmd, '.Rmd', '.qmd')\n\nCan I vectorize? No, but a loop works. Git commit first!\n\nfor (i in 1:length(allrmd)) {\n  knitr::convert_chunk_header(input = allrmd[i], output = allqmd[i])\n}\n\nNow, if you want to really go for it, delete the rmds. That makes git happier because then it can treat this as a rename and keep tracking the files.\nDangerous- make sure you’ve git-committed. I’m commenting out and eval: false ing this\n\n# file.remove(allrmd)"
  },
  {
    "objectID": "small_helpers/zip_downloading.html",
    "href": "small_helpers/zip_downloading.html",
    "title": "Download Zip helper",
    "section": "",
    "text": "When we download files from the internet, we often feed in a url, and it returns a zip, which we then want to unzip to access. There’s a fairly simple way to do that, but we can write a quicky function to do it and clean up the directory afterwards."
  },
  {
    "objectID": "small_helpers/zip_downloading.html#the-function",
    "href": "small_helpers/zip_downloading.html#the-function",
    "title": "Download Zip helper",
    "section": "The function",
    "text": "The function\nwe want to give it the dirname for the file(s), the datadir that contains our data, and the URL. Then it checks if it exists, and downloads, unzips, and cleans up.\n\nzip_load <- function(dirname, datadir, sourceurl,  \n                      existing_dirs = list.files(datadir)) {\n  print(existing_dirs)\n  if (!(dirname %in% existing_dirs)) {\n    \n    zippath <- file.path(datadir, paste0(dirname, '.zip'))\n    download.file(sourceurl, destfile = zippath)\n    \n    unzip(zippath, exdir = file.path(datadir, dirname))\n    \n    file.remove(zippath)\n  }\n}"
  },
  {
    "objectID": "small_helpers/zip_downloading.html#an-example",
    "href": "small_helpers/zip_downloading.html#an-example",
    "title": "Download Zip helper",
    "section": "An example",
    "text": "An example\nGet the Murray-Darling basin boundary\n\nzip_load('mdb_boundary', 'data', \"https://data.gov.au/data/dataset/4ede9aed-5620-47db-a72b-0b3aa0a3ced0/resource/8a6d889d-723b-492d-8c12-b8b0d1ba4b5a/download/sworkingadhocjobsj4430dataoutputsmdb_boundarymdb_boundary.zip\")\n\ncharacter(0)\n\n\nWarning in download.file(sourceurl, destfile = zippath): URL\nhttps://data.gov.au/data/dataset/4ede9aed-5620-47db-a72b-0b3aa0a3ced0/resource/8a6d889d-723b-492d-8c12-b8b0d1ba4b5a/download/sworkingadhocjobsj4430dataoutputsmdb_boundarymdb_boundary.zip:\ncannot open destfile 'data/mdb_boundary.zip', reason 'No such file or directory'\n\n\nWarning in download.file(sourceurl, destfile = zippath): download had nonzero\nexit status\n\n\nWarning in unzip(zippath, exdir = file.path(datadir, dirname)): error 1 in\nextracting from zip file\n\n\nWarning in file.remove(zippath): cannot remove file 'data/mdb_boundary.zip',\nreason 'No such file or directory'\n\n\n[1] FALSE\n\n\nI’ve saved this in functions/ so I have easy access to it everywhere."
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html",
    "href": "vicwater/vicwater_api_howtocall.html",
    "title": "Vicwater api crude testing",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#access-victoria-water-data-through-api",
    "href": "vicwater/vicwater_api_howtocall.html#access-victoria-water-data-through-api",
    "title": "Vicwater api crude testing",
    "section": "Access Victoria water data through API",
    "text": "Access Victoria water data through API\nWe want to access victorian water data for a set of sites. That requires using the api at https://data.water.vic.gov.au/cgi/webservice.exe?[JSON_request] , but it’s poorly documented, and I’ve maybe done one API call ever. Time to figure this out. Will start by piggybacking on the mdba-gauge-getter python that gets water levels as a starting point and then try to get other data.\nFirst, how do we make an API request? Most tutorials use twitter or github, which are well-documented. But let’s try something similar.\npurrr conflicts with jsonlite::flatten, so don’t load tidyverse.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(httr)\nlibrary(jsonlite)\n\n# Actually end up using\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.2.2\n\n\nLooks like the first thing is the base url. The web says it’s this, and the mdba-gauge-getter uses the same, and then appends json_data\n\nvicurl <- \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\n\nI guess I need to specify something to get. But there is no documentation I can find for what the parameters are. The gauge-getter has a few, so I guess start picking things apart.\n\nparams <- list(\"site_list\" = '232202')\n\n\nresponse <- GET(vicurl, query = params)\nresponse\n\nResponse [https://data.water.vic.gov.au/cgi/webservice.exe?site_list=232202]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: text/html\n  Size: 103 B\n\n\nFollowing the R api vignette,\n\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nInteresting. It looked like it returned 200 (good) when I printed response and when I look at it in the View, but actually had errors. Where ARE these results?\nso, can we add those missing ‘top-level’ items? I see now that the gauge-getter has a two-level dict\n\nparams <- list(\"version\" = '2')\nresponse <- GET(vicurl, params = params)\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\nparsed\n\nTry the example. can’t get it to even be a character vector\n\n# demourl <- https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}\n\n\nparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\nresponse <- GET(vicurl, query = params)\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 122\n\n$error_msg\n[1] \"Parameter error(s) for function get_db_info:Missing: table_name\"\n\n\nWell, that’s a start. at least I’m not getting the top-level errors. Can i just smash that whole demo into a single params list? without the sublists of ‘params’ and ‘filter_values’?\n\n# params <- list(\"function\" = 'get_db_info',\n#                \"version\" = \"3\",\n#                \"table_name\" = \"site\",\n#                \"station\" = \"221001\")\n# response <- GET(vicurl, query = params)\n# \n# # The parsed barfs\n# # parsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n# # parsed\n# \n# response\n\nWhat am I actually asking for here? GET is using modify_url\n\nmodify_url(vicurl, query = params)\n\n[1] \"https://data.water.vic.gov.au/cgi/webservice.exe?function=get_db_info&version=3\"\n\n\nSo that’s using the ’conventional parameter pairs’ option here, not the json . How do I generate some json so I can see if I’m matching the format? auto_unbox = TRUE is needed to not wrap the second values in brackets.\n\ntoJSON(params, auto_unbox = TRUE)\n\n{\"function\":\"get_db_info\",\"version\":\"3\"} \n\n\nOK, so that looks vaguely right, but not leveled. Can we do lists of lists?\n\nnestparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"hash\",\n                               \"filter_values\" = list(\"station\" = \"221001\")))\ntoJSON(nestparams, auto_unbox = TRUE)\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}} \n\n\nWhoa! that looks right. Now, let’s try it. It immediately fails to just use GET. try looking at modify_url to see why.\n\njsonbit <- toJSON(nestparams, auto_unbox = TRUE)\n\n\nmodify_url(vicurl, query = jsonbit)\n\n[1] \"https://data.water.vic.gov.au/cgi/webservice.exe?{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\n\nmodify_url(vicurl, path = jsonbit)\n\n[1] \"https://data.water.vic.gov.au/{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\nGetting a lot of slashes. does it matter? Maybe?\n\nmodify_url(vicurl, scheme = nestparams)\n\n[1] \"get_db_info://data.water.vic.gov.au/cgi/webservice.exe\"                                                                                    \n[2] \"3://data.water.vic.gov.au/cgi/webservice.exe\"                                                                                              \n[3] \"list(table_name = \\\"site\\\", return_type = \\\"hash\\\", filter_values = list(station = \\\"221001\\\"))://data.water.vic.gov.au/cgi/webservice.exe\"\n\n\nIs httpbin a way to test?\n\nbinurl <- \"http://httpbin.org/get\"\n\nbinr <- GET(binurl, query = jsonbit)\nbinr\n\nResponse [http://httpbin.org/get?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: application/json\n  Size: 689 B\n{\n  \"args\": {\n    \"{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name...\n  }, \n  \"headers\": {\n    \"Accept\": \"application/json, text/xml, application/xml, */*\", \n    \"Accept-Encoding\": \"deflate, gzip\", \n    \"Host\": \"httpbin.org\", \n    \"User-Agent\": \"libcurl/7.64.1 r-curl/4.3.3 httr/1.4.4\", \n    \"X-Amzn-Trace-Id\": \"Root=1-638ffc12-7dab7950495d406217e5071c\"\n...\n\n\n\nparsedB <- fromJSON(content(binr, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsedB\n\n$args\n$args$`{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}`\n[1] \"\"\n\n\n$headers\n$headers$Accept\n[1] \"application/json, text/xml, application/xml, */*\"\n\n$headers$`Accept-Encoding`\n[1] \"deflate, gzip\"\n\n$headers$Host\n[1] \"httpbin.org\"\n\n$headers$`User-Agent`\n[1] \"libcurl/7.64.1 r-curl/4.3.3 httr/1.4.4\"\n\n$headers$`X-Amzn-Trace-Id`\n[1] \"Root=1-638ffc12-7dab7950495d406217e5071c\"\n\n\n$origin\n[1] \"180.222.17.154\"\n\n$url\n[1] \"http://httpbin.org/get?{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\nThat call looks right.\n\nresponse <- GET(vicurl, query = jsonbit, encode = 'json')\nresponse\n\nResponse [https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: text/html\n  Size: 99 B\n\n\n\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Request is not well-formed JSON\\r\\nInput request was not valid JSON\"\n\n\nI pasted it in to notebook++ and it’s exactly the same as the example. So, why isn’t it working?\n\nmodurl <- modify_url(vicurl, query = jsonbit)\nresponse <- GET(modurl)\nresponse\n\nResponse [https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: text/html\n  Size: 99 B\n\n\n\nparsed <- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Request is not well-formed JSON\\r\\nInput request was not valid JSON\""
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#httr2",
    "href": "vicwater/vicwater_api_howtocall.html#httr2",
    "title": "Vicwater api crude testing",
    "section": "httr2",
    "text": "httr2\nHmmm. I see Hadley has released a v2. And it has a req_body_json. See if that works\n\nlibrary(httr2)\n\nThe req_dry_run lets us see what it’s passing. THat looks right? I think?\n\nreq <- request(vicurl)\nreq %>% \n  req_body_json(nestparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 129\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}\n\n\n\nresp <- req %>% \n  req_body_json(nestparams) %>% \n  req_perform()\n\n\nresp %>% resp_raw()\n\nHTTP/1.1 200 OK\nDate: Wed, 07 Dec 2022 02:36:05 GMT\nContent-Type: text/html\nContent-Length: 972\nConnection: keep-alive\nContent-Encoding: gzip\nVary: Accept-Encoding\nServer: Microsoft-IIS/10.0\nContent: \nAccess-Control-Allow-Origin: *\n\n{\"error_num\":0,\"return\":{\"rows\":{\"221001\":{\"category20\":\"\",\"category19\":\"\",\"category18\":\"\",\"category17\":\"\",\"category16\":\"\",\"category15\":\"\",\"category14\":\"\",\"category13\":\"\",\"category12\":\"\",\"category11\":\"\",\"category10\":\"N\\/A\",\"active\":false,\"northing\":\"5887218.000\",\"timezone\":\"10.0\",\"shortname\":\"GENOA R @ ROCKTON\",\"datecreate\":18991230,\"elevdatum\":\"\",\"stname\":\"GENOA RIVER @ ROCKTON\",\"category9\":\"N\\/A\",\"category8\":\"G(S)\",\"category7\":\"G\",\"category6\":\"2WD\\/4WD\",\"category5\":\"10\",\"category4\":\"150\",\"category3\":\"GRAVEL\",\"category2\":\"V_70D3\",\"category1\":\"23\",\"elevacc\":\"1\",\"dbver47\":false,\"quarter\":\"Y\",\"section\":0,\"commence\":19930526,\"parent\":\"\",\"mapname\":\"MAFFRA\",\"meridian\":\"\",\"spare5\":\"\",\"spare4\":\"\",\"spare3\":\"\",\"spare2\":\"N\\/A\",\"spare1\":\"N\\/A\",\"posacc\":\"9\",\"timemod\":1642,\"region\":\"221\",\"grdatum\":\"UTM\",\"township\":\"\",\"longitude\":\"149.317296100\",\"comment\":\" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011\\/07\\/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\",\"lldatum\":\"WGS84\",\"station\":\"221001\",\"datemod\":20221117,\"timecreate\":0,\"orgcode\":\"DSE\",\"barcode\":\"Bombala\",\"zone\":55,\"elev\":\"429.000\",\"cease\":18991230,\"local_map\":\"CRAIGIE\",\"latitude\":\"-37.138654990\",\"range\":\"\",\"qquarter\":\"Y\",\"easting\":\"705825.000\",\"stntype\":\"VIR\"}}}}\n\n\nI thought it was json? but resp_body_json fails with defaults\n\nrj <- resp %>% resp_body_json(check_type = FALSE)\nrj\n\n$error_num\n[1] 0\n\n$return\n$return$rows\n$return$rows$`221001`\n$return$rows$`221001`$category20\n[1] \"\"\n\n$return$rows$`221001`$category19\n[1] \"\"\n\n$return$rows$`221001`$category18\n[1] \"\"\n\n$return$rows$`221001`$category17\n[1] \"\"\n\n$return$rows$`221001`$category16\n[1] \"\"\n\n$return$rows$`221001`$category15\n[1] \"\"\n\n$return$rows$`221001`$category14\n[1] \"\"\n\n$return$rows$`221001`$category13\n[1] \"\"\n\n$return$rows$`221001`$category12\n[1] \"\"\n\n$return$rows$`221001`$category11\n[1] \"\"\n\n$return$rows$`221001`$category10\n[1] \"N/A\"\n\n$return$rows$`221001`$active\n[1] FALSE\n\n$return$rows$`221001`$northing\n[1] \"5887218.000\"\n\n$return$rows$`221001`$timezone\n[1] \"10.0\"\n\n$return$rows$`221001`$shortname\n[1] \"GENOA R @ ROCKTON\"\n\n$return$rows$`221001`$datecreate\n[1] 18991230\n\n$return$rows$`221001`$elevdatum\n[1] \"\"\n\n$return$rows$`221001`$stname\n[1] \"GENOA RIVER @ ROCKTON\"\n\n$return$rows$`221001`$category9\n[1] \"N/A\"\n\n$return$rows$`221001`$category8\n[1] \"G(S)\"\n\n$return$rows$`221001`$category7\n[1] \"G\"\n\n$return$rows$`221001`$category6\n[1] \"2WD/4WD\"\n\n$return$rows$`221001`$category5\n[1] \"10\"\n\n$return$rows$`221001`$category4\n[1] \"150\"\n\n$return$rows$`221001`$category3\n[1] \"GRAVEL\"\n\n$return$rows$`221001`$category2\n[1] \"V_70D3\"\n\n$return$rows$`221001`$category1\n[1] \"23\"\n\n$return$rows$`221001`$elevacc\n[1] \"1\"\n\n$return$rows$`221001`$dbver47\n[1] FALSE\n\n$return$rows$`221001`$quarter\n[1] \"Y\"\n\n$return$rows$`221001`$section\n[1] 0\n\n$return$rows$`221001`$commence\n[1] 19930526\n\n$return$rows$`221001`$parent\n[1] \"\"\n\n$return$rows$`221001`$mapname\n[1] \"MAFFRA\"\n\n$return$rows$`221001`$meridian\n[1] \"\"\n\n$return$rows$`221001`$spare5\n[1] \"\"\n\n$return$rows$`221001`$spare4\n[1] \"\"\n\n$return$rows$`221001`$spare3\n[1] \"\"\n\n$return$rows$`221001`$spare2\n[1] \"N/A\"\n\n$return$rows$`221001`$spare1\n[1] \"N/A\"\n\n$return$rows$`221001`$posacc\n[1] \"9\"\n\n$return$rows$`221001`$timemod\n[1] 1642\n\n$return$rows$`221001`$region\n[1] \"221\"\n\n$return$rows$`221001`$grdatum\n[1] \"UTM\"\n\n$return$rows$`221001`$township\n[1] \"\"\n\n$return$rows$`221001`$longitude\n[1] \"149.317296100\"\n\n$return$rows$`221001`$comment\n[1] \" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011/07/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\"\n\n$return$rows$`221001`$lldatum\n[1] \"WGS84\"\n\n$return$rows$`221001`$station\n[1] \"221001\"\n\n$return$rows$`221001`$datemod\n[1] 20221117\n\n$return$rows$`221001`$timecreate\n[1] 0\n\n$return$rows$`221001`$orgcode\n[1] \"DSE\"\n\n$return$rows$`221001`$barcode\n[1] \"Bombala\"\n\n$return$rows$`221001`$zone\n[1] 55\n\n$return$rows$`221001`$elev\n[1] \"429.000\"\n\n$return$rows$`221001`$cease\n[1] 18991230\n\n$return$rows$`221001`$local_map\n[1] \"CRAIGIE\"\n\n$return$rows$`221001`$latitude\n[1] \"-37.138654990\"\n\n$return$rows$`221001`$range\n[1] \"\"\n\n$return$rows$`221001`$qquarter\n[1] \"Y\"\n\n$return$rows$`221001`$easting\n[1] \"705825.000\"\n\n$return$rows$`221001`$stntype\n[1] \"VIR\"\n\n\nIf I hack it together to check, first note that the resp_body is raw, as we see in the str of resp_raw and in resp_body_raw\n\nresp %>% resp_raw() %>% str()\n\nHTTP/1.1 200 OK\nDate: Wed, 07 Dec 2022 02:36:05 GMT\nContent-Type: text/html\nContent-Length: 972\nConnection: keep-alive\nContent-Encoding: gzip\nVary: Accept-Encoding\nServer: Microsoft-IIS/10.0\nContent: \nAccess-Control-Allow-Origin: *\n\n{\"error_num\":0,\"return\":{\"rows\":{\"221001\":{\"category20\":\"\",\"category19\":\"\",\"category18\":\"\",\"category17\":\"\",\"category16\":\"\",\"category15\":\"\",\"category14\":\"\",\"category13\":\"\",\"category12\":\"\",\"category11\":\"\",\"category10\":\"N\\/A\",\"active\":false,\"northing\":\"5887218.000\",\"timezone\":\"10.0\",\"shortname\":\"GENOA R @ ROCKTON\",\"datecreate\":18991230,\"elevdatum\":\"\",\"stname\":\"GENOA RIVER @ ROCKTON\",\"category9\":\"N\\/A\",\"category8\":\"G(S)\",\"category7\":\"G\",\"category6\":\"2WD\\/4WD\",\"category5\":\"10\",\"category4\":\"150\",\"category3\":\"GRAVEL\",\"category2\":\"V_70D3\",\"category1\":\"23\",\"elevacc\":\"1\",\"dbver47\":false,\"quarter\":\"Y\",\"section\":0,\"commence\":19930526,\"parent\":\"\",\"mapname\":\"MAFFRA\",\"meridian\":\"\",\"spare5\":\"\",\"spare4\":\"\",\"spare3\":\"\",\"spare2\":\"N\\/A\",\"spare1\":\"N\\/A\",\"posacc\":\"9\",\"timemod\":1642,\"region\":\"221\",\"grdatum\":\"UTM\",\"township\":\"\",\"longitude\":\"149.317296100\",\"comment\":\" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011\\/07\\/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\",\"lldatum\":\"WGS84\",\"station\":\"221001\",\"datemod\":20221117,\"timecreate\":0,\"orgcode\":\"DSE\",\"barcode\":\"Bombala\",\"zone\":55,\"elev\":\"429.000\",\"cease\":18991230,\"local_map\":\"CRAIGIE\",\"latitude\":\"-37.138654990\",\"range\":\"\",\"qquarter\":\"Y\",\"easting\":\"705825.000\",\"stntype\":\"VIR\"}}}}\nList of 5\n $ method     : chr \"POST\"\n $ url        : chr \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\n $ status_code: int 200\n $ headers    :List of 9\n  ..$ Date                       : chr \"Wed, 07 Dec 2022 02:36:05 GMT\"\n  ..$ Content-Type               : chr \"text/html\"\n  ..$ Content-Length             : chr \"972\"\n  ..$ Connection                 : chr \"keep-alive\"\n  ..$ Content-Encoding           : chr \"gzip\"\n  ..$ Vary                       : chr \"Accept-Encoding\"\n  ..$ Server                     : chr \"Microsoft-IIS/10.0\"\n  ..$ Content                    : chr \"\"\n  ..$ Access-Control-Allow-Origin: chr \"*\"\n  ..- attr(*, \"class\")= chr \"httr2_headers\"\n $ body       : raw [1:1460] 7b 22 65 72 ...\n - attr(*, \"class\")= chr \"httr2_response\"\n\n\n\nresp %>% resp_body_raw()\n\n   [1] 7b 22 65 72 72 6f 72 5f 6e 75 6d 22 3a 30 2c 22 72 65 74 75 72 6e 22 3a\n  [25] 7b 22 72 6f 77 73 22 3a 7b 22 32 32 31 30 30 31 22 3a 7b 22 63 61 74 65\n  [49] 67 6f 72 79 32 30 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 39 22 3a\n  [73] 22 22 2c 22 63 61 74 65 67 6f 72 79 31 38 22 3a 22 22 2c 22 63 61 74 65\n  [97] 67 6f 72 79 31 37 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 36 22 3a\n [121] 22 22 2c 22 63 61 74 65 67 6f 72 79 31 35 22 3a 22 22 2c 22 63 61 74 65\n [145] 67 6f 72 79 31 34 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 33 22 3a\n [169] 22 22 2c 22 63 61 74 65 67 6f 72 79 31 32 22 3a 22 22 2c 22 63 61 74 65\n [193] 67 6f 72 79 31 31 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 30 22 3a\n [217] 22 4e 5c 2f 41 22 2c 22 61 63 74 69 76 65 22 3a 66 61 6c 73 65 2c 22 6e\n [241] 6f 72 74 68 69 6e 67 22 3a 22 35 38 38 37 32 31 38 2e 30 30 30 22 2c 22\n [265] 74 69 6d 65 7a 6f 6e 65 22 3a 22 31 30 2e 30 22 2c 22 73 68 6f 72 74 6e\n [289] 61 6d 65 22 3a 22 47 45 4e 4f 41 20 52 20 40 20 52 4f 43 4b 54 4f 4e 22\n [313] 2c 22 64 61 74 65 63 72 65 61 74 65 22 3a 31 38 39 39 31 32 33 30 2c 22\n [337] 65 6c 65 76 64 61 74 75 6d 22 3a 22 22 2c 22 73 74 6e 61 6d 65 22 3a 22\n [361] 47 45 4e 4f 41 20 52 49 56 45 52 20 40 20 52 4f 43 4b 54 4f 4e 22 2c 22\n [385] 63 61 74 65 67 6f 72 79 39 22 3a 22 4e 5c 2f 41 22 2c 22 63 61 74 65 67\n [409] 6f 72 79 38 22 3a 22 47 28 53 29 22 2c 22 63 61 74 65 67 6f 72 79 37 22\n [433] 3a 22 47 22 2c 22 63 61 74 65 67 6f 72 79 36 22 3a 22 32 57 44 5c 2f 34\n [457] 57 44 22 2c 22 63 61 74 65 67 6f 72 79 35 22 3a 22 31 30 22 2c 22 63 61\n [481] 74 65 67 6f 72 79 34 22 3a 22 31 35 30 22 2c 22 63 61 74 65 67 6f 72 79\n [505] 33 22 3a 22 47 52 41 56 45 4c 22 2c 22 63 61 74 65 67 6f 72 79 32 22 3a\n [529] 22 56 5f 37 30 44 33 22 2c 22 63 61 74 65 67 6f 72 79 31 22 3a 22 32 33\n [553] 22 2c 22 65 6c 65 76 61 63 63 22 3a 22 31 22 2c 22 64 62 76 65 72 34 37\n [577] 22 3a 66 61 6c 73 65 2c 22 71 75 61 72 74 65 72 22 3a 22 59 22 2c 22 73\n [601] 65 63 74 69 6f 6e 22 3a 30 2c 22 63 6f 6d 6d 65 6e 63 65 22 3a 31 39 39\n [625] 33 30 35 32 36 2c 22 70 61 72 65 6e 74 22 3a 22 22 2c 22 6d 61 70 6e 61\n [649] 6d 65 22 3a 22 4d 41 46 46 52 41 22 2c 22 6d 65 72 69 64 69 61 6e 22 3a\n [673] 22 22 2c 22 73 70 61 72 65 35 22 3a 22 22 2c 22 73 70 61 72 65 34 22 3a\n [697] 22 22 2c 22 73 70 61 72 65 33 22 3a 22 22 2c 22 73 70 61 72 65 32 22 3a\n [721] 22 4e 5c 2f 41 22 2c 22 73 70 61 72 65 31 22 3a 22 4e 5c 2f 41 22 2c 22\n [745] 70 6f 73 61 63 63 22 3a 22 39 22 2c 22 74 69 6d 65 6d 6f 64 22 3a 31 36\n [769] 34 32 2c 22 72 65 67 69 6f 6e 22 3a 22 32 32 31 22 2c 22 67 72 64 61 74\n [793] 75 6d 22 3a 22 55 54 4d 22 2c 22 74 6f 77 6e 73 68 69 70 22 3a 22 22 2c\n [817] 22 6c 6f 6e 67 69 74 75 64 65 22 3a 22 31 34 39 2e 33 31 37 32 39 36 31\n [841] 30 30 22 2c 22 63 6f 6d 6d 65 6e 74 22 3a 22 20 5c 72 5c 6e 54 75 72 6e\n [865] 20 72 69 67 68 74 20 66 72 6f 6d 20 4d 6f 6e 61 72 6f 20 48 69 67 68 77\n [889] 61 79 20 6f 6e 74 6f 20 49 6d 6c 61 79 20 72 6f 61 64 20 73 69 67 6e 70\n [913] 6f 73 74 65 64 20 38 32 20 6b 6d 20 74 6f 20 45 64 65 6e 2e 20 20 54 72\n [937] 61 76 65 6c 20 6f 76 65 72 20 6c 6f 77 20 6c 65 76 65 6c 20 62 72 69 64\n [961] 67 65 20 20 74 6f 20 61 20 73 68 61 72 70 20 6c 65 66 74 20 62 65 6e 64\n [985] 20 69 6e 20 74 68 65 20 72 6f 61 64 2e 20 20 41 20 70 69 6e 65 20 70 6c\n[1009] 61 6e 74 61 74 69 6f 6e 20 20 77 69 6c 6c 20 62 65 20 64 69 72 65 63 74\n[1033] 6c 79 20 6f 6e 20 74 68 65 20 72 69 67 68 74 2e 20 20 54 75 72 6e 20 64\n[1057] 6f 77 6e 20 69 6e 74 6f 20 70 6c 61 6e 74 61 74 69 6f 6e 20 61 6e 64 20\n[1081] 74 72 61 76 65 6c 20 61 70 70 72 6f 78 2e 20 31 30 30 6d 32 30 31 31 5c\n[1105] 2f 30 37 5c 2f 31 37 20 56 69 72 74 75 61 6c 20 73 69 74 65 20 65 6e 74\n[1129] 72 79 20 67 65 6e 65 72 61 74 65 64 20 62 79 20 44 53 45 56 49 52 54 55\n[1153] 41 4c 53 49 54 45 2e 48 53 43 20 66 72 6f 6d 20 64 65 74 61 69 6c 73 20\n[1177] 69 6e 20 32 32 31 30 30 31 41 5c 72 5c 6e 22 2c 22 6c 6c 64 61 74 75 6d\n[1201] 22 3a 22 57 47 53 38 34 22 2c 22 73 74 61 74 69 6f 6e 22 3a 22 32 32 31\n[1225] 30 30 31 22 2c 22 64 61 74 65 6d 6f 64 22 3a 32 30 32 32 31 31 31 37 2c\n[1249] 22 74 69 6d 65 63 72 65 61 74 65 22 3a 30 2c 22 6f 72 67 63 6f 64 65 22\n[1273] 3a 22 44 53 45 22 2c 22 62 61 72 63 6f 64 65 22 3a 22 42 6f 6d 62 61 6c\n[1297] 61 22 2c 22 7a 6f 6e 65 22 3a 35 35 2c 22 65 6c 65 76 22 3a 22 34 32 39\n[1321] 2e 30 30 30 22 2c 22 63 65 61 73 65 22 3a 31 38 39 39 31 32 33 30 2c 22\n[1345] 6c 6f 63 61 6c 5f 6d 61 70 22 3a 22 43 52 41 49 47 49 45 22 2c 22 6c 61\n[1369] 74 69 74 75 64 65 22 3a 22 2d 33 37 2e 31 33 38 36 35 34 39 39 30 22 2c\n[1393] 22 72 61 6e 67 65 22 3a 22 22 2c 22 71 71 75 61 72 74 65 72 22 3a 22 59\n[1417] 22 2c 22 65 61 73 74 69 6e 67 22 3a 22 37 30 35 38 32 35 2e 30 30 30 22\n[1441] 2c 22 73 74 6e 74 79 70 65 22 3a 22 56 49 52 22 7d 7d 7d 7d\n\n\nSo, if we get the raw, convert to char, then pass to JSON, it looks the same as what I’m getting out of resp_body_json.\n\nresp %>% resp_body_raw() %>% rawToChar() %>% fromJSON()\n\n$error_num\n[1] 0\n\n$return\n$return$rows\n$return$rows$`221001`\n$return$rows$`221001`$category20\n[1] \"\"\n\n$return$rows$`221001`$category19\n[1] \"\"\n\n$return$rows$`221001`$category18\n[1] \"\"\n\n$return$rows$`221001`$category17\n[1] \"\"\n\n$return$rows$`221001`$category16\n[1] \"\"\n\n$return$rows$`221001`$category15\n[1] \"\"\n\n$return$rows$`221001`$category14\n[1] \"\"\n\n$return$rows$`221001`$category13\n[1] \"\"\n\n$return$rows$`221001`$category12\n[1] \"\"\n\n$return$rows$`221001`$category11\n[1] \"\"\n\n$return$rows$`221001`$category10\n[1] \"N/A\"\n\n$return$rows$`221001`$active\n[1] FALSE\n\n$return$rows$`221001`$northing\n[1] \"5887218.000\"\n\n$return$rows$`221001`$timezone\n[1] \"10.0\"\n\n$return$rows$`221001`$shortname\n[1] \"GENOA R @ ROCKTON\"\n\n$return$rows$`221001`$datecreate\n[1] 18991230\n\n$return$rows$`221001`$elevdatum\n[1] \"\"\n\n$return$rows$`221001`$stname\n[1] \"GENOA RIVER @ ROCKTON\"\n\n$return$rows$`221001`$category9\n[1] \"N/A\"\n\n$return$rows$`221001`$category8\n[1] \"G(S)\"\n\n$return$rows$`221001`$category7\n[1] \"G\"\n\n$return$rows$`221001`$category6\n[1] \"2WD/4WD\"\n\n$return$rows$`221001`$category5\n[1] \"10\"\n\n$return$rows$`221001`$category4\n[1] \"150\"\n\n$return$rows$`221001`$category3\n[1] \"GRAVEL\"\n\n$return$rows$`221001`$category2\n[1] \"V_70D3\"\n\n$return$rows$`221001`$category1\n[1] \"23\"\n\n$return$rows$`221001`$elevacc\n[1] \"1\"\n\n$return$rows$`221001`$dbver47\n[1] FALSE\n\n$return$rows$`221001`$quarter\n[1] \"Y\"\n\n$return$rows$`221001`$section\n[1] 0\n\n$return$rows$`221001`$commence\n[1] 19930526\n\n$return$rows$`221001`$parent\n[1] \"\"\n\n$return$rows$`221001`$mapname\n[1] \"MAFFRA\"\n\n$return$rows$`221001`$meridian\n[1] \"\"\n\n$return$rows$`221001`$spare5\n[1] \"\"\n\n$return$rows$`221001`$spare4\n[1] \"\"\n\n$return$rows$`221001`$spare3\n[1] \"\"\n\n$return$rows$`221001`$spare2\n[1] \"N/A\"\n\n$return$rows$`221001`$spare1\n[1] \"N/A\"\n\n$return$rows$`221001`$posacc\n[1] \"9\"\n\n$return$rows$`221001`$timemod\n[1] 1642\n\n$return$rows$`221001`$region\n[1] \"221\"\n\n$return$rows$`221001`$grdatum\n[1] \"UTM\"\n\n$return$rows$`221001`$township\n[1] \"\"\n\n$return$rows$`221001`$longitude\n[1] \"149.317296100\"\n\n$return$rows$`221001`$comment\n[1] \" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011/07/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\"\n\n$return$rows$`221001`$lldatum\n[1] \"WGS84\"\n\n$return$rows$`221001`$station\n[1] \"221001\"\n\n$return$rows$`221001`$datemod\n[1] 20221117\n\n$return$rows$`221001`$timecreate\n[1] 0\n\n$return$rows$`221001`$orgcode\n[1] \"DSE\"\n\n$return$rows$`221001`$barcode\n[1] \"Bombala\"\n\n$return$rows$`221001`$zone\n[1] 55\n\n$return$rows$`221001`$elev\n[1] \"429.000\"\n\n$return$rows$`221001`$cease\n[1] 18991230\n\n$return$rows$`221001`$local_map\n[1] \"CRAIGIE\"\n\n$return$rows$`221001`$latitude\n[1] \"-37.138654990\"\n\n$return$rows$`221001`$range\n[1] \"\"\n\n$return$rows$`221001`$qquarter\n[1] \"Y\"\n\n$return$rows$`221001`$easting\n[1] \"705825.000\"\n\n$return$rows$`221001`$stntype\n[1] \"VIR\"\n\n\nNow can I clean that up? THere’s a lot of nesting but some of it is pointless?\n\nnames(rj)\n\n[1] \"error_num\" \"return\"   \n\n\nand we know error_num is a single int from the str above. And $return is length 1 with only rows and rows only has the gauge number. After that there’s actually some data.\n\nnames(rj$return)\n\n[1] \"rows\"\n\nnames(rj$return$rows)\n\n[1] \"221001\"\n\nnames(rj$return$rows$`221001`)\n\n [1] \"category20\" \"category19\" \"category18\" \"category17\" \"category16\"\n [6] \"category15\" \"category14\" \"category13\" \"category12\" \"category11\"\n[11] \"category10\" \"active\"     \"northing\"   \"timezone\"   \"shortname\" \n[16] \"datecreate\" \"elevdatum\"  \"stname\"     \"category9\"  \"category8\" \n[21] \"category7\"  \"category6\"  \"category5\"  \"category4\"  \"category3\" \n[26] \"category2\"  \"category1\"  \"elevacc\"    \"dbver47\"    \"quarter\"   \n[31] \"section\"    \"commence\"   \"parent\"     \"mapname\"    \"meridian\"  \n[36] \"spare5\"     \"spare4\"     \"spare3\"     \"spare2\"     \"spare1\"    \n[41] \"posacc\"     \"timemod\"    \"region\"     \"grdatum\"    \"township\"  \n[46] \"longitude\"  \"comment\"    \"lldatum\"    \"station\"    \"datemod\"   \n[51] \"timecreate\" \"orgcode\"    \"barcode\"    \"zone\"       \"elev\"      \n[56] \"cease\"      \"local_map\"  \"latitude\"   \"range\"      \"qquarter\"  \n[61] \"easting\"    \"stntype\"   \n\n\nNow, can we put that in a dataframe? Is each one length 1?\n\nactualdata <- rj$return$rows$`221001`\n\nall(purrr::map_int(actualdata, length) == 1)\n\n[1] TRUE\n\n\n\ntibout <- as_tibble(actualdata)\ntibout\n\n# A tibble: 1 × 62\n  category20 category19 catego…¹ categ…² categ…³ categ…⁴ categ…⁵ categ…⁶ categ…⁷\n  <chr>      <chr>      <chr>    <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n1 \"\"         \"\"         \"\"       \"\"      \"\"      \"\"      \"\"      \"\"      \"\"     \n# … with 53 more variables: category11 <chr>, category10 <chr>, active <lgl>,\n#   northing <chr>, timezone <chr>, shortname <chr>, datecreate <int>,\n#   elevdatum <chr>, stname <chr>, category9 <chr>, category8 <chr>,\n#   category7 <chr>, category6 <chr>, category5 <chr>, category4 <chr>,\n#   category3 <chr>, category2 <chr>, category1 <chr>, elevacc <chr>,\n#   dbver47 <lgl>, quarter <chr>, section <int>, commence <int>, parent <chr>,\n#   mapname <chr>, meridian <chr>, spare5 <chr>, spare4 <chr>, spare3 <chr>, …\n\n\nand toss cols with no data\n\ntibout %>% select(where(~!all(. == '')))\n\n# A tibble: 1 × 44\n  catego…¹ active north…² timez…³ short…⁴ datec…⁵ stname categ…⁶ categ…⁷ categ…⁸\n  <chr>    <lgl>  <chr>   <chr>   <chr>     <int> <chr>  <chr>   <chr>   <chr>  \n1 N/A      FALSE  588721… 10.0    GENOA …  1.90e7 GENOA… N/A     G(S)    G      \n# … with 34 more variables: category6 <chr>, category5 <chr>, category4 <chr>,\n#   category3 <chr>, category2 <chr>, category1 <chr>, elevacc <chr>,\n#   dbver47 <lgl>, quarter <chr>, section <int>, commence <int>, mapname <chr>,\n#   spare2 <chr>, spare1 <chr>, posacc <chr>, timemod <int>, region <chr>,\n#   grdatum <chr>, longitude <chr>, comment <chr>, lldatum <chr>,\n#   station <chr>, datemod <int>, timecreate <int>, orgcode <chr>,\n#   barcode <chr>, zone <int>, elev <chr>, cease <int>, local_map <chr>, …\n\n\nCool, I have data and can do stuff with it. NOW, how do I get the data I actually want, vs the data that happens to be in the one demo on the website?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#querying-options",
    "href": "vicwater/vicwater_api_howtocall.html#querying-options",
    "title": "Vicwater api crude testing",
    "section": "Querying options",
    "text": "Querying options\nBefore we go get data, we need to figure out what we can ask for. First, sort out those functions.\n\nDatasources\nLet’s try get_datasources_by_site. Takes site_list params. Dunno what versions it works for? Tried 2, gave error says has to be 1. I assume the “A” datasource means archive here, since that’s what it means in QLD.\n\nds_s_params <- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217\"))\n\n# req <- request(vicurl)\n\nreq %>% \n  req_body_json(ds_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 84\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217\"}}\n\nresp_ds_s <- req %>% \n  req_body_json(ds_s_params) %>% \n  req_perform()\n\nrbody_ds_s <- resp_ds_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 1\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\n\n\nSitelist\nOk, I could make that into a tibble easily enough. It tells me what that site has for datasources, how about another? Can I figure out how to use a sitelist? That’d be really nice, and applies in a lot of places. Cool. I had tried to do \"sitelist\" = c('site', 'site') , and that failed. But it works to have \"site, site\"\n\nds_s_params <- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\"))\n\n# req <- request(vicurl)\n\nreq %>% \n  req_body_json(ds_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 92\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328\"}}\n\nresp_ds_s <- req %>% \n  req_body_json(ds_s_params) %>% \n  req_perform()\n\nrbody_ds_s <- resp_ds_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 2\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405328\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\n\n\nVariables\nNow, for a given site, we want to know what variables are available. (and I also eventually want to know what all possible variables are, and what happens if we ask for variables that aren’t there). Let’s start with the same two sites. I’m\n\nv_s_params <- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\",\n                               \"datasource\" = \"A\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(v_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 103\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328\",\"datasource\":\"A\"}}\n\nresp_v_s <- req %>% \n  req_body_json(v_s_params) %>% \n  req_perform()\n\nrbody_v_s <- resp_v_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 2\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ variables   :List of 6\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"19610306171500\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"210.00\"\n  .. .. .. .. ..$ units       : chr \"pH\"\n  .. .. .. .. ..$ name        : chr \"Acidity/Alkalinity (pH)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"215.00\"\n  .. .. .. .. ..$ units       : chr \"ppm\"\n  .. .. .. .. ..$ name        : chr \"Dissolved Oxygen (ppm)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"233217\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221111094500\"\n  .. .. .. .. ..$ period_start: chr \"20091119170800\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. ..$ site        : chr \"405328\"\n\n\nSo, that gives me the number and name of each variable at each site. But it does not give derived variables (discharge being the main one)."
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#location-etc",
    "href": "vicwater/vicwater_api_howtocall.html#location-etc",
    "title": "Vicwater api crude testing",
    "section": "Location etc",
    "text": "Location etc\nSo, get_db_info seems like it should be useful, but kind of isn’t (see above). Maybe I’ll come back to that. It does let us do geofilters, but they seem both crude and complex. I think I’d probably rather do geofiltering myself and then turn that into a site_list. But might come back to this. The complex_filter might be useful if we can use it to choose sites based on something. But again, I’d probably do that myself? Again, come back to this maybe?\n\nCan we get a list of all sites and all variables?\nMaybe? Do we want to?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#getting-timeseries",
    "href": "vicwater/vicwater_api_howtocall.html#getting-timeseries",
    "title": "Vicwater api crude testing",
    "section": "Getting timeseries",
    "text": "Getting timeseries\nNow, let’s go back to get timeseries, now we know what the variables are. Just for the Barwon at first, and way fewer days. There’s a var_list option, or varfrom and varto. It’s unclear whether the from and to version is numerivally inclusive- ie if we have 100 and 10000, does it get everything? I’ll try with varto = 820, since that’s the highest number avail at the barwon. Gives cryptic error. Try 210? Also, why isn’t 141 available in teh lsit above? Again, cryptic “Assumed fail to reload varcon for 233217: 100.00-> 210.00, as we failed loading it last time”. does it work for 141? Yes.\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"100\",\n                               \"interval\" = \"day\",\n                               \"varto\" = \"141\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"100\",\"interval\":\"day\",\"varto\":\"141\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 150: chr \"Rating extrapolated above 1.5x maximum flow gauged.\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"15.93\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"14.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"11.23\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.81\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.75\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.09\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"4.13\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"2.48\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.88\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"12.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"9.87\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.42\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.45\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"9.74\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Discharge (Ml/d)\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"141.00\"\n  .. .. .. ..$ units     : chr \"megalitres/day\"\n  .. .. .. ..$ name      : chr \"Stream Discharge (Ml/d)\"\n\n\nDo the other vars work if we ask for them separately? Try pH (which failed above)\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"210\",\n                               \"interval\" = \"day\",\n                               \"varto\" = \"210\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"210\",\"interval\":\"day\",\"varto\":\"210\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nInteresting. How about a var_list?\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nThat works, seems to set the varfrom and varto to each value in the list. I wonder if things like 141 are done in the from/to way because they are derived from 100. But how do we find them when they don’t appear in the get_variable_list? Can I include them in var_list? Hmm. No. what’s going on? see table 3 in qld doc- they are derived, and it gives numbers. Is there a get_available_varcons or similar?\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,141,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\",\n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 227\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,141,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nHow about asking for variables that don’t exist- ie can we just ask for all of them, and it just gives us whatever’s available? The other site (Steavenson, 405328) only has variable 100, so ask for some others. Just gives 100.\n\nsparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"405328\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(sparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"405328\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nresps <- req %>% \n  req_body_json(sparams) %>% \n  req_perform()\n\nrbodys <- resps %>% resp_body_json(check_type = FALSE)\n\nstr(rbodys)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ longitude : chr \"145.773503100\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. .. ..$ latitude  : chr \"-37.525797590\"\n  .. .. .. ..$ org_name  : chr \"Victorian Rural Water Corporation\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.738\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.736\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.734\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.755\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.739\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.735\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.759\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.742\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.757\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"405328\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\n\nCan we get derived (141, etc) with varlist\nIt looks like it’s really just 140 (cumecs) and 141 (ML/d) we’d want, if Vic matches QLD. There are a couple other varcons, but they’re about groundwater.\nLet’s see what get_varcon gives us. I can’t get this not to error, and the examples online have square brackets mixed in the json. I think some combo of c and list might do it but not worth it.\n\nvc_params <- list(\"function\" = 'get_varcon',\n               \"version\" = \"2\",\n               \"params\" = list(\"varcons\" = list(\"varfrom\" = \"100\",\n                               \"varto\" = \"141\",\n                               \"site_list\" = \"233217, 405328\",\n                               \"datasource\" = \"A\",\n                               \"requests\" = list(\"qf1\" = \"1\", \n                                                 \"t1\" = \"20200101000000\",\n                                                 \"t2\" = \"20200131000000\"))))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(vc_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 205\n\n{\"function\":\"get_varcon\",\"version\":\"2\",\"params\":{\"varcons\":{\"varfrom\":\"100\",\"varto\":\"141\",\"site_list\":\"233217, 405328\",\"datasource\":\"A\",\"requests\":{\"qf1\":\"1\",\"t1\":\"20200101000000\",\"t2\":\"20200131000000\"}}}}\n\nresp_vc <- req %>% \n  req_body_json(vc_params) %>% \n  req_perform()\n\nrbody_vc <- resp_vc %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_vc)\n\nList of 3\n $ error_num: int 124\n $ error_msg: chr \"Access violation at address 005203ED in module 'webservice.exe'. Read of address 00000008\"\n $ return   :List of 1\n  ..$ varcons: list()\n\n\nThis isn’t worth it. If we ask for 141 or 140, just do another round with varfrom and varto. Or always get 100, 140, 141, then only sometimes get the others if asked?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#geolocation",
    "href": "vicwater/vicwater_api_howtocall.html#geolocation",
    "title": "Vicwater api crude testing",
    "section": "Geolocation",
    "text": "Geolocation\nSo, get_db_info seems to have a way to get sites by radius or bounding box.. And the flipside is we might want to get sites within a polygon, and so need their locations, which should be available as geoJSON. Try to figure both those out.\nI think get_site_geojson is going to be simpler. start there. Not sure why the site_list can’t have a c(), but the fields has to use it. Works though, gives a feature list. I think those are readable by sf, so that’s good. Not sure what the fields even are though. The help says “Any field that is part of the site table”. So I guess we need to sort that out. On to get_db_info.\n\ng_j_params <- list(\"function\" = 'get_site_geojson',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\",\n                               \"get_elev\" = \"1\",\n                               \"fields\" = c(\"zone\",\"region\")))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(g_j_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 127\n\n{\"function\":\"get_site_geojson\",\"version\":\"2\",\"params\":{\"site_list\":\"233217, 405328\",\"get_elev\":\"1\",\"fields\":[\"zone\",\"region\"]}}\n\nresp_g_j <- req %>% \n  req_body_json(g_j_params) %>% \n  req_perform()\n\nrbody_g_j <- resp_g_j %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_g_j)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 2\n  ..$ features:List of 2\n  .. ..$ :List of 4\n  .. .. ..$ properties:List of 2\n  .. .. .. ..$ region: chr \"233\"\n  .. .. .. ..$ zone  : int 55\n  .. .. ..$ geometry  :List of 2\n  .. .. .. ..$ coordinates:List of 3\n  .. .. .. .. ..$ : num 144\n  .. .. .. .. ..$ : num -38.2\n  .. .. .. .. ..$ : int 0\n  .. .. .. ..$ type       : chr \"Point\"\n  .. .. ..$ id        : chr \"233217\"\n  .. .. ..$ type      : chr \"Feature\"\n  .. ..$ :List of 4\n  .. .. ..$ properties:List of 2\n  .. .. .. ..$ region: chr \"405\"\n  .. .. .. ..$ zone  : int 55\n  .. .. ..$ geometry  :List of 2\n  .. .. .. ..$ coordinates:List of 3\n  .. .. .. .. ..$ : num 146\n  .. .. .. .. ..$ : num -37.5\n  .. .. .. .. ..$ : int 0\n  .. .. .. ..$ type       : chr \"Point\"\n  .. .. ..$ id        : chr \"405328\"\n  .. .. ..$ type      : chr \"Feature\"\n  ..$ type    : chr \"FeatureCollection\""
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#db-info",
    "href": "vicwater/vicwater_api_howtocall.html#db-info",
    "title": "Vicwater api crude testing",
    "section": "DB info",
    "text": "DB info\nI was using get_db_info to test above, so let’s just go back to that as a start and think a bit more about what we want. look at the barown. Cannot feed it a list of sites for fitler_values. It does give lat/long/northing, etc, so could use this instead of geoJSON, but geoJSON probably better if we want geo. Using return_type = hash is not noticably different than return_type = array. All examples use hash, so I guess keep using that moving forward. I think we can filter on lots of things in this list, both here and in the geojson.\n\ndbparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"station\" = \"233217\")))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(dbparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 130\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"station\":\"233217\"}}}\n\nrespdb <- req %>% \n  req_body_json(dbparams) %>% \n  req_perform()\n\nrbodydb <- respdb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodydb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 1\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5772691.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON @ GEELONG\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"0\"\n  .. .. ..$ category4 : chr \"150\"\n  .. .. ..$ category3 : chr \"SEALED\"\n  .. .. ..$ category2 : chr \"V_93G4\"\n  .. .. ..$ category1 : chr \"0\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19601118\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"BW\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. ..$ comment   : chr \"\\r\\n\\r\\n\\r\\nBarwon Water flood monitoring stationFrom the intersection of the Fyans St and La Trobe Terrace, he\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233217\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Geelong\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"267562.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n\n\n\nGeofiltering the db to select sites\nOK, so there are lots of ways to filter (sitename, date, name, etc). Some of those like Name or region or active might be useful, but for now let’s try the geo filters (boudning box and radius).\nTry circle (lat, long, radius in degrees)- use the Barwon lat/long.\nKeeps crashing with timeouts. is it just too much to ask for? Or is the json not right?\n\n# dbparams <- list(\"function\" = 'get_db_info',\n#                \"version\" = \"3\",\n#                \"params\" = list(\"table_name\" = \"site\",\n#                                \"return_type\" = \"hash\",\n#                                \"geo_filter\" = list(\"circle\" = c(\"-38.16\", \"144.35\", \"0.25\"))))\n# \n# req <- request(vicurl)\n# \n# req %>% \n#   req_body_json(dbparams) %>% \n#   req_dry_run()\n# \n# respdb <- req %>% \n#   req_body_json(dbparams) %>% \n#   req_perform()\n# \n# rbodydb <- respdb %>% resp_body_json(check_type = FALSE)\n# \n# str(rbodydb)\n\nLet’s try one of the other geofilters. Otherwise this will work better to write my own if I can et the geojson of all the sites.\nUgh. the rectangle (and region) need nested square brackets. I can make one with c(),\n\ndbparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"hash\",\n                               \"geo_filter\" = list(\"rectangle\" = \n                                                     c(c(\"-38.126\", \"144.282\"),\n                                                       c(\"-38.223\", \"144.406\")))))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(dbparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 161\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"geo_filter\":{\"rectangle\":[\"-38.126\",\"144.282\",\"-38.223\",\"144.406\"]}}}\n\n# \n# respdb <- req %>% \n#   req_body_json(dbparams) %>% \n#   req_perform()\n# \n# rbodydb <- respdb %>% resp_body_json(check_type = FALSE)\n# \n# str(rbodydb)\n\nthe locations of all guages it’d be\nOK, generating the json for these geo selections is horrible. If I can pull faster to do it myself.\nCan I get a complete gaugelist, nad then pull geojson?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#get-all-gauges",
    "href": "vicwater/vicwater_api_howtocall.html#get-all-gauges",
    "title": "Vicwater api crude testing",
    "section": "Get all gauges",
    "text": "Get all gauges\nWhat’s the best way? with get_db_info? With get_sites_by_datasource? The latter would assume we know all datasources. We probably do just want those in ‘A’ but not positive.\nSo, how about db_info, but maybe not all columns? Tempted to get lat/long or easting/northing.\nTakes a while, but it does run. 189,464 sites??? Yikes. WHY? Clearly i need to filter on something.\n\ndbparams <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"field_list\" = c(\"station\", \"stname\", \"shortname\")))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(dbparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 139\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"field_list\":[\"station\",\"stname\",\"shortname\"]}}\n\nrespdb <- req %>% \n  req_body_json(dbparams) %>% \n  req_perform()\n\nrbodydb <- respdb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodydb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 189491\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"0\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"044079\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"044387\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045407\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045621\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045652\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045717\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"092825\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"097421\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"097962\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100000\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100001\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100002\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100003\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100004\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100005\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100006\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100007\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100008\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100009\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100010\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100011\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100012\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100013\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100014\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100015\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100016\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100017\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100018\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100019\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100020\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100021\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100022\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100023\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100024\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100025\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100026\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100028\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100029\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100030\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100031\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100032\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100033\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100034\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100035\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100036\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100037\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100038\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100039\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100040\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100041\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100042\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100043\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100044\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100045\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100046\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100047\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100048\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100049\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100050\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100051\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100054\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100055\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100056\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100057\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100058\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100059\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100060\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100061\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100062\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100063\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100064\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100065\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100066\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100067\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100068\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100069\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100070\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100071\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100072\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100073\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100074\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100075\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100076\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100077\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100078\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100079\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100080\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100081\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100082\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100083\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100084\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100085\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100086\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100087\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100088\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100089\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100090\"\n  .. .. [list output truncated]\n\n\nwhat are the variables at some of those sites? Can we figure out what’s up that way? I have a feeling some are groundewater, but there’s no obvious field for that.\n\nv_s_params <- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"100089, 100079\",\n                               \"datasource\" = \"A\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(v_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 103\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"100089, 100079\",\"datasource\":\"A\"}}\n\nresp_v_s <- req %>% \n  req_body_json(v_s_params) %>% \n  req_perform()\n\nrbody_v_s <- resp_v_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 2\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"\"\n  .. .. .. ..$ name      : chr \"\"\n  .. .. ..$ variables   : list()\n  .. .. ..$ site        : chr \"100089\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"\"\n  .. .. .. ..$ name      : chr \"\"\n  .. .. ..$ variables   : list()\n  .. .. ..$ site        : chr \"100079\"\n\n\nUhhh, those have no variables? WHat’s going on here?"
  },
  {
    "objectID": "vicwater/vicwater_testing.html",
    "href": "vicwater/vicwater_testing.html",
    "title": "Testing VicWater API",
    "section": "",
    "text": "# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#access-victoria-water-data-through-api",
    "href": "vicwater/vicwater_testing.html#access-victoria-water-data-through-api",
    "title": "Testing VicWater API",
    "section": "Access Victoria water data through API",
    "text": "Access Victoria water data through API\nThis document is my testing and development of functions to include in the {vicwater} package. Basically, it’s where I interactively sorted through how to hit the API functions, the formats of the lists, and how to unpack the returned lists. It is a work in progress, since that package is under development.\nWe want to access victorian water data for a set of sites. That requires using the api at https://data.water.vic.gov.au/cgi/webservice.exe?[JSON_request] , but it’s poorly documented. I think I got it mostly figured out in a testing document, but there’s a lot of extra testing in there that needs to be skipped over and cleaned up. My plan is to make this a package, but it needs more development. I’m moving further testing here so I can get to the point a bit quicker.\nLibraries. Do I still need jsonlite now that I’ve moved ot httr2?\n\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.2.2\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.2.2\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#set-up-params",
    "href": "vicwater/vicwater_testing.html#set-up-params",
    "title": "Testing VicWater API",
    "section": "Set up params",
    "text": "Set up params\n\nURL\nThe base url that everything gets attached to is: and we use httr2::request to start building the request.\n\nvicurl <- \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\nreqvic <- request(vicurl)\n\n\n\nSite lists\nI want to test with one, two, and several sites in a site list. I had tried to do \"sitelist\" = c('site', 'site') , and that failed. But it works to have \"site, site\"\nThe upper steavenson is 405328, Barwon is 233217 (and has Temp), Taggerty 405331 only ran 2010-2013, And the Marysville golf course 405837 (only rainfall). That hits some things we want to make sure we pick up- no longer running gauges, gauges with only rain, gauges with lots of variables, etc.\nI only make one site_list here with multiple, but can do the str_c inside the calls usually.\n\nbarwon <- '233217'\nsteavenson <- '405328'\ntaggerty <- '405331'\ngolf <- '405837'\n\nallsites <- str_c(barwon, steavenson, taggerty, golf, sep = \", \")"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#api-functions",
    "href": "vicwater/vicwater_testing.html#api-functions",
    "title": "Testing VicWater API",
    "section": "API functions",
    "text": "API functions\nAnd how to call each- including multiple values.\nI finally found a couple sources of documentation that will hopefully be helpful: https://kisters.com.au/doco/hydllp.htm and https://water-monitoring.information.qld.gov.au/wini/Documents/RDMW_API_doco.pdf.\nThe first thing to do is to figure out what basic information is there, so we can ask for it. What we really want is get_ts_traces, but it has a lot of parameters (see Kisters docs). Some are relatively straightforward to meaning, though how to get them to be correct JSON can be tricky (e.g. site_list, while others are opaque, e.g. varfrom, varto, datasource, either to their meaning or what the options are we can ask for. We can try to figure that out with some querying of the other functions.\n\nDatasources\nCan we figure out what datasource means by asking for some by site?\nversion has to be 1.\n\nds_s_params <- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites))\n\nreqvic %>% \n  req_body_json(ds_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 108\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\"}}\n\nresp_ds_s <- reqvic %>% \n  req_body_json(ds_s_params) %>% \n  req_perform()\n\nrbody_ds_s <- resp_ds_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 4\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405328\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405331\"\n  .. .. ..$ datasources:List of 2\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405837\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\nI’ll need to sort out how to unpack that list later, but for now, let’s just look at it and see that all of them have options A and TELEM, and a couple have TELEMCOPY.\nAccording to the QLD pdf, the datasource distinguishes things like Archive and Telemetry. That’s similar in Vic, though QLD also had codes for back-filled holes, which don’t seem to be here (at least at these sites).\nSeems like it will be safest to ask for ‘A’ or ‘TELEM’.\nAnd the different variables can be in a var_list or varto and varfrom (though not always- see below). The numbers are for different variables, but again, no guarantee they’re the same in Vic.\n\nunpacking the list\nI might as well do this and build the function. Should be able to do it with unnest, and then maybe drop dumb columns? Nope, some of the lists unpack into lists of mixed type. But unnest_wider and unnest_longer might be the trick. Will need to test with single sites in case the structure changes.\nFor the function, we probably want to just print the error value or something, and not return it in the df. Or if it errors, return that, if it doesn’t, just give df. That’s probably best.\n\na <- as_tibble(rbody_ds_s[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # error and a `return` list\n  unnest_wider(col = where(is.list)) %>% # error, site, and a `datasources` list\n  unnest_longer(col = where(is.list)) # fully unpacked into a long df\na\n\n# A tibble: 11 × 2\n   site   datasources\n   <chr>  <chr>      \n 1 233217 A          \n 2 233217 TELEM      \n 3 233217 TELEMCOPY  \n 4 405328 A          \n 5 405328 TELEM      \n 6 405328 TELEMCOPY  \n 7 405331 A          \n 8 405331 TELEM      \n 9 405837 A          \n10 405837 TELEM      \n11 405837 TELEMCOPY  \n\n\nMight actually be better as a table or pivot_wider? Depends what the point is? Pivot wider is kind of a pain, use table? But table actually unpacks longer when I as_tibble or as.data.frame it. Which is annoying.\n\nb <- table(a$site, a$datasources)\nb\n\n        \n         A TELEM TELEMCOPY\n  233217 1     1         1\n  405328 1     1         1\n  405331 1     1         0\n  405837 1     1         1\n\n\nI think just return the long tibble, and do the table as a plot or explicitly a table or something. Which orientation makes most sense? Not sure. Probably gauges on x? But plots of actual data will have gauges on y, time on x, so maybe stay consistent.\n\nc <- b %>% \n  as_tibble(.name_repair = 'unique') %>% \n  rename(gauge = `...1`, datasource = `...2`)\n\nNew names:\n• `` -> `...1`\n• `` -> `...2`\n\nc %>% \n  mutate(n = as.logical(n)) %>% \nggplot2::ggplot(ggplot2::aes(x = datasource, y = gauge, fill = n)) + \n  ggplot2::geom_tile(colour=\"white\", size=0.25) +\n  ggplot2::scale_fill_discrete(type = c('firebrick', 'dodgerblue')) +\n  ggplot2::labs(fill = NULL) +\n  ggplot2::coord_equal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nCan I just build that plot from a? Yes, but would be just as annoying. maybe.\n\nallopts <- a %>%\n  expand(site, datasources)\n\na2 <- a %>% \n  mutate(indata = TRUE) %>% \n  dplyr::right_join(allopts) %>% \n  mutate(indata = ifelse(is.na(indata), FALSE, indata))\n\nJoining, by = c(\"site\", \"datasources\")\n\n  ggplot(a2, aes(x = datasources, y = site, fill = indata)) + \n  ggplot2::geom_tile(colour=\"white\", linewidth=0.25) +\n  ggplot2::scale_fill_discrete(type = c('firebrick', 'dodgerblue')) +\n  ggplot2::labs(fill = NULL) +\n  ggplot2::coord_equal()\n\n\n\n\nGood enough for this one- turn that into a function in a package.\n\n\n\nTest from the package\nI’m using devtools::load_all() to load the package repo in here for interactive testing and poking.\nObviously, hard paths are a bad idea, but I’m going to do it here since I typically do relative within repos, and these are across repos. Still, temporary and hacky.\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\n\n\nreturntib <- get_datasources_by_site(vicurl, allsites)\n\n\nreturntib\n\n# A tibble: 11 × 2\n   site   datasource\n   <chr>  <chr>     \n 1 233217 A         \n 2 233217 TELEM     \n 3 233217 TELEMCOPY \n 4 405328 A         \n 5 405328 TELEM     \n 6 405328 TELEMCOPY \n 7 405331 A         \n 8 405331 TELEM     \n 9 405837 A         \n10 405837 TELEM     \n11 405837 TELEMCOPY \n\nplot_datasources_by_site(returntib)\n\nJoining, by = c(\"site\", \"datasource\")\n\n\n\n\n\n\n\nSites by datasource\nHaven’t written this one before, might blow things up. But if we want a list of sites, it might be better than the way I did this before of just asking for everything in the db, lots of which had no data.\nAnd now we know the datasource options. I think? I suppose it’s possible there’s another type I’m not aware of.\nFor some weird reason, sitelists should be \"site, site, site\", while the datasources should be c('source', 'source'). The latter yields JSON array ['source', 'source'], while the former yields JSON 'site', 'site'\nThis works. The list truncates, but it did work.\n\nds_wanted <- c('A', 'TELEM')\ns_ds_params <- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = ds_wanted))\n\nreqvic %>% \n  req_body_json(s_ds_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}\n\nresp_s_ds <- reqvic %>% \n  req_body_json(s_ds_params) %>% \n  req_perform()\n\nrbody_s_ds <- resp_s_ds %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_s_ds)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ datasources:List of 2\n  .. ..$ :List of 2\n  .. .. ..$ datasource: chr \"A\"\n  .. .. ..$ sites     :List of 4044\n  .. .. .. ..$ : chr \"100023\"\n  .. .. .. ..$ : chr \"100500\"\n  .. .. .. ..$ : chr \"100503\"\n  .. .. .. ..$ : chr \"100504\"\n  .. .. .. ..$ : chr \"101708\"\n  .. .. .. ..$ : chr \"102621\"\n  .. .. .. ..$ : chr \"102827\"\n  .. .. .. ..$ : chr \"102828\"\n  .. .. .. ..$ : chr \"102829\"\n  .. .. .. ..$ : chr \"102830\"\n  .. .. .. ..$ : chr \"102831\"\n  .. .. .. ..$ : chr \"103811\"\n  .. .. .. ..$ : chr \"104929\"\n  .. .. .. ..$ : chr \"104930\"\n  .. .. .. ..$ : chr \"105134\"\n  .. .. .. ..$ : chr \"105222\"\n  .. .. .. ..$ : chr \"105287\"\n  .. .. .. ..$ : chr \"105317\"\n  .. .. .. ..$ : chr \"105480\"\n  .. .. .. ..$ : chr \"105484\"\n  .. .. .. ..$ : chr \"105936\"\n  .. .. .. ..$ : chr \"107631\"\n  .. .. .. ..$ : chr \"107970\"\n  .. .. .. ..$ : chr \"107971\"\n  .. .. .. ..$ : chr \"108201\"\n  .. .. .. ..$ : chr \"108203\"\n  .. .. .. ..$ : chr \"108319\"\n  .. .. .. ..$ : chr \"108320\"\n  .. .. .. ..$ : chr \"108321\"\n  .. .. .. ..$ : chr \"108898\"\n  .. .. .. ..$ : chr \"108899\"\n  .. .. .. ..$ : chr \"108917\"\n  .. .. .. ..$ : chr \"108944\"\n  .. .. .. ..$ : chr \"109133\"\n  .. .. .. ..$ : chr \"109461\"\n  .. .. .. ..$ : chr \"109462\"\n  .. .. .. ..$ : chr \"109644\"\n  .. .. .. ..$ : chr \"109645\"\n  .. .. .. ..$ : chr \"109769\"\n  .. .. .. ..$ : chr \"109770\"\n  .. .. .. ..$ : chr \"109778\"\n  .. .. .. ..$ : chr \"109779\"\n  .. .. .. ..$ : chr \"110151\"\n  .. .. .. ..$ : chr \"110152\"\n  .. .. .. ..$ : chr \"110153\"\n  .. .. .. ..$ : chr \"110171\"\n  .. .. .. ..$ : chr \"110186\"\n  .. .. .. ..$ : chr \"110464\"\n  .. .. .. ..$ : chr \"110721\"\n  .. .. .. ..$ : chr \"110724\"\n  .. .. .. ..$ : chr \"110731\"\n  .. .. .. ..$ : chr \"110739\"\n  .. .. .. ..$ : chr \"110745\"\n  .. .. .. ..$ : chr \"110943\"\n  .. .. .. ..$ : chr \"110978\"\n  .. .. .. ..$ : chr \"111543\"\n  .. .. .. ..$ : chr \"111549\"\n  .. .. .. ..$ : chr \"111551\"\n  .. .. .. ..$ : chr \"111691\"\n  .. .. .. ..$ : chr \"111692\"\n  .. .. .. ..$ : chr \"112182\"\n  .. .. .. ..$ : chr \"112235\"\n  .. .. .. ..$ : chr \"112236\"\n  .. .. .. ..$ : chr \"112237\"\n  .. .. .. ..$ : chr \"112459\"\n  .. .. .. ..$ : chr \"112708\"\n  .. .. .. ..$ : chr \"112803\"\n  .. .. .. ..$ : chr \"112804\"\n  .. .. .. ..$ : chr \"113004\"\n  .. .. .. ..$ : chr \"113124\"\n  .. .. .. ..$ : chr \"113125\"\n  .. .. .. ..$ : chr \"113467\"\n  .. .. .. ..$ : chr \"113694\"\n  .. .. .. ..$ : chr \"113695\"\n  .. .. .. ..$ : chr \"113705\"\n  .. .. .. ..$ : chr \"113706\"\n  .. .. .. ..$ : chr \"114158\"\n  .. .. .. ..$ : chr \"114169\"\n  .. .. .. ..$ : chr \"115732\"\n  .. .. .. ..$ : chr \"115872\"\n  .. .. .. ..$ : chr \"116382\"\n  .. .. .. ..$ : chr \"116459\"\n  .. .. .. ..$ : chr \"116460\"\n  .. .. .. ..$ : chr \"116802\"\n  .. .. .. ..$ : chr \"116803\"\n  .. .. .. ..$ : chr \"119329\"\n  .. .. .. ..$ : chr \"119330\"\n  .. .. .. ..$ : chr \"119337\"\n  .. .. .. ..$ : chr \"119338\"\n  .. .. .. ..$ : chr \"119339\"\n  .. .. .. ..$ : chr \"119340\"\n  .. .. .. ..$ : chr \"119341\"\n  .. .. .. ..$ : chr \"119342\"\n  .. .. .. ..$ : chr \"119347\"\n  .. .. .. ..$ : chr \"119348\"\n  .. .. .. ..$ : chr \"119366\"\n  .. .. .. ..$ : chr \"119367\"\n  .. .. .. ..$ : chr \"119377\"\n  .. .. .. ..$ : chr \"120248\"\n  .. .. .. .. [list output truncated]\n  .. ..$ :List of 2\n  .. .. ..$ datasource: chr \"TELEM\"\n  .. .. ..$ sites     :List of 2093\n  .. .. .. ..$ : chr \"100023\"\n  .. .. .. ..$ : chr \"100500\"\n  .. .. .. ..$ : chr \"100503\"\n  .. .. .. ..$ : chr \"100504\"\n  .. .. .. ..$ : chr \"100731\"\n  .. .. .. ..$ : chr \"101708\"\n  .. .. .. ..$ : chr \"102621\"\n  .. .. .. ..$ : chr \"102827\"\n  .. .. .. ..$ : chr \"102828\"\n  .. .. .. ..$ : chr \"102829\"\n  .. .. .. ..$ : chr \"102830\"\n  .. .. .. ..$ : chr \"102831\"\n  .. .. .. ..$ : chr \"103811\"\n  .. .. .. ..$ : chr \"104929\"\n  .. .. .. ..$ : chr \"104930\"\n  .. .. .. ..$ : chr \"105134\"\n  .. .. .. ..$ : chr \"105222\"\n  .. .. .. ..$ : chr \"105484\"\n  .. .. .. ..$ : chr \"105936\"\n  .. .. .. ..$ : chr \"107631\"\n  .. .. .. ..$ : chr \"107970\"\n  .. .. .. ..$ : chr \"108201\"\n  .. .. .. ..$ : chr \"108203\"\n  .. .. .. ..$ : chr \"108319\"\n  .. .. .. ..$ : chr \"108320\"\n  .. .. .. ..$ : chr \"108321\"\n  .. .. .. ..$ : chr \"108944\"\n  .. .. .. ..$ : chr \"109133\"\n  .. .. .. ..$ : chr \"109462\"\n  .. .. .. ..$ : chr \"109644\"\n  .. .. .. ..$ : chr \"109645\"\n  .. .. .. ..$ : chr \"109769\"\n  .. .. .. ..$ : chr \"109770\"\n  .. .. .. ..$ : chr \"110151\"\n  .. .. .. ..$ : chr \"110152\"\n  .. .. .. ..$ : chr \"110153\"\n  .. .. .. ..$ : chr \"110171\"\n  .. .. .. ..$ : chr \"110186\"\n  .. .. .. ..$ : chr \"110464\"\n  .. .. .. ..$ : chr \"110721\"\n  .. .. .. ..$ : chr \"110724\"\n  .. .. .. ..$ : chr \"110731\"\n  .. .. .. ..$ : chr \"110739\"\n  .. .. .. ..$ : chr \"110745\"\n  .. .. .. ..$ : chr \"110943\"\n  .. .. .. ..$ : chr \"110978\"\n  .. .. .. ..$ : chr \"111543\"\n  .. .. .. ..$ : chr \"111549\"\n  .. .. .. ..$ : chr \"111551\"\n  .. .. .. ..$ : chr \"111691\"\n  .. .. .. ..$ : chr \"111692\"\n  .. .. .. ..$ : chr \"112182\"\n  .. .. .. ..$ : chr \"112185\"\n  .. .. .. ..$ : chr \"112235\"\n  .. .. .. ..$ : chr \"112236\"\n  .. .. .. ..$ : chr \"112237\"\n  .. .. .. ..$ : chr \"112459\"\n  .. .. .. ..$ : chr \"112708\"\n  .. .. .. ..$ : chr \"112803\"\n  .. .. .. ..$ : chr \"112804\"\n  .. .. .. ..$ : chr \"113004\"\n  .. .. .. ..$ : chr \"113124\"\n  .. .. .. ..$ : chr \"113125\"\n  .. .. .. ..$ : chr \"113467\"\n  .. .. .. ..$ : chr \"113694\"\n  .. .. .. ..$ : chr \"113695\"\n  .. .. .. ..$ : chr \"113705\"\n  .. .. .. ..$ : chr \"113706\"\n  .. .. .. ..$ : chr \"114158\"\n  .. .. .. ..$ : chr \"114169\"\n  .. .. .. ..$ : chr \"114975\"\n  .. .. .. ..$ : chr \"115732\"\n  .. .. .. ..$ : chr \"116382\"\n  .. .. .. ..$ : chr \"116802\"\n  .. .. .. ..$ : chr \"116803\"\n  .. .. .. ..$ : chr \"119329\"\n  .. .. .. ..$ : chr \"119337\"\n  .. .. .. ..$ : chr \"119340\"\n  .. .. .. ..$ : chr \"119341\"\n  .. .. .. ..$ : chr \"119342\"\n  .. .. .. ..$ : chr \"119347\"\n  .. .. .. ..$ : chr \"119348\"\n  .. .. .. ..$ : chr \"119366\"\n  .. .. .. ..$ : chr \"119367\"\n  .. .. .. ..$ : chr \"119377\"\n  .. .. .. ..$ : chr \"120248\"\n  .. .. .. ..$ : chr \"121019\"\n  .. .. .. ..$ : chr \"122152\"\n  .. .. .. ..$ : chr \"123140\"\n  .. .. .. ..$ : chr \"126975\"\n  .. .. .. ..$ : chr \"129744\"\n  .. .. .. ..$ : chr \"129746\"\n  .. .. .. ..$ : chr \"134949\"\n  .. .. .. ..$ : chr \"137194\"\n  .. .. .. ..$ : chr \"137195\"\n  .. .. .. ..$ : chr \"137197\"\n  .. .. .. ..$ : chr \"137198\"\n  .. .. .. ..$ : chr \"137199\"\n  .. .. .. ..$ : chr \"137200\"\n  .. .. .. .. [list output truncated]\n\n\nCan I tibble that up?\n\ns <- as_tibble(rbody_s_ds[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # sites, and a `datasource` list\n  unnest_longer(col = where(is.list)) # fully unpacked into a long df\ns\n\n# A tibble: 6,137 × 2\n   datasource sites \n   <chr>      <chr> \n 1 A          100023\n 2 A          100500\n 3 A          100503\n 4 A          100504\n 5 A          101708\n 6 A          102621\n 7 A          102827\n 8 A          102828\n 9 A          102829\n10 A          102830\n# … with 6,127 more rows\n\n\nWhat are the number of sites in each?\n\ns %>% group_by(datasource) %>% summarise(n = n())\n\n# A tibble: 2 × 2\n  datasource     n\n  <chr>      <int>\n1 A           4044\n2 TELEM       2093\n\n\nWay more in Archive. Are there any in Telem that aren’t in A?\n\ntsites <- s %>% \n  filter(datasource == 'TELEM') %>% \n  select(sites) %>% \n  pull()\n\nasites <- s %>% \n  filter(datasource == 'A') %>% \n  select(sites) %>% \n  pull()\n\nall(tsites %in% asites)\n\n[1] FALSE\n\nsum(!(tsites %in% asites))\n\n[1] 96\n\n\nsee if I can blow up ggplot. Oof the plurals\n\ns <- s %>% \n  rename(site = sites, datasources = datasource)\n\nUnreadable. Not surprisingly.\n\nplot_datasources_by_site(s) + coord_flip()\n\nyikes.\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\nsxd <- get_sites_by_datasource(datasources = c('A', 'TELEM'))\n\n\nsxd\n\n# A tibble: 6,172 × 2\n   datasource site  \n   <chr>      <chr> \n 1 A          100023\n 2 A          100500\n 3 A          100503\n 4 A          100504\n 5 A          101708\n 6 A          102621\n 7 A          102827\n 8 A          102828\n 9 A          102829\n10 A          102830\n# … with 6,162 more rows\n\n\nPlot still needs work.\n\nplot_datasources_by_site(sxd) + coord_flip()\n\nJoining, by = c(\"datasource\", \"site\")"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#variables",
    "href": "vicwater/vicwater_testing.html#variables",
    "title": "Testing VicWater API",
    "section": "Variables",
    "text": "Variables\nor do I go straight for get_ts_ and then back this back out? Or get_db_info?\nI think I’m going to want to use get_variable_list both in get_ts_traces and to generate a set of possible variables.\nI’d like to get all sites, then make a master list of datasources and variables. But I need to get this usable for get_ts_traces, I think.\n\nGet_variable_list\nFeeding this a c(datasource, datasource) makes it return only some of the results, but throws no errors. So do one at a time.\n\nv_s_params <- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites,\n                               \"datasource\" = \"A\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(v_s_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 119\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\",\"datasource\":\"A\"}}\n\nresp_v_s <- req %>% \n  req_body_json(v_s_params) %>% \n  req_perform()\n\nrbody_v_s <- resp_v_s %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 4\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ variables   :List of 6\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"19610306171500\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"210.00\"\n  .. .. .. .. ..$ units       : chr \"pH\"\n  .. .. .. .. ..$ name        : chr \"Acidity/Alkalinity (pH)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"215.00\"\n  .. .. .. .. ..$ units       : chr \"ppm\"\n  .. .. .. .. ..$ name        : chr \"Dissolved Oxygen (ppm)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"233217\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221202090000\"\n  .. .. .. .. ..$ period_start: chr \"20091119170800\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. ..$ site        : chr \"405328\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"TAGGERTY R LADY TALT\"\n  .. .. .. ..$ name      : chr \"TAGGERTY RV @ LADY TALBOT DRIVE NEAR MARYSVILLE\"\n  .. .. ..$ variables   :List of 4\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"405331\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"R.G. MARYSVILLE\"\n  .. .. .. ..$ name      : chr \"RAINGAUGE @ MARYSVILLE GOLF CLUB\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221202081730\"\n  .. .. .. .. ..$ period_start: chr \"20010621142700\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"10.00\"\n  .. .. .. .. ..$ units       : chr \"mm\"\n  .. .. .. .. ..$ name        : chr \"Rainfall (mm)\"\n  .. .. ..$ site        : chr \"405837\"\n\n\nUnpack that\n\ns <- as_tibble(rbody_v_s[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # sites, and a `datasource` list\n  unnest_wider(col = site_details) %>% # site details in new cols\n  unnest_longer(col = variables) %>% # one line per variable, details of variables in a list\n  rename(long_name = name) %>% # variables have names too, avoid conflicts\n  unnest_wider(col = variables) %>% # columns for each attribute of the variables\n  rename(var_name = name)\ns\n\n# A tibble: 12 × 10\n   timezone short_…¹ long_…² perio…³ perio…⁴ subdesc varia…⁵ units var_n…⁶ site \n   <chr>    <chr>    <chr>   <chr>   <chr>   <chr>   <chr>   <chr> <chr>   <chr>\n 1 10.0     BARWON … BARWON… 202211… 196103… Availa… 100.00  metr… Stream… 2332…\n 2 10.0     BARWON … BARWON… 202211… 201007… Availa… 210.00  pH    Acidit… 2332…\n 3 10.0     BARWON … BARWON… 202211… 201007… Availa… 215.00  ppm   Dissol… 2332…\n 4 10.0     BARWON … BARWON… 202211… 201007… Availa… 450.00  Degr… Water … 2332…\n 5 10.0     BARWON … BARWON… 202211… 201007… Availa… 810.00  NTU   Turbid… 2332…\n 6 10.0     BARWON … BARWON… 202211… 201007… Availa… 820.00  µS/c… Conduc… 2332…\n 7 10.0     STEAVEN… STEAVE… 202212… 200911… Availa… 100.00  metr… Stream… 4053…\n 8 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 100.00  metr… Stream… 4053…\n 9 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 450.00  Degr… Water … 4053…\n10 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 810.00  NTU   Turbid… 4053…\n11 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 820.00  µS/c… Conduc… 4053…\n12 10.0     R.G. MA… RAINGA… 202212… 200106… Availa… 10.00   mm    Rainfa… 4058…\n# … with abbreviated variable names ¹​short_name, ²​long_name, ³​period_end,\n#   ⁴​period_start, ⁵​variable, ⁶​var_name\n\n\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\nvl <- get_variable_list(site_list = allsites, datasource = 'A')\n\n\nvl\n\n# A tibble: 12 × 11\n   site   short_…¹ long_…² varia…³ units var_n…⁴ perio…⁵ perio…⁶ subdesc datas…⁷\n   <chr>  <chr>    <chr>   <chr>   <chr> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 233217 BARWON … BARWON… 100.00  metr… Stream… 196103… 202211… Availa… A      \n 2 233217 BARWON … BARWON… 210.00  pH    Acidit… 201007… 202211… Availa… A      \n 3 233217 BARWON … BARWON… 215.00  ppm   Dissol… 201007… 202211… Availa… A      \n 4 233217 BARWON … BARWON… 450.00  Degr… Water … 201007… 202211… Availa… A      \n 5 233217 BARWON … BARWON… 810.00  NTU   Turbid… 201007… 202211… Availa… A      \n 6 233217 BARWON … BARWON… 820.00  µS/c… Conduc… 201007… 202211… Availa… A      \n 7 405328 STEAVEN… STEAVE… 100.00  metr… Stream… 200911… 202212… Availa… A      \n 8 405331 TAGGERT… TAGGER… 100.00  metr… Stream… 201007… 201302… Availa… A      \n 9 405331 TAGGERT… TAGGER… 450.00  Degr… Water … 201007… 201302… Availa… A      \n10 405331 TAGGERT… TAGGER… 810.00  NTU   Turbid… 201007… 201302… Availa… A      \n11 405331 TAGGERT… TAGGER… 820.00  µS/c… Conduc… 201007… 201302… Availa… A      \n12 405837 R.G. MA… RAINGA… 10.00   mm    Rainfa… 200106… 202212… Availa… A      \n# … with 1 more variable: timezone <chr>, and abbreviated variable names\n#   ¹​short_name, ²​long_name, ³​variable, ⁴​var_name, ⁵​period_start, ⁶​period_end,\n#   ⁷​datasource\n\n\nTry with two datasources\n\nv2 <- get_variable_list(site_list = allsites, datasource = c('A', 'TELEM'))\n\n\nv2\n\n# A tibble: 27 × 11\n   site   short_…¹ long_…² varia…³ units var_n…⁴ perio…⁵ perio…⁶ subdesc datas…⁷\n   <chr>  <chr>    <chr>   <chr>   <chr> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 233217 BARWON … BARWON… 100.00  metr… Stream… 196103… 202211… Availa… A      \n 2 233217 BARWON … BARWON… 210.00  pH    Acidit… 201007… 202211… Availa… A      \n 3 233217 BARWON … BARWON… 215.00  ppm   Dissol… 201007… 202211… Availa… A      \n 4 233217 BARWON … BARWON… 450.00  Degr… Water … 201007… 202211… Availa… A      \n 5 233217 BARWON … BARWON… 810.00  NTU   Turbid… 201007… 202211… Availa… A      \n 6 233217 BARWON … BARWON… 820.00  µS/c… Conduc… 201007… 202211… Availa… A      \n 7 405328 STEAVEN… STEAVE… 100.00  metr… Stream… 200911… 202212… Availa… A      \n 8 405331 TAGGERT… TAGGER… 100.00  metr… Stream… 201007… 201302… Availa… A      \n 9 405331 TAGGERT… TAGGER… 450.00  Degr… Water … 201007… 201302… Availa… A      \n10 405331 TAGGERT… TAGGER… 810.00  NTU   Turbid… 201007… 201302… Availa… A      \n# … with 17 more rows, 1 more variable: timezone <chr>, and abbreviated\n#   variable names ¹​short_name, ²​long_name, ³​variable, ⁴​var_name,\n#   ⁵​period_start, ⁶​period_end, ⁷​datasource\n\n\nI think now go to get_ts_traces, and then go back and write some helpers that can call get_datasources and get_variable and geo-locate, and use them to allow passing things like variables = ‘all’"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#get-traces",
    "href": "vicwater/vicwater_testing.html#get-traces",
    "title": "Testing VicWater API",
    "section": "get traces",
    "text": "get traces\nThe basic format is this, will need to do some testing.\n\nb1params <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(b1params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 219\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb1 <- req %>% \n  req_body_json(b1params) %>% \n  req_perform()\n\nrbodyb1 <- respb1 %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb1)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nand with two params\n\nb2params <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(b2params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb2 <- req %>% \n  req_body_json(b2params) %>% \n  req_perform()\n\nrbodyb2 <- respb2 %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb2)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nTwo params, two sites (one site only has one var, but not an error, I hope)\n\nb22params <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = str_c(barwon, steavenson, sep = \", \"),\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(b22params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 231\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217, 405328\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb22 <- req %>% \n  req_body_json(b22params) %>% \n  req_perform()\n\nrbodyb22 <- respb22 %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb22)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 3\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ longitude : chr \"145.773503100\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. .. ..$ latitude  : chr \"-37.525797590\"\n  .. .. .. ..$ org_name  : chr \"Victorian Rural Water Corporation\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.738\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.736\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.734\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.755\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.739\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.735\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.759\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.742\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.757\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"405328\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nI wasn’t able to get discharge (140, 141) from a varlist. Double check\n\nbparamsd <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,140\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparamsd) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,140\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbd <- req %>% \n  req_body_json(bparamsd) %>% \n  req_perform()\n\nrbodybd <- respbd %>% resp_body_json(check_type = FALSE)\n\nstr(rbodybd)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nKisters has an example of asking for 140.01? Try that? No, just do the varfrom/varto method for discharge and stage.\n\nbparams <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,140.01\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparams) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 226\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,140.01\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb <- req %>% \n  req_body_json(bparams) %>% \n  req_perform()\n\nrbodyb <- respb %>% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nvarfrom-varto check. Does it give us both, or do we have to ask for the from separately`?\n\nbparamsft <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"100\",\n                               \"varto\" = \"140\", \n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparamsft) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"100\",\"varto\":\"140\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbft <- req %>% \n  req_body_json(bparamsft) %>% \n  req_perform()\n\nrbodybft <- respbft %>% resp_body_json(check_type = FALSE)\n\nstr(rbodybft)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 150: chr \"Rating extrapolated above 1.5x maximum flow gauged.\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.18443\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.16332\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.12997\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.10202\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.08706\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07044\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.04782\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.02872\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07961\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.14628\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.11425\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.09740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07464\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.11269\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Discharge (m3/sec)\"\n  .. .. .. ..$ precision : chr \"0.000010\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"140.00\"\n  .. .. .. ..$ units     : chr \"cubic metres/second\"\n  .. .. .. ..$ name      : chr \"Stream Discharge (m3/s)\"\n\n\nThat looks like it might have only given us varto.\nWhat happens if we ask for a varto/from for a site that doesn’t have it? Earlier we saw that the varlist just skips, but does this?\n\nbparamse <- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = golf,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq <- request(vicurl)\n\nreq %>% \n  req_body_json(bparamse) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"405837\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbe <- req %>% \n  req_body_json(bparamse) %>% \n  req_perform()\n\nrbodybe <- respbe %>% resp_body_json(check_type = FALSE)\n\nstr(rbodybe)\n\nList of 2\n $ error_num: int 125\n $ error_msg: chr \"No data for specified variable in file\"\n\n\nOk, that errors. So I’ll need to capture and skip errors.\n\nunpack different formats\n(varlist with multiple, varfrom/to, etc). b1params (one param, one site), b2params (two params, varlist), b22params (two params, two sites- only one param at one site) bparamsft (varfrom/varto), bparamse (error),\nstart simple- grab errors. Should have been doing this all along.\n\ner1 <- rbodyb1[1]\nere <- rbodybe[1]\ner1\n\n$error_num\n[1] 0\n\nere\n\n$error_num\n[1] 125\n\n\nUnpack the single. Yeesh\nI’m ignoring quality codes for the moment. They’re a definition list, and so maybe should be unpacked separately, matched to varto (I think not varfrom), and then joined? But I want to see how they work with more complex structures first.\n\ns <- as_tibble(rbodyb1[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %>% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\nTry two variables. still works. still not sure why I need both varfrom and varto when they match.\n\ns <- as_tibble(rbodyb2[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %>% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\nTwo variables, two sites\n\ns <- as_tibble(rbodyb22[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %>% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\nAnd finally, the one where we do have a varto\n\ns <- as_tibble(rbodybft[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %>% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\nFinally, some new q values. and pretty clear we don’t need the varfroms.\nNeed to change the time column to dates\nDo we want to split up or return stacked or return wide? Make an option.\nDo we want to return NA days as NA or skip them?\n\n\nCleaning that up\nsafest is code x site x varto. though I think don’t need site, we’ll have expanded there by the time we get varto.\n\ns <- as_tibble(rbodyb22[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% # complex set of lists\n  unnest_wider(col = site_details) %>% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %>% \n  # there are name conflicts between site and varfrom and varto. \n  # and we can drop varfrom\n  select(-varfrom_details) %>% \n  unnest_wider(col = varto_details) %>% \n  rename_with(~(paste0('variable_', .)), \n              c(short_name, name)) \n\n# break in here to get the quality codes to match\nqc <- s %>% \n  select(quality_codes, site, variable) %>% \n  unnest_longer(col = quality_codes) %>% \n  mutate(quality_codes_id = as.integer(quality_codes_id))\n\n# finish unpacking\ns <- s %>%\n  select(-quality_codes) %>% \n  unnest_longer(col = trace) %>% \n  unnest_wider(col = trace)\n\n# clean up\ns <- s %>% \n  rename(value = v, time = t, quality_codes_id = q) %>% \n  mutate(time = lubridate::ymd_hms(time)) %>% \n  left_join(qc, by = c('quality_codes_id', 'site', 'variable')) %>% \n  mutate(across(c(longitude, latitude, value), as.numeric)) # leaving some others because they either are names (gauges, variable) or display better (precision)\n\nCan I make that wide? Works without using id_cols but messy because too many info cols. Would end up being better to cut and join the info back on. But then I lose the quality codes, because they apply to each variable differently. Just return like this for now with a warning.\n\nsw <- s %>% pivot_wider(names_from = variable, values_from = value, id_cols = c(time, site))\n\nWhat I could do though is break it up into a list, potentially by sites and/or variables.\n\nslist <- split(s, s$site)\nvlist <- split(s, s$variable)\nsvlist <- split(s, interaction(s$site, s$variable))\n\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\n\nOne site, one variable\n\nbs <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = '100', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nWarning: executing %dopar% sequentially: no parallel backend registered\n\n\nCan I pass decimals? it’s how they come out of get_variable_list\n\nbsdec <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = '100.00', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOne site, derived variables\n\nbsd <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = c('100', '140'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOnly derived\n\nbsod <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = '140', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nSome more variables, derived and not\n\nbsdv <- get_ts_traces(site_list = barwon, datasource = 'A', var_list = c('100', '140', '210', '450'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nAnd multi-sites too- does it correctly collapse the vector?\n\nbsdvs <- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nAnd finally everything including rain at golf. Careful though- does mean make sense for that? Probably better as a sum? Tried that and threw an error but told me the options:\nMean/Max/Min/Start/End/First/Last/Tot/MaxMin/Point/Cum\nDefinitely need two calls if need two different values at least for now- total temp is nonsense.\n\nbsdvs <- get_ts_traces(site_list = allsites, \n                       datasource = 'A', \n                       var_list = c('10', '100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nand return a list\n\nbsdvsl <- get_ts_traces(site_list = allsites, \n                       datasource = 'A', \n                       var_list = c('10', '100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'sxvlist')"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#a-variable-and-time-aware-version",
    "href": "vicwater/vicwater_testing.html#a-variable-and-time-aware-version",
    "title": "Testing VicWater API",
    "section": "A variable and time-aware version",
    "text": "A variable and time-aware version\n\npossibles <- get_variable_list(site_list = allsites, datasource = 'A') %>% \n  dplyr::select(site, datasource, variable, period_start, period_end)\n\n\nposs140 <- possibles[possibles$variable == '100.00', ] \n\nposs141 <- poss140\nposs140$variable <- '140.00'\nposs141$variable <- '141.00'\n\npossibles <- bind_rows(possibles, poss140, poss141)\n\nall the tests above should run\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\n\nOne site, one variable\n\nbs <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '100', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nCan I pass decimals? it’s how they come out of get_variable_list\n\nbsdec <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '100.00', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOne site, derived variables\n\nbsd <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = c('100', '140', '141'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOnly derived\n\nbsod <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '140', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nSome more variables, derived and not\n\nbsdv <- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = c('100', '140', '210', '450'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nAnd multi-sites too- does it correctly collapse the vector?\n\nbsdvs <- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nDo the ‘all’ settings work? Let’s bump to year so I don’t have so much data\n\nbsdvs <- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = \"all\", \n                       start_time = \"all\", \n                       end_time = \"all\", \n                       interval = 'year', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nWarning: `var_list = 'all'` is *very* dangerous, since it applies the same\n`data_type` to all variables, which is rarely appropriate. Check the variables\navailable for your sites and make sure you want to do this.\n\n\nCan I throw something wrong to interval to see if it tells me what it can do? Kisters says\nyear, month, day, hour, minute, second,\nperiod,\ndefault\n\nbiw <- get_ts_traces2(site_list = barwon, \n                       datasource = 'A', \n                       var_list = \"100\", \n                       start_time = \"20200101\", \n                       end_time = \"20211231\", \n                       interval = 'eon', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nerror is Invalid interval, must be YEAR, MONTH, DAY, HOUR, MINUTE or SECOND"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#benchmark",
    "href": "vicwater/vicwater_testing.html#benchmark",
    "title": "Testing VicWater API",
    "section": "Benchmark",
    "text": "Benchmark\nThis likely varies a lot depending on what I’m asking for. Should be done more systematically, and use microbenchmark.\nThey should be roughly the same for a single?\n\nsystem.time(b1 <- get_ts_traces(site_list = barwon, \n                       datasource = 'A', \n                       var_list = '100', \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.10    0.00    0.86 \n\nsystem.time(b2 <- get_ts_traces2(site_list = barwon, \n                       datasource = 'A', \n                       var_list = '100', \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.17    0.01    1.50 \n\n\ninteresting. so the second is faster locally, but higher network, I think.\n\nsystem.time(bsdvs1 <- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.16    0.02    1.71 \n\nsystem.time(bsdvs2 <- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.25    0.01    5.69 \n\n\nOof. That’s pretty bad. Can I speed it up? probably.\nHow about parallel?\n\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nWarning: package 'foreach' was built under R version 4.2.2\n\n\nLoading required package: future\n\n\nWarning: package 'future' was built under R version 4.2.2\n\nregisterDoFuture()\nplan(multisession)\n\nsystem.time(bsdvs1p <- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.04    0.00    2.81 \n\nsystem.time(bsdvs2p <- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.06    0.00    3.67"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#get-db-info",
    "href": "vicwater/vicwater_testing.html#get-db-info",
    "title": "Testing VicWater API",
    "section": "Get db info",
    "text": "Get db info\n\nGeofiltering\nThe get_db_info API function can do a LOT. I’m primarily looking for using it for geofiltering, but it may turn out that that’s better done in other ways anyway. The filter_values sitelist_filter, etc are likely also useful, and will be worth adding as we go.\nThe area from Teesdale (-38, 144 at top left to Leopold (-38.2, 144.5) should contain 5 sites.\nOh no it doesn’t. It actually contains a bazillion, because there’s a bunch of stntype: \"GW\". Turning eval: false to we don’t do that again.\n\n# need a matrix to get the double brackets.\ntopleft <- c('-38', '144')\nbottomright <- c('-38.2', '144.5')\n\nrectbox <- rbind(topleft, bottomright)\ngeo_params <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %>% \n  req_body_json(geo_params) %>% \n  req_dry_run()\n\nresp_geo <- reqvic %>% \n  req_body_json(geo_params) %>% \n  req_perform()\n\nrbody_geo <- resp_geo %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nSo, that is WAAY too much, so we need to filter by surface water station types. Assuming the structure is the same, let’s figure out how to parse that into a table, and then get the stntype that matches surface and develop that capacity. And document how to choose which.\nAnd use the field_list to not return all the ‘category’ nonsense.\n\ndb <- as_tibble(rbody_geo[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% \n  select(station, stntype, stname, everything(), -starts_with('category'))\n\nSo, what are the possible stntypes?\n\ntable(db$stntype)\n\nWhat are those? GW has got to be groundwater. HYD is Hydrology, presumably??? SOB I think is the State Observation Bore Network. VIR???.\nLet’s knock off the GW and see if we can figure the others out.\n\n##| rows.print: 30\ndbNoGW <- db %>% \n  filter(stntype != 'GW') %>% \n  select(station, stntype, stname, shortname, parent, commence, cease, everything())\n\ndbNoGW\n\nSo, the SOB don’t have any info, the HYD seem to be A and B versions of the VIR. Do they have different data??? If we get traces for 233217A and B, do they differ from 233217?\nLet’s clean that up a bit first before figuring that out.\n\n##| rows.print = 30\ndbNoGW <- dbNoGW %>% \n  filter(stntype != 'SOB') %>% \n  arrange(station)\ndbNoGW\n\nInteresting.\nSo, some things to do\nTODO-\n\nsort out the filters (filter_values I think?) to only get HYD and VIR (or whatever the user wants)\n\nand tell the user what the options are- apparently, ‘GW’, ‘SOB’, ‘HYD’, and ‘VIR’\n\nsort out field_list to only get a subset of useful fields\n\nAnd alert the user about what fields are being dropped\n\nfigure out whether the data differs between HYD and VIR (should be able to just pass the A,B, numbers to the get_ts_traces)?\nOther geo filters\n\n\n\nOther filtering\nTry filter_values and field_list. \"filter_values\" = list(\"stntype\" = 'HYD, VIR') does not work- figure out why.\n\ntopleft <- c('-38', '144')\nbottomright <- c('-38.2', '144.5')\n\nrectbox <- rbind(topleft, bottomright)\ngeo_params <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"stntype\" = 'VIR'),\n                               \"field_list\" = \"station, stntype, stname, shortname, commence, cease, active, northing, easting, longitude, latitude, lldatum\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %>% \n  req_body_json(geo_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 313\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"stntype\":\"VIR\"},\"field_list\":\"station, stntype, stname, shortname, commence, cease, active, northing, easting, longitude, latitude, lldatum\",\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo <- reqvic %>% \n  req_body_json(geo_params) %>% \n  req_perform()\n\nrbody_geo <- resp_geo %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 6\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5780762.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"MOORABOOL @ BATESFOR\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"MOORABOOL RIVER @ BATESFORD\"\n  .. .. ..$ category9 : chr \"YES\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"0\"\n  .. .. ..$ category4 : chr \"50\"\n  .. .. ..$ category3 : chr \"S/U\"\n  .. .. ..$ category2 : chr \"V_93F3\"\n  .. .. ..$ category1 : chr \"0\"\n  .. .. ..$ elevacc   : chr \"9\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19080101\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"03 5276 1201\"\n  .. .. ..$ spare1    : chr \"Derwent Hotel\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"232\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.278224920\"\n  .. .. ..$ comment   : chr \"\\r\\n\\r\\n\\r\\nDerwent Hotel BatesfordFrom Geelong West, head west along Ballarat Highway to Batesford. Just befor\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"232202\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Batesford\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"29.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.089311520\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"261303.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"\"\n  .. .. ..$ active    : logi FALSE\n  .. .. ..$ northing  : chr \"5779117.700\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"WASTEWATER DR @ FORD\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ category9 : chr \"\"\n  .. .. ..$ category8 : chr \"\"\n  .. .. ..$ category7 : chr \"\"\n  .. .. ..$ category6 : chr \"\"\n  .. .. ..$ category5 : chr \"\"\n  .. .. ..$ category4 : chr \"\"\n  .. .. ..$ category3 : chr \"\"\n  .. .. ..$ category2 : chr \"\"\n  .. .. ..$ category1 : chr \"\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"N\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 20060630\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEELONG\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"\"\n  .. .. ..$ posacc    : chr \"1\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"232\"\n  .. .. ..$ grdatum   : chr \"ANG\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.355800000\"\n  .. .. ..$ comment   : chr \"\\r\\nStation operated for Ford Aus. Project No 1563532011/07/17 Virtual site entry generated by DSEVIRTUALSITE.H\"| __truncated__\n  .. .. ..$ lldatum   : chr \"AGD66\"\n  .. .. ..$ station   : chr \"232711\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"VRW\"\n  .. .. ..$ barcode   : chr \"\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 20071029\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.105900000\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"N\"\n  .. .. ..$ easting   : chr \"268159.900\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5774521.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON POLLOCKSFORD\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"B\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"4WD\"\n  .. .. ..$ category5 : chr \"50\"\n  .. .. ..$ category4 : chr \"150\"\n  .. .. ..$ category3 : chr \"GRAVEL\"\n  .. .. ..$ category2 : chr \"V_93D4\"\n  .. .. ..$ category1 : chr \"18\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19060701\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"N/A\"\n  .. .. ..$ spare1    : chr \"N/A\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.187100410\"\n  .. .. ..$ comment   : chr \"\\r\\nFrom Inverliegh, travel east along Hamilton Highway. Turn south down Pollocksford Road and travel 3.1 km to\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233200\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Moriac\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.143395970\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"253492.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5779017.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"LEIGH @ INVERLEIGH\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"20\"\n  .. .. ..$ category4 : chr \"100\"\n  .. .. ..$ category3 : chr \"SEALED\"\n  .. .. ..$ category2 : chr \"V_93B3\"\n  .. .. ..$ category1 : chr \"1\"\n  .. .. ..$ elevacc   : chr \"9\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"N\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19460315\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"N/A\"\n  .. .. ..$ spare1    : chr \"N/A\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1101\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.062212190\"\n  .. .. ..$ comment   : chr \"\\r\\nIt is underneath the Hamilton Highway Bridge that passes over the Leigh River.2011/07/17 Virtual site entry\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233209\"\n  .. .. ..$ datemod   : int 20220427\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Inverleigh\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"64.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.099828090\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"n\"\n  .. .. ..$ easting   : chr \"242392.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5772691.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON @ GEELONG\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"0\"\n  .. .. ..$ category4 : chr \"150\"\n  .. .. ..$ category3 : chr \"SEALED\"\n  .. .. ..$ category2 : chr \"V_93G4\"\n  .. .. ..$ category1 : chr \"0\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19601118\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"BW\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. ..$ comment   : chr \"\\r\\n\\r\\n\\r\\nBarwon Water flood monitoring stationFrom the intersection of the Fyans St and La Trobe Terrace, he\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233217\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Geelong\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"267562.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"\"\n  .. .. ..$ active    : logi FALSE\n  .. .. ..$ northing  : chr \"5777388.300\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON RIVER\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ category9 : chr \"\"\n  .. .. ..$ category8 : chr \"\"\n  .. .. ..$ category7 : chr \"\"\n  .. .. ..$ category6 : chr \"\"\n  .. .. ..$ category5 : chr \"\"\n  .. .. ..$ category4 : chr \"\"\n  .. .. ..$ category3 : chr \"\"\n  .. .. ..$ category2 : chr \"\"\n  .. .. ..$ category1 : chr \"\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"N\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19680208\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEELONG\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"\"\n  .. .. ..$ posacc    : chr \"1\"\n  .. .. ..$ timemod   : int 1558\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.150000000\"\n  .. .. ..$ comment   : chr \"2011/07/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 233219A\\r\\n\"\n  .. .. ..$ lldatum   : chr \"\"\n  .. .. ..$ station   : chr \"233219\"\n  .. .. ..$ datemod   : int 20141001\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"VRW\"\n  .. .. ..$ barcode   : chr \"\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 19690911\n  .. .. ..$ local_map : chr \"QUEENSCLIFF\"\n  .. .. ..$ latitude  : chr \"-38.116666670\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"N\"\n  .. .. ..$ easting   : chr \"250148.900\"\n  .. .. ..$ stntype   : chr \"VIR\"\n\n\ncleanup\n\ndb <- as_tibble(rbody_geo[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% \n  select(station, stntype, stname, everything(), -starts_with('category'))\ndb\n\n# A tibble: 6 × 42\n  station stntype stname  active north…¹ timez…² short…³ datec…⁴ elevd…⁵ elevacc\n  <chr>   <chr>   <chr>   <lgl>  <chr>   <chr>   <chr>     <int> <chr>   <chr>  \n1 232202  VIR     MOORAB… TRUE   578076… 10.0    MOORAB…  1.90e7 \"\"      9      \n2 232711  VIR     WASTEW… FALSE  577911… 10.0    WASTEW…  1.90e7 \"\"      1      \n3 233200  VIR     BARWON… TRUE   577452… 10.0    BARWON…  1.90e7 \"\"      1      \n4 233209  VIR     LEIGH … TRUE   577901… 10.0    LEIGH …  1.90e7 \"\"      9      \n5 233217  VIR     BARWON… TRUE   577269… 10.0    BARWON…  1.90e7 \"\"      1      \n6 233219  VIR     BARWON… FALSE  577738… 10.0    BARWON…  1.90e7 \"\"      1      \n# … with 32 more variables: dbver47 <lgl>, quarter <chr>, section <int>,\n#   commence <int>, parent <chr>, mapname <chr>, meridian <chr>, spare5 <chr>,\n#   spare4 <chr>, spare3 <chr>, spare2 <chr>, spare1 <chr>, posacc <chr>,\n#   timemod <int>, region <chr>, grdatum <chr>, township <chr>,\n#   longitude <chr>, comment <chr>, lldatum <chr>, datemod <int>,\n#   timecreate <int>, orgcode <chr>, barcode <chr>, zone <int>, elev <chr>,\n#   cease <int>, local_map <chr>, latitude <chr>, range <chr>, …\n\n\n\nnames(db)\n\n [1] \"station\"    \"stntype\"    \"stname\"     \"active\"     \"northing\"  \n [6] \"timezone\"   \"shortname\"  \"datecreate\" \"elevdatum\"  \"elevacc\"   \n[11] \"dbver47\"    \"quarter\"    \"section\"    \"commence\"   \"parent\"    \n[16] \"mapname\"    \"meridian\"   \"spare5\"     \"spare4\"     \"spare3\"    \n[21] \"spare2\"     \"spare1\"     \"posacc\"     \"timemod\"    \"region\"    \n[26] \"grdatum\"    \"township\"   \"longitude\"  \"comment\"    \"lldatum\"   \n[31] \"datemod\"    \"timecreate\" \"orgcode\"    \"barcode\"    \"zone\"      \n[36] \"elev\"       \"cease\"      \"local_map\"  \"latitude\"   \"range\"     \n[41] \"qquarter\"   \"easting\"   \n\n\nThat seems to have worked for filter vaues but not field list. and it doesn’t work for filtering to multiple values. Figure both those things out.\nHow about c() on both. Shorten the field list for the moment. Runs but no data.\nc() on the field list works for a single stntype. Now to figure out the stntypes\n\ntopleft <- c('-38', '144')\nbottomright <- c('-38.2', '144.5')\n\nrectbox <- rbind(topleft, bottomright)\ngeo_params <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"stntype\" = 'HYD'),\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %>% \n  req_body_json(geo_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"stntype\":\"HYD\"},\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo <- reqvic %>% \n  req_body_json(geo_params) %>% \n  req_perform()\n\nrbody_geo <- resp_geo %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 12\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"MOORABOOL RIVER AT BATESFORD\"\n  .. .. ..$ station: chr \"232202A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ station: chr \"232711A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ station: chr \"233209A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217C\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217D\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ station: chr \"233219A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Waurn Ponds Creek @ Brearly Reserve\"\n  .. .. ..$ station: chr \"233221A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Native Hut Creek @ Turtle Bend Park Teesdale\"\n  .. .. ..$ station: chr \"233242A\"\n  .. .. ..$ stntype: chr \"HYD\"\n\n\n\ndb <- as_tibble(rbody_geo[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list)) %>% \n  select(station, stntype, stname, everything(), -starts_with('category'))\ndb\n\n# A tibble: 12 × 3\n   station stntype stname                                      \n   <chr>   <chr>   <chr>                                       \n 1 232202A HYD     MOORABOOL RIVER AT BATESFORD                \n 2 232711A HYD     WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG \n 3 233200A HYD     BARWON RIVER @ POLLOCKSFORD                 \n 4 233200B HYD     BARWON RIVER @ POLLOCKSFORD                 \n 5 233209A HYD     LEIGH RIVER @ INVERLEIGH                    \n 6 233217A HYD     BARWON RIVER @ GEELONG                      \n 7 233217B HYD     BARWON RIVER @ GEELONG                      \n 8 233217C HYD     BARWON RIVER @ GEELONG                      \n 9 233217D HYD     BARWON RIVER @ GEELONG                      \n10 233219A HYD     BARWON RIVER @ MURGHEBOLUC                  \n11 233221A HYD     Waurn Ponds Creek @ Brearly Reserve         \n12 233242A HYD     Native Hut Creek @ Turtle Bend Park Teesdale\n\n\n\n\nTrying for the stntype\nTried to use sitelist_filter = \"GROUP(PLUVIO_STATIONS), which qld made look like would work, but nothing. maybe get_groups is a way to figure out what the groups are?\n\ngeo_params <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"sitelist_filter\" = \"GROUP(PLUVIO_STATIONS)\",\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %>% \n  req_body_json(geo_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 241\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"sitelist_filter\":\"GROUP(PLUVIO_STATIONS)\",\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo <- reqvic %>% \n  req_body_json(geo_params) %>% \n  req_perform()\n\nrbody_geo <- resp_geo %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows: list()\n\n\n\n\nAside- get_groups\nCan we use hydstra groupings, as defined in the db? What are those groupings?\n\ngrp_params <- list(\"function\" = 'get_groups',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites))\n\nreqvic %>% \n  req_body_json(grp_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 95\n\n{\"function\":\"get_groups\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\"}}\n\nresp_grp <- reqvic %>% \n  req_body_json(grp_params) %>% \n  req_perform()\n\nrbody_grp <- resp_grp %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_grp)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 31\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"to be at the top of the list\"\n  .. ..$ group       : chr \"AB\"\n  .. ..$ value       : chr \"CTS\"\n  .. ..$ value_decode: chr \"list of site A files that have changed in the last 24hrs\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"to be at the top of the list\"\n  .. ..$ group       : chr \"AB\"\n  .. ..$ value       : chr \"FLOOD\"\n  .. ..$ value_decode: chr \"All VIR Sites for Flood Zoom\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"10PLUS\"\n  .. ..$ value_decode: chr \"sites with records longer than 10 years\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"30PLUS\"\n  .. ..$ value_decode: chr \"sites with records longer than 30 years\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"ARCHIVE\"\n  .. ..$ value_decode: chr \"Archive files\"\n  .. ..$ stations    :List of 4\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"FLOWA\"\n  .. ..$ value_decode: chr \"All sites with flow (Vir and Hyd)\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"LEVEL\"\n  .. ..$ value_decode: chr \"All sites with level (100.00)\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"VFLOW\"\n  .. ..$ value_decode: chr \"Virtual sites with flow\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Goups for the CMA areas\"\n  .. ..$ group       : chr \"CMA\"\n  .. ..$ value       : chr \"CCMA\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"daily on change generated site lists\"\n  .. ..$ group       : chr \"HYVIRTUAL\"\n  .. ..$ value       : chr \"TS\"\n  .. ..$ value_decode: chr \"list of site A files that have changed in the last 24hrs\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Telemetry Sites\"\n  .. ..$ group       : chr \"TELEMETRY\"\n  .. ..$ value       : chr \"RAWDATA\"\n  .. ..$ stations    :List of 4\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Yukkie\"\n  .. ..$ group       : chr \"WDTF\"\n  .. ..$ value       : chr \"LEVEL\"\n  .. ..$ value_decode: chr \"All sites with level (100.00)\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Yukkie\"\n  .. ..$ group       : chr \"WDTF\"\n  .. ..$ value       : chr \"RAIN\"\n  .. ..$ value_decode: chr \"All rain monitoring sites\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Yukkie\"\n  .. ..$ group       : chr \"WDTF\"\n  .. ..$ value       : chr \"WQSITES\"\n  .. ..$ value_decode: chr \"all sites that have WQ in sample table\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"CMA\"\n  .. ..$ group       : chr \"WEB_GW_CMA\"\n  .. ..$ value       : chr \"CMA01\"\n  .. ..$ value_decode: chr \"Corangamite CMA\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"CMA\"\n  .. ..$ group       : chr \"WEB_GW_CMA\"\n  .. ..$ value       : chr \"CMA04\"\n  .. ..$ value_decode: chr \"Goulburn CMA\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"RWC\"\n  .. ..$ group       : chr \"WEB_GW_RWC\"\n  .. ..$ value       : chr \"GMW\"\n  .. ..$ value_decode: chr \"Goulburn Murray Water\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"RWC\"\n  .. ..$ group       : chr \"WEB_GW_RWC\"\n  .. ..$ value       : chr \"SRW\"\n  .. ..$ value_decode: chr \"Southern Rural Water\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites\"\n  .. ..$ group       : chr \"WEB_SW\"\n  .. ..$ value       : chr \"B_233_BARWON\"\n  .. ..$ value_decode: chr \"233-Barwon Basin\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites\"\n  .. ..$ group       : chr \"WEB_SW\"\n  .. ..$ value       : chr \"B_405_GOULBURN\"\n  .. ..$ value_decode: chr \"405-Goulburn Basin\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites - active telem only\"\n  .. ..$ group       : chr \"WEB_SW_TELEM\"\n  .. ..$ value       : chr \"B_233_BARWON\"\n  .. ..$ value_decode: chr \"233-Barwon Basin\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites - active telem only\"\n  .. ..$ group       : chr \"WEB_SW_TELEM\"\n  .. ..$ value       : chr \"B_405_GOULBURN\"\n  .. ..$ value_decode: chr \"405-Goulburn Basin\"\n  .. ..$ stations    :List of 2\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Test Sites\"\n  .. ..$ group       : chr \"WEB_TEST\"\n  .. ..$ value       : chr \"WEB_TEST\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites\"\n  .. ..$ group       : chr \"WMIS2\"\n  .. ..$ value       : chr \"RF\"\n  .. ..$ value_decode: chr \"Rainfall\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites\"\n  .. ..$ group       : chr \"WMIS2\"\n  .. ..$ value       : chr \"SW\"\n  .. ..$ value_decode: chr \"Surface Water\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites\"\n  .. ..$ group       : chr \"WMIS2\"\n  .. ..$ value       : chr \"WQ\"\n  .. ..$ value_decode: chr \"Water Quality\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites version 2\"\n  .. ..$ group       : chr \"WMIS2_2\"\n  .. ..$ value       : chr \"RF\"\n  .. ..$ value_decode: chr \"Rainfall\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites version 2\"\n  .. ..$ group       : chr \"WMIS2_2\"\n  .. ..$ value       : chr \"SW\"\n  .. ..$ value_decode: chr \"Surface Water\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites version 2\"\n  .. ..$ group       : chr \"WMIS2_2\"\n  .. ..$ value       : chr \"WQ\"\n  .. ..$ value_decode: chr \"Water Quality\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 CMAs\"\n  .. ..$ group       : chr \"WMIS2_CMA\"\n  .. ..$ value       : chr \"CMA01\"\n  .. ..$ value_decode: chr \"Corangamite CMA\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 CMAs\"\n  .. ..$ group       : chr \"WMIS2_CMA\"\n  .. ..$ value       : chr \"CMA04\"\n  .. ..$ value_decode: chr \"Goulburn CMA\"\n  .. ..$ stations    :List of 2\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n\n\nUnpack\n\ndb <- as_tibble(rbody_grp[2]) %>% # the [2] drops the error column\n  unnest_wider(col = where(is.list)) %>% # a `return` list\n  unnest_longer(col = where(is.list))\n\nDrop the stations- what are the groups?\n\ndb %>% select(-stations) %>% distinct()\n\n# A tibble: 31 × 4\n   group_decode                         group     value   value_decode          \n   <chr>                                <chr>     <chr>   <chr>                 \n 1 to be at the top of the list         AB        CTS     list of site A files …\n 2 to be at the top of the list         AB        FLOOD   All VIR Sites for Flo…\n 3 All sites in archive                 ALLA      10PLUS  sites with records lo…\n 4 All sites in archive                 ALLA      30PLUS  sites with records lo…\n 5 All sites in archive                 ALLA      ARCHIVE Archive files         \n 6 All sites in archive                 ALLA      FLOWA   All sites with flow (…\n 7 All sites in archive                 ALLA      LEVEL   All sites with level …\n 8 All sites in archive                 ALLA      VFLOW   Virtual sites with fl…\n 9 Goups for the CMA areas              CMA       CCMA    <NA>                  \n10 daily on change generated site lists HYVIRTUAL TS      list of site A files …\n# … with 21 more rows\n\n\nNo obvious groupings there to select on except maybe surface water sites, but that drops rainfall.\n\n\nHow about sitelist_filter?\nwith one site\n\ngeo_params <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"sitelist_filter\" = \"233217\",\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %>% \n  req_body_json(geo_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 225\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"sitelist_filter\":\"233217\",\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo <- reqvic %>% \n  req_body_json(geo_params) %>% \n  req_perform()\n\nrbody_geo <- resp_geo %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 1\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217\"\n  .. .. ..$ stntype: chr \"VIR\"\n\n\nwith multiple sites, it just returns the first one. And using c(\"site1\", \"site2\") errors out.\n\ngeo_params <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"sitelist_filter\" = \"233217, 405331\",\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %>% \n  req_body_json(geo_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 233\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"sitelist_filter\":\"233217, 405331\",\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo <- reqvic %>% \n  req_body_json(geo_params) %>% \n  req_perform()\n\nrbody_geo <- resp_geo %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 1\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217\"\n  .. .. ..$ stntype: chr \"VIR\"\n\n\n\n\nusing complex_filter\nis this easier or harder than just running it twice (or however many times)\nIt’s more flexible, but pretty terrible to sort out. And absurdly slow.\n\ngeo_params <- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"complex_filter\" = list(list('fieldname' = 'stntype',\n                                                         'value' = \"HYD\",\n                                                         'operator' = 'EQ'),\n                                                    list('combine' = 'OR',\n                                                         'fieldname' = 'stntype',\n                                                         'value' = \"VIR\",\n                                                         'operator' = 'EQ')),\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %>% \n  req_body_json(geo_params) %>% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 340\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"complex_filter\":[{\"fieldname\":\"stntype\",\"value\":\"HYD\",\"operator\":\"EQ\"},{\"combine\":\"OR\",\"fieldname\":\"stntype\",\"value\":\"VIR\",\"operator\":\"EQ\"}],\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo <- reqvic %>% \n  req_body_json(geo_params) %>% \n  req_perform()\n\nrbody_geo <- resp_geo %>% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 18\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"MOORABOOL RIVER @ BATESFORD\"\n  .. .. ..$ station: chr \"232202\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"MOORABOOL RIVER AT BATESFORD\"\n  .. .. ..$ station: chr \"232202A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ station: chr \"232711\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ station: chr \"232711A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ station: chr \"233209\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ station: chr \"233209A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217C\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217D\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ station: chr \"233219\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ station: chr \"233219A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Waurn Ponds Creek @ Brearly Reserve\"\n  .. .. ..$ station: chr \"233221A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Native Hut Creek @ Turtle Bend Park Teesdale\"\n  .. .. ..$ station: chr \"233242A\"\n  .. .. ..$ stntype: chr \"HYD\"\n\n\nthat seems to have worked. Unpack- remember we’ve used a field_list here so it’s simpler and we don’t need to have the select we had above. In the package we should probably allow doing that both ways.\n\ndb <- as_tibble(rbody_geo[2]) %>% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %>% # a `return` list\n  unnest_wider(col = where(is.list))\n\nTODO clean that up into a function and include. And test where we know there’s rain gauges- am I losing those with the stntypes I’ve gone with?\nallow field_list\nand geo_filter- all three kinds\nhow to do stntype- could have one option that allows actually directly passing lists to complex filter (or filter_values).\nOR (and?) could have an option that builds that list for the user somehow. would need to limit it to a few common things like stntype. *could I do this by saying if filter_values gets a c() of things for any of its keys, build an OR complex_filter. ie filter_values can only have one stntype. So if it gets c(stntype1, stntype2), just short-circuit and use complex instead. Would need to be careful if user passes more than one filtering thing in. In that case, just make them build the complex filter themselves. maybe provide a test_complex_filter function that just returns req_dry_run?"
  },
  {
    "objectID": "website_notes/quarto_website_github.html",
    "href": "website_notes/quarto_website_github.html",
    "title": "Quarto website",
    "section": "",
    "text": "I want to use quarto to build a website hosted on github pages. I have a few goals for it, but step one is to figure out how to do it.\nI’ve already started a github pages repo, and had started putting things in it before I realised I was probably not working in the best way (and some things needed to be private). So before any commits, I moved the work out and want to just start clean and see how to do it. I’ll walk through the process here.\nI’ll start by following the quarto docs, but may diverge. Using the Rstudio version, but will likely use a bit of VS too for python."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#set-up-github-pages",
    "href": "website_notes/quarto_website_github.html#set-up-github-pages",
    "title": "Quarto website",
    "section": "Set up github pages",
    "text": "Set up github pages\nI did this a while ago, will come back to it.\nClone the repo locally."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#start-as-a-website-project",
    "href": "website_notes/quarto_website_github.html#start-as-a-website-project",
    "title": "Quarto website",
    "section": "Start as a website project",
    "text": "Start as a website project\nCould I have converted from a normal project? Probably. And I could only get to the ‘quarto website’ option if I made a new directory. So even though I already cloned the repo from github, I put the project in a new dir, and then will copy it into the repo (I guess?).\nI actually made it in a dir, then started a git repo in that dir and set its remote to hit the url of my github.io repo following the instructions on github for starting a local repo and pointing it to github.\nProject seems to work when I click render, though it renders in browser not in Viewer pane (which is actually nicer, just not what the docs say).\nI had my repo in dropbox, as that seems like it usually works fine for other repos and gives another layer of backup. But it was failing here with lots of errors about files being in use by other processes. Moved it to Documents and seems to work fine."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#setting-up-nav",
    "href": "website_notes/quarto_website_github.html#setting-up-nav",
    "title": "Quarto website",
    "section": "Setting up nav",
    "text": "Setting up nav\nI’m not entirely sure what I want the structure to be, but likely a brief home page, navbar at top with things like ‘Research’, ‘About’, ‘Code examples’, etc. Lots of options here, I guess just cobble something together quickly.\nOne question I have is what happens when I start committing to git. Does it auto-publish? It looks like no, according to quarto, if I set up to render to docs and don’t push master. Or if I publish from a gh-pages branch though that’s not working on windows.\nThe .yaml seems to be where all the website structure goes- nav bars, search, etc.\nI think I’m going to end up with something fairly complex for nav, but for now, maybe try broad categories across the top, then specifics down the side. Add additional nesting later.\nSeems reasonably ok, with ability to have sections within contents in the sidebar (I think)."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#questions",
    "href": "website_notes/quarto_website_github.html#questions",
    "title": "Quarto website",
    "section": "Questions",
    "text": "Questions\nIf I render a single file, does it render the whole website? seems like yes. If I want to render single pages (like to test them without having to re-render everything), can use the terminal quarto render filename.qmd or a subdir quarto render subdir/. The output ends up in the _site directory."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#pushing-to-github",
    "href": "website_notes/quarto_website_github.html#pushing-to-github",
    "title": "Quarto website",
    "section": "Pushing to github",
    "text": "Pushing to github\nThe publishing the gh-pages branch seems the nicest, but there’s a bit warning not to do that on Windows. So, I guess I’ll do the render to docs way.\nadd output-dir: docs to the _quarto.yml and then create a .nojekyll file. Then quarto render to render to docs. I think I’ll also add a _quarto.yml.local with\nexecute:\n  cache: true\nto cache output and avoid long re-renders ( I hope). Seems to- re-clicking render was much faster.\nTo set to docs, go to repo, then settings –> Pages (on left) –> deploy from a branch, and choose the branch (likely Main) and /docs instead of /root.\nSo, I’ve been developing on dev, I guess I’ll merge main and see what happens.\nDon’t forget to merge and push ‘main’ if using the publish to /docs method. Otherwise no changes will actually appear."
  },
  {
    "objectID": "website_notes/updating_website.html",
    "href": "website_notes/updating_website.html",
    "title": "Updating website",
    "section": "",
    "text": "I often work on a bunch of pages and actually merge and push rarely. To identify pages that are present in the repo but not yet in the yaml (and so will not be included in the website), I wrote a little function that finds the files not in the yaml and also qmds in the yml but without a file, as when we change filenames. It’s available in the functions dir and here:\n\nfind_missing_pages <- function() {\n  qmd_in_proj <- list.files(pattern = \"*.qmd\", recursive = TRUE)\n  qmd_in_yml <- readLines(\"_quarto.yml\") |> \n    purrr::keep(\\(x) grepl(\".qmd\", x)) |> \n    gsub('\\\\s', '', x = _) |> \n    gsub('-', '', x = _) |> \n    gsub('href:', '', x = _)\n  # The values in the `render` section are still there, but that's ok\n  \n  not_present <- which(!(qmd_in_proj %in% qmd_in_yml))\n  \n  missing_pages <- qmd_in_proj[not_present]\n  \n  no_file <- which(!(qmd_in_yml %in% qmd_in_proj))\n  \n  broken_links <- qmd_in_yml[no_file]\n\n  \n  return(list(missing_pages = missing_pages, no_file_present = broken_links))\n}\n\n\n\n\nI keep getting dead links, because I think I don’t understand how links of the form\n[words](path/to/quarto_doc.qmd)\nwork. Do I need the path? Should the path be relative to the root, or the particular document being rendered?\nTo test, this doc is in the /website_notes folder, as is quarto_website_github.qmd, while another set of small helpers is in /small_helpers/smallpieces.qmd. I’m going to try to link to both of those in a couple different ways and see what works. Actual text is in code chunks so it’s visible.\nno outer dir, same folder quarto_website_github.qmd : WORKS\n[quarto_website_github.qmd](quarto_website_github.qmd)\nno outer dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](smallpieces.qmd)\nouter dir, same folder quarto_website_github.qmd : FAILS\n[quarto_website_github.qmd](website_notes/quarto_website_github.qmd)\nouter dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](small_helpers/smallpieces.qmd)\nreferenced to file dir, different folder smallpieces.qmd : (doesn’t make sense to do this for the same folder, since the implication is that the reference is to the directory with the qmd in it). WORKS\n[smallpieces.qmd](../small_helpers/smallpieces.qmd)\nSo, LINKS NEED TO BE RELATIVE TO THE LOCATION OF THE FILE. That’s really annoying, and I thought would have been controlled by setting execute-dir: project in the yaml, but that doesn’t seem to be the case.\nIs there a good way to find and fix dead links? Need to look.\n\n\n\nVery frequently pushes to main will still not seem to work- the website will remain in the old state, despite being sure we’ve pushed to main and the local render and preview has all the new content. The issue seems to be caching, and I’m not sure why it’s so aggressive. In chrome, you can open the Inspector, then right-click on the refresh button and then “empty cache and hard reload”. Or incognito window, or clear the cache some other way. It’s very annoying though.\n\n\n\nI had thought that pushing the ‘Render’ button in Rstudio built the whole site- it throws errors for other qmds than the one I currently build. BUT, if that’s all I do and then push and publish, pages that I haven’t specifically rendered are outdated- e.g. the sidebar and content don’t show updates. So it seems that we have to actually quarto render at the terminal first? I think- I’m trying to do that and all the R-python pages have broken.\nAh. I see now that there’s a warning about this in the quarto docs:\n\nAs you preview your site, pages will be rendered and updated. However, if you make changes to global options (e.g. _quarto.yml or included files) you need to fully re-render your site to have all of the changes reflected. Consequently, you should always fully quarto render your site before deploying it, even if you have already previewed changes to some pages with the preview server.\n\n\n\n\nIf there is a folder called ‘website’ in the repo, quarto will throw an error on render ‘unsupported project type ../website’. The solution seems to be to change the folder name."
  }
]