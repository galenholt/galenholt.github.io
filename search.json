[
  {
    "objectID": "website_notes/updating_website.html",
    "href": "website_notes/updating_website.html",
    "title": "Updating website",
    "section": "",
    "text": "I often work on a bunch of pages and actually merge and push rarely. To identify pages that are present in the repo but not yet in the yaml (and so will not be included in the website), I wrote a little function that finds the files not in the yaml and also qmds in the yml but without a file, as when we change filenames. It’s available in the functions dir and here:\n\nfind_missing_pages &lt;- function(files = NULL, pattern = \"_quarto.*.yml\") {\n\n  # This deals with things like profiles with multiple ymls\n  if (is.null(files)) {\n    files &lt;- list.files(pattern = pattern)\n  }\n\n  qmd_in_proj &lt;- list.files(pattern = \"*.qmd\", recursive = TRUE)\n\n  qmd_in_yml &lt;- vector(mode = 'character')\n  for (f in files) {\n    qmd_in_file &lt;- readLines(f) |&gt;\n      purrr::keep(\\(x) grepl(\".qmd\", x)) |&gt;\n      gsub('\\\\s', '', x = _) |&gt;\n      gsub('-', '', x = _) |&gt;\n      gsub('href:', '', x = _)\n\n    qmd_in_yml &lt;- c(qmd_in_yml, qmd_in_file)\n  }\n\n  qmd_in_yml &lt;- unique(qmd_in_yml)\n\n  # The values in the `render` section are still there, but that's ok\n\n  not_present &lt;- which(!(qmd_in_proj %in% qmd_in_yml))\n\n  missing_pages &lt;- qmd_in_proj[not_present]\n\n  no_file &lt;- which(!(qmd_in_yml %in% qmd_in_proj))\n\n  broken_links &lt;- qmd_in_yml[no_file]\n\n\n  return(list(missing_pages = missing_pages, no_file_present = broken_links))\n}\n\n\n\n\nI keep getting dead links, because I think I don’t understand how links of the form\n[words](path/to/quarto_doc.qmd)\nwork. Do I need the path? Should the path be relative to the root, or the particular document being rendered?\nTo test, this doc is in the /website_notes folder, as is quarto_website_github.qmd, while another set of small helpers is in /small_helpers/smallpieces.qmd. I’m going to try to link to both of those in a couple different ways and see what works. Actual text is in code chunks so it’s visible.\nno outer dir, same folder quarto_website_github.qmd : WORKS\n[quarto_website_github.qmd](quarto_website_github.qmd)\nno outer dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](smallpieces.qmd)\nouter dir, same folder quarto_website_github.qmd : FAILS\n[quarto_website_github.qmd](website_notes/quarto_website_github.qmd)\nouter dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](small_helpers/smallpieces.qmd)\nreferenced to file dir, different folder smallpieces.qmd : (doesn’t make sense to do this for the same folder, since the implication is that the reference is to the directory with the qmd in it). WORKS\n[smallpieces.qmd](../small_helpers/smallpieces.qmd)\nSo, LINKS NEED TO BE RELATIVE TO THE LOCATION OF THE FILE. That’s really annoying, and I thought would have been controlled by setting execute-dir: project in the yaml, but that doesn’t seem to be the case.\nIs there a good way to find and fix dead links? Need to look.\n\n\n\nVery frequently pushes to main will still not seem to work- the website will remain in the old state, despite being sure we’ve pushed to main and the local render and preview has all the new content. The issue seems to be caching, and I’m not sure why it’s so aggressive. In chrome, you can open the Inspector, then right-click on the refresh button and then “empty cache and hard reload”. Or incognito window, or clear the cache some other way. It’s very annoying though.\n\n\n\nI had thought that pushing the ‘Render’ button in Rstudio built the whole site- it throws errors for other qmds than the one I currently build. BUT, if that’s all I do and then push and publish, pages that I haven’t specifically rendered are outdated- e.g. the sidebar and content don’t show updates. So it seems that we have to actually quarto render at the terminal first? I think- I’m trying to do that and all the R-python pages have broken.\nAh. I see now that there’s a warning about this in the quarto docs:\n\nAs you preview your site, pages will be rendered and updated. However, if you make changes to global options (e.g._quarto.yml or included files) you need to fully re-render your site to have all of the changes reflected. Consequently, you should always fully quarto render your site before deploying it, even if you have already previewed changes to some pages with the preview server.\n\nSo, since I’m using profiles, run\nquarto render --profile fullsite\n\n\n\nI’m now using\nquarto publish gh-pages\nAnd, since I use profiles, that will only build the first profile. So, pass the profile argument:\nquarto publish gh-pages --profile fullsite\n\n\n\nIf there is a folder called ‘website’ in the repo, quarto will throw an error on render ‘unsupported project type ../website’. The solution seems to be to change the folder name.\n\n\n\nI get a lot of errors rendering this site. Often the solution is to trash the _cache and _filesdirectories for the offending files, along with any generated .html or .md . What seems to work the best though, is to delete everything in .Rproj.user. And often restart Rstudio.\nSome common errors are\nError: path for html_dependency not found: C:/Users/username/AppData/Local/Temp/Rtmp...\nAn error about permissions being denied (os error 5) trying to move site_libs: ERROR: PermissionDenied: Access is denied. (os error 5), rename 'path/to/repo/site_libs' -&gt; 'path/to/repo/docs/site_libs'\nPart of the issue seems to be Dropbox, even if the code is only in a backup directory and not a fully-synced directory. It sometimes works to quit dropbox, but sometimes we need to do that and restart.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Updating and maintaining website"
    ]
  },
  {
    "objectID": "website_notes/updating_website.html#website",
    "href": "website_notes/updating_website.html#website",
    "title": "Updating website",
    "section": "",
    "text": "I often work on a bunch of pages and actually merge and push rarely. To identify pages that are present in the repo but not yet in the yaml (and so will not be included in the website), I wrote a little function that finds the files not in the yaml and also qmds in the yml but without a file, as when we change filenames. It’s available in the functions dir and here:\n\nfind_missing_pages &lt;- function(files = NULL, pattern = \"_quarto.*.yml\") {\n\n  # This deals with things like profiles with multiple ymls\n  if (is.null(files)) {\n    files &lt;- list.files(pattern = pattern)\n  }\n\n  qmd_in_proj &lt;- list.files(pattern = \"*.qmd\", recursive = TRUE)\n\n  qmd_in_yml &lt;- vector(mode = 'character')\n  for (f in files) {\n    qmd_in_file &lt;- readLines(f) |&gt;\n      purrr::keep(\\(x) grepl(\".qmd\", x)) |&gt;\n      gsub('\\\\s', '', x = _) |&gt;\n      gsub('-', '', x = _) |&gt;\n      gsub('href:', '', x = _)\n\n    qmd_in_yml &lt;- c(qmd_in_yml, qmd_in_file)\n  }\n\n  qmd_in_yml &lt;- unique(qmd_in_yml)\n\n  # The values in the `render` section are still there, but that's ok\n\n  not_present &lt;- which(!(qmd_in_proj %in% qmd_in_yml))\n\n  missing_pages &lt;- qmd_in_proj[not_present]\n\n  no_file &lt;- which(!(qmd_in_yml %in% qmd_in_proj))\n\n  broken_links &lt;- qmd_in_yml[no_file]\n\n\n  return(list(missing_pages = missing_pages, no_file_present = broken_links))\n}\n\n\n\n\nI keep getting dead links, because I think I don’t understand how links of the form\n[words](path/to/quarto_doc.qmd)\nwork. Do I need the path? Should the path be relative to the root, or the particular document being rendered?\nTo test, this doc is in the /website_notes folder, as is quarto_website_github.qmd, while another set of small helpers is in /small_helpers/smallpieces.qmd. I’m going to try to link to both of those in a couple different ways and see what works. Actual text is in code chunks so it’s visible.\nno outer dir, same folder quarto_website_github.qmd : WORKS\n[quarto_website_github.qmd](quarto_website_github.qmd)\nno outer dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](smallpieces.qmd)\nouter dir, same folder quarto_website_github.qmd : FAILS\n[quarto_website_github.qmd](website_notes/quarto_website_github.qmd)\nouter dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](small_helpers/smallpieces.qmd)\nreferenced to file dir, different folder smallpieces.qmd : (doesn’t make sense to do this for the same folder, since the implication is that the reference is to the directory with the qmd in it). WORKS\n[smallpieces.qmd](../small_helpers/smallpieces.qmd)\nSo, LINKS NEED TO BE RELATIVE TO THE LOCATION OF THE FILE. That’s really annoying, and I thought would have been controlled by setting execute-dir: project in the yaml, but that doesn’t seem to be the case.\nIs there a good way to find and fix dead links? Need to look.\n\n\n\nVery frequently pushes to main will still not seem to work- the website will remain in the old state, despite being sure we’ve pushed to main and the local render and preview has all the new content. The issue seems to be caching, and I’m not sure why it’s so aggressive. In chrome, you can open the Inspector, then right-click on the refresh button and then “empty cache and hard reload”. Or incognito window, or clear the cache some other way. It’s very annoying though.\n\n\n\nI had thought that pushing the ‘Render’ button in Rstudio built the whole site- it throws errors for other qmds than the one I currently build. BUT, if that’s all I do and then push and publish, pages that I haven’t specifically rendered are outdated- e.g. the sidebar and content don’t show updates. So it seems that we have to actually quarto render at the terminal first? I think- I’m trying to do that and all the R-python pages have broken.\nAh. I see now that there’s a warning about this in the quarto docs:\n\nAs you preview your site, pages will be rendered and updated. However, if you make changes to global options (e.g._quarto.yml or included files) you need to fully re-render your site to have all of the changes reflected. Consequently, you should always fully quarto render your site before deploying it, even if you have already previewed changes to some pages with the preview server.\n\nSo, since I’m using profiles, run\nquarto render --profile fullsite\n\n\n\nI’m now using\nquarto publish gh-pages\nAnd, since I use profiles, that will only build the first profile. So, pass the profile argument:\nquarto publish gh-pages --profile fullsite\n\n\n\nIf there is a folder called ‘website’ in the repo, quarto will throw an error on render ‘unsupported project type ../website’. The solution seems to be to change the folder name.\n\n\n\nI get a lot of errors rendering this site. Often the solution is to trash the _cache and _filesdirectories for the offending files, along with any generated .html or .md . What seems to work the best though, is to delete everything in .Rproj.user. And often restart Rstudio.\nSome common errors are\nError: path for html_dependency not found: C:/Users/username/AppData/Local/Temp/Rtmp...\nAn error about permissions being denied (os error 5) trying to move site_libs: ERROR: PermissionDenied: Access is denied. (os error 5), rename 'path/to/repo/site_libs' -&gt; 'path/to/repo/docs/site_libs'\nPart of the issue seems to be Dropbox, even if the code is only in a backup directory and not a fully-synced directory. It sometimes works to quit dropbox, but sometimes we need to do that and restart.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Updating and maintaining website"
    ]
  },
  {
    "objectID": "website_notes/render_pdf.html",
    "href": "website_notes/render_pdf.html",
    "title": "Quarto render pdfs",
    "section": "",
    "text": "Just like the word version, sometimes we want to render something other than just html. This is working through specific issues with pdf.\nformat:\n  pdf: default\nIt was just working for me to use elsevier-pdf: default in my header, but now it’s not working to just use simple pdf. I get the error\ncompilation failed- no matching packages\nLaTeX Error: File `scrartcl.cls' not found.\nConsensus seems to be to reinstall tinytex. quarto install tinytex says it’s installed but will update.\nThat did get it to work. I think somehow I had tinytex, but not the KOMA Script documentclasses Quarto uses by default.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Rendering to pdf"
    ]
  },
  {
    "objectID": "website_notes/quarto_profiles.html",
    "href": "website_notes/quarto_profiles.html",
    "title": "Quarto profiles",
    "section": "",
    "text": "Quarto projects with lots of pieces (e.g. websites or books) are great, but once they get big, rendering single documents can be a pain. By default, both the Rstudio ‘Render’ button and quarto render document_name.qmd at the command line render the whole project as defined in _quarto.yml. They use pre-renders for other pages, but always seem to end up re-rendering a lot of other pages. If some of those pages break (e.g. works in progress), the whole process falls down. If we’re working on a single document and want to test its rendering, that can be very annoying. For example, if I want to render this document to check it, I don’t want to render an entire website, including other pages that are still in development.\nQuarto profiles are a partial solution, but need a bit of tweaking to set up.\nThe basic idea of profiles is to keep the common bits in _quarto.yml, and then have changes in _quarto-profilename.yml. The examples are all about rendering different versions of complex projects, with defined sets of pages to be rendered in the different profiles. Here, we have some specific requirements that aren’t obvious from the examples:\nNote- if using profiles, the argument needs to be passed to publish as well as render, e.g",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Quarto profiles for incremental render"
    ]
  },
  {
    "objectID": "website_notes/quarto_profiles.html#implementation",
    "href": "website_notes/quarto_profiles.html#implementation",
    "title": "Quarto profiles",
    "section": "Implementation",
    "text": "Implementation\nWe set up a general _quarto.yml file that contains the website headers and overall formatting code. We then have a _quarto-fullsite.yml with the full website build (structure of the pages). Finally, we have a _quarto-singlefile.yml without the structure of the website.\nIn the main _quarto.yml, I set\nprofile:\n  group:\n    - [singlefile, fullsite]\nso that pressing the render button or using quarto render at the terminal defaults to the singlefile.\nThen, to render the whole thing, use quarto render --profile fullsite. And quarto publish gh-pages --profile fullsite works for the github pages publish.\nThe division of what goes in the main _quarto.yml and _quarto-fullsite.yml will be project-dependent. The main issue here is how to specify _quarto-singlefile.yml.\n\nSinglefile - simple\nOne option is for _quarto-singlefile.yml to consist only of\nproject:\n  render:\n    - \"!*.qmd\"\nwhich is a bit surprising- it has all rendering turned off. This takes advantage of quarto rendering the active page even when it is not supposed to be part of a project.\nThe catch is that this approach works by just bypassing the rest of the yaml options. And so the rest of the website structure (color schemes, headers, etc) aren’t there, and more critically, if the execute_dir has been set to project (as I typically do), that gets lost and the working directory reverts to the file directory. Note that it doesn’t work to put execute-dir: project in the _quarto-singlefile.yml , because the workaround here bypasses all the render arguments.\nTo check the working directory as we try different things,\n\ngetwd()\n\n[1] \"C:/Users/galen/Documents/code/web_testing/galen_website\"\n\n\n\n\nSinglefile- extra step\nThese issues go away if we explicitly put the filename in the render: slot of the yaml instead of \"!*.qmd. However, the point is to render the active document, and so hardcoding this isn’t an option.\nA temporary workaround is to write the yaml from R and then render. Make a simple yaml. This can’t be in the notebook as here, because it won’t exist until during the render process, and it’s needed to control that process. So, this would need to be run at the console pre-render. I have this simple version commented out, because the later one is better.\n```{r}\nmake_simpleyml &lt;- function(renderfile) {\n    simple_yaml &lt;- list()\n    simple_yaml$project &lt;- list()\n    simple_yaml$project$render &lt;- list(renderfile)\n  yaml::write_yaml(simple_yaml, '_quarto-singlefile.yml')\n}\n```\nThen, calling this creates the singlefile we want.\n```{r}\nmake_simpleyml('website_notes/quarto_profiles.qmd')\n\n```\nAnd if we use rstudio, we can use it to auto-generate, but only interactively (rstudio is not running when quarto renders). In that case, this works if we run it ad-hoc or if we pre run all before rendering.\n\nmake_simpleyml &lt;- function(renderfile = 'auto') {\n\n  if (renderfile == 'auto') {\n    if (rstudioapi::isAvailable()) {\n      projpath &lt;- rstudioapi::getActiveProject()\n      docpath &lt;- rstudioapi::documentPath()\n      projdir &lt;- sub(\".*/([^/]+)$\", \"\\\\1\", projpath)\n      reldocpath &lt;- sub(paste0(\".*\", projdir, \"/\"), \"\", docpath)\n      renderfile &lt;- reldocpath\n    } else {\n      rlang::inform(\"Rstudio not running, do not want new profiles created while rendering, skipping\")\n      return(invisible())\n    }\n\n\n  }\n\n\n  simple_yaml &lt;- list()\n  simple_yaml$project &lt;- list()\n  simple_yaml$project$render &lt;- list(renderfile)\n  yaml::write_yaml(simple_yaml, '_quarto-singlefile.yml')\n}\n\nAnd now it works to just call that function with a document open. It can be done in a chunk, as here, but that only works while interactively working with the notebook. Rendering fails because it needs Rstudio to be running to use the renderfile = 'auto'. However, it can also be done from the console and still grabs the active file, which is probably fine because this needs to happen pre-render anyway.\n\nmake_simpleyml()\n\nRstudio not running, do not want new profiles created while rendering, skipping\n\n\nNote that using full vs relative paths for renderfile matters. If we use full paths (e.g. C://…) It overrides what _quarto.yml has in execute_dir, and sets it back to the file.\n\n# Works\n# make_simpleyml(reldocpath)\n\n# Sets the working directory to the file\n# make_simpleyml(fulldocpath)",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Quarto profiles for incremental render"
    ]
  },
  {
    "objectID": "website_notes/quarto_profiles.html#other-approaches-that-dont-work-yet",
    "href": "website_notes/quarto_profiles.html#other-approaches-that-dont-work-yet",
    "title": "Quarto profiles",
    "section": "Other approaches that don’t work (yet)",
    "text": "Other approaches that don’t work (yet)\nCan we use lua? It will print to markdown, but will it work in an R chunk? First, install lua-env with quarto add mcanouil/quarto-lua-env, then put\n# filters: \n#   - lua-env\nin the header. Then\n\n# {{&lt; lua-env quarto.doc.input_file &gt;}}\n\ngives the input_file. But I can’t get it to work in an R chunk and it’s REALLY buggy even in markdown so I’ve had to put it in an R block and comment it out to even run this file without errors. Ideally, we want to auto-detect the active script, and generate the needed yml when the Render button is pressed.\nPre-render scripts look like they have some useful environment variables, but in trying them I can’t seem to access those variables, and they seem to run after the yaml setup and before the render, so also don’t work for yaml modification.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Quarto profiles for incremental render"
    ]
  },
  {
    "objectID": "website_notes/quarto_change_yml.html",
    "href": "website_notes/quarto_change_yml.html",
    "title": "Switching quarto ymls",
    "section": "",
    "text": "They’re not perfect, but Quarto profiles work better than what’s below. I’ve mostly been able to get them to work, and the issues they have are less annoying that the issues with the solution here. So, use that (and the function make_simpleyml(). I’m leaving this here because it’s a bit more flexible.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Avoiding rendering whole site for testing"
    ]
  },
  {
    "objectID": "website_notes/quarto_change_yml.html#end-result",
    "href": "website_notes/quarto_change_yml.html#end-result",
    "title": "Switching quarto ymls",
    "section": "",
    "text": "They’re not perfect, but Quarto profiles work better than what’s below. I’ve mostly been able to get them to work, and the issues they have are less annoying that the issues with the solution here. So, use that (and the function make_simpleyml(). I’m leaving this here because it’s a bit more flexible.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Avoiding rendering whole site for testing"
    ]
  },
  {
    "objectID": "website_notes/quarto_change_yml.html#old-attempts",
    "href": "website_notes/quarto_change_yml.html#old-attempts",
    "title": "Switching quarto ymls",
    "section": "Old attempts",
    "text": "Old attempts\nQuarto projects with lots of pieces (e.g. this website) are great, but once they get big, rendering single documents can be a pain. By default, both the Rstudio ‘Render’ button and quarto render document_name.qmd at the command line render the whole project as defined in _quarto.yml. They supposedly use pre-renders for other pages, but always seem to end up re-rendering a bunch of things. If we’re working on a single document and want to test its rendering, that can be very annoying. For example, if I want to render this document to check it, I don’t want to render the entire website. I’m surprised quarto itself doesn’t have a way to switch behaviour. (I think profiles are that way.See the testing doc.\nThe workaround developed below does a file rename/swap with _quarto.yml- have one simple version and one complex version, and trade them out.\nWe want to have a directory _yml to store various .yaml files, and then have a simple call to choose between them. To get there, we need to\n\nBuild the _yml directory\nMove existing .yaml files in (presumably the main project definition)\nGenerate a minimal .yaml to use to render single notebooks\nWrite a simple function to move the desired .yaml to the main project directory and name it _quarto.yaml.\n\nCould this be done most cleanly inside Quarto itself? Yes. It would be really nice to be able to do something like quarto render filename.qmd -simple or quarto render filename.qmd -website. But until that’s the case, maybe this will work.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Avoiding rendering whole site for testing"
    ]
  },
  {
    "objectID": "website_notes/quarto_change_yml.html#setup",
    "href": "website_notes/quarto_change_yml.html#setup",
    "title": "Switching quarto ymls",
    "section": "Setup",
    "text": "Setup\nFirst, set up a new structure programatically. You could always just add files to this manually, if you want a bunch of different yamls. This assumes a single _quarto_project.yml with the complete project definition. In the simplest case, make_multi_yaml just makes the _yml directory. But it can also copy over an existing _quarto.yml and make a simple version.\nNote- this shouldn’t be dangerous to run again- the overwrite argument to file.copy is FALSE by default, so it shouldn’t overwrite the _quarto_project.yml with a simpler version later, but still makes me nervous.\n\nmake_multi_yaml &lt;- function(yamdir = '_yml', \n                            copy_orig = TRUE, \n                            make_simple = TRUE,\n                            leave_orig = TRUE) {\n  \n  \n  if (!dir.exists(yamdir)) {dir.create(yamdir)}\n  \n  if (copy_orig) {\n      file.copy('_quarto.yml', file.path(yamdir, '_quarto_project.yml'))\n      if (!leave_orig) {file.remove('_quarto.yml')}\n  }\n\n  if (make_simple) {\n    make_simple_yaml(file.path(yamdir, '_quarto_project.yml'), yamdir = yamdir)\n  }\n}\n\nThis function just makes a simple yaml from the main one.\n\nmake_simple_yaml &lt;- function(proj_yaml_file = NULL,\n                             yamdir = getwd(), \n                             simple_file = NULL) {\n  # By default, assume yaml is in working directory/_quarto.yml\n  if (is.null(proj_yaml_file)) {\n    proj_yaml_file &lt;- file.path(yamdir, '_quarto.yml')\n  }\n  \n  proj_yaml &lt;- yaml::read_yaml(proj_yaml_file)\n  \n  simple_yaml &lt;- list() \n  simple_yaml$project &lt;- proj_yaml$project\n  # kill the type in case it's complex (e.g. website, book)\n  simple_yaml$project$type &lt;- NULL\n  # kill render options- only rendering a single doc should allow rendering that doc and no others\n  simple_yaml$project$render &lt;- NULL\n  \n  if (is.null(simple_file)) {\n    simple_file &lt;- file.path(yamdir, '_quarto_simple.yml')\n  }\n  \n  yaml::write_yaml(simple_yaml, simple_file)\n  \n}",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Avoiding rendering whole site for testing"
    ]
  },
  {
    "objectID": "website_notes/quarto_change_yml.html#switching",
    "href": "website_notes/quarto_change_yml.html#switching",
    "title": "Switching quarto ymls",
    "section": "Switching",
    "text": "Switching\nThe main functionality here is to simply copy over a desired .yaml file to the project directory and name it _quarto.yaml so it gets used on render.\n\nuse_quarto_yaml &lt;- function(yamfile = 'project', yamdir = '_yml') {\n  yamfiles &lt;- list.files(yamdir)\n  whichyam &lt;- grepl(yamfile, yamfiles)\n  if (sum(whichyam) &gt; 1) {stop('too many matching yaml files')}\n \n  file.copy(file.path(yamdir, yamfiles[whichyam]), '_quarto.yml', overwrite = TRUE)\n  \n  return(invisible())\n}\n\nThat uses partial matching, so we can run use_quarto_yaml('simple') to switch to that version prior to rendering. This will work whether or not we use the earlier functions to set up the directory- as long as we have unique names for the various yaml options, calling use_quarto_yaml switches which is active, e.g. 31b8e172-b470-440e-83d8-e6b185028602:dAB5AHAAZQA6AE8AUQBCAGoAQQBHAEkAQQBOAHcAQQA1AEEARwBVAEEATgBnAEIAagBBAEMAMABBAE8AQQBBADQAQQBEAGcAQQBaAEEAQQB0AEEARABRAEEAWQBRAEEAdwBBAEcAVQBBAEwAUQBBADUAQQBHAE0AQQBPAFEAQgBpAEEAQwAwAEEAWgBnAEIAbQBBAEQAWQBBAE4AdwBCAGoAQQBHAFEAQQBOAEEAQQB3AEEARABRAEEAWgBRAEEAMgBBAEQAQQBBAAoAcABvAHMAaQB0AGkAbwBuADoATgBBAEEANABBAEQAYwBBAE4AdwBBAD0ACgBwAHIAZQBmAGkAeAA6AAoAcwBvAHUAcgBjAGUAOgBZAEEAQgBnAEEARwBBAEEAZQB3AEIANwBBAEgASQBBAGYAUQBCADkAQQBBAG8AQQBkAFEAQgB6AEEARwBVAEEAWAB3AEIAeABBAEgAVQBBAFkAUQBCAHkAQQBIAFEAQQBiAHcAQgBmAEEASABrAEEAWQBRAEIAdABBAEcAdwBBAEsAQQBBAG4AQQBIAE0AQQBhAFEAQgB0AEEASABBAEEAYgBBAEIAbABBAEMAYwBBAEsAUQBBAEsAQQBHAEEAQQBZAEEAQgBnAEEAQQA9AD0ACgBzAHUAZgBmAGkAeAA6AA==:31b8e172-b470-440e-83d8-e6b185028602\nThis is not run here, because it would reset the yaml on the fly during render, which would be bad. It should be run interactively, just prior to rendering.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Avoiding rendering whole site for testing"
    ]
  },
  {
    "objectID": "website_notes/commenting_quarto.html",
    "href": "website_notes/commenting_quarto.html",
    "title": "Commenting quarto",
    "section": "",
    "text": "I’d like to be able to comment quarto documents (or at least the html output). The commenting section of the quarto docs suggests we can use Hypothes.is, Utterances, and Giscus. It’s unclear which is better, but all their other demos use Hypothes.is so I think I’ll start there.\nBased on a bit of testing, it looks like putting\ncomments:\n  hypothesis: true\nin the main yml file is all it takes to turn it on across a project. It also works in the yaml headers for single files. I’ve added it to the yaml header in this file, enabling commenting here.\nformat:\n  html:\n    comments:\n      hypothesis: true\nWhat’s nice is that turning on hypothes.is works on local renders, private repos, and public. I’m not entirely sure how it works on local renders, but it does. However it’s working, it persists across renders- if I render this file, comment on it, delete it, and re-render, the comments are still there. They go away if all the commented text is deleted, but come back if it’s re-added. Comments can always be deleted though.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Adding comments to html"
    ]
  },
  {
    "objectID": "website_notes/commenting_quarto.html#adding-commenting-ability",
    "href": "website_notes/commenting_quarto.html#adding-commenting-ability",
    "title": "Commenting quarto",
    "section": "",
    "text": "I’d like to be able to comment quarto documents (or at least the html output). The commenting section of the quarto docs suggests we can use Hypothes.is, Utterances, and Giscus. It’s unclear which is better, but all their other demos use Hypothes.is so I think I’ll start there.\nBased on a bit of testing, it looks like putting\ncomments:\n  hypothesis: true\nin the main yml file is all it takes to turn it on across a project. It also works in the yaml headers for single files. I’ve added it to the yaml header in this file, enabling commenting here.\nformat:\n  html:\n    comments:\n      hypothesis: true\nWhat’s nice is that turning on hypothes.is works on local renders, private repos, and public. I’m not entirely sure how it works on local renders, but it does. However it’s working, it persists across renders- if I render this file, comment on it, delete it, and re-render, the comments are still there. They go away if all the commented text is deleted, but come back if it’s re-added. Comments can always be deleted though.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Adding comments to html"
    ]
  },
  {
    "objectID": "website_notes/commenting_quarto.html#issues",
    "href": "website_notes/commenting_quarto.html#issues",
    "title": "Commenting quarto",
    "section": "Issues",
    "text": "Issues\nYou do need to sign up for Hypothes.is. There are free and education accounts, but it’s unclear what the differences are. Free seems to be working for me so far, but it hasn’t been very long.\nThe public/private annotations are confusing and pointless, and ‘Post to only me’ in Public doesn’t seem to work. It does seem to work to make a Private Group and just annotate there. In the dropdown in the annotation area, choose ‘New private group’ and then post in that (e.g. in the below, I’d select the JustMe group).\n\n\n\nHypothes.is dropdown\n\n\nGiscus looks like it’s more about leaving comments at the bottom of blog posts, not annotating documents. And it says it requires a public repo. I wonder if that’s actually true, or if commenting is just limited to people with repo access? Either way, it might be good at what it does, but I’m looking for highlights, comments on manuscripts, and so Hypothes.is looks like the only game in town.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Adding comments to html"
    ]
  },
  {
    "objectID": "stats_probability/spatial_ac.html",
    "href": "stats_probability/spatial_ac.html",
    "title": "Analysing spatially autocorrelated data with spaMM",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\nlibrary(tidyverse)\nlibrary(spaMM)\nlibrary(foreach)\ndevtools::load_all()\n\nℹ Loading galenR\nI often work with spatially autocorrelated data, and want to estimate fixed effects while dealing with autocorrelation (and typically, estimate the autocorrelation as well). I also often work with the spaMM package, which was built for this, but usually use it for other things. Here, I’ll see if it works for the sorts of data we often encounter. They’re often not quite what we’d think of as geostatistics, but and might be any sort of autocorrelated sequence, e.g. sections of stream.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Regressions with spatial autocorrelation"
    ]
  },
  {
    "objectID": "stats_probability/spatial_ac.html#testing",
    "href": "stats_probability/spatial_ac.html#testing",
    "title": "Analysing spatially autocorrelated data with spaMM",
    "section": "Testing",
    "text": "Testing\nI’ll test with a modeled autocorrelated sequence with known AC, and then modify with fixed effects.\nI only want a line, at least at first, so grab one row. (ac2d should be fixed to work with 1)\n\noned &lt;- ac2d(n_x = 100, n_y = 10, rho_x = 0.5, printStats = TRUE)\n\n[1] \"Mean of all points is 0.016\"\n[1] \"Var of all points is 1.139\"\n[1] \"Mean y AC is -0.109\"\n[1] \"Mean x AC is 0.45\"\n\noned &lt;- oned[1, ]\n\n\ncheckac &lt;- acf(oned)\n\n\n\n\n\n\n\ncheckac$acf[2]\n\n[1] 0.5322618\n\n\n\nJust the AC, no fixed effects\nCan spaMM just pick that up without a fixed effect? Ie the only thing there is the data, with no predictor.\n\nonedf &lt;- tibble(obs = oned, x = 1:length(obs))\n\n\nno_fix &lt;- fitme(obs ~ 1 + Matern(1|x), data = onedf, fixed = list(nu=0.5))\n\nAccording to spaMM, Matern with \\(\\nu = 0.5\\) fits a spatial correlation \\(\\exp(-\\rho d)\\) , which superficially looks a lot like what we have for the correlation length, \\(\\tau\\), where the autocorrelation at distance d is \\(A(d) = e^{(-d/\\tau)}\\). Since \\(A(d) = \\rho^d\\), \\(\\tau = -1/ln(\\rho)\\) (and \\(\\rho = exp{(-1/\\tau)}\\)). So is this really just fitting \\(1/\\tau\\) and calling it \\(\\rho\\)?\n\nget_ranPars(no_fix, which = 'corrPars')[[1]]$rho\n\n[1] 0.55831\n\n# used realised lag-1 rho rather than defined\n1/(-1/log(checkac$acf[2]))\n\n[1] 0.6306197\n\n\nSure looks like it.\n\nalltib &lt;- foreach(i = 0:9, \n                  .combine = bind_rows,\n                  .multicombine = TRUE) %do% {\n  oned &lt;- ac2d(n_x = 100, n_y = 10, rho_x = i/10, printStats = FALSE)\n  # doing this on the matrix does all the crosses. Don't want htat\n  rhotib &lt;- foreach(j = 1:nrow(oned),\n                    .combine = bind_rows,\n                    .multicombine = TRUE) %do% {\n        checkac &lt;- acf(oned[j, ], plot = FALSE)\n        onedf &lt;- tibble(obs = oned[j, ], x = 1:length(obs))\n        no_fix &lt;- fitme(obs ~ 1 + Matern(1|x), data = onedf, fixed = list(nu=0.5))\n        spamout &lt;- get_ranPars(no_fix, which = 'corrPars')[[1]]$rho\n        acftau &lt;- 1/(-1/log(checkac$acf[2]))\n        \n        tibble(spamout = spamout, acftau = acftau, \n               rho = i/10, tau = 1/(-1/log(i/10)))\n\n                    }\n  rhotib\n\n}\n\nWarning in log(checkac$acf[2]): NaNs produced\nWarning in log(checkac$acf[2]): NaNs produced\nWarning in log(checkac$acf[2]): NaNs produced\nWarning in log(checkac$acf[2]): NaNs produced\nWarning in log(checkac$acf[2]): NaNs produced\nWarning in log(checkac$acf[2]): NaNs produced\nWarning in log(checkac$acf[2]): NaNs produced\nWarning in log(checkac$acf[2]): NaNs produced\nWarning in log(checkac$acf[2]): NaNs produced\n\n\n\nalltib |&gt; \n  pivot_longer(-rho) |&gt; \nggplot(aes(x = rho, y = value, color = name)) +\n  stat_summary() +\n  ggtitle(\"All are 1/tau in the usual sense\")\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_summary()`).\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\n\n\nSo, spaMM does a good job once there’s about a rho of 0.2. Perhaps that’s step 1: ask if there’s enough AC to care. And if we’re getting 1/taus of &gt; 2 or 3, maybe say it’s likely not tellng us much? That’s rhos of\n\nexp(-1/ (1/2))\n\n[1] 0.1353353\n\nexp(-1/ (1/3))\n\n[1] 0.04978707\n\n\nWhich we wouldn’t call sig- see the ac plot above- it doesn’t go sig until 0.2, which is a 1/tau of\n\n1/(-1/log(0.2))\n\n[1] 1.609438\n\n\nObviously sample size will change that. Not sure if larger sample size will help spaMM, it’s slow when things get big.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Regressions with spatial autocorrelation"
    ]
  },
  {
    "objectID": "stats_probability/spatial_ac.html#testing-covariates",
    "href": "stats_probability/spatial_ac.html#testing-covariates",
    "title": "Analysing spatially autocorrelated data with spaMM",
    "section": "Testing covariates",
    "text": "Testing covariates\nOK, now let’s see if this continues to work and returns useful estimates for covariates. Lets say there’s some covariate. This is really just hand-building the example in the spaMM intro, but a bit more relevant to the issues we have here.\n\noned &lt;- ac2d(n_x = 100, n_y = 10, rho_x = 0.5, printStats = TRUE)\n\n[1] \"Mean of all points is 0.021\"\n[1] \"Var of all points is 0.941\"\n[1] \"Mean y AC is -0.066\"\n[1] \"Mean x AC is 0.411\"\n\noned &lt;- oned[1, ]\n\n\nfdf &lt;- tibble(obs = oned, x = 1:length(obs))\n\n\nCategorical\nso far, same as above. Now we want a categorical fixed effect (spaMM does quant, but we’ll really want both, I think)\n\nfdf$cat_q &lt;- sample(3, nrow(fdf), replace = TRUE)\nfdf &lt;- fdf |&gt; \n  # make additive, otherwise I'm adjusting the group variance, not the group means\n  mutate(resp_c = obs + cat_q,\n         cat = paste0('c', cat_q)) \n\n\nwithfix &lt;- fitme(resp_c ~ cat + Matern(1|x), data = fdf, fixed = list(nu=0.5))\n\n\nsummary(withfix)\n\nformula: resp_c ~ cat + Matern(1 | x)\nML: Estimation of corrPars, lambda and phi by ML.\n    Estimation of fixed effects by ML.\nEstimation of lambda and phi by 'outer' ML, maximizing logL.\nfamily: gaussian( link = identity ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)   0.6565   0.2148   3.057\ncatc2         1.2136   0.2245   5.405\ncatc3         2.1992   0.2204   9.976\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n     1.nu     1.rho \n0.5000000 0.7352398 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x  :  1.052  \n# of obs: 100; # of groups: x, 100 \n -------------- Residual variance  ------------\nphi estimate was 0.146829 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -141.2862\n\n\nThat’s pretty good, really\n\nggplot(fdf, aes(x = x, y = resp_c, color = cat)) + geom_point()\n\n\n\n\n\n\n\n\nHow does that compare to when I ignore ac?\n\nflm &lt;- lm(resp_c ~ cat, data = fdf)\nsummary(flm)\n\n\nCall:\nlm(formula = resp_c ~ cat, data = fdf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.78904 -0.81428  0.03124  0.83761  2.43251 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.9074     0.1824   4.974 2.84e-06 ***\ncatc2         0.7196     0.2731   2.634  0.00981 ** \ncatc3         1.9149     0.2598   7.369 5.72e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.095 on 97 degrees of freedom\nMultiple R-squared:  0.3626,    Adjusted R-squared:  0.3494 \nF-statistic: 27.59 on 2 and 97 DF,  p-value: 3.275e-10\n\n\nThat’s just as bad, really\n\n\nQuantitative, independent\nNow, as in the spaMM intro, let’s assume we have an independent quant covariate\n\nfdf$quant &lt;- sample(nrow(fdf))\nfdf &lt;- fdf |&gt; \n  mutate(resp_q = obs+quant*0.1)\n\nwithq &lt;- fitme(resp_q ~ quant + Matern(1|x), data = fdf, fixed = list(nu=0.5))\n\n\nsummary(withq)\n\nformula: resp_q ~ quant + Matern(1 | x)\nML: Estimation of corrPars, lambda and phi by ML.\n    Estimation of fixed effects by ML.\nEstimation of lambda and phi by 'outer' ML, maximizing logL.\nfamily: gaussian( link = identity ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)  -0.4451 0.226094  -1.969\nquant         0.1047 0.003067  34.143\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n     1.nu     1.rho \n0.5000000 0.9007638 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x  :  1.166  \n# of obs: 100; # of groups: x, 100 \n -------------- Residual variance  ------------\nphi estimate was 0.00183086 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -140.7463\n\n\nThat’s pretty good for both the rho and the estimate\n\n\nTrend\nNow, let’s say there’s a trend with x itself.\n\nfdf &lt;- fdf |&gt; \n  mutate(resp_trend = obs + x*0.1)\n\nwitht &lt;- fitme(resp_trend ~ x + Matern(1|x), data = fdf, fixed = list(nu=0.5))\n\n\nsummary(witht)\n\nformula: resp_trend ~ x + Matern(1 | x)\nML: Estimation of corrPars, lambda and phi by ML.\n    Estimation of fixed effects by ML.\nEstimation of lambda and phi by 'outer' ML, maximizing logL.\nfamily: gaussian( link = identity ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)  -0.3349 0.334140  -1.002\nx             0.1025 0.005722  17.905\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n     1.nu     1.rho \n0.5000000 0.7461973 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x  :  0.9383  \n# of obs: 100; # of groups: x, 100 \n -------------- Residual variance  ------------\nphi estimate was 0.227616 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -141.6942\n\n\nAlso does a good job with that.\n\n\nEverything?\nWhat happens if I just hit it with everything?\n\nfdf &lt;- fdf |&gt; \n  mutate(resp_all = obs + x*0.1 + 1*cat_q + quant*0.1)\n\nwithall &lt;- fitme(resp_all ~ x + cat + quant + Matern(1|x),\n                 data = fdf, fixed = list(nu=0.5))\n\n\nsummary(withall)\n\nformula: resp_all ~ x + cat + quant + Matern(1 | x)\nML: Estimation of corrPars, lambda and phi by ML.\n    Estimation of fixed effects by ML.\nEstimation of lambda and phi by 'outer' ML, maximizing logL.\nfamily: gaussian( link = identity ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)   0.2678 0.399366  0.6707\nx             0.1032 0.005860 17.6148\ncatc2         1.2070 0.221550  5.4482\ncatc3         2.1965 0.216833 10.1300\nquant         0.1046 0.003036 34.4505\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n     1.nu     1.rho \n0.5000000 0.8399614 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x  :  1.183  \n# of obs: 100; # of groups: x, 100 \n -------------- Residual variance  ------------\nphi estimate was 1.65693e-05 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -140.0889\n\n\nThat’s pretty good. Though noting that I don’t actually have any residual error on any of this. Well, except that the 2d ac has error variance epsilon. It’s not quite the same, but it’s something.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Regressions with spatial autocorrelation"
    ]
  },
  {
    "objectID": "stats_probability/spatial_ac.html#residuals",
    "href": "stats_probability/spatial_ac.html#residuals",
    "title": "Analysing spatially autocorrelated data with spaMM",
    "section": "Residuals",
    "text": "Residuals\n\nfdf &lt;- fdf |&gt; \n  mutate(resp_resid = obs + x*0.1 + 1*cat_q + quant*0.1 + rnorm(nrow(fdf), sd = sqrt(1)))\n\nwithresid &lt;- fitme(resp_resid ~ x + cat + quant + Matern(1|x),\n                 data = fdf, fixed = list(nu=0.5))\n\n\nsummary(withresid)\n\nformula: resp_resid ~ x + cat + quant + Matern(1 | x)\nML: Estimation of corrPars, lambda and phi by ML.\n    Estimation of fixed effects by ML.\nEstimation of lambda and phi by 'outer' ML, maximizing logL.\nfamily: gaussian( link = identity ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)  0.72999 0.618223   1.181\nx            0.09691 0.009226  10.505\ncatc2        1.32735 0.317452   4.181\ncatc3        2.42895 0.306107   7.935\nquant        0.09907 0.004381  22.613\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n     1.nu     1.rho \n0.5000000 0.2558873 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x  :  0.8774  \n# of obs: 100; # of groups: x, 100 \n -------------- Residual variance  ------------\nphi estimate was 1.21494 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -171.5012\n\n\nThat’s pretty good still. The random AC inflates some. Obviously it will change depending on the size of that residual variance. The sd of the data without the residual is\n\nsd(fdf$resp_all)\n\n[1] 4.209415",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Regressions with spatial autocorrelation"
    ]
  },
  {
    "objectID": "stats_probability/fitting_truncated.html",
    "href": "stats_probability/fitting_truncated.html",
    "title": "Fitting partial distributions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(fitdistrplus)\nlibrary(foreach)\nWe have some data that has a lot of zeros in it, but is otherwise reasonably modelled by a lognormal. We want to be able to shift the distribution to change the number of zeros, and so we need to fit that lognormal, and then shift it around.\nOne way to think about the problem is that there is some detection limit or process limit below which we get zeros in the data, but where theoretically the distribution continues. Specifically, we’re dealing with water flow, and so maybe it drops below the ability of the gauges to detect, but continues dropping below that. It may even drop underground (e.g river is dry), but to some approximation, water remains underground, and the amount can continue dropping.\nThus, we want to take the data we have, and say that any zeros are actually some small but unknown number, and fit the distribution. We can identify the zero/undetectable threshold as the smallest nonzero number. Then, we can simulate new data with a shifted distribution, which will have a different number of points below that threshold.\nHere, we want to figure out how to shift the data.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Shifting and fitting truncated lognormals"
    ]
  },
  {
    "objectID": "stats_probability/fitting_truncated.html#generate-data",
    "href": "stats_probability/fitting_truncated.html#generate-data",
    "title": "Fitting partial distributions",
    "section": "Generate data",
    "text": "Generate data\nMake the ‘true’ data\n\ntruedata &lt;- rlnorm(10000, 5, 1.5)\n\nSet the detection limit. Making this a variable since we need it in a few places and might want to change it.\n\ndetectionlimit &lt;- 10\n\n\ncensdata &lt;- truedata\ncensdata[censdata &lt; detectionlimit] &lt;- 0\n\nPlot those densities- big spike at 0 in censored (obviously).\n\ntc &lt;- tibble(truedata, censdata)\n\nggplot(tc) +\n  geom_density(aes(x = truedata), color = 'black') + \n  geom_density(aes(x = censdata), color = 'firebrick') + \n  xlim(-1, 100)\n\nWarning: Removed 6104 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 6104 rows containing non-finite outside the scale range\n(`stat_density()`).",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Shifting and fitting truncated lognormals"
    ]
  },
  {
    "objectID": "stats_probability/fitting_truncated.html#naive-fit",
    "href": "stats_probability/fitting_truncated.html#naive-fit",
    "title": "Fitting partial distributions",
    "section": "Naive fit",
    "text": "Naive fit\nWe can just use fitdistr to get a naive fit- it should be right for truedata but wrong for censdata. It’s not way off, but it is off.\n\nfit_true &lt;- fitdistr(truedata, densfun = 'lognormal')\nfit_cens &lt;- fitdistr(censdata[censdata &gt; 0], densfun = 'lognormal')\n\nfit_true\n\n    meanlog       sdlog   \n  5.01228084   1.49945363 \n (0.01499454) (0.01060274)\n\nfit_cens\n\n     meanlog        sdlog   \n  5.135346982   1.376120151 \n (0.014012165) (0.009908097)\n\n\nHow far off?\n\ndf_cdf &lt;- tibble(x = seq(0,1000, by = 0.1), \n                 cdf_true = plnorm(x, \n                             fit_true$estimate['meanlog'],\n                             fit_true$estimate['sdlog']),\n                 cdf_cens = plnorm(x, \n                             fit_cens$estimate['meanlog'],\n                             fit_cens$estimate['sdlog']))\n\n\nggplot() + \n  stat_ecdf(data = tc, mapping = aes(x = truedata), color = 'forestgreen') + \n  stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_true), color = 'darkseagreen') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens), color = 'deeppink2') +\n  coord_cartesian(xlim = c(-1, 2*detectionlimit), ylim = c(0, 0.1))\n\n\n\n\n\n\n\n\nThe empirical cdfs fit exactly after 10, because of the probability mass at 0 for the censored data. The fitted distributions are clearly very good for the true data, but quite a ways off for the censored.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Shifting and fitting truncated lognormals"
    ]
  },
  {
    "objectID": "stats_probability/fitting_truncated.html#censored-fit",
    "href": "stats_probability/fitting_truncated.html#censored-fit",
    "title": "Fitting partial distributions",
    "section": "Censored fit",
    "text": "Censored fit\nWe should be able to use fitdistrplus::fitdistcens, though we need some weird data manipulation. Our data is either left-censored or true. From the help, left should be NA for left-censored, or observed for non-censored, and right should be the right bound of the interval for interval-censored or the data for non-censored. I think that means we set it to the detection limit for the censored observations- e.g. it’s the right bound of a censoring interval from -infinity to detection limit. But the interval censoring is confusing me.\n\ncensframe &lt;- tc |&gt; \n  dplyr::mutate(left = ifelse(censdata == 0, NA, censdata),\n                right = ifelse(censdata == 0, detectionlimit, censdata)) |&gt; \n  dplyr::select(left, right)\n\nNow, fit that. There’s a bug where an internal error check fails for tibbles, so send it an old-fashioned df. (it asks for length(censdata[,1]), which is the length of the vector for data.frames, but because indexing remains a tibble, fails for tibbles).\n\nfit_cens2 &lt;- fitdistcens(censdata = data.frame(censframe), distr = 'lnorm')\n\nfit_cens2\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n        estimate\nmeanlog 5.013691\nsdlog   1.496170\n\n\nAdd that to the cdfs\n\ndf_cdf &lt;- df_cdf |&gt; \n  mutate(cdf_cens2 = plnorm(x, \n                             fit_cens2$estimate['meanlog'],\n                             fit_cens2$estimate['sdlog']),)\n\nAnd plot it\n\nggplot() + \n  stat_ecdf(data = tc, mapping = aes(x = truedata), color = 'forestgreen') + \n  stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_true), color = 'darkseagreen') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens), color = 'deeppink2') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens2), color = 'purple') +\n  coord_cartesian(xlim = c(-1, 2*detectionlimit), ylim = c(0, 0.1))",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Shifting and fitting truncated lognormals"
    ]
  },
  {
    "objectID": "stats_probability/fitting_truncated.html#shifted-to-censor",
    "href": "stats_probability/fitting_truncated.html#shifted-to-censor",
    "title": "Fitting partial distributions",
    "section": "Shifted to censor",
    "text": "Shifted to censor\nThe above is working, but when I go to apply it, there’s a slightly different issue. The data above is still bounded by 0, we’re just saying we can’t see it below some limit. And so the censored fitting allows us to fill in that bit of distribution between 0 and the limit.\nThe issue I actually encounter with my real data is that it looks like the curve simply cuts off at zero, but should go below. In other words, it looks like this:\n\ntc &lt;- tc |&gt; \n  mutate(shiftdata = truedata-detectionlimit,\n         shiftcens = ifelse(shiftdata &gt; 0, shiftdata, 0))\n\n\nggplot(tc) +\n  stat_ecdf(mapping = aes(x = shiftdata), color = 'grey80') +\n  stat_ecdf(mapping = aes(x = shiftcens), color = 'black') +\n  stat_ecdf(mapping = aes(x = truedata), color = 'forestgreen') + \n  stat_ecdf(mapping = aes(x = censdata), color = 'firebrick') +\n  coord_cartesian(xlim = c(-detectionlimit-1, 10*detectionlimit), ylim = c(0, 0.5))\n\n\n\n\n\n\n\n\nSo, it looks like the same problem, but we run into problems when we try to fit it. Because any function we try to fit will still be bounded by 0, not allow some unknown pseudo-tail to go lower than 0.\nFit it. where do we put the detection limit in this case? It shouldn’t be the same as above, necessarily, because we do have numbers between 0 and 10 (or whatever it is). I’ll choose 1, but even that’s arbitrary. Changing the left to NA anything less than whatever that value is so we don’t have a few numbers and then the spike.\n\nshiftdetect &lt;- 1\ncensframeshift &lt;- tc |&gt; \n  dplyr::mutate(left = ifelse(shiftcens &lt; shiftdetect, NA, shiftcens),\n                right = ifelse(shiftcens == 0, shiftdetect, shiftcens)) |&gt; \n  dplyr::select(left, right)\n\nNow, fit that.\n\nfit_censshift &lt;- fitdistcens(censdata = data.frame(censframeshift), distr = 'lnorm')\n\nfit_censshift\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n        estimate\nmeanlog 4.752459\nsdlog   1.914297\n\n\nThat is quite a bit different than the previous\n\nfit_cens2\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n        estimate\nmeanlog 5.013691\nsdlog   1.496170\n\n\nAdd that to the cdfs\n\ndf_cdf &lt;- df_cdf |&gt; \n  mutate(cdf_censshift = plnorm(x, \n                             fit_censshift$estimate['meanlog'],\n                             fit_censshift$estimate['sdlog']))\n\nAnd plot it\n\nggplot() + \n  stat_ecdf(data = tc, mapping = aes(x = truedata), color = 'forestgreen') + \n  stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  stat_ecdf(data = tc, mapping = aes(x = shiftcens), color = 'black') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_true), \n            color = 'darkseagreen') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens), \n            color = 'deeppink2') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens2),\n            color = 'purple') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_censshift),\n            color = 'magenta') +\n  coord_cartesian(xlim = c(-1, 2*detectionlimit), ylim = c(0, 0.1))\n\n\n\n\n\n\n\n\nWe can see that the magenta line is a terrible fit, because it is trying to fit an actual lognormal, which is necessarily bound by 0.\nSo, we want to find a way to fit the bit of the lognormal we have, assuming that some portion has gone below 0.\n\nShift the distribution\nThis approach exactly parallels how we created the dummy data- we shift the distribution up some amount, call that the detection limit, and fit (and then shift the fit back down). Let’s first do that for the case we know should work- exactly mirroring the shift we used to create the data. Assuming that works, we’ll need to then explore the situation where we don’t know how far to shift.\nThis is a bit silly, because we are just recovering the test data, but to make the process clear (and retain the zeros in shiftcens), we create a shifted version of it. This should match the censored true data censdata for this precise re-shift, with the slight change in the location of the initial probability mass.\n\ntc &lt;- tc |&gt; \n  mutate(shiftcensback = shiftcens + detectionlimit)\n\n\nggplot(tc) + \n    stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  stat_ecdf(data = tc, mapping = aes(x = shiftcensback), color = 'cyan', linetype = 2) + xlim(c(0,100))\n\nWarning: Removed 6104 rows containing non-finite outside the scale range\n(`stat_ecdf()`).\nRemoved 6104 rows containing non-finite outside the scale range\n(`stat_ecdf()`).\n\n\n\n\n\n\n\n\n\nNow, we fit that shifted data. We’re back to censoring at the detectionlimit (or whatever the shift back is). Now, instead of a mass at 0, all that mass gets pushed to detectonlimit (e.g. min(tc$shiftcensback) is 10, not 0). So we have to use that as the censoring value.\n\ncensshiftback &lt;- tc |&gt; \n  dplyr::mutate(left = ifelse(shiftcensback &lt;= detectionlimit, NA, shiftcensback),\n                right = ifelse(shiftcensback &lt;= detectionlimit, detectionlimit, shiftcensback)) |&gt; \n  dplyr::select(left, right)\n\nfit_censshiftback &lt;- fitdistcens(censdata = data.frame(censshiftback), distr = 'lnorm')\n\nfit_censshiftback\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n        estimate\nmeanlog 5.013691\nsdlog   1.496170\n\n\nThat is the same as fit_cens2, which we know works. Plot to double check. Need to add to df_cdf. And make a shifted x that we can use to re-backshift the new cdf. If that makes any sense.\n\ndf_cdf &lt;- df_cdf |&gt; \n  mutate(cdf_censshiftback = plnorm(x, \n                             fit_censshiftback$estimate['meanlog'],\n                             fit_censshiftback$estimate['sdlog']),\n         x_shiftback = x-detectionlimit)\n\nNow, cdf_censshiftback should match the shifted-up distribution, and shifting it back down should match the ‘original’ distribution that looks like it should go below 0.\n\nggplot() + \n  stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  stat_ecdf(data = tc, mapping = aes(x = shiftcens), color = 'black') +\n   stat_ecdf(data = tc, mapping = aes(x = shiftcensback), \n             color = 'cyan', linetype = 2) +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_censshiftback), \n            color = 'deeppink2') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_censshift),\n            color = 'magenta') +\n    geom_line(data = df_cdf, mapping = aes(x = x_shiftback, y = cdf_censshiftback),\n            color = 'dodgerblue') +\n  coord_cartesian(xlim = c(-1, 2*detectionlimit), ylim = c(0, 0.1))\n\n\n\n\n\n\n\n\nSo, that seems to be working. Now, though, what if we don’t know how much to shift? E.g. we’ll have data with no obvious ‘true’ shift. Can we just shift it way off to make sure the cdf hits zero, and then shift it back?\nI think at this point I probably need to use a clean dataframe and set of variables- detectionlimit doesn’t really mean the same thing as ‘arbitrary shift’.\n\n\nUnknown shift\nI’m going to start all the way back at the beginning with a clean set of truedata and then shift it.\n\ntrueshift &lt;- 10\n\n\ndf_shift &lt;- tibble(truedata = truedata, \n                   shiftdata = truedata-trueshift, \n                   shiftcens = ifelse(shiftdata &gt; 0, shiftdata, 0))\n\nIs that what I think it is? Yes\n\nggplot(df_shift) +\n  stat_ecdf(aes(x = truedata), color = 'black') +\n  stat_ecdf(aes(x = shiftdata), color = 'firebrick') +\n  stat_ecdf(aes(x = shiftcens), color = 'dodgerblue') +\n  coord_cartesian(xlim = c(-10, 100), ylim = c(0, 0.5))\n\n\n\n\n\n\n\n\nSO, the blue line there (shiftcens) is what we think we have for the real data. We know from above that it works to fit it if we bump it back up by exactly what we bumped down to make it, because, unsurprisingly, it becomes exactly what it was.\nSo the question now is, can we shift that blue line back up some arbitrary amount (as long as it’s greater than the downshift), and still recover the distribution?\n\nupshift &lt;- 100\n\nNow, the min of that is upshift, not 0.\n\ndf_shift &lt;- df_shift |&gt; \n  mutate(shiftup = shiftcens + upshift)\n\nGet the fit for that. It clearly does not give the same values, by necessity.\n\nupcens &lt;- df_shift |&gt; \n  dplyr::mutate(left = ifelse(shiftup &lt;= upshift, NA, shiftup),\n                right = ifelse(shiftup &lt;= upshift, upshift, shiftup)) |&gt; \n  dplyr::select(left, right)\n\nfit_upcens &lt;- fitdistcens(censdata = data.frame(upcens), distr = 'lnorm')\n\nfit_upcens\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n         estimate\nmeanlog 5.6890233\nsdlog   0.9478606\n\n\nDo we at least do a good job fitting that shifted distribution?\n\ncdf_up &lt;- tibble(x = seq(0,1000, by = 0.1), \n                 cdf_shiftup = plnorm(x, \n                             fit_upcens$estimate['meanlog'],\n                             fit_upcens$estimate['sdlog']))\n\nThat’s terrible.\n\nggplot() +\n  stat_ecdf(data = df_shift, mapping = aes(x = shiftup)) +\n  geom_line(data = cdf_up, mapping = aes(x = x, y = cdf_shiftup), color = 'purple') +\n  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 0.5))\n\n\n\n\n\n\n\n\nWrite a function and loop to look at this over some range of shifts and clean up the dataframe.\n\nshiftfun &lt;- function(indata, upshift) {\n  # Shift the data\n  shiftdf &lt;- tibble(clipped = indata, shiftdat = indata + upshift, upshift)\n  \n  # Create censored dataset and fit\n  \n  # Handle the zero case- we just use the next value up\n  if (upshift == 0) {rightlim &lt;- min(indata[indata&gt;0])} else {rightlim &lt;- upshift}\n  \n  upcens &lt;- shiftdf |&gt; \n  dplyr::mutate(left = ifelse(shiftdat &lt;= upshift, NA, shiftdat),\n                right = ifelse(shiftdat &lt;= upshift, rightlim, shiftdat)) |&gt; \n  dplyr::select(left, right)\n\n  fit_up &lt;- fitdistcens(censdata = data.frame(upcens), distr = 'lnorm')\n\n  # This isn't ideal, but we can shove the cdf on here too, it just has rows that don't mean the same thing. prevents us saving a list though.\n  shiftdf &lt;- shiftdf |&gt; \n    mutate(x = row_number()/10,\n           cdf_up = plnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           pdf_up = dlnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           # Some diagnostics\n           fitloglik = fit_up$loglik,\n           fitsemean = fit_up$sd['meanlog'],\n           fitsesd = fit_up$sd['sdlog'])\n\n}\n\nLoop over a few values of the shift, from as-is, below true, true, and above true. Indata is the dummy for the data we actually have (ending at 0, but too high). Here, it’s in df_shift$shiftcens.\n\nabshift &lt;- foreach(upshift = c(0, 1, 5, 10, 20, 50, 100), .combine = bind_rows) %do% {\n  shiftfun(df_shift$shiftcens, upshift)\n}\n\n\nggplot(abshift) +\n  stat_ecdf(aes(x = shiftdat, color = factor(upshift))) +\n  geom_line(aes(x = x, y = cdf_up, color = factor(upshift)), linetype = 2) +\n  coord_cartesian(xlim = c(0,200), ylim = c(0, 0.5))\n\n\n\n\n\n\n\n\nI think maybe we need to minimize Kullback-Leibler, but we might be able to just look at the fit diagnostics\n\nabshift |&gt; \n  distinct(upshift, fitloglik, fitsemean, fitsesd) |&gt; \nggplot(aes(x = upshift, y = fitloglik)) + \n  geom_line() + geom_point() + \n  geom_label(aes(label = upshift), hjust = -0.1, vjust = -0.1, alpha = 0.2)\n\n\n\n\n\n\n\n\n\nabshift |&gt; \n  distinct(upshift, fitloglik, fitsemean, fitsesd) |&gt; \nggplot(aes(x = upshift)) + \n  geom_line(aes(y = fitsemean)) + geom_point(aes(y = fitsemean)) + \n  geom_line(aes(y = fitsesd), color = 'firebrick') + \n  geom_point(aes(y = fitsesd), color = 'firebrick') + \n  geom_label(aes(y = fitsemean, label = upshift), hjust = -0.1, vjust = -0.1, alpha = 0.2)\n\n\n\n\n\n\n\n\nSo, unsurprisingly, the thing to do is maximise log-likelihood. I could probably figure out a way to include a shift in the main function and use whatever the optimiser is doing internally.\nOr, I can use optim on the output?\n\nindata &lt;- df_shift$shiftcens\nshiftoptim &lt;- function(upshift) {\n  # Shift the data\n  shiftdf &lt;- tibble(clipped = indata, shiftdat = indata + upshift, upshift)\n  \n  # Create censored dataset and fit\n  \n  # Handle the zero case- we just use the next value up\n  if (upshift == 0) {rightlim &lt;- min(indata[indata&gt;0])} else {rightlim &lt;- upshift}\n  \n  upcens &lt;- shiftdf |&gt; \n  dplyr::mutate(left = ifelse(shiftdat &lt;= upshift, NA, shiftdat),\n                right = ifelse(shiftdat &lt;= upshift, rightlim, shiftdat)) |&gt; \n  dplyr::select(left, right)\n\n  suppressWarnings(fit_up &lt;- fitdistcens(censdata = data.frame(upcens), distr = 'lnorm'))\n  \n  return(-fit_up$loglik)\n}\n\nThis says to use optimize\n\noptim(1, shiftoptim)\n\nWarning in optim(1, shiftoptim): one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\n$par\n[1] 10.275\n\n$value\n[1] 67581.24\n\n$counts\nfunction gradient \n      28       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\noptimize(shiftoptim, interval = c(0, 1000))\n\n$minimum\n[1] 10.26533\n\n$objective\n[1] 67581.24\n\n\nLet’s write a version that takes the data, then optimizes to get the right upshift.\n\nfitshift &lt;- function(rawdata, shift_up) {\n  # Handle the zero case- we just use the next value up\n    if (shift_up == 0) {rightlim &lt;- min(rawdata[rawdata&gt;0])\n    } else {\n      rightlim &lt;- shift_up}\n  \n  inshift &lt;- rawdata + shift_up\n    \n    upcens &lt;- tibble(left = ifelse(inshift &lt;= shift_up, NA, inshift),\n                right = ifelse(inshift &lt;= shift_up, rightlim, inshift))\n    \n    suppressWarnings(fit_up &lt;- fitdistcens(censdata = data.frame(upcens),\n                                           distr = 'lnorm'))\n    \n    return(fit_up)\n}\n\n\nopt_up &lt;- function(shift_up, rawdata) {\n  \n  fit_up &lt;- fitshift(rawdata, shift_up)\n  \n  return(-fit_up$loglik)\n}\n\n\noptshift &lt;- function(rawdata) {\n  \n  \n  # get the optimal shift\n  shift &lt;- optimize(opt_up, interval = c(0, 1000), rawdata = rawdata)\n  \n  # Get the fit at that shift (would be nice to kick this out of opt_up somehow)\n  \n  fit_up &lt;- fitshift(rawdata, shift$minimum)\n  \n # Create a df for output\n  # The shifted data\n  shiftdf &lt;- tibble(orig_data = rawdata, \n                    shift_data = rawdata + shift$minimum, \n                    optimum_shift = shift$minimum)\n  \n\n   # This isn't ideal, but we can shove the cdf on here too, it just has rows that don't mean the same thing. prevents us saving a list though.\n  shiftdf &lt;- shiftdf |&gt; \n    mutate(x = row_number()/10,\n           cdf_up = plnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           pdf_up = dlnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           # Some diagnostics\n           fitloglik = fit_up$loglik)\n  \n  # and a shifted-back version of the cdf/pdf just needs a shifted x. The\n  # backshift of the data is just the original `rawdata`.\n  shiftdf &lt;- shiftdf |&gt; \n    mutate(x_back = x-shift$minimum)\n\n}\n\n\noptimal_fit &lt;- optshift(df_shift$shiftcens)\n\nThat’s not exactly 10, but is it close? The CDF looks pretty good.\n\nggplot(optimal_fit) +\n  stat_ecdf(aes(x = shift_data)) +\n  geom_line(aes(x = x, y = cdf_up), linetype = 2) +\n  coord_cartesian(xlim = c(0,400), ylim = c(0, 0.5))\n\n\n\n\n\n\n\n\nPDF not as perfect, but OK.\n\nggplot(optimal_fit) +\n  geom_density(aes(x = shift_data)) +\n  geom_line(aes(x = x, y = pdf_up), linetype = 2) +\n  xlim(c(0,1000))\n\nWarning: Removed 1032 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nAnd the backshift (e.g. back to the original position of the data)\n\nggplot(optimal_fit) +\n  stat_ecdf(aes(x = orig_data)) +\n  geom_line(aes(x = x_back, y = cdf_up), linetype = 2) +\n  coord_cartesian(xlim = c(0,400), ylim = c(0, 0.5))",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Shifting and fitting truncated lognormals"
    ]
  },
  {
    "objectID": "stats_probability/expected_shannon.html",
    "href": "stats_probability/expected_shannon.html",
    "title": "Expected Shannon Diversity",
    "section": "",
    "text": "We have a community that we’re describing with Shannon diversity, \\(-\\sum{p_ilogp_i}\\) , but we’re doing so for a lot of sub-communities of different size. These diversities change with size in a way that is clearly not what we’d expect if they were simply samples from the regional pool, but to show that we need to see that expectation.\nWe should be able to get an analytical expected value given a set of regional proportions \\(p_i…p_k\\) and size of the sub-community \\(N\\). Most likely as a transformation of a multinomial distribution. But we’re under a bit of a time crunch, so I’ll throw together a bootstrap version.\nFundamentally, the null expectation for a sub-community of size \\(N\\) is defined as the identity of each individual being chosen from a categorical distribution with regional probabilities (pooled over all sub-communities) \\(p_1…p_k\\) , where \\(k\\) is the number of species. We could construct this with the Bernouilli, but the {extraDistr} package just has a random number generator.\nextraDistr::rcat(10, c(0.1, 0.2, 0.3, 0.4))\n\n [1] 2 4 4 4 4 4 1 3 3 3\nSo, to bootstrap the expected shannon, for each n, we will do an rcat some large number of times, calculate shannon. That will give the probability distribution of the shannon, and the mean will be approximately the expectation. We might want other moments too, as sd should shrink as \\(N\\) increases.\nexpected_shannon &lt;- function(N, regional_probs, \n                             n_boots = 1000, returntype = 'mean') {\n  \n  H &lt;- matrix(NA, ncol = length(N), nrow = n_boots)\n  colnames(H) &lt;- paste0('N_', as.character(N))\n  \n  for (i in 1:length(N)) {\n    for (j in 1:n_boots) {\n      # This loses the names, but that doesn't matter.\n      localcomp &lt;- extraDistr::rcat(N[i], regional_probs)\n      localprops &lt;- table(localcomp)/N[i]\n      H[j,i] &lt;- -sum(localprops*log(localprops))\n    }\n  }\n  \n  if (returntype == 'dist') {return(H)}\n  \n  if (returntype == 'mean') {return(apply(H, 2, mean))}\n  \n}\nA quick test\npooled_probs &lt;- c(0.1, 0.5, 0.1, 0.3)\nEs &lt;- expected_shannon(1:100, regional_probs = pooled_probs)\nplot(Es)\nLarger N makes that take much larger, so we might want to do something like\nEslong &lt;- expected_shannon(c(1:50, 100, 250, 500), \n                           regional_probs = pooled_probs)\nplot(Eslong, type = 'lines')\n\nWarning in plot.xy(xy, type, ...): plot type 'lines' will be truncated to first\ncharacter",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Bootstrapping the expected shannon diversity"
    ]
  },
  {
    "objectID": "stats_probability/expected_shannon.html#simple--max-and-min",
    "href": "stats_probability/expected_shannon.html#simple--max-and-min",
    "title": "Expected Shannon Diversity",
    "section": "Simple- max and min",
    "text": "Simple- max and min\nFor reference, the max and min diversity occur at even p’s and only one p, respectively.\n\nmaxmin_shannon &lt;- function(sp_range) {\n  \n  sp &lt;- 1:sp_range\n  \n  maxshan &lt;- rep(0, sp_range)\n  minshan &lt;- maxshan\n  for (i in 1:sp_range) {\n    minsp &lt;- rep(0, i)\n    minsp[1] &lt;- 1\n    maxsp &lt;- rep(1/i, i)\n    \n    maxshan[i] &lt;- -sum(maxsp*log(maxsp))\n    minshan[i] &lt;- -sum(minsp*log(minsp))\n  }\n  \n  return(tibble::lst(minshan, maxshan))\n  \n}\n\nThe min is always 0, but the max depends nonlinearly on n species.\n\nmms &lt;- maxmin_shannon(100)\n\n\nplot(mms$maxshan)\n\n\n\n\n\n\n\n\nAnalytically, for \\(N\\) species, the maximum shannon diversity is \\[-N(1/N\\ln{1/N})\\], which collapses to \\(\\ln(N)\\).",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Bootstrapping the expected shannon diversity"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "An R package providing consistent modelling of response to flows in the Murrray-Darling Basin, incorporating scenario analysis and automated workflows, as well as consistent, safe scaling along space, time, and ecological causal networks. HydroBOT forms the core of the climate adaptation toolkit developed for the climate adaptation theme of the Murray–Darling Water and Environment Research Program. HydroBOT is described at (Holt et al. 2025), with documentation here.",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "software.html#hydrobot",
    "href": "software.html#hydrobot",
    "title": "Software",
    "section": "",
    "text": "An R package providing consistent modelling of response to flows in the Murrray-Darling Basin, incorporating scenario analysis and automated workflows, as well as consistent, safe scaling along space, time, and ecological causal networks. HydroBOT forms the core of the climate adaptation toolkit developed for the climate adaptation theme of the Murray–Darling Water and Environment Research Program. HydroBOT is described at (Holt et al. 2025), with documentation here.",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "software.html#hydrogauge",
    "href": "software.html#hydrogauge",
    "title": "Software",
    "section": "hydrogauge",
    "text": "hydrogauge\nAn R package for querying BoM and Australian state water gauge APIs. Useful not only for pulling flow data, but also identifying available parameters and periods of record.",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "software.html#efloweval",
    "href": "software.html#efloweval",
    "title": "Software",
    "section": "eFlowEval",
    "text": "eFlowEval\nA workflow and R package for modelling ecological response in flexible but standardised ways in wetlands in the Murray-Darling Basin, with capacity for spatial scaling and synthesis. See Holt, Macqueen, and Lester (2024).",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "software.html#peeler",
    "href": "software.html#peeler",
    "title": "Software",
    "section": "peeler",
    "text": "peeler\nAn R package implementing the bvstep algorithm from Clarke and Warwick (1998), enabling reproducible and auutomated use. Also provides the ability to ‘peel’ those outcomes (not recommended).",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "software.html#other",
    "href": "software.html#other",
    "title": "Software",
    "section": "Other",
    "text": "Other\nI have a number of other repositories consisting of data analyses for published papers and demonstration/testing/template repositories (often discussed in more detail in code demos.)",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "software.html#private-packages",
    "href": "software.html#private-packages",
    "title": "Software",
    "section": "Private packages",
    "text": "Private packages\nI develop a number of other packages and data analysis repositories, but most are private due to data embargoes.",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "small_helpers/vdiffr_networks.html",
    "href": "small_helpers/vdiffr_networks.html",
    "title": "SVG vdiffr testing",
    "section": "",
    "text": "For some situations, I need to use golden tests for figures that aren’t made by ggplot or base or otherwise have a print() method as required by vdiffr::write_svg(). Specifically, I encounter this issue with networks created by DiagrammeR, which are html. The solution is to write a custom writer function. Note that for this to work, we also need to install DiagrammeRsvg and rsvg; they are only suggests for DiagrammeR and so are not automatically installed.\n\nsvgnetwriter &lt;- function(plot, file, title = \"\") {\n  plot |&gt;\n    DiagrammeR::export_graph(file_name = paste0(file))\n}\n\nThen we can call expect_doppleganger\n\nvdiffr::expect_doppelganger(title = \"plot_name\", fig = plot_object, \n                            writer = svgnetwriter)\n\nThis is a bit confusing, because the order of arguments changes for expect_doppleganger and writer, and change meaning as well. The title argument to expect_doppleganger becomes the file argument of writer (with '.svg' appended). So writer needs plot, then filename, then a title (which I skip here). The file should not have .svg on it, as that gets added automatically to \"plot_name\" in expect_doppleganger. The title argument for writer is only used for ggplot objects in vdiffr::print_plot().",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing network plots with vdiffr"
    ]
  },
  {
    "objectID": "small_helpers/renv_github.html",
    "href": "small_helpers/renv_github.html",
    "title": "Renv and github",
    "section": "",
    "text": "Sometimes we need to use renv to install from github. in the simple form it works just as usual, e.g.\nrenv::install(galenholt/hydrogauge) . But there are complications, and the syntax is really poorly documented. It uses remotes under the hood but does not always use the same syntax.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Installing from github with renv"
    ]
  },
  {
    "objectID": "small_helpers/renv_github.html#authentication",
    "href": "small_helpers/renv_github.html#authentication",
    "title": "Renv and github",
    "section": "Authentication",
    "text": "Authentication\nAuthentication to private repos is not well documented. There are two options, SSH and HTTPS. Both are possible, but each have tricks that are not clear from the renv docs.\n\nHTTPS\nThis is what is usually the default, but to get it to work with renv, there are some tricks. In all cases for private repos, you’ll need to set up a github PAT (I found the usethis instructions easier to follow than those at github).\nFor the longest time I wasn’t able to authenticate to private repos. Until I was reading the usethis docs, which note that remotes (which renv uses for github) doesn’t hit gitcreds to obtain the GitHub PAT. So, the solution is to install gitcreds (or devtools or usethis, really; we almost always need at least one of them), run gitcreds::gitcreds_get() in the session to activate the PAT (‘tickle’ it, in the usethis docs), and then we can install renv::install('private/repo').\nThe other solution is to use pak , which seems to be where the tidyverse crew is moving. pak uses gitcreds, so just works if we use pak::pkg_install('private/repo') . To set that up with renv put options(renv.config.pak.enabled = TRUE) in .Rprofile and restart. The catch here, at least as far as I can tell, is that pak doesn’t hit the renv cache, so if you have multiple projects, it redownloads packages each time instead of linking to cached versions. I’d really like to figure out how to get pak to avoid unneeded downloads a la renv. I.e. why doesn’t options(renv.pak.enabled = TRUE) just use pak for all the downloading and building but still use the versioning and caching? It seems to lose at least the caching.\n\n\nSSH\nTo use ssh (with authentication), the inbuilt git2r is failing, so we need git = 'external'.\nThen we can use the ssh link. This assumes you’ve set up ssh.\nrenv::install('git@github.com:ACCOUNT/repo_name.git', git = 'external')\nIf we’re scripting, we likely want to add prompt = FALSE, and we can control upgrading other packages with upgrade = 'always' (this is documented).",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Installing from github with renv"
    ]
  },
  {
    "objectID": "small_helpers/renv_github.html#rebuilding",
    "href": "small_helpers/renv_github.html#rebuilding",
    "title": "Renv and github",
    "section": "Rebuilding",
    "text": "Rebuilding\nIt is really hard to force it to build from source if the version number hasn’t changed. rebuild = TRUE usually works, but I also try to index version numbers with every commit now in packages I own.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Installing from github with renv"
    ]
  },
  {
    "objectID": "small_helpers/renv_github.html#using-branches",
    "href": "small_helpers/renv_github.html#using-branches",
    "title": "Renv and github",
    "section": "Using branches",
    "text": "Using branches\nTo get to a branch or ref, use @ref, not a ref argument as in remotes.\n\nrenv::install('git@github.com:ACCOUNT/repo_name.git@branch_name', \n              rebuild = TRUE, \n              upgrade = 'always', \n              git = 'external', \n              prompt = FALSE)",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Installing from github with renv"
    ]
  },
  {
    "objectID": "small_helpers/private_github_notes.html",
    "href": "small_helpers/private_github_notes.html",
    "title": "Github notes",
    "section": "",
    "text": "Authorisation can happen a couple ways- ssh and https with a github PAT.\n\n\nGo to https://github.com/settings/tokens and create a PAT with at least repo scope and copy it. SAVE IT SOMEWHERE OTHER THAN PLAINTEXT. Then, to authorise, the simplest but most dangerous is to use auth_token = 'YOUR GITHUB PAT'.\nThe better option is to use credentials::set_github_pat() to set your PAT using the github signin, which doesn’t require you to have it in plaintext. That sets the GITHUB_PAT environment variable, which is the default for auth_token.\nSo, assuming you’ve created a PAT in github,\n# install.packages(\"devtools\")\ncredentials::set_github_pat()\ndevtools::install_github(\"USER/repo\")\n\n\n\nSee the github documents for creating ssh keys. Note that R and helpers want them to be the default names (and often the RSA version). So don’t do anything different with names, and if get some errors about not finding them, check if you made ed25519 and not rsa keys.\nThen, to install,\n# install.packages(\"devtools\")\ndevtools::install_git(\"git@github.com:MDBAuth/WERP_toolkit.git\", ref = 'master', force = TRUE, upgrade = 'ask')\nBut, if using R 4.3, the {git2r} package does not support ssh, and so you have to clone the directory, and use\n`devtools::install_local('path/to/repo', force = TRUE, upgrade = 'ask')`\nHopefully that’s fixed soon (or install_git moves away from {git2r}- {gert} works with ssh, but install_git can’t use it)",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Dealing with private github"
    ]
  },
  {
    "objectID": "small_helpers/private_github_notes.html#private-repos-and-access",
    "href": "small_helpers/private_github_notes.html#private-repos-and-access",
    "title": "Github notes",
    "section": "",
    "text": "Authorisation can happen a couple ways- ssh and https with a github PAT.\n\n\nGo to https://github.com/settings/tokens and create a PAT with at least repo scope and copy it. SAVE IT SOMEWHERE OTHER THAN PLAINTEXT. Then, to authorise, the simplest but most dangerous is to use auth_token = 'YOUR GITHUB PAT'.\nThe better option is to use credentials::set_github_pat() to set your PAT using the github signin, which doesn’t require you to have it in plaintext. That sets the GITHUB_PAT environment variable, which is the default for auth_token.\nSo, assuming you’ve created a PAT in github,\n# install.packages(\"devtools\")\ncredentials::set_github_pat()\ndevtools::install_github(\"USER/repo\")\n\n\n\nSee the github documents for creating ssh keys. Note that R and helpers want them to be the default names (and often the RSA version). So don’t do anything different with names, and if get some errors about not finding them, check if you made ed25519 and not rsa keys.\nThen, to install,\n# install.packages(\"devtools\")\ndevtools::install_git(\"git@github.com:MDBAuth/WERP_toolkit.git\", ref = 'master', force = TRUE, upgrade = 'ask')\nBut, if using R 4.3, the {git2r} package does not support ssh, and so you have to clone the directory, and use\n`devtools::install_local('path/to/repo', force = TRUE, upgrade = 'ask')`\nHopefully that’s fixed soon (or install_git moves away from {git2r}- {gert} works with ssh, but install_git can’t use it)",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Dealing with private github"
    ]
  },
  {
    "objectID": "small_helpers/list_names.html",
    "href": "small_helpers/list_names.html",
    "title": "List names as variables",
    "section": "",
    "text": "I often end up wanting to build a list with named items, and the names come in as variables.\nFor example, instead of the typical\nlist(a = 'first', b = 'second', d = c(3,4))\n\n$a\n[1] \"first\"\n\n$b\n[1] \"second\"\n\n$d\n[1] 3 4\nI might have the names defined elsewhere. This is particularly common inside functions.\nname1 &lt;- 'a'\nname2 &lt;- 'b'\nname3 &lt;- 'd'\nWe can’t pass those in as usual- this uses name# as the name, not the value of the variable.\nlist(name1 = 'first', name2 = 'second', name3 = c(3,4))\n\n$name1\n[1] \"first\"\n\n$name2\n[1] \"second\"\n\n$name3\n[1] 3 4",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Naming rules for lists"
    ]
  },
  {
    "objectID": "small_helpers/list_names.html#various-solutions",
    "href": "small_helpers/list_names.html#various-solutions",
    "title": "List names as variables",
    "section": "Various solutions",
    "text": "Various solutions\nWe can use setNames\n\nsetNames(list('first', 'second',c(3,4)), c(name1, name2, name3))\n\n$a\n[1] \"first\"\n\n$b\n[1] \"second\"\n\n$d\n[1] 3 4\n\n\nwe can do it in two steps with names , which I think is what setNames wraps, and is just extra verbose and requires carrying data copies around.\n\nbarelist &lt;- list('first', 'second',c(3,4))\nnames(barelist) &lt;- c(name1, name2, name3)\nbarelist\n\n$a\n[1] \"first\"\n\n$b\n[1] \"second\"\n\n$d\n[1] 3 4\n\n\nCan we unquote/eval?\nI can almost never get !! or !!! to work. this doesn’t, as usual.\n\nlist(!!name1 = 'first', !!name2 = 'second', !!name3 = c(3,4))\n\nNor this\n\nlist(eval(name1) = 'first', eval(name2) = 'second', eval(name3) = c(3,4))\n\nNor this, despite the eval working\n\nrlang::eval_bare(name1)\n\n[1] \"a\"\n\n\n\nlist(rlang::eval_bare(name1) = 'first', rlang::eval_bare(name2) = 'second', rlang::eval_bare(name3) = c(3,4))\n\nHow about tibble::lst ? I often use it for lists of variables because it self-names them, so maybe it’s the answer here. Yep. That’s just cleaner.\n\ntibble::lst(name1 = 'first', name2 = 'second', name3 = c(3,4))\n\n$name1\n[1] \"first\"\n\n$name2\n[1] \"second\"\n\n$name3\n[1] 3 4\n\n\nAnd, that self-naming I was describing, which solves a different problem- having to write list(name = name, age = age).\n\nname &lt;- c('David', 'Susan')\nage &lt;- c(1,2)\n\ntibble::lst(name, age)\n\n$name\n[1] \"David\" \"Susan\"\n\n$age\n[1] 1 2",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Naming rules for lists"
    ]
  },
  {
    "objectID": "small_helpers/json_api_construction.html",
    "href": "small_helpers/json_api_construction.html",
    "title": "JSON API coding",
    "section": "",
    "text": "I’m working on an API that uses JSON in the body, but getting it to come out right with square brackets, curly brackets, commas, etc where they’re supposed to be has been trial and error. I’m going to put what I’ve figured out here. I’m using examples from the {hydrogauge} package, but the main point is to show how to get different sorts of output.\nUsing the httr2 examples with req_dry_run to see what the request looks like and check the formats.\n\nlibrary(httr2)\nreq &lt;- request(\"http://httpbin.org/post\")\n\n\n\nTo pass simple one to one key-value pairs, wrapped in {} use a list.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\n\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 40\n\n{\"function\":\"get_db_info\",\"version\":\"3\"}\n\n\n\n\n\nTo pass nested key-value pairs, use nested lists\n\nparams &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '123abc',\n                               \"datasource\" = \"A\"))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 95\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"123abc\",\"datasource\":\"A\"}}\n\n\n\n\n\nThese cannot be created with c(), because that does something else (square brackets- see below).\n\nparams &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '233217, 405328, 405331'))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 100\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331\"}}\n\n\n\n\n\nTo get square brackets, we need a vector. So, typically c() the bits together in the call (or previously).\n\nparams &lt;- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = c('A', 'TELEM')))\n\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}\n\n\n\n\n\nTo get patterns like [['a', 'b'],['c', 'd']], use a matrix (and I think maybe a df). Which makes sense if we think of that as a group of vectors. The pattern is [[row1], [row2], [row_n]].\n\ntopleft &lt;- c('-35', '148')\nbottomright &lt;- c('-36', '149')\n\nrectbox &lt;- rbind(topleft, bottomright)\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"148\"],[\"-36\",\"149\"]]}}}\n\n\n\n\nGives some horrible combination of curly and square braces including column and row names.\n\nrectdf &lt;- data.frame(rectbox)\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 208\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"X1\":\"-35\",\"X2\":\"148\",\"_row\":\"topleft\"},{\"X1\":\"-36\",\"X2\":\"149\",\"_row\":\"bottomright\"}]}}}\n\n\nTibbles aren’t really any different, but the names are a bit cleaner\n\nrectdf &lt;- tibble::as_tibble(rectbox)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}\n\n\n\n\n\n\nThere are arguments to toJSON that alter how matrices and dfs get parsed. Matrices are by default row-wise, but we can change to cols (e.g. [['col1'], ['col2']] with matrix = 'columnmajor'.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %&gt;%\n  req_body_json(params, matrix = 'columnmajor') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"-36\"],[\"148\",\"149\"]]}}}\n\n\nSimilarly, we can alter how dfs work, which might actually be fairly useful in the way it handles named columns especially. The default (above) is dataframe = 'rows' , which is kind of a mess (or at least not how my brain parses what a dataframe means). But dataframe = 'columns' ends up with named vectors. I don’t currently need that, but it sure makes more sense.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params, dataframe = 'columns') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 160\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":{\"V1\":[\"-35\",\"-36\"],\"V2\":[\"148\",\"149\"]}}}}\n\n\nUsing dataframe = 'values' is again a confusing list.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params, datafraem = 'values') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}\n\n\n\n\n\nTo get square brackets around multiple sets of curlies, e.g. `[{‘key’: ‘value’}, {‘key2’: ‘value2’}], use a list of lists.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \n                               \"complex_filter\" = list(list('fieldname' = 'stntype', \n                                                       'value' = 'HYD'),\n                                                   list('combine' = 'OR',\n                                                        'fieldname' = 'stntype',\n                                                        'value' = 'VIR'))))\nreq %&gt;%\n  req_body_json(params, datafraem = 'values') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 203\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"complex_filter\":[{\"fieldname\":\"stntype\",\"value\":\"HYD\"},{\"combine\":\"OR\",\"fieldname\":\"stntype\",\"value\":\"VIR\"}]}}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Building JSON syntax from R for API calls"
    ]
  },
  {
    "objectID": "small_helpers/json_api_construction.html#simple-key-value",
    "href": "small_helpers/json_api_construction.html#simple-key-value",
    "title": "JSON API coding",
    "section": "",
    "text": "To pass simple one to one key-value pairs, wrapped in {} use a list.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\n\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 40\n\n{\"function\":\"get_db_info\",\"version\":\"3\"}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Building JSON syntax from R for API calls"
    ]
  },
  {
    "objectID": "small_helpers/json_api_construction.html#nested-key-value",
    "href": "small_helpers/json_api_construction.html#nested-key-value",
    "title": "JSON API coding",
    "section": "",
    "text": "To pass nested key-value pairs, use nested lists\n\nparams &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '123abc',\n                               \"datasource\" = \"A\"))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 95\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"123abc\",\"datasource\":\"A\"}}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Building JSON syntax from R for API calls"
    ]
  },
  {
    "objectID": "small_helpers/json_api_construction.html#comma-separated-strings",
    "href": "small_helpers/json_api_construction.html#comma-separated-strings",
    "title": "JSON API coding",
    "section": "",
    "text": "These cannot be created with c(), because that does something else (square brackets- see below).\n\nparams &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '233217, 405328, 405331'))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 100\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331\"}}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Building JSON syntax from R for API calls"
    ]
  },
  {
    "objectID": "small_helpers/json_api_construction.html#square-brackets",
    "href": "small_helpers/json_api_construction.html#square-brackets",
    "title": "JSON API coding",
    "section": "",
    "text": "To get square brackets, we need a vector. So, typically c() the bits together in the call (or previously).\n\nparams &lt;- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = c('A', 'TELEM')))\n\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Building JSON syntax from R for API calls"
    ]
  },
  {
    "objectID": "small_helpers/json_api_construction.html#double-square-brackets",
    "href": "small_helpers/json_api_construction.html#double-square-brackets",
    "title": "JSON API coding",
    "section": "",
    "text": "To get patterns like [['a', 'b'],['c', 'd']], use a matrix (and I think maybe a df). Which makes sense if we think of that as a group of vectors. The pattern is [[row1], [row2], [row_n]].\n\ntopleft &lt;- c('-35', '148')\nbottomright &lt;- c('-36', '149')\n\nrectbox &lt;- rbind(topleft, bottomright)\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"148\"],[\"-36\",\"149\"]]}}}\n\n\n\n\nGives some horrible combination of curly and square braces including column and row names.\n\nrectdf &lt;- data.frame(rectbox)\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 208\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"X1\":\"-35\",\"X2\":\"148\",\"_row\":\"topleft\"},{\"X1\":\"-36\",\"X2\":\"149\",\"_row\":\"bottomright\"}]}}}\n\n\nTibbles aren’t really any different, but the names are a bit cleaner\n\nrectdf &lt;- tibble::as_tibble(rectbox)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Building JSON syntax from R for API calls"
    ]
  },
  {
    "objectID": "small_helpers/json_api_construction.html#orientation-of-dfs-and-matrices",
    "href": "small_helpers/json_api_construction.html#orientation-of-dfs-and-matrices",
    "title": "JSON API coding",
    "section": "",
    "text": "There are arguments to toJSON that alter how matrices and dfs get parsed. Matrices are by default row-wise, but we can change to cols (e.g. [['col1'], ['col2']] with matrix = 'columnmajor'.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %&gt;%\n  req_body_json(params, matrix = 'columnmajor') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"-36\"],[\"148\",\"149\"]]}}}\n\n\nSimilarly, we can alter how dfs work, which might actually be fairly useful in the way it handles named columns especially. The default (above) is dataframe = 'rows' , which is kind of a mess (or at least not how my brain parses what a dataframe means). But dataframe = 'columns' ends up with named vectors. I don’t currently need that, but it sure makes more sense.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params, dataframe = 'columns') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 160\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":{\"V1\":[\"-35\",\"-36\"],\"V2\":[\"148\",\"149\"]}}}}\n\n\nUsing dataframe = 'values' is again a confusing list.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params, datafraem = 'values') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Building JSON syntax from R for API calls"
    ]
  },
  {
    "objectID": "small_helpers/json_api_construction.html#square-brackets-around-curly",
    "href": "small_helpers/json_api_construction.html#square-brackets-around-curly",
    "title": "JSON API coding",
    "section": "",
    "text": "To get square brackets around multiple sets of curlies, e.g. `[{‘key’: ‘value’}, {‘key2’: ‘value2’}], use a list of lists.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \n                               \"complex_filter\" = list(list('fieldname' = 'stntype', \n                                                       'value' = 'HYD'),\n                                                   list('combine' = 'OR',\n                                                        'fieldname' = 'stntype',\n                                                        'value' = 'VIR'))))\nreq %&gt;%\n  req_body_json(params, datafraem = 'values') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 203\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"complex_filter\":[{\"fieldname\":\"stntype\",\"value\":\"HYD\"},{\"combine\":\"OR\",\"fieldname\":\"stntype\",\"value\":\"VIR\"}]}}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Building JSON syntax from R for API calls"
    ]
  },
  {
    "objectID": "small_helpers/failing_snapshot_check.html",
    "href": "small_helpers/failing_snapshot_check.html",
    "title": "Snapshot failures on check",
    "section": "",
    "text": "Sometimes I have all tests pass with devtools::test(), but then encounter failures with devtools::check(). When these are for golden tests (snapshots, whether from testthat itself or vdiffr, the output says\n* Run `testthat::snapshot_review('name_of_file')` to interactively review the change.\nBut nothing happens, because the filename.new.md or filename.new.svg aren’t available in the main package directory.\nTo get it to work, go find the packagename.Rcheck directory, then go to /tests/testthat/snaps and copy the offending .new files into the main working project tests/testthat/snaps . Then you can run testthat::snapshot_review('name_of_file') .\nI tend to find the packagename.Rcheck directory one level up from the package project directory, though I’m not sure if that’s consistent.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Snapshot failures on check"
    ]
  },
  {
    "objectID": "small_helpers/error_handling.html",
    "href": "small_helpers/error_handling.html",
    "title": "Catching, passing, handling errors",
    "section": "",
    "text": "There’s a lot out there on handling errors. This will mostly be testing things as they come up when I need to use them. Will rely heavily on Hadley, as usual.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Intro to error handling"
    ]
  },
  {
    "objectID": "small_helpers/error_handling.html#try-and-passing",
    "href": "small_helpers/error_handling.html#try-and-passing",
    "title": "Catching, passing, handling errors",
    "section": "Try and passing",
    "text": "Try and passing\nIf we just use try, the error should get printed but everything keeps moving\n\noutvec &lt;- vector(mode = 'numeric', length = 10)\nfor (i in 1:10) {outvec[i] &lt;- try(err_even(i))}\n\nError in err_even(i) : Even numbers are error\nError in err_even(i) : Even numbers are error\nError in err_even(i) : Even numbers are error\nError in err_even(i) : Even numbers are error\nError in err_even(i) : Even numbers are error\n\noutvec\n\n [1] \"1\"                                              \n [2] \"Error in err_even(i) : Even numbers are error\\n\"\n [3] \"3\"                                              \n [4] \"Error in err_even(i) : Even numbers are error\\n\"\n [5] \"5\"                                              \n [6] \"Error in err_even(i) : Even numbers are error\\n\"\n [7] \"7\"                                              \n [8] \"Error in err_even(i) : Even numbers are error\\n\"\n [9] \"9\"                                              \n[10] \"Error in err_even(i) : Even numbers are error\\n\"\n\n\nHuh. I thought try just printed the values but let things keep going. Changing the non-failures to character isn’t ideal. But I guess then I’d use tryCatch? For now though, this is exactly what i need, so I’ll stop here.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Intro to error handling"
    ]
  },
  {
    "objectID": "small_helpers/error_handling.html#trycatch",
    "href": "small_helpers/error_handling.html#trycatch",
    "title": "Catching, passing, handling errors",
    "section": "tryCatch",
    "text": "tryCatch\nI actually want to capture errors, warnings, or passing to assess some code\n\nerr_even_warn5 &lt;- function(x) {\n  if ((x %% 2) == 0) {\n    stop('Even numbers are error')\n  } else if (x == 5) {\n    warning('5 throws a warning')\n  } else {x}\n}\n\nI want to use this for recording, so\n\nrecorder &lt;- vector(mode = 'character', length = 10)\nfor (i in 1:10) {\n  recorder[i] &lt;- tryCatch(err_even_warn5(i),\n                        error = function(c) c$message,\n                        warning = function(c) c$message,\n                        message = function(c) c$message)\n}\nrecorder\n\n [1] \"1\"                      \"Even numbers are error\" \"3\"                     \n [4] \"Even numbers are error\" \"5 throws a warning\"     \"Even numbers are error\"\n [7] \"7\"                      \"Even numbers are error\" \"9\"                     \n[10] \"Even numbers are error\"\n\n\nAnd to be even more explicit, can I do some mods in the call to just say if it passed?\n\nrecorder2 &lt;- vector(mode = 'character', length = 10)\nfor (i in 1:10) {\n  recorder2[i] &lt;- tryCatch(if(is.numeric(err_even_warn5(i))) {'pass'},\n                        error = function(c) c$message,\n                        warning = function(c) c$message,\n                        message = function(c) c$message)\n}\nrecorder2\n\n [1] \"pass\"                   \"Even numbers are error\" \"pass\"                  \n [4] \"Even numbers are error\" \"5 throws a warning\"     \"Even numbers are error\"\n [7] \"pass\"                   \"Even numbers are error\" \"pass\"                  \n[10] \"Even numbers are error\"\n\n\nNow, for packages, sometimes I want to ignore certain warnings and messages that I know are OK, while letting other unexpected ones bubble up. I think according to Hadley here and here, I want withCallingHandlers() rather than tryCatch because I want the code flow to proceed unimpeded.\nSo, let’s say I have a function that throws ‘expected’ warnings and messages and ‘unexpected’ ones, as well as errors (which should still error).\n\nexpect_unexpect &lt;- function(x) {\n  if ((x == 10)) {\n    stop('10 is error')\n  } else if (x == 2) {\n    warning('expected warning at 2')\n  } else if (x == 3) {\n    warning('unexpected warning at 3')\n  } else if (x == 4) {\n    message('expected message at 4')\n  } else if (x == 5) {\n    message('unexpected message at 5')\n  } else {x}\n  \n  x\n}\n\nSo, how does withCallingHandlers work?\n\ntestvec &lt;- 1:6*NA\n\nfor (i in 1:6) {\n  testvec[i] &lt;- withCallingHandlers(\n    warning = function(cnd) {\n      i *2\n    }, \n    message = function(cnd) {\n      i * 3\n    },\n    expect_unexpect(i)\n  )\n}\n\nWarning in expect_unexpect(i): expected warning at 2\n\n\nWarning in expect_unexpect(i): unexpected warning at 3\n\n\nexpected message at 4\n\n\nunexpected message at 5\n\ntestvec\n\n[1] 1 2 3 4 5 6\n\n\nSo that’s clearly doing what it should in that it passes the actual output, but the bits where it’s managing cnds needs work.\nWhat I actually want to do is ignore those warnings and messages when they are expected.\nWhat do the objects look like?\n\nrlang::catch_cnd(expect_unexpect(4))\n\n&lt;simpleMessage in message(\"expected message at 4\"): expected message at 4\n&gt;\n\nrlang::catch_cnd(expect_unexpect(4))$message\n\n[1] \"expected message at 4\\n\"\n\nrlang::catch_cnd(expect_unexpect(4))$call\n\nmessage(\"expected message at 4\")\n\n\nDo I need that? or do I just need to do a check?\nThis muffles warnings but not messages\n\ntestvec &lt;- 1:6*NA\n\nfor (i in 1:6) {\n  testvec[i] &lt;- withCallingHandlers(\n    warning = function(cnd) {\n        rlang::cnd_muffle(cnd)\n    },\n    message = function(cnd) {\n      i\n    },\n    expect_unexpect(i)\n  )\n}\n\nexpected message at 4\n\n\nunexpected message at 5\n\ntestvec\n\n[1] 1 2 3 4 5 6\n\n\nBut how do I muffle only some warnings? I thought I needed rlang::catch_cnd(), but that’s an extra unnecessary layer- it’s null, the cnd being passed around here is already caught.\n\ntestvec &lt;- 1:6*NA\n\nfor (i in 1:6) {\n  testvec[i] &lt;- withCallingHandlers(\n    warning = function(cnd) {\n      print(cnd)\n      print(rlang::catch_cnd(cnd))\n      print(cnd$message)\n    },\n    message = function(cnd) {\n      i\n    },\n    expect_unexpect(i)\n  )\n}\n\n&lt;simpleWarning in expect_unexpect(i): expected warning at 2&gt;\nNULL\n[1] \"expected warning at 2\"\n\n\nWarning in expect_unexpect(i): expected warning at 2\n\n\n&lt;simpleWarning in expect_unexpect(i): unexpected warning at 3&gt;\nNULL\n[1] \"unexpected warning at 3\"\n\n\nWarning in expect_unexpect(i): unexpected warning at 3\n\n\nexpected message at 4\n\n\nunexpected message at 5\n\ntestvec\n\n[1] 1 2 3 4 5 6\n\n\nThese don’t have useful classes, so just use grep on the message. When I go to do this for real, will need to use rlang::catch_cnd() to look at what I’m dealin with and see if I can do a better condition.\n\ntestvec &lt;- 1:6*NA\n\nfor (i in 1:6) {\n  testvec[i] &lt;- withCallingHandlers(\n    warning = function(cnd) {\n      if (grepl('^expect', cnd$message)) {\n        rlang::cnd_muffle(cnd)\n      }\n    },\n    message = function(cnd) {\n      if (grepl('^expect', cnd$message)) {\n        rlang::cnd_muffle(cnd)\n      }\n    },\n    expect_unexpect(i)\n  )\n}\n\nWarning in expect_unexpect(i): unexpected warning at 3\n\n\nunexpected message at 5\n\ntestvec\n\n[1] 1 2 3 4 5 6",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Intro to error handling"
    ]
  },
  {
    "objectID": "small_helpers/error_handling.html#asidesspecific-cases",
    "href": "small_helpers/error_handling.html#asidesspecific-cases",
    "title": "Catching, passing, handling errors",
    "section": "Asides/specific cases",
    "text": "Asides/specific cases\nFor purrr::map and similar functions, we can use purrr::safely and purrr::possibly to pass errors without failing a whole run. The output then needs to be unpacked and cleaned up.\nFor foreach::foreach, we can use the .errorhandling argument to pass errors through without failing the whole run.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Intro to error handling"
    ]
  },
  {
    "objectID": "small_helpers/breaking_whiles.html",
    "href": "small_helpers/breaking_whiles.html",
    "title": "‘Break’-ing ‘while’ loops in debug",
    "section": "",
    "text": "I’m getting unexpected behaviour with break in a while loop. Basically, it isn’t breaking out of the while, or even out of that iteration, but returning control to an earlier point in the same iteration. I think it might have to do with nested if statements?\nOutcome: break itself works fine. The issue is that it does not work the same while debugging. For some reason break in a debug doesn’t break out of the loop, but just reiterates. So the behaviour was weird and unexpected while stepping in debug, but worked fine when the code just ran.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "While loops, break, and debugging"
    ]
  },
  {
    "objectID": "small_helpers/breaking_whiles.html#testing--this-all-works",
    "href": "small_helpers/breaking_whiles.html#testing--this-all-works",
    "title": "‘Break’-ing ‘while’ loops in debug",
    "section": "Testing- this all works",
    "text": "Testing- this all works\nThis behaves as expected\n\ncount &lt;- 1\nwhile(count &lt; 100) {\n  print(glue::glue(\"Count is {count}\"))\n  if (count == 10) {\n    break()\n  }\n  \n  count &lt;- count + 1\n}\n\nCount is 1\nCount is 2\nCount is 3\nCount is 4\nCount is 5\nCount is 6\nCount is 7\nCount is 8\nCount is 9\nCount is 10\n\n\nThis is the situation I have that’s failing, but it seems to work here.\n\ncount &lt;- 1\nwhile(count &lt; 100) {\n  print(glue::glue(\"Count is {count}\"))\n  if ((count %% 2) == 0) {\n    print(glue::glue(\"{count} is an even number\"))\n    if (count == 10) {\n      break()\n    }\n  }\n  \n  \n  count &lt;- count + 1\n}\n\nCount is 1\nCount is 2\n2 is an even number\nCount is 3\nCount is 4\n4 is an even number\nCount is 5\nCount is 6\n6 is an even number\nCount is 7\nCount is 8\n8 is an even number\nCount is 9\nCount is 10\n10 is an even number\n\n\nJust making the while more complicated doesn’t change it. Still works as expected.\n\ncount &lt;- 1\nwhile(count &lt; 100) {\n  print(glue::glue(\"Count is {count}\"))\n  if ((count %% 2) == 0) {\n    print(glue::glue(\"{count} is an even number\"))\n    \n    if (count == 10) {\n      print('10')\n    }\n    \n    if (count == 10) {\n      print('breaking')\n      break()\n    }\n    \n    \n  }\n  \n  if ((count %% 2) == 1) {\n    print(glue::glue(\"{count} is an odd number\"))\n  }\n  \n  count &lt;- count + 1\n}\n\nCount is 1\n1 is an odd number\nCount is 2\n2 is an even number\nCount is 3\n3 is an odd number\nCount is 4\n4 is an even number\nCount is 5\n5 is an odd number\nCount is 6\n6 is an even number\nCount is 7\n7 is an odd number\nCount is 8\n8 is an even number\nCount is 9\n9 is an odd number\nCount is 10\n10 is an even number\n[1] \"10\"\n[1] \"breaking\"\n\n\nBut if I put those in a function, and step in with a debug, it doesn’t break, it keeps iterating. Just a quirk, I guess that I need to be aware of.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "While loops, break, and debugging"
    ]
  },
  {
    "objectID": "simmodelling/negbin_autocorr_testing.html",
    "href": "simmodelling/negbin_autocorr_testing.html",
    "title": "Negative binomial autocorr",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\nlibrary(tidyverse)\nlibrary(fitdistrplus)\nI have data that is almost certainly autocorrelated and negative binomial distributed. I’m analysing it elsewhere, but bringing over two simplified datasets here. Here, x is a spatial location, and n is a count at that location.\nThe questions are:\nI’m guessing that last bit is harder than the typical Gaussian AR sequence defined by \\[X_{t+1} = \\rho X_{t} + \\epsilon_t\\]\nwhere if we have rho and the \\(\\mu\\) and \\(\\sigma\\) for the \\(X\\) distribution, we can calculate the appropriate \\(\\mu_\\epsilon\\) and \\(\\sigma_\\epsilon\\) using standard, well-known equations.\nnb_a &lt;- readRDS('data/negbin_testing/nb_testA.rds')\nnb_s &lt;- readRDS('data/negbin_testing/nb_testS.rds')"
  },
  {
    "objectID": "simmodelling/negbin_autocorr_testing.html#the-distributions",
    "href": "simmodelling/negbin_autocorr_testing.html#the-distributions",
    "title": "Negative binomial autocorr",
    "section": "The distributions",
    "text": "The distributions\nFirst, let’s look at the distributions we have, including their negative binomial fits and AC.\nI could do this with dplyr and grouping, but trying to get to first principles here for the building and fitting.\n\nNegative binomial fits\n\na_fit &lt;- fitdist(nb_a$count, 'nbinom')\ns_fit &lt;- fitdist(nb_s$count, 'nbinom')\n\n\nfreq_a &lt;- nb_a |&gt; \n  summarise(freq = n()/nrow(nb_a), .by = count) |&gt; \n  arrange(count)\n\nfreq_s &lt;- nb_s |&gt; \n  summarise(freq = n()/nrow(nb_s), .by = count) |&gt; \n  arrange(count)\n\na_dist &lt;- tibble(count = 0:max(freq_a$count),\n                 pdf = dnbinom(count, \n                               size = a_fit$estimate[1],\n                               mu = a_fit$estimate[2]))\n\ns_dist &lt;- tibble(count = 0:max(freq_s$count),\n                 pdf = dnbinom(count, \n                               size = s_fit$estimate[1],\n                               mu = s_fit$estimate[2]))\n\n\nggplot(freq_a, aes(x = count, y = freq)) +\n  geom_line() +\n  geom_point() +\n  geom_line(data = a_dist, aes(y = pdf), color = 'dodgerblue')\n\n\n\n\n\n\n\n\nNote that while it looks like a big discrepancy in the tail here, there’s only a single data point past about 110.\n\nggplot(freq_s, aes(x = count, y = freq)) +\n  geom_line() +\n  geom_point() +\n  geom_line(data = s_dist, aes(y = pdf), color = 'dodgerblue')\n\n\n\n\n\n\n\n\n\n\nAutocorrelation\n\nac_a &lt;- acf(nb_a$count)\n\n\n\n\n\n\n\n\n\nac_s &lt;- acf(nb_s$count)"
  },
  {
    "objectID": "simmodelling/negbin_autocorr_testing.html#generating-data",
    "href": "simmodelling/negbin_autocorr_testing.html#generating-data",
    "title": "Negative binomial autocorr",
    "section": "Generating data",
    "text": "Generating data\nNow, we want to generate more data with those properties, e.g. for the ‘a’ data, negative binomial size 0.5073501, \\(\\mu\\) 12.4241402, and \\(\\rho\\) 0.5508952.\nHow do we do that?\nThere are a few papers on it, e.g. here and here and here. I’ll try to cobble something together from them. The paper by Gouriéroux and Lu (2019) looks the best so far, so let’s try that. It’s not simple though, so let’s figure out what we have and what we need to estimate and how. And then how to generate new realisations.\nWe have some process \\(X_t\\) that is negative binomial and autoregressive.\nTo get to \\(X_{t+1}\\) they go through an ‘intensity process’ \\(Y_{t+1}\\), which is a first-order AR gamma.\nThe define distributions\nPoisson: \\(P(\\lambda)\\)\nGamma: \\(\\gamma(\\delta,\\beta,c)\\) where \\(\\delta\\) is degree of freedom, \\(\\beta\\) is non-centrality, and \\(c\\) is scale\nNegative binomial: \\(NB(\\delta,\\beta)\\) where \\(\\delta\\) is the df of the gamma and \\(\\beta\\) is the scale of the intensity.\nThe appendix has details of those distributions, as well as some simulations, though I’ll need to sort through how they got those.\nIn various places (eq 2.1, 2.2, and appendix), they say \\(\\rho = \\beta c\\) and there is not a way to determine both, so set \\(c = 1\\). Thus, if I’m reading that correctly, \\(\\beta = \\rho\\) .\nThey do say that they use a parameterisation with \\(\\beta\\) to get directly at the Poisson-Gamma mixture, and \\(p = \\frac{\\beta}{\\beta + 1}\\) . The definiton of the negbin in R is given in help, but we are here using the \\(\\mu\\) version because that’s what fitdistrplus returns. The size parameter in R (\\(n\\)) is defined as the “dispersion parameter (the shape parameter of the gamma mixing distribution)”, and so seems to equal . The \\(\\mu\\) parameter has \\(p = \\frac{n}{n+\\mu}\\). The easiest thing to do probably is to translate back and forth through \\(p\\), where \\(\\beta= -\\frac{p}{p-1}\\) and \\(\\mu = \\frac{n(1-p)}{p}\\) .\nSo, that lets us move back and forth for the parameterisation notation, but leaves two important questions:\n\nHow do we actually simulate?\nHow does \\(\\beta\\) simultanously define the \\(p\\) of the negative binomial and the autocorrelation \\(\\rho\\)?\n\n\nSimulation\nDeal with 1 first.- before we try to match a distribution, try to simulate anything.\nTo simulate, they say “simulation of such trajectories is rather straightforward and it suffices to draw alternatively from Poisson and gamma, or Poisson, gamma and Wishart distributions”.\nThey use \\(c = 1\\), \\(\\beta = 0.69\\) and \\(\\delta = 1.3\\). Try that.\nIf I’m reading definition 1 correctly, we: start with an \\(X_t\\), then \\(Y_{t+1}\\) is \\(\\gamma(\\delta+X_t, 0, c)\\), and then \\(X_{t+1}\\) is \\(Pois(\\beta Y_{t+1})\\).\nLet’s try it.\nFirst, some translation functions\n\np_from_beta &lt;- function(beta) {\n  beta/(beta + 1)\n}\n\np_from_mu &lt;- function(mu, n) {\n  n / (n + mu)\n}\n\nbeta_from_p &lt;- function(p) {\n  -p/(p-1)\n}\n\nmu_from_p &lt;- function(p, n) {\n  (n*(1-p))/p\n}\n\nbeta_from_mu &lt;- function(mu, n) {\n  p &lt;- p_from_mu(mu, n)\n  beta_from_p(p)\n}\n\nmu_from_beta &lt;- function(beta, n) {\n  p &lt;- p_from_beta(beta)\n  mu_from_p(p, n)\n}\n\n\ndelta &lt;- 1.3\nbeta &lt;- 0.69\nc_param &lt;- 1\n# just initialise the whole vector\nX &lt;- rnbinom(1000, size = 1.3, prob = p_from_beta(0.69))\nY &lt;- X*NA\n\nfor (i in 2:length(X)) {\n  Y[i] &lt;- rgamma(1, shape = delta+X[i-1], scale = c_param)\n  X[i] &lt;- rpois(1, lambda = beta*Y[i])\n}\n\nCheck\n\nsim_fit &lt;- fitdist(X, 'nbinom')\nsim_fit\n\nFitting of the distribution ' nbinom ' by maximum likelihood \nParameters:\n     estimate Std. Error\nsize 1.289241 0.09379129\nmu   2.816988 0.09469248\n\n\nconvert to beta to check. The n is right on, but the beta is way off.\n\nbeta_from_mu(mu = sim_fit$estimate[2], n = sim_fit$estimate[1]) |&gt; \n  setNames('beta')\n\n     beta \n0.4576666 \n\n\nSimilarly, if we ask what the mu we expect is, it’s way off the mu we estimate.\n\nmu_from_beta(0.69, 1.3)\n\n[1] 1.884058\n\n\nWhat’s the ac? That’s about right.\n\nsim_ac &lt;- acf(X)\n\n\n\n\n\n\n\nsim_ac$acf[2]\n\n[1] 0.6362625\n\n\nIs the issue that I’m using a gamma, but I’m supposed to be using a centered gamma? Well, a non-centered gamma with centering term = 0? Is the issue that c \\neq 1?\nWait, property A.1 in appendix says unconditional dist of X is \\(NB(\\delta, \\frac{\\beta c}{1-\\beta c})\\).\n\ntransbeta &lt;- beta/(1-beta)\ntransbeta\n\n[1] 2.225806\n\nmu_from_beta(transbeta, 1.3)\n\n[1] 0.584058\n\n\nTransbeta is kind of close, but still pretty far off? But did they just change the parameterisation mid-paper?\nDo I need to try to use their fitting algorithm?\n\npmf_to_fit &lt;- function(beta, delta, xt, xtm1) {\n  (beta^xt * gamma(delta + xt + xtm1)) /\n    (gamma(delta + xtm1) * (beta + 1)^(delta+xt+xtm1) * factorial(xt))\n}\n\neachlog &lt;- X * NA\neachlog[1] &lt;- 0\n\nfor (i in 2:length(X)) {\n  eachlog[i] &lt;- log(pmf_to_fit(beta, delta, X[i], X[i-1]))\n}\n\nlogl &lt;- sum(eachlog)\n\nI don’t think I can use optimize, since it’s single-parameter. Maybe nlm or optim. OR, can I estimate one some other way?\n\nmake_ll &lt;- function(betadelta, series) {\n  eachlog &lt;- series * NA\n  eachlog[1] &lt;- 0\n  \n  beta &lt;- betadelta[1]\n  delta &lt;- betadelta[2]\n\nfor (i in 2:length(series)) {\n  eachlog[i] &lt;- log(pmf_to_fit(beta, delta, series[i], series[i-1]))\n}\n\n  # optim likes to minimize, so use negative\nlogl &lt;- -sum(eachlog)\n\nreturn(logl)\n}\n\nSo that basically recovers the parameterisation. But is the issue still that we’re too bound up between rho and beta?\n\nfit_sims &lt;- optim(c(0.69, 1.3), make_ll, series = X)\nfit_sims\n\n$par\n[1] 0.6791434 1.3340810\n\n$value\n[1] 1940.01\n\n$counts\nfunction gradient \n      37       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nCan I plot those and see what I’m dealing with?\n\nfreq_x &lt;- tibble(count = X) |&gt; \n  summarise(freq = n()/length(X), .by = count) |&gt; \n  arrange(count)\n\nx_dist &lt;- tibble(count = 0:max(freq_x$count),\n                 pdf = dnbinom(count, \n                               size = sim_fit$estimate[1],\n                               mu = sim_fit$estimate[2]),\n                 pdf_beta = dnbinom(count, \n                                    size = delta, \n                                    prob = p_from_beta(beta)),\n                 pdf_beta_mu = dnbinom(count, \n                                    size = delta, \n                                    mu = mu_from_beta(beta, delta)),\n                 pdf_beta_fit = dnbinom(count, \n                                    size =  fit_sims$par[2], \n                                    prob = p_from_beta(fit_sims$par[1])),\n                 pdf_beta_mu_fit = dnbinom(count, \n                                    size = fit_sims$par[2], \n                                    mu = mu_from_beta(fit_sims$par[1],\n                                                      fit_sims$par[2]))) |&gt; \n  left_join(freq_x)\n\nJoining with `by = join_by(count)`\n\n\nWell, the pdf is the only one that fits. Which makes sense, the others are pulling the mean too low, as we saw above. Clearly that’s not an artifact.\n\nx_dist |&gt; \n  pivot_longer(-count) |&gt; \n  ggplot(aes(x = count, y = value, color = name)) +\n  geom_line()\n\n\n\n\n\n\n\n\nDoes the optim fit work? Even if the fits are wrong, are we recovering the right distribution? They’re close, but not exact, though that’s expected. And the mu vs p parameterisations are dead on.\n\nx_dist |&gt; \n  pivot_longer(-count) |&gt; \n  filter(grepl(\"_\", name)) |&gt; \n  ggplot(aes(x = count, y = value, color = name, linetype = name)) +\n  geom_line(linewidth = 2)\n\n\n\n\n\n\n\n\nSo, where to from here? If we play with C, does that help? Ie if \\(\\rho = \\beta c\\), but the negbin only has beta and the gamma only has c, can we get there?\nie can we do something roundabout like saying we know rho, we can get the beta from mu from the overall negbin, then c has to be obtainable from that?, i.e. \\(c = \\frac{\\rho}{\\beta}\\).\n\nrho = 0.69\nmu = 2.8\ndelta = 1.3\n\nbeta = beta_from_mu(mu, delta)\n\nc_param = rho/beta\n\n\n# just initialise the whole vector\nX &lt;- rnbinom(1000, size = delta, mu = mu)\nY &lt;- X*NA\n\nfor (i in 2:length(X)) {\n  Y[i] &lt;- rgamma(1, shape = delta+X[i-1], scale = c_param)\n  X[i] &lt;- rpois(1, lambda = beta*Y[i])\n}\n\n\nsim_fit &lt;- fitdist(X, 'nbinom')\nsim_fit\n\nFitting of the distribution ' nbinom ' by maximum likelihood \nParameters:\n     estimate Std. Error\nsize 1.540803 0.11666398\nmu   2.892248 0.09122759\n\n\n\nsim_ac &lt;- acf(X)\n\n\n\n\n\n\n\nsim_ac$acf[2]\n\n[1] 0.6656559\n\n\nHOLY COW DID THAT JUST WORK???\nI should also recover rho and delta with the optim. Pretty decent.\n\nfit_sims &lt;- optim(c(0.69, 1.3), make_ll, series = X)\nfit_sims\n\n$par\n[1] 0.6682954 1.4363979\n\n$value\n[1] 1923.407\n\n$counts\nfunction gradient \n      41       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nPlot again\n\nfreq_x &lt;- tibble(count = X) |&gt; \n  summarise(freq = n()/length(X), .by = count) |&gt; \n  arrange(count)\n\nx_dist &lt;- tibble(count = 0:max(freq_x$count),\n                 pdf = dnbinom(count, \n                               size = sim_fit$estimate[1],\n                               mu = sim_fit$estimate[2]),\n                 pdf_mu = dnbinom(count, size = delta, mu = mu),\n                 pdf_beta = dnbinom(count, \n                                    size = delta, \n                                    prob = p_from_beta(beta)),\n                 pdf_beta_mu = dnbinom(count, \n                                    size = delta, \n                                    mu = mu_from_beta(beta, delta)),\n                 pdf_beta_fit = dnbinom(count, \n                                    size =  fit_sims$par[2], \n                                    prob = p_from_beta(fit_sims$par[1])),\n                 pdf_beta_mu_fit = dnbinom(count, \n                                    size = fit_sims$par[2], \n                                    mu = mu_from_beta(fit_sims$par[1],\n                                                      fit_sims$par[2]))) |&gt; \n  left_join(freq_x)\n\nJoining with `by = join_by(count)`\n\n\nPlot, but too much overplotting, so leave the data on there and wrap the estimates. They all work except pdf_beta_fit and pdf_beta_mu_fit, which are the ones that use fit_sims (i.e. the likelihood fit from the paper).\n\nx_dist |&gt; \n  pivot_longer(-count) |&gt; \n  filter(name != 'freq') |&gt; \n  ggplot(aes(x = count, y = value, color = name)) +\n  geom_line() +\n  geom_line(data = x_dist, aes(y = freq), color = 'black') + \n  facet_wrap('name')\n\n\n\n\n\n\n\n\nSo, now to develop a clean method and test with data."
  },
  {
    "objectID": "simmodelling/negbin_autocorr_testing.html#clean-simulation-given-mu-or-p-n-delta-and-rho",
    "href": "simmodelling/negbin_autocorr_testing.html#clean-simulation-given-mu-or-p-n-delta-and-rho",
    "title": "Negative binomial autocorr",
    "section": "Clean simulation given \\(\\mu\\) or \\(p\\), \\(n\\) (\\(\\delta\\)), and \\(\\rho\\)",
    "text": "Clean simulation given \\(\\mu\\) or \\(p\\), \\(n\\) (\\(\\delta\\)), and \\(\\rho\\)\nSo, we want to be able to fit an autocorrelated negative binomial distribution given the distribution of the overall distribution and the AR(1) \\(\\rho\\).\nWe’ll use R’s nbinom parameterisations, which use size and either prob or mu. I will refer to size as \\(\\delta\\) because we are using it here as the dispersion parameter (shape parameter of the gamma mixing distribution). Like R, I’ll allow either prob or mu, though in practice I expect mu will be more common, since it is what’s returned by fitdistrplus::fitdist(), and so will be easier to use when we have empirical distributions and for checking.\nWith a set of those parameters,\n\nrho &lt;- 0.75\ndelta &lt;- 1.5\nmu &lt;- 5\n\nThen we define a function to generate the AR(1) sequence\n\nrnbinomAR &lt;- function(n, size, prob, mu, rho, return_Y = FALSE) {\n  \n  if (!missing(prob) & !missing(mu)) {\n    rlang::abort('Use either `prob` or `mu`')\n  }\n  \n  if (!missing(prob)) {\n    beta &lt;- beta_from_p(prob)\n  }\n  if (!missing(prob)) {\n    beta &lt;- beta_from_mu(mu, size)\n  }\n  \n  # This differs from Gourieroux & Lu, as theirs (c = 1) yielded incorrect means.\n  c_param = rho/beta\n  \n  # just initialise the whole vector\n  if (!missing(prob)) {\n    X &lt;- rnbinom(n, size = size, prob = prob)\n  }\n  if (!missing(mu)) {\n    X &lt;- rnbinom(n, size = size, mu = mu)\n  }\n  \n  # Initialise the intensity process\n  Y &lt;- X*NA\n  \n  # Build the sequence one step at a time according to Gourieroux & Lu Definition 1.\n  for (i in 2:length(X)) {\n    Y[i] &lt;- rgamma(1, shape = size+X[i-1], scale = c_param)\n    X[i] &lt;- rpois(1, lambda = beta*Y[i])\n  }\n  \n  if (returnY) {\n    return(list(X = X, Y = Y))\n  } else {\n    return(X)\n  }\n}\n\nA standard set of checks are then to fit that and return the mu, size, and rho. Could do the acf plot too, but that’s just using acf.\n\nfit_nbinomAR &lt;- function(X) {\n  # check the size and mu\n  musize &lt;- fitdistrplus::fitdist(X, 'nbinom')\n  # check the AR\n  ac_x1 &lt;- acf(X)$acf[2]\n  \n  # return tibble\n  nbinar_est &lt;- tibble::tibble(term = c(names(musize$estimate), 'rho'),\n                               estimate = c(musize$estimate, ac_x1),\n                               std_error = c(musize$sd, NA))\n  \n  return(nbinar_est)\n  \n}\n\nStandard checks using the frequencies and then a chi square or a plot. These just check the distribution, not the AR. We’ll have to take the rho estimation from acf’s word on that.\n\nnbin_emp_pdf &lt;- function(X, size, prob, mu) {\n  \n  freq_x &lt;- tibble::tibble(count = X) |&gt; \n  dplyr::summarise(freq = dplyr::n()/length(X),\n                   .by = count) |&gt; \n  dplyr::arrange(count)\n\n    if (!missing(prob) & !missing(mu)) {\n    rlang::abort('Use either `prob` or `mu`')\n  }\n  \n  if (!missing(prob)) {\n    x_dist &lt;- tibble::tibble(count = 0:max(freq_x$count),\n                         pmf = dnbinom(count,\n                                       size = size,\n                                       prob = prob))\n  }\n  if (!missing(prob)) {\n    x_dist &lt;- tibble::tibble(count = 0:max(freq_x$count),\n                         pmf = dnbinom(count,\n                                       size = size,\n                                       mu = mu))\n  }\n  \n  # Do I want to be able to do this for the *given* params and the *fitted* params? Or just do it twice? I think probably do it twice, otherwise this gets VERY specific.\n\n\n# Join into one dataframe\nx_dist &lt;- x_dist |&gt; \n  dplyr::left_join(freq_x) |&gt; \n  tidyr::pivot_longer(-count)\n\nreturn(x_dist)\n}"
  },
  {
    "objectID": "setup/rstudio_themes.html",
    "href": "setup/rstudio_themes.html",
    "title": "Editing Rstudio themes",
    "section": "",
    "text": "I really don’t like how faint the selection colours are for all of the dark themes. When I do a ‘find’, I don’t want to hunt around for dark grey on black. So to fix that, I need to edit the theme.\n\n\nI went to this massive list of themes, clicked ‘gallery’, chose the one I wanted (just a simple edit of Tomorrow Night, will save a more complex hunt for another day). Then the pane in the middle lets us change all the colours. I clicked the ‘General’ tab, and changed the lineHighlight and selection to a nice blue. I also changed the comment colour to green, not sure if I like that or not.\n\nThen, go to the ‘Info’ tab and change the name before downloading. Otherwise Rstudio won’t find it under a new name.\nDownload. This saves as a .tmTheme file, which I think might just be able to be used directly (see new Posit documentation, but I was looking at something old and so used rstudioapi::convertTheme('setup/Tomorrow Night HL.tmTheme', outputLocation = 'setup') to create a .rstheme file.\nThen global options, add, and select the theme. I had to restart Rstudio a couple times for it to take. The edited theme is available in the git for this website.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Editing Rstudio themes"
    ]
  },
  {
    "objectID": "setup/rstudio_themes.html#creating-a-new-theme",
    "href": "setup/rstudio_themes.html#creating-a-new-theme",
    "title": "Editing Rstudio themes",
    "section": "",
    "text": "I went to this massive list of themes, clicked ‘gallery’, chose the one I wanted (just a simple edit of Tomorrow Night, will save a more complex hunt for another day). Then the pane in the middle lets us change all the colours. I clicked the ‘General’ tab, and changed the lineHighlight and selection to a nice blue. I also changed the comment colour to green, not sure if I like that or not.\n\nThen, go to the ‘Info’ tab and change the name before downloading. Otherwise Rstudio won’t find it under a new name.\nDownload. This saves as a .tmTheme file, which I think might just be able to be used directly (see new Posit documentation, but I was looking at something old and so used rstudioapi::convertTheme('setup/Tomorrow Night HL.tmTheme', outputLocation = 'setup') to create a .rstheme file.\nThen global options, add, and select the theme. I had to restart Rstudio a couple times for it to take. The edited theme is available in the git for this website.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Editing Rstudio themes"
    ]
  },
  {
    "objectID": "setup/R_in_VS.html",
    "href": "setup/R_in_VS.html",
    "title": "R in VS code",
    "section": "",
    "text": "I typically use Rstudio, and am very used to it. But I need to use VScode for working on Azure, and am trying to sort that out. There are also idiosyncracies with using Azure, but I’ll try to hold those for somewhere else and just keep this about VS.",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Using R in VScode"
    ]
  },
  {
    "objectID": "setup/R_in_VS.html#the-issue",
    "href": "setup/R_in_VS.html#the-issue",
    "title": "R in VS code",
    "section": "",
    "text": "I typically use Rstudio, and am very used to it. But I need to use VScode for working on Azure, and am trying to sort that out. There are also idiosyncracies with using Azure, but I’ll try to hold those for somewhere else and just keep this about VS.",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Using R in VScode"
    ]
  },
  {
    "objectID": "setup/R_in_VS.html#the-basics",
    "href": "setup/R_in_VS.html#the-basics",
    "title": "R in VS code",
    "section": "The basics",
    "text": "The basics\nThe VS code documentation gives a pretty good overview of the basics- install VScode, install languageserver, and install the R extension. That gets us up and running. Though it is worth noting that if you tend to work in renv for everything, it’s probably better to install languageserver globally (ie in a non-renv-managed session).\nNow, supposedly that provides linting, debugging, code completion, help, etc. And the add-ons (radian and httpgd look good too in terms of nicer terminal and visualisations). The question now is, HOW do we actually use all that functionality. I’m so used to Rstudio, it’ll take some playing. I’ll try to write down here what I try and how to get it to work.\nI tried to install.packages('languageserver') globally, but it gets grumpy sometimes and can’t find it inside a renv- managed repo because the .libPaths don’t have wherever the global package directory is. It seems to have worked on Windows, but not Azure/Unix.\n\nRadian\nI tried installing radian as the terminal. Assuming I don’t want it to mess up the project-level python environments, I installed it globally through the git-bash in windows terminal. It works when I type radian into VS bash, but does not run when I try to actually run something from a .R file. In the command pallette -&gt; settings -&gt; extensions -&gt; R, there’s an option for Rterm:Windows that says it can be the path to radian. I tried where radian in git-bash (which radian on unix), and it gave me two paths in py-env/shims. The one with the .bat works on windows (C:\\Users\\galen\\.pyenv\\pyenv-win\\shims\\radian.bat). On Unix, it’s just a standard usr/bin/…. I’ve turned radian back off, though, because it throws a really annoying amount of weird errors that don’t actually stop the code from running, and that don’t appear in the base R terminal. Things like ‘unexpected & in }’, when there are no ‘&’ symbols in the code (and it still completes (usually). I think it works in .R scripts, but not in quarto. It’s something about bracketed paste not working right in notebooks I think. Working on sorting that out.\n\n\nlinting\nThis is something that (weirdly, I think) isn’t included in Rstudio. It supposedly is in VScode, but I’m not seeing obvious signs of it.\nInteresting. I don’t know what changed, but it has suddenly started linting. Maybe I turned something on in the settings-&gt;extensions-&gt;R section?\nAnd now all the blue lines are super annoying. Would be nice to at least have a turn off for comments setting. Or a good way to wrap comments a la Rstudio ctrl-shift /.\nGuess I need to figure out how to step through lintr and fix issues.",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Using R in VScode"
    ]
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My background is in theoretical community ecology, where I use analytical theory and probabilistic simulation models to better understand how environmental variation in space and time interact with life history to maintain diversity and community structure. Current work extends this approach to nonstationary environmental variation (e.g. climate change), with a particular focus on how movement and growth across a heterogeneous landscape determines population trajectories.\n\n\n\nBranching",
    "crumbs": [
      "Research",
      "Overview"
    ]
  },
  {
    "objectID": "research.html#analysis-and-methods-development",
    "href": "research.html#analysis-and-methods-development",
    "title": "Research",
    "section": "Analysis and methods development",
    "text": "Analysis and methods development\nCapturing the data needed for linking theory to empirical work or building large-scale management models often requires developing new methods and analytical techniques or applying them in new ways. I enjoy the creativity inherent in this process, whether it is to adapt beta-binomial logistic regression for testing density-dependence in infection rates or modifying cameras to take underwater night vision movies of insect egg-laying.\nIn the course of my work I develop R packages and analysis workflows (some of which are available publicly and some not (yet)). These typically are the core output of my theoretical and modelling work, as well as the analysis of empirical data. Others simply arise as side projects or infrastructure to allow other projects to proceed. As is likely typical of most people writing code, the final, working, code is the end product of quite a lot of experimentation and figuring out new approaches. I have begun documenting this process both for my future self and others.",
    "crumbs": [
      "Research",
      "Overview"
    ]
  },
  {
    "objectID": "publishing/reference_sections.html",
    "href": "publishing/reference_sections.html",
    "title": "Reference sections",
    "section": "",
    "text": "library(ggplot2)",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Sections and numbering"
    ]
  },
  {
    "objectID": "publishing/reference_sections.html#bibliography-style",
    "href": "publishing/reference_sections.html#bibliography-style",
    "title": "Reference sections",
    "section": "1 Bibliography style",
    "text": "1 Bibliography style\nUse a csl file to change the referencing style, both in-text and in-bib. Put\ncsl: ecology.csl\nin the header to use a different file. Get CSL files from their github, and look at what they actually look like from CSL’s visual editor (which seems to kick over to something managed by Zotero.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Sections and numbering"
    ]
  },
  {
    "objectID": "publishing/reference_sections.html#references-bibliography",
    "href": "publishing/reference_sections.html#references-bibliography",
    "title": "Reference sections",
    "section": "2 References (bibliography)",
    "text": "2 References (bibliography)\nTo put the bibliography wherever you want, include a div ::: {#refs} wherever you want it to be. If we cite Holt and Chesson (2018), we can put the bib right here.\nNote, we need square brackets around the citation to get the author in parentheses. This one has square brackets (Holt and Chesson 2018) and this one Holt and Chesson (2018) does not.\n\n\nHolt, Galen, and Peter Chesson. 2018. “The Role of Branching in the Maintenance of Diversity in Watersheds.” Freshwater Science 37 (4): 712–30. https://doi.org/10.1086/700680.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Sections and numbering"
    ]
  },
  {
    "objectID": "publishing/reference_sections.html#figures-and-tables-with-sections",
    "href": "publishing/reference_sections.html#figures-and-tables-with-sections",
    "title": "Reference sections",
    "section": "3 Figures and tables with sections",
    "text": "3 Figures and tables with sections\nI want to have figs and tables in different sections; e.g. in main text and supplement or different chapters. It seems to work different for figs generated from code and images loaded in.\nIn the main text, we just use tbl and fig, referenced with @tbl-iris (Table 1) for tables, @fig-branching (Figure 1) for read-in images, and @fig-iris (Figure 2) for code-generated figures:\n\n```{r}\n#| label: tbl-iris\n#| tbl-cap: Head of iris dataframe\nhead(iris) |&gt; knitr::kable()\n```\n\n\n\nTable 1: Head of iris dataframe\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\n\n![An image of branching](../_images/branching.png){#fig-branching} gives\n\n\n\n\n\n\nFigure 1: An image of branching\n\n\n\n\n```{r}\n#| label: fig-iris\n#| fig-cap: A figure of the iris dataframe\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n```\n\n\n\n\n\n\n\nFigure 2: A figure of the iris dataframe",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Sections and numbering"
    ]
  },
  {
    "objectID": "publishing/reference_sections.html#new-section",
    "href": "publishing/reference_sections.html#new-section",
    "title": "Reference sections",
    "section": "4 New section",
    "text": "4 New section\nNow, according to the Quarto docs, we can add a new kind of crossref. So I’ve put the following from that site in the yaml header:\ncrossref:\n  custom:\n    - kind: float\n      key: suppfig\n      latex-env: suppfig\n      reference-prefix: Figure S\n      space-before-numbering: false\n    - kind: float\n      key: supptbl\n      latex-env: supptbl\n      reference-prefix: Table S\n      space-before-numbering: false\n      caption-location: top\nSee docs for options.\nNow, the trick, which I missed, is that it only works for fenced divs (see github issues). See quarto docs.\nNow, it should work to use suppfig to reference loaded figures such as @suppfig-elephant (Figure S1) for read-in images, and @suppfig-iris (Figure S2) for code-generated figures and tables @supptbl-iris (Table S1) Only if we use divs:\n![An image of Surus](../_images/surus.png){#suppfig-elephant} gives\n\n\n\n\n\n\nFigure S1: An image of Surus\n\n\n\nWapping this in ::: {#suppfig-iris} and putting the caption inside:\n\n\n\n\n```{r}\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Petal.Width)) +\n  geom_point()\n```\n\n\n\n\n\n\n\n\n\n\nFigure S2: Another iris plot\n\n\n\nNow tables. It captions at the bottom though unles we set caption-location: top in the yml.\n\n\n\nTable S1: Tail of the iris df.\n\n\n\n```{r}\ntail(iris) |&gt; knitr::kable()\n```\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n145\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n146\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n147\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n148\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n149\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n150\n5.9\n3.0\n5.1\n1.8\nvirginica",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Sections and numbering"
    ]
  },
  {
    "objectID": "publishing/reference_sections.html#unnumbered-sections",
    "href": "publishing/reference_sections.html#unnumbered-sections",
    "title": "Reference sections",
    "section": "Unnumbered sections",
    "text": "Unnumbered sections\nIf we have number-sections: true in the yaml, we can turn it off for some by putting {.unnumbered} next to their names (as I have for this section).",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Sections and numbering"
    ]
  },
  {
    "objectID": "publishing/multi_csl.html",
    "href": "publishing/multi_csl.html",
    "title": "Multi-csl?",
    "section": "",
    "text": "Does it work to set multiple csl files? and multiple reference sections? Why would I want to do that? Because the ecology.csl isn’t producing what I’d expect (or what Zotero produces with it). Instead of DOIs printing, it turns the titles into links. So I want to make a huge doc that shows how the different CSLs actually export from Quarto.\nTry to cite (Holt and Chesson 2018) here with the default csl\n\n\n\nWe know it works to set csl in the quarto header, but can we set it later and swap? Insert a yaml block\n---\ncsl: nature.csl\n---\nTry that again. Do we get the earlier cite too, cite a different paper to check Holt et al. (2021).\n\n\n\nThat overrode the default even above. What happens if we add yet another explicit CSL?\n---\ncsl: ecology.csl\n---\nAnd cite yet another Holt, Macqueen, and Lester (2024).\n\n\n\nAnd now everything is in Ecology format. So whatever’s last overrides earlier, and it doesn’t seem possible. Unless perhaps we actually make new docs and glue them together.\nI doubt it; the includes webpage says\n\nIt also means that if the included file has a metadata block, that block will take effect in all included files. In most cases, having metadata blocks in an included file will cause unexpected behavior.\n\nThis probably isn’t worth much more effort. I’ll just look individually at the most likely csls to work.\n---\ncsl: freshwater-biology.csl\n---\n\n\n\n---\ncsl: apa.csl\n---\n\n\n\n\n\n\n\nReferences\n\nHolt, Galen, and Peter Chesson. 2018. “The Role of Branching in the Maintenance of Diversity in Watersheds.” Freshwater Science 37 (4): 712–30. https://doi.org/10.1086/700680.\n\n\nHolt, Galen, Georgia K. Dwyer, Courtney Bourke, Ty G. Matthews, Ashley Macqueen, and Rebecca E. Lester. 2021. “Characteristics and Consequences of a Disease Outbreak in Aquatic Insects.” Freshwater Biology 66 (7): 1267–81. https://doi.org/10.1111/fwb.13715.\n\n\nHolt, Galen, Ashley Macqueen, and Rebecca E. Lester. 2024. “A Flexible Consistent Framework for Modelling Multiple Interacting Environmental Responses to Management in Space and Time.” Journal of Environmental Management 367 (September): 122054. https://doi.org/10.1016/j.jenvman.2024.122054.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Multi-csl?"
    ]
  },
  {
    "objectID": "publishing/flextable_word.html",
    "href": "publishing/flextable_word.html",
    "title": "Flextable in word",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nWarning: package 'flextable' was built under R version 4.4.2\n\n\nComp\n\n\nGeneral HydroBOT componentsTypeGeneral component definitionsSpecific components used in our exampleInput dataNot part of HydroBOTHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. Modified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)ControllerWorkflowInterface between input data, response model, and other toolkit components. Sets up run(s).Sets up links to data and parameters for EWR tool and aggregations.Response modelsExternal, integratedA model of the response of values, e.g.  social, cultural, environmental, or economic values in response to hydrologic drivers.EWR toolAggregatorWorkflowAggregates response model results to scales across the dimensions of time, space, and theme. Response model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).ComparerWorkflowCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.Comparison of environmental values at various theme scales for the example climate and adaptation scenariosCausal networksExternal, integratedDescribe causal relationships between values.Long Term Water Plan (LTWP)Spatial dataExternal, integratedDescribe spatial relationshipsGauge locations, Sustainable Diversion Limits (SDL) units, Murray-Darling Basin"
  },
  {
    "objectID": "publishing/flextable_word.html#tables",
    "href": "publishing/flextable_word.html#tables",
    "title": "Flextable in word",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nWarning: package 'flextable' was built under R version 4.4.2\n\n\nComp\n\n\nGeneral HydroBOT componentsTypeGeneral component definitionsSpecific components used in our exampleInput dataNot part of HydroBOTHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. Modified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)ControllerWorkflowInterface between input data, response model, and other toolkit components. Sets up run(s).Sets up links to data and parameters for EWR tool and aggregations.Response modelsExternal, integratedA model of the response of values, e.g.  social, cultural, environmental, or economic values in response to hydrologic drivers.EWR toolAggregatorWorkflowAggregates response model results to scales across the dimensions of time, space, and theme. Response model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).ComparerWorkflowCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.Comparison of environmental values at various theme scales for the example climate and adaptation scenariosCausal networksExternal, integratedDescribe causal relationships between values.Long Term Water Plan (LTWP)Spatial dataExternal, integratedDescribe spatial relationshipsGauge locations, Sustainable Diversion Limits (SDL) units, Murray-Darling Basin"
  },
  {
    "objectID": "plotting/tweaks_tricks.html",
    "href": "plotting/tweaks_tricks.html",
    "title": "Theming and saving",
    "section": "",
    "text": "library(tidyverse) # Overkill, but easier than picking and choosing",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Saving and themeing plots"
    ]
  },
  {
    "objectID": "plotting/tweaks_tricks.html#theming",
    "href": "plotting/tweaks_tricks.html#theming",
    "title": "Theming and saving",
    "section": "Theming",
    "text": "Theming\nI tend to establish a theme to set the basic plot look, including font sizes. I start with theme_bw() because the default ggplot grey background doesn’t look good in pubs. I used to set the sizes separately for each sort of text (commented out), but that is typically easier to just use the base_size argument and let ggplot handle the relative adjustments.\nCan also set theme differently for presentations, including doing things like setting font to match a ppt theme.\nDeveloping themes is now done more extensively in ggplot themes, where I develop them as more flexible functions.\nTypically, very few fonts are loaded into R and available for use. See fonts.Rmd for figuring out how to work with that. The short answer is that we use showtext to load what we need (if anything). If this step is skipped, will default to the default font and throw a warning about “fontfamily not found” because we haven’t loaded the selected font yet.\nWe could load fonts by hand Using functions from showtext and sysfonts, and specify the text = element_text(family=\"Ink Free\") with a manual character vector. It’s way easier to automate though, and saves issues of loading the wrong font.\nFirst, load the function I wrote that simplifies finding the files and their names to load them. And tell R to use showtext to render fonts.\n\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galenR\n\nshowtext::showtext_auto()\n\nThen, load the font(s) we want\n\ntalkfont &lt;- 'Ink Free'\npubfont &lt;- 'Cambria'\nloadfonts(fontvec = c(talkfont, pubfont))\n# loadfonts()\n\nNote that we could also just loadfonts() with no arguments to read in ALL available fonts\nPass talkfont and pubfont to the themes.\n\ntalktheme &lt;- theme_bw(base_size = 18) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=talkfont)) # Replace with fontname used in PPT\n\n# \n        # axis.text = element_text(size = 18),\n        # axis.title = element_text(size = 24),\n        # strip.text = element_text(size = 24),\n        # plot.title = element_text(size = 24))\n\npubtheme &lt;- theme_bw(base_size = 10) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=pubfont))\n\nAs an example, let’s make a simple plot with iris, and then look at the themed versions.\n\nbaseiris &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\nbaseiris\n\n\n\n\n\n\n\n\nNow, what does a publication version look like?\n\nbaseiris + pubtheme\n\n\n\n\n\n\n\n\nNote that further theme changes can happen later on, e.g. Note that it’s easy to get in trouble with the internal legend positions when it comes time to save- as the dimensions change on export vs whatever arbitrary size you have the Rstudio plot pane, what looks good will changes as well.\n\nbaseiris + pubtheme +\n  theme(legend.title = element_text(face = 'bold'),\n        legend.position = c(0.8,0.2))\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nFor talks, we use talktheme. Terrible font, but easy to see that it’s been shifted from default.\n\nbaseiris + talktheme\n\n\n\n\n\n\n\n\nWe can update parts of the theme including the font while keeping the rest. Though if we haven’t loaded all fonts, will need to load the new ones now.\n\n# load new font\nloadfonts(fontvec = 'Elephant')\n\ntalktheme &lt;- talktheme + \n  theme(text = element_text(family = 'Elephant')) # Replace with fontname used in PPT\n\nAnd to show that it worked, plot again.\n\nbaseiris + talktheme",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Saving and themeing plots"
    ]
  },
  {
    "objectID": "plotting/rgb_to_hex.html",
    "href": "plotting/rgb_to_hex.html",
    "title": "RGB to hex",
    "section": "",
    "text": "library(dplyr)\nlibrary(colorspace)\nlibrary(ggplot2)\n\nLet’s say we have a dataframe with R, G, B values, but we want the hex. Why? Maybe we want to use scale_color_identity to plot the values in some other columns. Let’s say x and y.\n\nset.seed(17)\nrgbtib &lt;- tidyr::expand_grid(x = 1:10, y = 1:10) %&gt;% \n  mutate(R = sample(0:255, 100),\n                         G = sample(0:255, 100),\n                         B = sample(0:255, 100))\nrgbtib\n\n# A tibble: 100 × 5\n       x     y     R     G     B\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1   177   166    44\n 2     1     2   231    80    48\n 3     1     3   224   188   136\n 4     1     4   221   168   181\n 5     1     5   107    82    93\n 6     1     6   238   111   179\n 7     1     7   246   242   124\n 8     1     8   230    57   213\n 9     1     9   172   151   130\n10     1    10   166    93    84\n# ℹ 90 more rows\n\n\nWe can use {colorspace}, but it’s convoluted- have to make an RGB object first, and then convert to hex. And the RGB need to be on 0-1, not 0-255.\nWriting that out doesn’t work because the colorspace RGB object can’t get stuffed in the dataframe. But this is the idea\n\nrgbtib_writeout &lt;- rgbtib %&gt;% \n  # Convert to 0-1\n  mutate(across(all_of(c('R', 'G', 'B')), ~./255)) %&gt;%\n  # Create the rgb object\n  mutate(rgbobj = colorspace::RGB(R, G, B)) %&gt;% \n  # Get the hex values\n  mutate(hexval = colorspace::hex(rgbobj))\n\nSo, make a function. Have a maxval the user can pass (don’t assume it’s 1 or 255).\n\nrgb2hex &lt;- function(R, G, B, maxval = 255) {\n  rgbobj &lt;- colorspace::RGB(R/maxval, G/maxval, B/maxval)\n  hexval &lt;- colorspace::hex(rgbobj)\n  return(hexval)\n}\n\nTest that\n\nrgb2hex(177, 41, 147)\n\n[1] \"#D970C8\"\n\n\napparently quarto doesn’t do the cool printing of color thing in output, just input.\n\n\"#D970C8\"\n\n[1] \"#D970C8\"\n\n\nNow, use that in the mutate\n\nrgbtib &lt;- rgbtib %&gt;% \n  mutate(hexvals = rgb2hex(R, G, B))\nrgbtib\n\n# A tibble: 100 × 6\n       x     y     R     G     B hexvals\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  \n 1     1     1   177   166    44 #D9D373\n 2     1     2   231    80    48 #F49878\n 3     1     3   224   188   136 #F1DFC1\n 4     1     4   221   168   181 #EFD4DB\n 5     1     5   107    82    93 #AD9AA3\n 6     1     6   238   111   179 #F7B0DA\n 7     1     7   246   242   124 #FBF9B9\n 8     1     8   230    57   213 #F482EC\n 9     1     9   172   151   130 #D6CABD\n10     1    10   166    93    84 #D3A39B\n# ℹ 90 more rows\n\n\nPlot to show it works\n\nggplot(rgbtib, aes(x = x, y = y, fill = hexvals)) + geom_tile() + theme(legend.position = 'none')",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Converting RGB to hex colors"
    ]
  },
  {
    "objectID": "plotting/math_in_ggplot.html",
    "href": "plotting/math_in_ggplot.html",
    "title": "Math and greek in legends",
    "section": "",
    "text": "I often need math in ggplot labels. Sometimes it’s actual math, but often just things like greek letters, subscripts, fractions, hats, bars, etc. THere’s ways to do most of that with expression and paste and bquote, but none of it is ever very intuitive for me. I’d like to be able to just send it latex and have it work.\nDoes latex2exp work?\n\n\n\nlibrary(ggplot2) \nlibrary(latex2exp)\n\nLet’s just try to do some things with that. Make the usual iris plot.\n\ntestplot &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\ntestplot\n\n\n\n\n\n\n\n\nNow let’s add some math. I’d typically use labs for the x,y, and color, so try that. Spacing not great, but it works.\nBasically following the manual, it’s pretty self-explanatory.\nThe r says to use raw strings so don’t have to escape slashes.\n\ntestplot &lt;- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho})'),\n       color = TeX(r'($\\left{ \\int_0^\\inf \\exp{\\eta x} dx \\right})'))\n\ntestplot\n\n\n\n\n\n\n\n\nCan I use amsmath in latex? Maybe, but not for linebreaks- this errors.\n\ntestplot &lt;- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho}$)'),\n       color = TeX(r'($\\begin{split}\\left{ \\int_0^\\inf \\\\ \\exp{\\eta x} dx \\right}\\end{split}$)'))\n\ntestplot",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Latex in ggplot with latex2exp"
    ]
  },
  {
    "objectID": "plotting/math_in_ggplot.html#using-latex2exp",
    "href": "plotting/math_in_ggplot.html#using-latex2exp",
    "title": "Math and greek in legends",
    "section": "",
    "text": "library(ggplot2) \nlibrary(latex2exp)\n\nLet’s just try to do some things with that. Make the usual iris plot.\n\ntestplot &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\ntestplot\n\n\n\n\n\n\n\n\nNow let’s add some math. I’d typically use labs for the x,y, and color, so try that. Spacing not great, but it works.\nBasically following the manual, it’s pretty self-explanatory.\nThe r says to use raw strings so don’t have to escape slashes.\n\ntestplot &lt;- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho})'),\n       color = TeX(r'($\\left{ \\int_0^\\inf \\exp{\\eta x} dx \\right})'))\n\ntestplot\n\n\n\n\n\n\n\n\nCan I use amsmath in latex? Maybe, but not for linebreaks- this errors.\n\ntestplot &lt;- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho}$)'),\n       color = TeX(r'($\\begin{split}\\left{ \\int_0^\\inf \\\\ \\exp{\\eta x} dx \\right}\\end{split}$)'))\n\ntestplot",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Latex in ggplot with latex2exp"
    ]
  },
  {
    "objectID": "plotting/hcl_exploration.html",
    "href": "plotting/hcl_exploration.html",
    "title": "hcl exploration",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\nUsing knitr::inline_expr(r message = FALSE, warning = FALSE) hopefully stops it printing all the package messages\nlibrary(tidyverse) # Overkill, but easier than picking and choosing\nlibrary(colorspace)\nFinding colors to use for a given plot can be a pain. I’m trying to find some good color ramps for a project, and also sort out manipulating those colors to allow fading. This is me playing around to try to understand how to do those manipulations and looking at the various potential color palettes.\nColorspace (https://colorspace.r-forge.r-project.org/index.html) is a particularly useful package (though it is not the only color package I use).\nColorspace uses a hue-chroma-luminance specification for colors that is really powerful. It also has built-in palettes. For some other work, I was interested in exploring moving along those dimensions and generating color palettes for plotting.\nPreviously (for the project that gave rise to looking at fading colors), I was using purples and emerald, so let’s start there. But for simplicity switch to greens so constant hue.\nswatchplot(\n  'Purples' = sequential_hcl(8, 'Purples'),\n  'Greens' = sequential_hcl(8, 'Greens'))\nI actually like those single-hue fades a lot for showing more or less of something. But it SHOULD be possible to do a hue shift from green to purple for one axis? will that make sense?",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Exploring HCL colorspace"
    ]
  },
  {
    "objectID": "plotting/hcl_exploration.html#hue-sequences",
    "href": "plotting/hcl_exploration.html#hue-sequences",
    "title": "hcl exploration",
    "section": "Hue sequences",
    "text": "Hue sequences\nI’d like to be able to specify the endpoints of a hue sequence and just shift along that axis. I’ll try it out with the purple and green above.\nFirst, I want to try to get the hue values (and the L and C as well) to make the endpoints. I can’t find a straightforward extraction in colorspace to get the HCLs though. So, since I know the endpoints are coming from those palettes above, I want their values. Make the palette, turn it into RGB, then turn the RGB into polarLUV to get the three axis values. Here, rows are the 8 fades in the palettes above.\n\nrgbpurps &lt;- hex2RGB(sequential_hcl(8, 'Purples'))\n\nluvpurps &lt;- as(rgbpurps, 'polarLUV')\nluvpurps\n\n            L         C        H\n[1,] 19.88570 55.128356 274.8415\n[2,] 34.37280 69.304529 274.3131\n[3,] 47.99202 56.799744 273.3506\n[4,] 60.90031 43.200021 272.3221\n[5,] 72.77975 31.772302 271.4182\n[6,] 83.46538 21.076091 271.6285\n[7,] 92.78865 10.863733 268.7090\n[8,] 98.79258  2.985742 276.3941\n\n\nThat’s sure roundabout, going palette that’s polarLUV under the hood but returns in hex to rgb and then back to polarLUV. Seems to work though.\n\nswatchplot(hex(luvpurps))\n\n\n\n\n\n\n\n\n\nrgbgrns &lt;- hex2RGB(sequential_hcl(8, 'Greens'))\n\nluvgrns &lt;- as(rgbgrns, 'polarLUV')\nluvgrns\n\n            L         C        H\n[1,] 25.06952 33.792199 132.8916\n[2,] 40.15678 49.456834 132.0640\n[3,] 54.06676 63.854764 129.4059\n[4,] 66.47833 62.340742 126.5380\n[5,] 77.49000 47.581607 123.8001\n[6,] 86.86700 33.248323 120.6451\n[7,] 93.95644 19.112933 117.4570\n[8,] 98.08100  5.367478 116.7639\n\n\nI can swatchplot them up together.\n\nswatchplot(hex(luvpurps), hex(luvgrns))\n\n\n\n\n\n\n\n\nNow, the goal is actually to identify those dark colors and transition between them. Now, can I get from purple to green? The L and C are quite different, unfortunately. Pick something the middle?\nHardcode numbers for now, though ideally we’ll get to a function that takes a start and end value.\n\npg &lt;- polarLUV(L = 20, C = 40, H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\nWarning in max(nchar(rnam) - 1L): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\n\nThat fails. So, now we learned the ranges of the other axes matter. Likely chroma?\n\n# Fails\nmax_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20)\n\n[1] 29.55000 19.90429 16.62571 16.05429 17.74000 23.18000 41.07429 66.11000\n\n\nCan I just use the minimum max_chroma? Not really…\n\n# Guessing I can't just go with 16, but let's try\npg &lt;- polarLUV(L = 20, C = 16, H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\n\n\n\n\n\n\n\nIf I try to fix how dark that is with chroma, it doesn’t work very well and I still lose one.\n\npg &lt;- polarLUV(L = 20, \n               C = max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20),\n               H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\n\n\n\n\n\n\n\nUsing a matrix isn’t the answer- same thing, though a floor argument puts the missing color back\n\nhclmat &lt;- cbind(20, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20, floor = TRUE),\n      seq(from = 130, to = 275, length.out = 8))\n\npg &lt;- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\n\n\n\n\nGuessing I don’t want to just turn up luminance, but let’s see what that does to get a better sense how this all works.\n\nhclmat &lt;- cbind(80, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 80, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg &lt;- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\n\n\n\n\nLower luminance does work OK, but it’s still ‘darker’ in the middle and the shift to blue on the right is abrupt. The darker middle is likely why a lot of the colorspace palettes have triangular luminance. I don’t particularly want to get so fine-tuned here. I was looking for a way to programatically define these sequences, and getting into tweaking luminance in a nonlinear and nonmonotonic way could get very bespoke very quickly. Likely better to just use the built-in palettes where someone who understands color theory has already done that.\n\nhclmat &lt;- cbind(50, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 50, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg &lt;- polarLUV(hclmat)\nswatchplot(hex(pg))",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Exploring HCL colorspace"
    ]
  },
  {
    "objectID": "plotting/hcl_exploration.html#fading",
    "href": "plotting/hcl_exploration.html#fading",
    "title": "hcl exploration",
    "section": "Fading",
    "text": "Fading\nI also want to make faded versions of palettes, and control levels of fade. The particular use I have in mind is to illustrate levels of uncertainty, but it could be any bivariate outcomes.\nI originally thought that I would need to manually adjust the chroma and luminance manually. But the exploration above suggests they interact and so it’s unlikely to just shift one or the other. Still, colorspace provides lighten, darken (which both shift luminance), and desaturate, which shifts chroma. I should be able to play with these to see how they work using either a homebrew base palette as above or the inbuilt ones.\nIn either case, we need the hex values\n\nhexcols &lt;- hex(pg)\n\nLighten (increase luminance)\n\nswatchplot('orig' = hexcols,\n           '25' = lighten(hexcols, amount = 0.25),\n           '50' = lighten(hexcols, amount = 0.5),\n           '75' = lighten(hexcols, amount = 0.75),\n           '100' = lighten(hexcols, amount = 1))\n\n\n\n\n\n\n\n\nDarken (decrease luminance)\n\nswatchplot('orig' = hexcols,\n           '25' = darken(hexcols, amount = 0.25),\n           '50' = darken(hexcols, amount = 0.5),\n           '75' = darken(hexcols, amount = 0.75),\n           '100' = darken(hexcols, amount = 1))\n\n\n\n\n\n\n\n\nDesaturate (adjust chroma)\n\nswatchplot('orig' = hexcols,\n           '25' = desaturate(hexcols, amount = 0.25),\n           '50' = desaturate(hexcols, amount = 0.5),\n           '75' = desaturate(hexcols, amount = 0.75),\n           '100' = desaturate(hexcols, amount = 1))\n\n\n\n\n\n\n\n\nFor my particular use, I like desaturating better, in that it implies less information. But it also makes the values look more similar across the range, and we don’t want that. That gets captured better by lightening.\nAs a bit of an aside, the ends of the lightened versions are effectively ‘Purples’ and ‘Greens’, reading down instead of across. What does it look like if I desaturate those built-in palettes?\n\npurp8 &lt;- sequential_hcl(8, 'Purples')\nswatchplot('orig' = purp8,\n           '25' = desaturate(purp8, amount = 0.25),\n           '50' = desaturate(purp8, amount = 0.5),\n           '75' = desaturate(purp8, amount = 0.75),\n           '100' = desaturate(purp8, amount = 1))\n\n\n\n\n\n\n\n\nIt does remove color, but it perceptually darkens as well, which is NOT what I want.\nWhat about choosing a pre-built set of colors and lightening/darkening? Start with viridis, we know it has good properties in greyscale, etc.\n\nvir8 &lt;- sequential_hcl(8, 'Viridis')\nswatchplot('orig' = vir8,\n                 '25' = lighten(vir8, amount = 0.25),\n                 '50' = lighten(vir8, amount = 0.5),\n                 '75' = lighten(vir8, amount = 0.75),\n                 '100' = lighten(vir8, amount = 1))\n\n\n\n\n\n\n\n\nThat actually works pretty well, even though the original had a luminance ramp on it already (https://colorspace.r-forge.r-project.org/articles/approximations.html), this just shifts it each time, I think. We can compare using specplot.\n\nspecplot(vir8, lighten(vir8, amount = 0.75))\n\n\n\n\n\n\n\n\nWhat does a desaturated viridis look like?\n\nswatchplot('orig' = vir8,\n           '25' = desaturate(vir8, amount = 0.25),\n           '50' = desaturate(vir8, amount = 0.5),\n           '75' = desaturate(vir8, amount = 0.75),\n           '100' = desaturate(vir8, amount = 1))\n\n\n\n\n\n\n\n\nAgain, makes them more similar, though the underlying luminance ramp helps. I don’t like that the first level still ends up darker though.\n\nInteracting chroma and luminance\nSo, changing luminance makes colors brighter or darker, while adjusting chroma removes color but tends to make them darker. Neither is exactly what I want- a color ramp that look the same, just “faded”. Is the answer to control this interaction? Does a simultaneous lighten and desaturate give me what I want by avoiding the perceptual darkening from the desaturation?\n\nswatchplot('orig' = vir8,\n           '25' = desaturate(vir8, amount = 0.25) %&gt;%\n             lighten(amount = 0.25),\n           '50' = desaturate(vir8, amount = 0.5) %&gt;%\n             lighten(amount = 0.5),\n           '75' = desaturate(vir8, amount = 0.75) %&gt;%\n             lighten(amount = 0.75),\n           '100' = desaturate(vir8, amount = 1) %&gt;%\n             lighten(amount = 1))\n\n\n\n\n\n\n\n\nThat works really well, actually. Does the order of operations matter? No:\n\nswatchplot('orig' = vir8,\n           '25' = lighten(vir8, amount = 0.25) %&gt;%\n             desaturate(amount = 0.25),\n           '50' = lighten(vir8, amount = 0.5) %&gt;%\n             desaturate(amount = 0.5),\n           '75' = lighten(vir8, amount = 0.75) %&gt;%\n             desaturate(amount = 0.75),\n           '100' = lighten(vir8, amount = 1) %&gt;%\n             desaturate(amount = 1))\n\n\n\n\n\n\n\n\nDid I just get lucky with viridis, or does it work with other palettes too? how about my ramp that I made from green to purple? Seems to:\n\nswatchplot('orig' = hexcols,\n           '25' = lighten(hexcols, amount = 0.25) %&gt;%\n             desaturate(amount = 0.25),\n           '50' = lighten(hexcols, amount = 0.5) %&gt;%\n             desaturate(amount = 0.5),\n           '75' = lighten(hexcols, amount = 0.75) %&gt;%\n             desaturate(amount = 0.75),\n           '100' = lighten(hexcols, amount = 1) %&gt;%\n             desaturate(amount = 1))\n\n\n\n\n\n\n\n\nDoes the lighten and desat work for the single-hue scales? Seems like it shouldn’t because they’re already changing along those axes.\n\nswatchplot('orig' = purp8,\n           '25' = lighten(purp8, amount = 0.25) %&gt;%\n             desaturate(amount = 0.25),\n           '50' = lighten(purp8, amount = 0.5) %&gt;%\n             desaturate(amount = 0.5),\n           '75' = lighten(purp8, amount = 0.75) %&gt;%\n             desaturate(amount = 0.75),\n           '100' = lighten(purp8, amount = 1) %&gt;%\n             desaturate(amount = 1))\n\n\n\n\n\n\n\n\nNot really. It basically does what it should, but the light end is just always light and so doesn’t contain info in the faded dimension and very similar colors appear in both dimensions- values at row n and col m are frequently very similar to row n + 1 and col m - 1.\nI suppose that might be OK for particular situations, but still not ideal. Might work ok though if we limited that lower end? ie don’t let it fall all the way to white in the original? Getting pretty hacky at that point and the diagonals are still too similar.\n\nswatchplot('orig' = purp8[1:6],\n           '25' = lighten(purp8[1:6], amount = 0.25) %&gt;%\n             desaturate(amount = 0.25),\n           '50' = lighten(purp8[1:6], amount = 0.5) %&gt;%\n             desaturate(amount = 0.5),\n           '75' = lighten(purp8[1:6], amount = 0.75) %&gt;%\n             desaturate(amount = 0.75),\n           '100' = lighten(purp8[1:6], amount = 1) %&gt;%\n             desaturate(amount = 1))\n\n\n\n\n\n\n\n\n\nTesting with other palettes\nViridis and the one I made are both fine, but look at a couple other palettes too. This is not comprehensive, mostly looking at those that have greens and purples for the use I have in mind.\nWrite a little function to do the fade and make this less cut and paste\n\npalcheck &lt;- function(palname, n = 8) {\n pal8 &lt;- sequential_hcl(n, palname)\n \n swatchplot('orig' = pal8,\n            '25' = lighten(pal8, amount = 0.25) %&gt;%\n              desaturate(amount = 0.25),\n            '50' = lighten(pal8, amount = 0.5) %&gt;%\n              desaturate(amount = 0.5),\n            '75' = lighten(pal8, amount = 0.75) %&gt;%\n              desaturate(amount = 0.75),\n            '100' = lighten(pal8, amount = 1) %&gt;%\n              desaturate(amount = 1))\n \n}\n\nPlasma\n\npalcheck('Plasma')\n\n\n\n\n\n\n\n\nGreen-based\nag_GrnYl is OK, but does get a bit of the diagonal issue\n\npalcheck('ag_GrnYl')\n\n\n\n\n\n\n\n\nditto Emrld, but might work?\n\npalcheck('Emrld')\n\n\n\n\n\n\n\n\nTerrains might be OK? 2 is less gaudy\n\npalcheck('Terrain')\n\n\n\n\n\n\n\npalcheck('Terrain2')\n\n\n\n\n\n\n\n\nmints and TealGrn fail diagonal test\n\npalcheck('Dark Mint')\n\n\n\n\n\n\n\npalcheck('Mint')\n\n\n\n\n\n\n\npalcheck('TealGrn')\n\n\n\n\n\n\n\n\nYlGn is pretty good, actually.\n\npalcheck('YlGn')\n\n\n\n\n\n\n\n\nFor the specific use, keep in mind that it will be two levels of fade, and so I can do something like orig and 75% and it’ll be pretty different. But here I’m trying to be fairly general.\nMeh\n\npalcheck('BluGrn')\n\n\n\n\n\n\n\n\nas expected, batlow and Hawaii are extreme, though might be OK?\n\npalcheck('Batlow')\n\n\n\n\n\n\n\npalcheck('Hawaii')\n\n\n\n\n\n\n\n\nPurple-based\nsingle hue doesn’t work\n\npalcheck('Purples')\n\n\n\n\n\n\n\npalcheck('Purples 3')\n\n\n\n\n\n\n\n\nthese are all maybes with tricky diagonals\n\npalcheck('Purple-Blu')\n\n\n\n\n\n\n\npalcheck('Purple-Ora')\n\n\n\n\n\n\n\npalcheck('Purp')\n\n\n\n\n\n\n\npalcheck('PurpOr')\n\n\n\n\n\n\n\npalcheck('Sunset')\n\n\n\n\n\n\n\npalcheck('Magenta')\n\n\n\n\n\n\n\npalcheck('SunsetDark')\n\n\n\n\n\n\n\n\npretty good, but have a fair amount of green in, so could be confusing\n\npalcheck('Purple-Yellow')\n\n\n\n\n\n\n\npalcheck('Viridis')\n\n\n\n\n\n\n\npalcheck('Mako')\n\n\n\n\n\n\n\n\nPlasma pretty good\n\npalcheck('Plasma')\n\n\n\n\n\n\n\n\nInferno might actually be pretty good if I cut off the first one\n\npalcheck('Inferno')\n\n\n\n\n\n\n\n\nag_Sunset is better on the diagonals than similar hue sequences\n\npalcheck('ag_Sunset')\n\n\n\n\n\n\n\n\nGood, but would need to cut the last one; too white. It is less gaudy/ more obviously a hue ramp than ag sunset. Diagonals are tricky too\n\npalcheck('RdPu')\n\n\n\n\n\n\n\n\nPretty good, but blue could be an issue getting confused with water for this project.\n\npalcheck('BuPu')\n\n\n\n\n\n\n\n\n\n\n\nContinuous hue from specified palettes\nIf I want to map values to colors continously, that gets tricky using the specified palettes because sequential_hcl takes an n argument.\nCan I get the endpoints and make my own (as I did above with green and purple?)\ndoes the one I’m using use a linear hue scale\n\nspecplot(sequential_hcl(8, 'ag_Sunset'))\n\n\n\n\n\n\n\n\nIt does, but doesn’t use linear chroma. and it has luminance shift too.\nCan I extract the hue from the ends? The same way I did right at the beginning for the greens and purples.\n\nspecplot(sequential_hcl(2, 'ag_Sunset'))\n\n\n\n\n\n\n\nrgbsun &lt;- hex2RGB(sequential_hcl(8, 'ag_Sunset'))\n\nluvsun &lt;- as(rgbsun, 'polarLUV')\nluvsun\n\n            L         C          H\n[1,] 25.00933  69.80052 274.922758\n[2,] 33.57582  78.49556 296.995075\n[3,] 42.09671  87.16488 318.944488\n[4,] 50.70304  96.81962 341.141446\n[5,] 59.33484 102.07413   3.730076\n[6,] 67.89723  89.83472  25.626328\n[7,] 76.47041  74.80664  47.677726\n[8,] 84.95182  45.16493  69.593540\n\n\nThis generates the wrong thing (roughly, viridis) because the hue crosses 0\n\nsunmat &lt;- cbind(seq(from = 85, to = 25, length.out = 8), \n                max_chroma(h = seq(from = 69, to = 275, length.out = 8), \n                           l = seq(from = 85, to = 25, length.out = 8), \n                           floor = TRUE),\n                seq(from = 69, to = 275, length.out = 8))\n\npgsun &lt;- polarLUV(sunmat)\nswatchplot(hex(pgsun))\n\n\n\n\n\n\n\n\nCan I fix the zero-crossing? I’m sure there’s a polar coord package, but for now, add a 360 and take it off\n\nhvec &lt;- seq(from = luvsun@coords[1, 3], to = 360+luvsun@coords[8,3], length.out = 8)\nhvec[hvec &gt; 360] &lt;- hvec[hvec&gt;360]-360\n\nlvec &lt;- seq(from = luvsun@coords[1, 1], to = luvsun@coords[8, 1], length.out = 8)\n\nThe max_chroma is intense, but not sure how else to choose the chromas if we’re trying to build a continuous ramp. Could just use n = 1000 or something to get pseudo-continuous\n\nsunmat &lt;- cbind(lvec, \n                max_chroma(h = hvec, \n                           l = lvec, \n                           floor = TRUE),\n                hvec)\n\npgsun &lt;- polarLUV(sunmat)\nswatchplot(hex(pgsun))\n\n\n\n\n\n\n\n\nSo, one option is just treating the built-in palettes as their endmembers like that and then doing it as I did before. But it does lose the actual built-in palettes, especially chroma or nonlinearity. Likely better to just use a large n for now and call it good.",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Exploring HCL colorspace"
    ]
  },
  {
    "objectID": "plotting/fonts.html",
    "href": "plotting/fonts.html",
    "title": "Fonts",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\nUsing knitr::inline_expr(r message = FALSE, warning = FALSE) hopefully stops it printing all the package messages\n\nlibrary(tidyverse) # Overkill, but easier than picking and choosing\n\nThese are mostly little plot tweaks and small things that I find and forget all the time.\n\nAccessing fonts\nIn the past, I’ve used extrafonts to use fonts within figures, but it’s failing for me (‘No FontName, skipping’ error as in https://github.com/wch/extrafont/issues/88).\nTry sysfonts. Actually, showtext on top of sysfonts. First, look at how it finds the fonts.\n\nlibrary(showtext)\n\nfontsIhave &lt;- font_files()\nfontsIhave\n\n\n  \n\n\nstr(fontsIhave)\n\n'data.frame':   372 obs. of  6 variables:\n $ path   : chr  \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" ...\n $ file   : chr  \"AGENCYB.TTF\" \"AGENCYR.TTF\" \"ALGER.TTF\" \"ANTQUAB.TTF\" ...\n $ family : chr  \"Agency FB\" \"Agency FB\" \"Algerian\" \"Book Antiqua\" ...\n $ face   : chr  \"Bold\" \"Regular\" \"Regular\" \"Bold\" ...\n $ version: chr  \"Version 1.01\" \"Version 1.01\" \"Version 1.57\" \"Version 2.35\" ...\n $ ps_name: chr  \"AgencyFB-Bold\" \"AgencyFB-Reg\" \"Algerian\" \"BookAntiqua-Bold\" ...\n\n\nI should be able to use font_add\nFirst, what fonts are CURRENTLY available in R?\n\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\nfont_families()\n\n[1] \"sans\"         \"serif\"        \"mono\"         \"wqy-microhei\"\n\n\nTest with one of the\n\nfont_add('Bookman Old Style', regular = 'BOOKOS.TTF', \n         italic = 'BOOKOSI.TTF', \n         bold = 'BOOKOSB.TTF', \n         bolditalic = 'BOOKOSBI.TTF')\n\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\nfont_families()\n\n[1] \"sans\"              \"serif\"             \"mono\"             \n[4] \"wqy-microhei\"      \"Bookman Old Style\"\n\n\nI’m not quite understanding how this object is organised. What is that last line? are the $xxxx the defaults?\nTo test, let’s make a plot and try to change font.\nThe help (https://cran.rstudio.com/web/packages/showtext/vignettes/introduction.html) says we need to tell R to use showtext for text.\n\nshowtext_auto()\n\n\nbaseiris &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\nbaseiris + theme(axis.title = element_text(family = 'Bookman Old Style'),\n                 legend.title = element_text(family = 'Bookman Old Style', face = 'bold.italic'))\n\n\n\n\n\n\n\n\nThat seems to work. Can I feed in all the fonts on my system automaticallly? Is that a bad idea? Might be if it takes a while and we only want one.\nFirst, though, can I give it a font name as a character and it load all of the faces automatically?\nNote: some of the fonts I have have weird faces. For now, just fail on those and stick with the ones supported by showtext. That should be fine.\n\nunique(fontsIhave$face)\n\n [1] \"Bold\"            \"Regular\"         \"Bold Italic\"     \"Italic\"         \n [5] \"Plain\"           \"Demibold\"        \"Demibold Italic\" \"Demibold Roman\" \n [9] \"Bold Oblique\"    \"Oblique\"         \"Light\"          \n\n\nThis is a) a useful thing to simplify adding single fonts, and b) precursor to loading them all.\n\n# chosen more or less at random for testing\nfamilyname &lt;- 'Candara'\n\n# I could use dplyr but this seems better to just use base logical indexing.\n# fontsIhave %&gt;%\n#   filter(family == familyname & face == 'Regular') %&gt;%\n#   select(file) %&gt;%\n#   pull()\n\n# Could do all the indexing in the function call to font_add(), but it just gets ridiculous\nregfile &lt;- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Regular'), 'file']\n\nitalfile &lt;- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Italic'), 'file']\n\nboldfile &lt;- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Bold'), 'file']\n\nbifile &lt;- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Bold Italic'), 'file']\n\n\n# NEED TO TEST WHEN THE FACE DOESN'T EXIST AND THROW NULL\n  # If not there, the value will be character(0). testing for that and returning NULL (which is what the function needs) is a bit tricky:\nnoface &lt;- function(x) {ifelse(rlang::is_empty(x), return(NULL), return(x))}\n\nfont_add(familyname, \n         regular = noface(regfile), \n         italic = noface(italfile), \n         bold = noface(boldfile), \n         bolditalic = noface(bifile))\n\nTest that with a figure\n\nbaseiris + theme(axis.title.x = element_text(family = familyname, face = 'italic'),\n                 axis.title.y = element_text(family = familyname, face = 'bold'),\n                 legend.text = element_text(family = familyname),\n                 legend.title = element_text(family = familyname, face = 'bold.italic'))\n\n\n\n\n\n\n\n\nHow bad an idea is it to just read them ALL in at once?\nEasy enough to feed the font_add above in a loop. Probably vectorizable too, but why bother?\nWrite it as a function, then it will work for all fonts or a subset if that’s a bad idea. Either feed it a dataframe of fonts or just use font_files() directly. It can also take NULL for fontvec, in which case it loads all the fonts.\n\nloadfonts &lt;- function(fontvec = NULL, fontframe = NULL) {\n  \n  # Get all fonts if no fontframe\n  if (is.null(fontframe)) {\n    fontframe &lt;- font_files()\n  }\n  \n  # Load all fonts if no fontvec\n  if (is.null(fontvec)) {\n    fontvec &lt;- unique(fontframe$family)\n  }\n  \n  # Loop over the font families\n  for (i in 1:length(fontvec)) {\n    familyname &lt;- fontvec[i]\n    regfile &lt;- fontframe[which(fontframe$family == familyname &\n                   fontframe$face == 'Regular'), 'file']\n\n    italfile &lt;- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Italic'), 'file']\n    \n    boldfile &lt;- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Bold'), 'file']\n    \n    bifile &lt;- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Bold Italic'), 'file']\n    \n## TODO: THROW A TRYCATCH ON HERE TO BYPASS AND ALERT FOR FAILURES\n    # For example, Berlin Sans FB Demi has no 'regular' and so fails. let's just skip those, this isn't supposed to be the most robust thing ever that handles all cases flawlessly.\n    try(font_add(fontvec[i], \n         regular = noface(regfile), \n         italic = noface(italfile), \n         bold = noface(boldfile), \n         bolditalic = noface(bifile)))\n    \n    # To avoid unforeseen carryover through the loop\n    rm(familyname, regfile, italfile, boldfile, bifile)\n  }\n  \n}\n\nLet’s try just reading everything in. Use try in the function above because there are failures for a few reasons, and instead of dealing with them I just want to quickly read in what’s easy to read in. I don’t have a ton of interest here in figuring out corner cases for weird fonts.\nTest the function with a vector of fontnames first, because can’t do that after try with everything\n\nloadfonts(fontvec = c('Consolas', 'Comic Sans MS', 'Tahoma'))\nfont_families()\n\n[1] \"sans\"              \"serif\"             \"mono\"             \n[4] \"wqy-microhei\"      \"Bookman Old Style\" \"Candara\"          \n[7] \"Consolas\"          \"Comic Sans MS\"     \"Tahoma\"           \n\n\nNow, go for it with everything. There are a million errors, so I’ve turned error reporting off for this code chunk.\n\nsystem.time(loadfonts())\n\nThat was pretty quick. What do I have?\n\nfont_families()\n\n  [1] \"sans\"                            \"serif\"                          \n  [3] \"mono\"                            \"wqy-microhei\"                   \n  [5] \"Bookman Old Style\"               \"Candara\"                        \n  [7] \"Consolas\"                        \"Comic Sans MS\"                  \n  [9] \"Tahoma\"                          \"Agency FB\"                      \n [11] \"Algerian\"                        \"Book Antiqua\"                   \n [13] \"Arial\"                           \"Arial Narrow\"                   \n [15] \"Arial Black\"                     \"Arial Rounded MT Bold\"          \n [17] \"Bahnschrift\"                     \"Baskerville Old Face\"           \n [19] \"Bauhaus 93\"                      \"Bell MT\"                        \n [21] \"Bernard MT Condensed\"            \"Bodoni MT\"                      \n [23] \"Bodoni MT Black\"                 \"Bodoni MT Condensed\"            \n [25] \"Bodoni MT Poster Compressed\"     \"Bradley Hand ITC\"               \n [27] \"Britannic Bold\"                  \"Berlin Sans FB\"                 \n [29] \"Broadway\"                        \"Bookshelf Symbol 7\"             \n [31] \"Calibri\"                         \"Calibri Light\"                  \n [33] \"Californian FB\"                  \"Calisto MT\"                     \n [35] \"Cambria\"                         \"Candara Light\"                  \n [37] \"Cascadia Code\"                   \"Cascadia Mono\"                  \n [39] \"Castellar\"                       \"Century Schoolbook\"             \n [41] \"Centaur\"                         \"Century\"                        \n [43] \"Chiller\"                         \"Colonna MT\"                     \n [45] \"Constantia\"                      \"Cooper Black\"                   \n [47] \"Copperplate Gothic Bold\"         \"Copperplate Gothic Light\"       \n [49] \"Corbel\"                          \"Corbel Light\"                   \n [51] \"Courier New\"                     \"Curlz MT\"                       \n [53] \"Dubai\"                           \"Dubai Light\"                    \n [55] \"Dubai Medium\"                    \"Ebrima\"                         \n [57] \"Elephant\"                        \"Engravers MT\"                   \n [59] \"Eras Bold ITC\"                   \"Eras Demi ITC\"                  \n [61] \"Eras Light ITC\"                  \"Eras Medium ITC\"                \n [63] \"Euclid\"                          \"Euclid Symbol\"                  \n [65] \"Euclid Extra\"                    \"Euclid Fraktur\"                 \n [67] \"Euclid Math One\"                 \"Euclid Math Two\"                \n [69] \"Felix Titling\"                   \"Forte\"                          \n [71] \"Franklin Gothic Book\"            \"Franklin Gothic Demi\"           \n [73] \"Franklin Gothic Demi Cond\"       \"Franklin Gothic Heavy\"          \n [75] \"Franklin Gothic Medium\"          \"Franklin Gothic Medium Cond\"    \n [77] \"Freestyle Script\"                \"French Script MT\"               \n [79] \"Footlight MT Light\"              \"Gabriola\"                       \n [81] \"Gadugi\"                          \"Garamond\"                       \n [83] \"Georgia\"                         \"Gigi\"                           \n [85] \"Gill Sans MT\"                    \"Gill Sans MT Condensed\"         \n [87] \"Gill Sans Ultra Bold Condensed\"  \"Gill Sans Ultra Bold\"           \n [89] \"Gloucester MT Extra Condensed\"   \"Gill Sans MT Ext Condensed Bold\"\n [91] \"Century Gothic\"                  \"Goudy Old Style\"                \n [93] \"Goudy Stout\"                     \"Harrington\"                     \n [95] \"Haettenschweiler\"                \"Microsoft Himalaya\"             \n [97] \"HoloLens MDL2 Assets\"            \"HP Simplified\"                  \n [99] \"HP Simplified Light\"             \"HP Simplified Hans Light\"       \n[101] \"HP Simplified Hans\"              \"HP Simplified Jpan Light\"       \n[103] \"HP Simplified Jpan\"              \"High Tower Text\"                \n[105] \"Impact\"                          \"Imprint MT Shadow\"              \n[107] \"Informal Roman\"                  \"Ink Free\"                       \n[109] \"Blackadder ITC\"                  \"Edwardian Script ITC\"           \n[111] \"Kristen ITC\"                     \"Javanese Text\"                  \n[113] \"Jokerman\"                        \"Juice ITC\"                      \n[115] \"Kunstler Script\"                 \"Lucida Sans Unicode\"            \n[117] \"Wide Latin\"                      \"Lucida Bright\"                  \n[119] \"Leelawadee UI\"                   \"Leelawadee UI Semilight\"        \n[121] \"Lucida Fax\"                      \"Lucida Sans\"                    \n[123] \"Lucida Sans Typewriter\"          \"Lucida Console\"                 \n[125] \"Maiandra GD\"                     \"Malgun Gothic\"                  \n[127] \"Malgun Gothic Semilight\"         \"Marlett\"                        \n[129] \"Matura MT Script Capitals\"       \"Microsoft Sans Serif\"           \n[131] \"MingLiU-ExtB\"                    \"Mistral\"                        \n[133] \"Myanmar Text\"                    \"Modern No. 20\"                  \n[135] \"Mongolian Baiti\"                 \"MS Gothic\"                      \n[137] \"Microsoft JhengHei\"              \"Microsoft JhengHei Light\"       \n[139] \"Microsoft YaHei\"                 \"Microsoft YaHei Light\"          \n[141] \"Microsoft Yi Baiti\"              \"MT Extra Tiger\"                 \n[143] \"Monotype Corsiva\"                \"MT Extra\"                       \n[145] \"MV Boli\"                         \"Niagara Engraved\"               \n[147] \"Niagara Solid\"                   \"Nirmala UI\"                     \n[149] \"Nirmala UI Semilight\"            \"Microsoft New Tai Lue\"          \n[151] \"OCR A Extended\"                  \"Old English Text MT\"            \n[153] \"Onyx\"                            \"MS Outlook\"                     \n[155] \"Palatino Linotype\"               \"Palace Script MT\"               \n[157] \"Papyrus\"                         \"Parchment\"                      \n[159] \"Perpetua\"                        \"Microsoft PhagsPa\"              \n[161] \"Playbill\"                        \"Poor Richard\"                   \n[163] \"Pristina\"                        \"Rage Italic\"                    \n[165] \"Ravie\"                           \"MS Reference Sans Serif\"        \n[167] \"MS Reference Specialty\"          \"Rockwell Condensed\"             \n[169] \"Rockwell\"                        \"Rockwell Extra Bold\"            \n[171] \"Sans Serif Collection\"           \"Script MT Bold\"                 \n[173] \"Segoe MDL2 Assets\"               \"Segoe Fluent Icons\"             \n[175] \"Segoe Print\"                     \"Segoe Script\"                   \n[177] \"Segoe UI\"                        \"Segoe UI Light\"                 \n[179] \"Segoe UI Semilight\"              \"Segoe UI Black\"                 \n[181] \"Segoe UI Emoji\"                  \"Segoe UI Historic\"              \n[183] \"Segoe UI Semibold\"               \"Segoe UI Symbol\"                \n[185] \"Segoe UI Variable\"               \"Showcard Gothic\"                \n[187] \"SimSun\"                          \"SimSun-ExtB\"                    \n[189] \"Sitka Text\"                      \"Snap ITC\"                       \n[191] \"Stencil\"                         \"Sylfaen\"                        \n[193] \"Symbol Tiger Expert\"             \"Symbol Tiger\"                   \n[195] \"Symbol\"                          \"Microsoft Tai Le\"               \n[197] \"Tw Cen MT\"                       \"Tw Cen MT Condensed\"            \n[199] \"Tw Cen MT Condensed Extra Bold\"  \"Tempus Sans ITC\"                \n[201] \"Tiger Expert\"                    \"Tiger\"                          \n[203] \"Times New Roman\"                 \"Trebuchet MS\"                   \n[205] \"Verdana\"                         \"Viner Hand ITC\"                 \n[207] \"Vladimir Script\"                 \"Webdings\"                       \n[209] \"Wingdings\"                       \"Wingdings 2\"                    \n[211] \"Wingdings 3\"                     \"Yu Gothic\"                      \n[213] \"Yu Gothic Light\"                 \"Yu Gothic Medium\"               \n[215] \"ZWAdobeF\"                       \n\n\nI’m sure if there were something that got bypassed that I really needed I could get it directly with font_add(), but this is sure quick to get them all. Test a couple of the new ones.\n\nbaseiris + theme(axis.title.x = element_text(family = 'Poor Richard', face = 'italic'),\n                 axis.title.y = element_text(family = 'Stencil', face = 'bold'),\n                 legend.text = element_text(family = 'Papyrus'),\n                 legend.title = element_text(family = 'Onyx', face = 'bold.italic'))\n\n\n\n\n\n\n\n\nI have also put loadfonts in the functions folder so I can use it elsewhere.",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Fonts"
    ]
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html",
    "href": "pix4d/read_pix4d_outputs.html",
    "title": "Pix4d outputs",
    "section": "",
    "text": "Trying to read in pix4d outputs- specifically point clouds, orthophotos, dsm_xyz, and dtm.\nI’m going to assume I need stars and sf.\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(stars)\n\nLoading required package: abind\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rayshader)\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galenR\nTypically the projdir would have a better name (likely similar/same as the project name)\nprojname &lt;- '4m_upandback'\nprojdir &lt;- file.path('data', 'pix4dout')",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Reading and plotting pix4d outputs"
    ]
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#raster-dsm",
    "href": "pix4d/read_pix4d_outputs.html#raster-dsm",
    "title": "Pix4d outputs",
    "section": "Raster DSM",
    "text": "Raster DSM\nThe settings for this seem to be in the DSM and orthomosaic tab and resolution matches the ortho, as far as I can tell.\n\ndsm_rpath &lt;- file.path(projdir, projname, '3_dsm_ortho', '1_dsm')\n\nread it in?\n\ndsm_r &lt;- read_stars(file.path(dsm_rpath, paste0(projname, '_dsm.tif')))\n\n\ndsm_r\n\nstars object with 2 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                           Min.   1st Qu.    Median      Mean   3rd Qu.\n4m_upandback_dsm.tif  -24.73554 -24.70843 -24.68753 -24.68809 -24.67387\n                           Max.  NA's\n4m_upandback_dsm.tif  -24.62663 96962\ndimension(s):\n  from   to  offset    delta                refsys point x/y\nx    1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny    1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\n\n\nReally not super clear why useRaster = TRUE isn’t working for these, but whatever.\n\nplot(dsm_r, useRaster = FALSE)\n\ndownsample set to 1\n\n\n\n\n\n\n\n\n\nThose are funny z-values. What is the reference- clearly not elevation.\nHow many pixels? 1.0983136^{7}. 11 million is a lot.",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Reading and plotting pix4d outputs"
    ]
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#grid-dsm",
    "href": "pix4d/read_pix4d_outputs.html#grid-dsm",
    "title": "Pix4d outputs",
    "section": "Grid DSM",
    "text": "Grid DSM\nThis is an xyz point file.The settings are in the Additional Outputs tab, and we can set the grid spacing there. This was done with a spacing of 100cm. That implies these are centers, and this is a much coarser grid than in the dsm raster, which is at GSD scale.\n\ndsm_xyzpath &lt;- file.path(projdir, projname, '3_dsm_ortho', '1_dsm')\n\n\ndsm_xyz &lt;- readr::read_csv(file.path(dsm_xyzpath, paste0(projname, '_dsm_1cm.xyz')),\n                                 col_names = c('x', 'y', 'z', 'r', 'g', 'b'))\n\nRows: 1490037 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): x, y, z, r, g, b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nassume the last 3 cols are rgb, use my function\n\ndsm_xyz &lt;- dsm_xyz %&gt;% \n  mutate(hexcol = rgb2hex(r,g,b))\n\nIgnore spatial for a minute, can I just slam it in ggplot? using z as fill first. If we want the rgb, it’ll either be an orthomosiac or we’ll need to do something 3d with color overlay\n\ndsm_xyzgg &lt;- ggplot(dsm_xyz, aes(x = x, y = y, fill = z)) + geom_raster() + coord_equal()\ndsm_xyzgg\n\n\n\n\n\n\n\n\nThat was really weirdly fast.\nCan I use the rgb to see the orthomosiac?\n\ndsm_xyzorthogg &lt;- ggplot(dsm_xyz, aes(x = x, y = y, fill = hexcol)) + geom_raster() + coord_equal() + scale_fill_identity()\ndsm_xyzorthogg\n\n\n\n\n\n\n\n\nWould be cool to do a 3d map with color, maybe {rayshader}? Would really be nice to be able to rotate it etc.\nThat has color and height- might be good to feed to a ML algorithm to ID rocks using both sets of info and their relationships to each other.\nThis is supposedly a 100cm grid, but it has 1.5 million pixels, which is a lot less than the dsm raster at GSD resolution, but is nowhere near the difference I’d expect. It’s approximately 10x less, but GSD is &lt; 1cm. And even at 1cm, I’d expect 10,000x fewer pixels. And those pictures are clearly not 1m pixels. What IS the spacing? Iterates fastest on x\n\ndsm_xyz[2, 'x'] - dsm_xyz[1, 'x']\n\n     x\n1 0.01\n\n# dsm_xyz[2, 'y'] - dsm_xyz[1, 'y']\n\nSo, what is 0.01? That’s a cm, isn’t it?\nAssuming the same crs as the raster dsm, the LENGTHUNIT is 1 meter. So, 0.01 would be 1cm, and that makes more sense.\n\nst_crs(dsm_r)\n\nCoordinate Reference System:\n  User input: WGS 84 / UTM zone 55S \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 55S\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 55S\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",147,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",10000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 144°E and 150°E, southern hemisphere between 80°S and equator, onshore and offshore. Australia. Papua New Guinea.\"],\n        BBOX[-80,144,0,150]],\n    ID[\"EPSG\",32755]]\n\n\nIt’s not exactly a translation from the raster, because the raster has all the NA off the edges, and a step of\n\ndiff(st_get_dimension_values(dsm_r, 'x')[1:2])\n\n[1] 0.00556\n\n\nSo, the raster should have about 4x, but also includes the NAs.\n\nsum(is.na(dsm_r[[1]]))\n\n[1] 5896746\n\n\nThat’s not entirely it. Still about 266k more pixels in the rnn than expected once we drop the NA and adjust by pixel size.\n\n#non-na in raster\nrnn  &lt;- (2416*4546)-sum(is.na(dsm_r[[1]]))\n#pixel difference- how many dsm_r pixels per dsm_xyz pixels?\npixdif &lt;- (0.01/diff(st_get_dimension_values(dsm_r, 'x')[1:2]))^2\n\nexpected_rnn &lt;- nrow(dsm_xyz)*pixdif\n\nexpected_rnn-rnn\n\n[1] -266385.2\n\n\n\nMake geographic\n\nsf\nCan I make it geographic (sf)? Assume the same crs as the dsm_r\n\ndsm_xyzsf &lt;- st_as_sf(dsm_xyz, coords = c('x', 'y'), crs = st_crs(dsm_r))\n\nuse the sf plot method? bad idea. takes forever. The points (next) are points, but it might make more sense to make these stars anyway- it is gridded.\n\nplot(dsm_xyzsf[,'z'])\n\n\n\n\n\n\n\n\n\n\nstars\nTry stars- first, confirm it is gridded- what’s the step?\nNeed to do a bit of cleanup- the x’s have negatives when they turnover y’s, and the y’s have a ton of zeros because they iterate slower.\n\nrawx &lt;- diff(pull(dsm_xyz[, 'x']))\ncleanx &lt;- rawx[rawx &gt; 0]\n# rounding error\ndifclean &lt;- abs(cleanx-cleanx[1])\n\n\n# huh- some are still out. by a lot. It seems systematic and strange\nall(difclean &lt; 1e-8)\n\n[1] FALSE\n\nwhich(difclean &gt; 1e-8)\n\n  [1]  191679  192281  192883  343775  344443  345109  375364  378772  378773\n [10]  380145  381514  382200  443113  443822  483744  484458  485170  485881\n [19]  486590  487296  527316  528041  530943  764429  765245  766058  766870\n [28]  767680  768486  773330  774145  774961  775777  822605  823429  824254\n [37]  825076  825896  877470  878310  879147  879982  884988  999784 1000608\n [46] 1001431 1002252 1002256 1003070 1003881 1004687 1070957 1071726 1136369\n [55] 1137116 1137859 1143776 1144521 1145270 1146022 1146781 1162799 1163554\n [64] 1205080 1205808 1206534 1207256 1207976 1212992 1333223 1384324 1385469\n [73] 1389993 1390552 1429954 1430434 1457519 1457948 1458373 1458795 1459216\n [82] 1459636 1460053 1460467 1460877 1461283 1461685 1462082 1462475 1462863\n [91] 1463247 1463626 1464003 1464376 1464744 1465107 1465466 1465821 1466172\n[100] 1466520 1466861 1467199 1467534 1467865 1468192 1468513 1468831 1469145\n[109] 1469455 1469760 1470062 1470360 1470655 1470946 1471233 1471516 1471796\n[118] 1472073 1472346 1472616 1472881 1473144 1473402 1473656 1473905 1474150\n[127] 1474392 1474629 1474862 1475092 1475316 1475537 1475754 1475966 1476174\n\ncleanx[which(difclean &gt; 1e-8)]\n\n  [1] 0.06 0.04 0.03 0.02 0.05 0.09 0.04 0.02 0.02 0.02 0.02 0.02 0.02 0.04 0.02\n [16] 0.06 0.08 0.10 0.13 0.15 0.03 0.04 0.04 0.02 0.04 0.07 0.08 0.11 0.14 0.03\n [31] 0.05 0.07 0.09 0.02 0.04 0.05 0.08 0.11 0.02 0.03 0.07 0.10 0.02 0.02 0.03\n [46] 0.05 0.06 0.03 0.14 0.15 0.20 0.03 0.06 0.02 0.03 0.06 0.29 0.25 0.22 0.17\n [61] 0.10 0.03 0.05 0.03 0.04 0.07 0.09 0.10 0.02 0.03 0.02 0.02 0.03 0.05 0.02\n [76] 0.04 0.02 0.05 0.08 0.10 0.10 0.11 0.11 0.14 0.17 0.21 0.24 0.27 0.30 0.33\n [91] 0.38 0.39 0.42 0.45 0.49 0.53 0.55 0.58 0.61 0.65 0.69 0.70 0.73 0.76 0.80\n[106] 0.84 0.86 0.90 0.93 0.97 1.00 1.01 1.05 1.09 1.12 1.15 1.17 1.21 1.25 1.28\n[121] 1.31 1.32 1.37 1.40 1.43 1.46 1.48 1.52 1.55 1.59 1.62 1.64 1.68 1.71 1.74\n\n\nIf I plot that does it become clear? Because I took diffs and cut out the big steps, I can’t just attach it. Will need to modify in place.\n\ndsm_xyz &lt;- dsm_xyz |&gt; \n  mutate(xdifs = x - lag(x, 1),\n         ydifs = y - lag(y, 1)) |&gt; \n  mutate(xdifs = ifelse(ydifs == 0, xdifs, NA),\n         ydifs = ifelse(ydifs == 0, NA, ydifs))\n\nThe steps are where there are weird things happening along angled edges.\n\ndsm_xyz_x &lt;- ggplot(dsm_xyz, aes(x = x, y = y, fill = xdifs)) + geom_raster() + coord_equal() + scale_fill_gradient(low = 'white', high = 'red')\ndsm_xyz_x\n\n\n\n\n\n\n\n\nCan we see it better with size?\n\ndsm_xyz_xp &lt;- ggplot(dsm_xyz, aes(x = x, y = y, color = xdifs, size = xdifs)) + geom_point() + coord_equal() + scale_color_gradient(low = 'white', high = 'red')\ndsm_xyz_xp\n\nWarning: Removed 2370 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYeah, it’s also centered on corners.\nCheck y in the same way\n\ndsm_xyz_yp &lt;- ggplot(dsm_xyz, aes(x = x, y = y, color = ydifs, size = ydifs)) + geom_point() + coord_equal() + scale_color_gradient(low = 'white', high = 'red')\ndsm_xyz_yp\n\nWarning: Removed 1487668 rows containing missing values or values outside the scale\nrange (`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe ydifs are all 0.01, and we can see the rows stepover on the left edge.\nI think it’s safe to ignore the few discontinuities.\nNow, actually make it stars\n\ndsm_xyzstars &lt;- st_as_stars(dsm_xyz, crs = st_crs(dtm))\n\n\nplot(dsm_xyzstars['z'])\n\n\n\n\n\n\n\n\n\nplot(dsm_xyzstars['hexcol'])\n\n\n\n\n\n\n\n\nggplot version- orthomosaic\n\ndsm_xyzstarsgg &lt;- ggplot() + \n  geom_stars(data = dsm_xyzstars, aes(fill = hexcol)) +\n  scale_fill_identity() +\n  theme(legend.position = 'none') + \n  coord_equal()\ndsm_xyzstarsgg\n\n\n\n\n\n\n\n\nggplot version- elevation\n\ndsm_xyzstarsgg_z &lt;- ggplot() + \n  geom_stars(data = dsm_xyzstars, aes(fill = z)) +\n  colorspace::scale_fill_continuous_sequential(palette = 'Terrain 2') +\n  theme(legend.position = 'none') + \n  coord_equal()\ndsm_xyzstarsgg_z\n\n\n\n\n\n\n\n\nIs it faster to just use geom_raster? Would need to have a non-spatial df as we did above. This just errors when I try it.\n\ndsm_xyzstarsgg_z_raster &lt;- ggplot() + \n  geom_raster(data = dsm_xyzstars, aes(fill = z)) +\n  colorspace::scale_fill_continuous_sequential(palette = 'Terrain 2') +\n  theme(legend.position = 'none') + \n  coord_equal()\ndsm_xyzstarsgg_z_raster\n\nWhat about geom_sf on the point version? I plotted it above, but haven’t tried ggplot. This is more specifically what the point cloud is, below. Change everything to color from fill.\n\ndsm_xyzsfgg_z &lt;- ggplot() + \n  geom_sf(data = dsm_xyzsf, aes(color = z)) +\n  colorspace::scale_color_continuous_sequential(palette = 'Terrain 2') +\n  theme(legend.position = 'none') + \n  coord_sf()\ndsm_xyzsfgg_z",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Reading and plotting pix4d outputs"
    ]
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#check-pixels-dsm-ortho",
    "href": "pix4d/read_pix4d_outputs.html#check-pixels-dsm-ortho",
    "title": "Pix4d outputs",
    "section": "Check pixels dsm & ortho",
    "text": "Check pixels dsm & ortho\n\nst_dimensions(dsm_r)\n\n  from   to  offset    delta                refsys point x/y\nx    1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny    1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\n\nst_dimensions(ortho)\n\n     from   to  offset    delta                refsys point x/y\nx       1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny       1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\nband    1    4      NA       NA                    NA    NA    \n\n\nConfirm they match\n\nall(st_get_dimension_values(dsm_r, 'x') == st_get_dimension_values(ortho, 'x'))\n\n[1] TRUE\n\nall(st_get_dimension_values(dsm_r, 'y') == st_get_dimension_values(ortho, 'y'))\n\n[1] TRUE\n\n\nCan we combine those into one stars? the catch is that the rgb vals aren’t on the same scale as the z. For the ortho, we have a pixel value for each of 4 bands (r,g,b, alpha). But z isn’t a band. So how would we do it? Make the bands attributes, probably. Or just call z a band?\nTo make the bands attributes and add z, this works. Will need to better define what’s needed later before deciding whether to then merge this back or what. And maybe we should be using hex (e.g. merging colmos and dsm_r).\n\northosplit &lt;- split(ortho, 'band') |&gt; \n  setNames(c('r', 'g','b', 'alpha'))\n\ndsm_r &lt;- setNames(dsm_r, 'z')\n\northodsm &lt;- c(orthosplit, dsm_r)\n\nMerging into a dim- takes a while. and it’s unclear what to call this dimension or its values- ‘band’ isn’t right, and the values aren’t 0-255 for the z. I think don’t do this unless we’re really sure we want to.\n\northodsm_dim &lt;- orthodsm |&gt; \n  merge()\n\nDoes it actually make more sense to add a z dimension? Maybe? we can’t just merge the dsm_r, because then it has no attributes. Can we control the merge above? Maybe, but it’ll be a bit tricky. ignore until we have a clearer definition of what we need. The few obvious things I’ve tried have failed.\nWhatever we do about the combination, the pixels match for the DSM raster and the orthophoto",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Reading and plotting pix4d outputs"
    ]
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#check-pixels-dtm-ortho",
    "href": "pix4d/read_pix4d_outputs.html#check-pixels-dtm-ortho",
    "title": "Pix4d outputs",
    "section": "Check pixels dtm & ortho",
    "text": "Check pixels dtm & ortho\nThese are unlikely to match-the dtm’s settings are done elsewhere and unless we were really particular to set them the same as the ortho, (which would require downgrading the ortho), they won’t match. AND, it’s unnecessary, since there’s a raster dsm that does match.\nLooking at the from-to, they clearly don’t match, and looking at dimensions confirms it\n\nst_dimensions(ortho)\n\n     from   to  offset    delta                refsys point x/y\nx       1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny       1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\nband    1    4      NA       NA                    NA    NA    \n\nst_dimensions(dtm)\n\n  from   to  offset    delta                refsys point x/y\nx    1 1932  263755  0.00695 WGS 84 / UTM zone 55S FALSE [x]\ny    1 3636 5774223 -0.00695 WGS 84 / UTM zone 55S FALSE [y]\n\n\nBut, the DTM is some sort of smoothed thing, and is set in the Additional Outputs tab. We could make the ortho matched, but it’d be contrived (and pointless, given the existence of the raster dsm.",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Reading and plotting pix4d outputs"
    ]
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#convert-dsm_xyz-to-image-with-z",
    "href": "pix4d/read_pix4d_outputs.html#convert-dsm_xyz-to-image-with-z",
    "title": "Pix4d outputs",
    "section": "Convert dsm_xyz to image with z?",
    "text": "Convert dsm_xyz to image with z?\nAgain, I don’t really see the point of this turning the dsm_xyz into an image (raster) is already done in the raster dsm + ortho. We can merge if we want. Just not sure exactly how we’d want to present that.",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Reading and plotting pix4d outputs"
    ]
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#can-i-get-rayshader-to-work",
    "href": "pix4d/read_pix4d_outputs.html#can-i-get-rayshader-to-work",
    "title": "Pix4d outputs",
    "section": "Can I get rayshader to work?",
    "text": "Can I get rayshader to work?\nWould be cool to do height with z and color with hexcol to actually map the stream in 3d with photo overlay. Pix4d does it, but would be nice to do here too.\n\nplot_gg(dsm_xyzstarsgg, ggobj_height = dsm_xyzstarsgg_z, scale = 50)\nrender_snapshot()\n\n\n\n\n\n\n\n\nGetting it to work directly isn’t happening for some reason, but gg is.",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Reading and plotting pix4d outputs"
    ]
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#why-does-the-point-cloud-not-include-the-water",
    "href": "pix4d/read_pix4d_outputs.html#why-does-the-point-cloud-not-include-the-water",
    "title": "Pix4d outputs",
    "section": "Why does the point cloud not include the water?",
    "text": "Why does the point cloud not include the water?\nIt can’t do the photo point matching on the moving surface. So the images are there but it can’t get parallax and so no z.",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Reading and plotting pix4d outputs"
    ]
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#to-do-make-a-package.",
    "href": "pix4d/read_pix4d_outputs.html#to-do-make-a-package.",
    "title": "Pix4d outputs",
    "section": "To do: make a package.",
    "text": "To do: make a package.\nHave funs for xyz and funs for tifs. But call those within specific funs for each output (e.g. dsm_xyz and point cloud should have their own funs, and the dsm_xyz should allow returning a raster in addition to points, but pc shouldn’t). Similar for a standard set of plot returns",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Reading and plotting pix4d outputs"
    ]
  },
  {
    "objectID": "parallelism/plans_and_hpc.html",
    "href": "parallelism/plans_and_hpc.html",
    "title": "Testing plans and systems",
    "section": "",
    "text": "library(dplyr)\nlibrary(tibble)\nlibrary(doFuture)\nregisterDoFuture()\nI want to test how tasks get split up in different plans (e.g. multisession, multicore, cluster, future.batchtools, future.callr). In a local case on Windows, I think it’s fairly clear we want plan(multisession) and on unix/mac, plan(multicore). But when we have access to something bigger, like an HPC, I want to know how the plans work where there are multiple nodes, each with several cores. Can we access more than one node? How? What’s the difference between cluster, batchtools and callr?\nI’ve done some haphazard poking at this, and at that time, if we just had a slurm script that requested multiple nodes, but then called a single R script, I couldn’t use workers on more than one node. My workaround was to use array jobs to do the node-parallelisation, and then {future} for the cores. But this splits the parallelisation into some being handled by shell scripts and slurm, and some handled by R and futures. And that makes it harder to control and load-balance. And in fact, I had an extra level, where I had shell scripts that started a set of SLURM array jobs, which then split into cores. This approach worked, but it’s very hardcodey, even when we auto-generate the batch and SLURM scripts. It tends to not be well-balanced without a lot of manual intervention every time anything changes. And it’s not portable- we have to restructure the code to run locally vs on the HPC.\nWhat would be nice is to auto-break-up the set of work into evenly-sized chunks, and then fire off the SLURM commands to start an appropriate number of nodes with some number of cores. And have that also work locally, where it would just parallelise over those things. Can I figure that out?\nI’m not sure how I’m going to test this in a quarto doc, since I need to run things on the HPC and return to stdout. Might have to copy-paste and treat this like onenote or something. But I should be able to set up the desired structure here, anyway.\nI think I’ll start similarly to some of my local parallel testing, where I just returned process IDs to see what resources are being used.\nAnd I think I’ll start by requesting resources with sbatch and slurm scripts that call Rscript and see what that gets me and whether we can access all the resources, and then move on to trying to get R to generate the SLURM calls, likely with future.batchtools.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Using single-machine plans on HPC"
    ]
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#setting-up-tests",
    "href": "parallelism/plans_and_hpc.html#setting-up-tests",
    "title": "Testing plans and systems",
    "section": "Setting up tests",
    "text": "Setting up tests\nI want to be able to see if I’m getting nodes and cores\nAs a first pass, I don’t particularly care about speed- deal with that after I figure out how it’s actually working.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Using single-machine plans on HPC"
    ]
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#structure",
    "href": "parallelism/plans_and_hpc.html#structure",
    "title": "Testing plans and systems",
    "section": "Structure",
    "text": "Structure\nI’ll write a slurm script to start an sbatch job that requests &gt; 1 node and all cores on those nodes. That will call a test R script that calls a plan and runs some foreach loops that return PIDs, as I did previously. These loops need to iterate over at least nodes X cores so we can see what gets used.\nThe easiest thing to do will be to just use print statements to print to stdout, I think, though I guess I could save rds files or something.\nCan I automate? Or can I call all the plans one after each other in the same script? Probably yes to both. What’s the best way to get the code over to the HPC? I usually use git/github to sync changes, but I don’t really want to git this whole website over there. It’s a hassle, but I might set up a template slurm testing repo and use that.\n\nThe R code\nBasically, I want to call plan(PLAN_NAME), and then run a loop, checking PID as before. That loop will be like the simple nested loop with %:% I built before at least to start. I might do more complex nesting and try plan(list(PLAN1, PLAN2) later. Why am I looping at all? In case there’s some built-in capacity to throw one set of loops on nodes and the other on cores. This is more likely to come in later, but might as well set it up now.\nThat nested function is\n\nnest_test &lt;- function(outer_size, inner_size, planname) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %:% \n    foreach(j = 1:inner_size,\n                       .combine = bind_rows) %dopar% {\n    \n                         thisproc &lt;- tibble(plan = planname,\n                                            outer_iteration = i,\n                                            inner_iteration = j, \n                                            pid = Sys.getpid())\n                       }\n  \n  return(outer_out)\n}\n\nAnd I also want to know availableWorkers() and availableCores(). Also try availableCores(methods = 'Slurm') to see how that differs and which matches the resources we actually use.\nCan I make the stdout auto-generate something in markdown I can copy-paste in here? That’d be nice.\nI think as a first pass, I’ll try this for the single-machine plans- sequential, multicore, and multiprocess. I have a feeling I’ll encounter more difficulty with the cluster, callr, and batchtools, so get the basics figured out first.\nSo, what should that R script look like? I think I’ll use a for over the plans.\nThe test HPC I’ll use has 20 nodes with 12 cores. I’ll request 2 nodes and 24 cores for testing so I can see if it uses multiple nodes. That means I’ll need at least 24 loops, and ideally more. Might as well go 50- this won’t actually take any time.\nI can run this locally too, so I guess do that?\n\nplannames &lt;- c('sequential', 'multisession', 'multicore')\n\n# The loopings\nnest_test &lt;- function(outer_size, inner_size, planname) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %:% \n    foreach(j = 1:inner_size,\n            .combine = bind_rows) %dopar% {\n              \n              thisproc &lt;- tibble(plan = planname,\n                                 outer_iteration = i,\n                                 inner_iteration = j, \n                                 pid = Sys.getpid())\n            }\n  \n  return(outer_out)\n}\n\n\nfor (planname in plannames) {\n  print(paste0(\"# \", planname))\n  plan(planname)\n  \n  print('## available Workers:')\n  print(availableWorkers())\n  \n  print('## available Cores:')\n  print(\"### non-slurm\")\n  print(availableCores())\n  print(\"### slurm method\")\n  print(availableCores(methods = 'Slurm'))\n  \n  # base R process id\n  print('## Main PID:')\n  print(Sys.getpid())\n  \n  looptib &lt;- nest_test(25, 25, planname)\n  \n  print('## Unique processes')\n  print(length(unique(looptib$pid)))\n  print(\"This should be the IDs of all cores used\")\n  print(unique(looptib$pid))\n  \n  print('## Full loop data')\n  print(looptib)\n  \n  \n}\n\n[1] \"# sequential\"\n[1] \"## available Workers:\"\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n[1] \"## available Cores:\"\n[1] \"### non-slurm\"\nsystem \n    20 \n[1] \"### slurm method\"\ncurrent \n      1 \n[1] \"## Main PID:\"\n[1] 16152\n[1] \"## Unique processes\"\n[1] 1\n[1] \"This should be the IDs of all cores used\"\n[1] 16152\n[1] \"## Full loop data\"\n# A tibble: 625 × 4\n   plan       outer_iteration inner_iteration   pid\n   &lt;chr&gt;                &lt;int&gt;           &lt;int&gt; &lt;int&gt;\n 1 sequential               1               1 16152\n 2 sequential               1               2 16152\n 3 sequential               1               3 16152\n 4 sequential               1               4 16152\n 5 sequential               1               5 16152\n 6 sequential               1               6 16152\n 7 sequential               1               7 16152\n 8 sequential               1               8 16152\n 9 sequential               1               9 16152\n10 sequential               1              10 16152\n# ℹ 615 more rows\n[1] \"# multisession\"\n[1] \"## available Workers:\"\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n[1] \"## available Cores:\"\n[1] \"### non-slurm\"\nsystem \n    20 \n[1] \"### slurm method\"\ncurrent \n      1 \n[1] \"## Main PID:\"\n[1] 16152\n[1] \"## Unique processes\"\n[1] 20\n[1] \"This should be the IDs of all cores used\"\n [1] 24128 26240 41872 25060 33500 43908 42332 44264  5716 44904 21424 43384\n[13] 34528 23156  4784 33776 30620 37632 31232 43172\n[1] \"## Full loop data\"\n# A tibble: 625 × 4\n   plan         outer_iteration inner_iteration   pid\n   &lt;chr&gt;                  &lt;int&gt;           &lt;int&gt; &lt;int&gt;\n 1 multisession               1               1 24128\n 2 multisession               1               2 24128\n 3 multisession               1               3 24128\n 4 multisession               1               4 24128\n 5 multisession               1               5 24128\n 6 multisession               1               6 24128\n 7 multisession               1               7 24128\n 8 multisession               1               8 24128\n 9 multisession               1               9 24128\n10 multisession               1              10 24128\n# ℹ 615 more rows\n[1] \"# multicore\"\n[1] \"## available Workers:\"\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n[1] \"## available Cores:\"\n[1] \"### non-slurm\"\nsystem \n    20 \n[1] \"### slurm method\"\ncurrent \n      1 \n[1] \"## Main PID:\"\n[1] 16152\n[1] \"## Unique processes\"\n[1] 1\n[1] \"This should be the IDs of all cores used\"\n[1] 16152\n[1] \"## Full loop data\"\n# A tibble: 625 × 4\n   plan      outer_iteration inner_iteration   pid\n   &lt;chr&gt;               &lt;int&gt;           &lt;int&gt; &lt;int&gt;\n 1 multicore               1               1 16152\n 2 multicore               1               2 16152\n 3 multicore               1               3 16152\n 4 multicore               1               4 16152\n 5 multicore               1               5 16152\n 6 multicore               1               6 16152\n 7 multicore               1               7 16152\n 8 multicore               1               8 16152\n 9 multicore               1               9 16152\n10 multicore               1              10 16152\n# ℹ 615 more rows\n\n\n\n\nThe slurm script\nwe need a batch script that calls the R script, requests resources (here, 2 nodes with 12 cores each so we can see node utilisation- or not).\n#!/bin/bash\n\n# # Resources on test system: 20 nodes, each with 12 cores. 70GB RAM\n\n#SBATCH --time=0:05:00 # request time (walltime, not compute time)\n#SBATCH --mem=8GB # request memory. 8 should be more than enough to test\n#SBATCH --nodes=2 # number of nodes. Need &gt; 1 to test utilisation\n#SBATCH --ntasks-per-node=12 # Cores per node\n\n#SBATCH -o node_core_%A_%a.out # Standard output\n#SBATCH -e node_core_%A_%a.err # Standard error\n\n# timing\nbegin=`date +%s`\n\nmodule load R\n\nRscript testing_plans.R\n\n\nend=`date +%s`\nelapsed=`expr $end - $begin`\n\necho Time taken for code: $elapsed\nThat’s then called with\nsbatch filename.sh\nFrom within the directory.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Using single-machine plans on HPC"
    ]
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#sequential",
    "href": "parallelism/plans_and_hpc.html#sequential",
    "title": "Testing plans and systems",
    "section": "sequential",
    "text": "sequential\n\navailable workers:\ngandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04\n\n\ntotal workers:\n24\n\n\nunique workers:\ngandalf-vm03 gandalf-vm04\n\n\navailable Cores:\n\nnon-slurm\n12\n\n\nslurm method\n12\n\n\n\nMain PID:\n1224603\n\n\nUnique processes\n1\nIDs of all cores used\n1224603",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Using single-machine plans on HPC"
    ]
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#multisession",
    "href": "parallelism/plans_and_hpc.html#multisession",
    "title": "Testing plans and systems",
    "section": "multisession",
    "text": "multisession\nℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2)\n\navailable workers:\ngandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04\n\n\ntotal workers:\n24\n\n\nunique workers:\ngandalf-vm03 gandalf-vm04\n\n\navailable Cores:\n\nnon-slurm\n12\n\n\nslurm method\n12\n\n\n\nMain PID:\n1224603\n\n\nUnique processes\n12\nIDs of all cores used\n1224723 1224717 1224715 1224716 1224718 1224725 1224724 1224721 1224720 1224722 1224719 1224726",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Using single-machine plans on HPC"
    ]
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#multicore",
    "href": "parallelism/plans_and_hpc.html#multicore",
    "title": "Testing plans and systems",
    "section": "multicore",
    "text": "multicore\n\navailable workers:\ngandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04\n\n\ntotal workers:\n24\n\n\nunique workers:\ngandalf-vm03 gandalf-vm04\n\n\navailable Cores:\n\nnon-slurm\n12\n\n\nslurm method\n12\n\n\n\nMain PID:\n1224603\n\n\nUnique processes\n12\nIDs of all cores used\n1225561 1225564 1225567 1225570 1225573 1225576 1225581 1225586 1225591 1225596 1225601 1225606\nTime taken for code: 15",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Using single-machine plans on HPC"
    ]
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html",
    "href": "parallelism/nested_parallel_hpc.html",
    "title": "Nested futures with batchtools",
    "section": "",
    "text": "Sometimes it makes sense to just send all parallel jobs to single cpus each. But there are other situations, like when we have some high-level parallelization over parameters, followed by additional parallelization over something else, where we can take advantage of the structure of HPC clusters to send biggish jobs to a node, and then further parallelize within it onto CPUS. Which is faster is going to be highly context-dependent, I think, but let’s figure out how and then set up some scripts that can be modified for speed testing.\nI’ll use the parallel inner and outer functions developed in testing nested paralellism, but modified as in most of the slurm_r_tests repo to just return PIDs and other diagnostics so we can see what resources get used.\nThe approach here is learning a lot from this github issue.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Nested parallel on HPC nodes and cores"
    ]
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html#creating-a-nested-plan",
    "href": "parallelism/nested_parallel_hpc.html#creating-a-nested-plan",
    "title": "Nested futures with batchtools",
    "section": "Creating a nested plan",
    "text": "Creating a nested plan\n\nUnnested template\nIf I was using a plan like this to assign jobs to CPUS\n\nplan(tweak(batchtools_slurm,\n           template = \"batchtools.slurm.tmpl\",\n           resources = list(time = 5,\n                            ntasks.per.node = 1, \n                            mem = 1000,\n                            job.name = 'NewName')))\n\n\n\nNested with list()\nI now need to do two things- wrap it in a list with a second plan type, and ask for more tasks per node. (It should also work to do this with a focus on tasks and cpus per tasks), but that should work similarly.\nSo, we can wrap that in a list\n\nplan(list(tweak(batchtools_slurm,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 5,\n                                 ntasks.per.node = 12, \n                                 mem = 1000,\n                                 job.name = 'NewName')),\n          multicore))",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Nested parallel on HPC nodes and cores"
    ]
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html#nested-functions-to-return-resource-use",
    "href": "parallelism/nested_parallel_hpc.html#nested-functions-to-return-resource-use",
    "title": "Nested futures with batchtools",
    "section": "Nested functions to return resource use",
    "text": "Nested functions to return resource use\nAnd set up nested functions that return the outer and inner PIDs, as well as some other stuff:\n\ninner_par &lt;- function(inner_size, outer_it, outer_pid) {\n  inner_out &lt;- foreach(j = 1:inner_size,\n                       .combine = bind_rows) %dorng% {\n                         thisproc &lt;- tibble(all_job_nodes = paste(Sys.getenv(\"SLURM_JOB_NODELIST\"),\n                                                                  collapse = \",\"),\n                                            node = Sys.getenv(\"SLURMD_NODENAME\"),\n                                            loop = \"inner\",\n                                            outer_iteration = outer_it,\n                                            outer_pid = outer_pid,\n                                            inner_iteration = j, \n                                            inner_pid = Sys.getpid(),\n                                            taskid = Sys.getenv(\"SLURM_LOCALID\"),\n                                            cpus_avail = Sys.getenv(\"SLURM_JOB_CPUS_PER_NODE\"))\n                         \n                       }\n  return(inner_out)\n}\n\n# The outer loop calls the inner one\nouter_par &lt;- function(outer_size, inner_size) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %dorng% {\n                         \n                         # do some stupid work so this isn't trivially nested\n                         a &lt;- 1\n                         b &lt;- 1\n                         d &lt;- a+b\n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- inner_par(inner_size = inner_size,\n                                                outer_it = i, outer_pid = Sys.getpid())\n                         \n                         inner_out\n                       }\n  \n  return(outer_out)\n}\n\nAnd it WORKS!--\nI did this in nested_plan.R in the slurm_r_tests repo, and running it for 25 loops in each of the nested functions shows that we’re getting and using 12 inner PIDS per outer PID\n## Nodes and pids simple\n# A tibble: 25 × 2\n   outer_pid n_inner\n       &lt;int&gt;   &lt;int&gt;\n 1    549832      12\n 2    549960      12\n 3    550097      12\n 4    550238      12\n 5   1507878      12\n 6   1508006      12\n 7   1508130      12\n 8   1508254      12\n 9   1508380      12\n10   1508502      12\n11   1508624      12\n12   3494670      12\n13   3494799      12\n14   3494921      12\n15   3495044      12\n16   3495168      12\n17   3495292      12\n18   3495415      12\n19   4189058      12\n20   4189200      12\n21   4189329      12\n22   4189455      12\n23   4189580      12\n24   4189702      12\n25   4189829      12\nFor more detail, run the code in the slurm_r_tests repo. But it shows that we get 25 nodes and 12 cores each (because of ntasks.per.node = 12 in resources), each of which gets used 2-3 times, which makes sense for a 25 outer x 25 inner looping.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Nested parallel on HPC nodes and cores"
    ]
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html#controlling-chunks-and-workers",
    "href": "parallelism/nested_parallel_hpc.html#controlling-chunks-and-workers",
    "title": "Nested futures with batchtools",
    "section": "Controlling chunks and workers",
    "text": "Controlling chunks and workers\nI’m not going to bother with trying to figure out array jobs (sounds like not easy or supported, and the batchtools framework basically works by spitting out sbatch calls instead of arrays. That’s not ideal, but it’s how it works at least for now. .\nIt probably is worth addressing chunking. One reason is just to not overload the cluster- if I ask for a million batch futures, that’s not going to make anyone happy, and it’s not efficient anyway. It looks like we can set this with a workers argument (which has default 100). That means however many futures we ask for, they get chunked into workers chunks, and that many jobs get submitted. So, we also might want to modify this if we have a job that naturally wants to be nested, and we know we want a node per outer loop, then we could say workers = OUTER_LOOP_LENGTH.\nAs a test, we can modify the plan above to have workers = 5 , which should only grab 5 nodes, and send 5 of the 25 outer loops to each of them. I’m going to up the time.\n\nplan(list(tweak(batchtools_slurm,\n                workers = 5,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 15,\n                                 ntasks.per.node = 12, \n                                 mem = 1000,\n                                 job.name = 'NewName')),\n          multicore))\n\nAnd that does what it’s supposed to- keep everything on 5 nodes\n## Nodes and pids simple\n# A tibble: 5 × 2\n  outer_pid n_inner\n      &lt;int&gt;   &lt;int&gt;\n1    552072      60\n2   1510060      60\n3   3496848      60\n4   4191978      60\n5   4192353      60",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Nested parallel on HPC nodes and cores"
    ]
  },
  {
    "objectID": "parallelism/nested_dependent_iterations.html",
    "href": "parallelism/nested_dependent_iterations.html",
    "title": "Nested dependencies with %:%",
    "section": "",
    "text": "I did a lot of nesting checking, but here I’m specifically interested in dependencies in the iterations in the nesting of foreach loops using %:%.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Dependencies of inner parallelism on outer"
    ]
  },
  {
    "objectID": "parallelism/nested_dependent_iterations.html#packages-and-setup",
    "href": "parallelism/nested_dependent_iterations.html#packages-and-setup",
    "title": "Nested dependencies with %:%",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\nlibrary(foreach)\nlibrary(dplyr)\nregisterDoFuture()\nplan(multisession)",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Dependencies of inner parallelism on outer"
    ]
  },
  {
    "objectID": "parallelism/interrogating_parallel.html",
    "href": "parallelism/interrogating_parallel.html",
    "title": "Interrogating parallel functions",
    "section": "",
    "text": "In testing parallel functions, especially on new machines, or if we’re trying to get a granular understanding of what they’re doing, we will want to do more than benchmark. We might, for example, want to know what cores and processes they’r using to make sure they’re taking full advantage of resources, or to better understand how resources get divided up, or to better understand the differences between plans.\nI’ve done some basic speed testing of parallel functions as well as some testing of nested functions Here, I’ll build on that to better understand how the workers get divided up in those nested functions, and use that to jump off into testing different plans.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Interrogating parallel resources"
    ]
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#packages-and-setup",
    "href": "parallelism/interrogating_parallel.html#packages-and-setup",
    "title": "Interrogating parallel functions",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}, because I tend to like writing for loops (there’s a reason I’ll try to write up sometime later). I test other packages in the {future} family (furrr, future_apply) where I try to better understand when they do and don’t give speed advantages.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nlibrary(foreach)\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tibble)\nlibrary(patchwork)\n\nregisterDoFuture()\nplan(multisession)",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Interrogating parallel resources"
    ]
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#workers-and-cores",
    "href": "parallelism/interrogating_parallel.html#workers-and-cores",
    "title": "Interrogating parallel functions",
    "section": "Workers and cores",
    "text": "Workers and cores\nWe get assigned workers and cores when we call plan . We can also get the outer process id\nnote on HPC, we need to use availableCores(methods = 'Slurm') .\n\navailableWorkers()\n\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n\navailableCores()\n\nsystem \n    20 \n\nSys.getpid()\n\n[1] 39732",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Interrogating parallel resources"
    ]
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#the-setup--nested-functions",
    "href": "parallelism/interrogating_parallel.html#the-setup--nested-functions",
    "title": "Interrogating parallel functions",
    "section": "The setup- nested functions",
    "text": "The setup- nested functions\nI’ll use the inner and outer parallel functions similar to my tests of nested functions, but instead of doing anything, they’ll just track the processes.\nWhat do I want to interrogate? The process id, for one- I can use Sys.getpid(). I think I might just skip all the actual processing and just get the IDs\nWhat do I want to check? How the processes get divvied up. Are the inner loops getting different processes? Are the outer, and then the inner all use that one? Does it change through the outer loop? Does it depend on the number of iterations?\n\ninner_par &lt;- function(outer_it, size) {\n  inner_out &lt;- foreach(j = 1:size,\n                       .combine = bind_rows) %dorng% {\n    \n                         thisproc &lt;- tibble(loop = \"inner\",\n                                            outer_iteration = outer_it,\n                                            inner_iteration = j, \n                                            pid = Sys.getpid())\n    # d &lt;- rnorm(size, mean = j)\n    # \n    # f &lt;- matrix(rnorm(size*size), nrow = size)\n    # \n    # g &lt;- d %*% f\n    # \n    # mean(g)\n    \n  }\n}\n\nFor the outer loop, let’s check the PID both before and after the inner loop runs.\n\nouter_par &lt;- function(outer_size, innerfun, inner_size) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %dorng% {\n                         \n                        outerpre &lt;- tibble(loop = 'outer_pre',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(outer_it = i, size = inner_size)\n                         \n                         outerpost &lt;- tibble(loop = 'outer_post',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         bind_rows(outerpre, inner_out, outerpost)\n                         \n                         \n                       }\n  \n  return(outer_out)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Interrogating parallel resources"
    ]
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#sequential-outer-loop",
    "href": "parallelism/interrogating_parallel.html#sequential-outer-loop",
    "title": "Interrogating parallel functions",
    "section": "sequential outer loop",
    "text": "sequential outer loop\nI assume if we make the outer loop sequential, the inner will then get PIDs\n\nouter_seq &lt;- function(outer_size, innerfun, inner_size) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %do% {\n                         \n                        outerpre &lt;- tibble(loop = 'outer_pre',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(outer_it = i, size = inner_size)\n                         \n                         outerpost &lt;- tibble(loop = 'outer_post',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         bind_rows(outerpre, inner_out, outerpost)\n                         \n                         \n                       }\n  \n  return(outer_out)\n}\n\n\ntestseq10 &lt;- outer_seq(outer_size = 10, innerfun = inner_par, inner_size = 10)\n\n\nggplot(testseq10, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0)) +\n  scale_x_continuous(breaks = 0:10) +\n  scale_color_binned(breaks = 0:10)\n\n\n\n\n\n\n\n\nAt least that- if we have a sequential outer, it lets the inner parallelise.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Interrogating parallel resources"
    ]
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#nested-with",
    "href": "parallelism/interrogating_parallel.html#nested-with",
    "title": "Interrogating parallel functions",
    "section": "Nested with %:%",
    "text": "Nested with %:%\nThe ‘proper’ way to nest foreach loops is with %:%. That’s not always possible, but I think we can here, to check what they’re getting.\n\nouter_nest &lt;- function(outer_size, innerfun, inner_size) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %:% \n    foreach(j = 1:inner_size,\n                       .combine = bind_rows) %dopar% {\n    \n                         thisproc &lt;- tibble(loop = \"\",\n                                            outer_iteration = i,\n                                            inner_iteration = j, \n                                            pid = Sys.getpid())\n                       }\n  \n  return(outer_out)\n}\n\nnote- inner_par isn’t doing anything here, since it’s not a function anymore\n\ntestnest &lt;- outer_nest(10, inner_par, 10)\n\n\nggplot(testnest, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0)) +\n  scale_x_continuous(breaks = 0:10) +\n  scale_color_binned(breaks = 0:10)\n\n\n\n\n\n\n\n\nNow, each outer loop is getting two PIDs to split up with its inner loop.\nSo that makes sense- this way foreach knows what’s coming and can split up workers. It looks like the outer loop is favored, though that shouldn’t matter when they’re specified this way.\nWe can check though\n\ntestnest50_10 &lt;- outer_nest(50, inner_par, 10)\ntestnest10_50 &lt;- outer_nest(10, inner_par, 50)\n\n\nplotnest50_10 &lt;- ggplot(testnest50_10, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplotnest10_50 &lt;- ggplot(testnest10_50, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplotnest50_10 + plotnest10_50\n\n\n\n\n\n\n\n\nThose look more similar than they are because of different axes. I think that is giving more PIDs to the outer when it has more iterations.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Interrogating parallel resources"
    ]
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#list-plans",
    "href": "parallelism/interrogating_parallel.html#list-plans",
    "title": "Interrogating parallel functions",
    "section": "List-plans",
    "text": "List-plans\n\nNaive- just defaults\nThere is information out there, largely related to using future.batchtools, that a list of plans lets us handle nested futures. Does that work with multisession?\nplan(\"list\") tells us what the plan is. This is super helpful for checking what’s going on.\n\nplan(list(multisession, multisession))\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(list(multisession, multisession))\n2. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(list(multisession, multisession))\n\n\nThen let’s use the same outer_par we tried earlier\nLet’s check an outer loop with 1, and inner with 10, and vice versa\n\ntest1_10_double &lt;- outer_par(outer_size = 1, \n                             innerfun = inner_par, inner_size = 10)\ntest10_1_double &lt;- outer_par(outer_size = 10, \n                             innerfun = inner_par, inner_size = 1)\n\n\nplot1_10_double &lt;- ggplot(test1_10_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot10_1_double &lt;- ggplot(test10_1_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot1_10_double + plot10_1_double\n\n\n\n\n\n\n\n\nThat didn’t work. BUT, plan(\"list\") shows that the first one uses workers = availableCores(). Does that eat all the cores?\n\n\nTweak outer plan\nIf we limit the outer plan, do the ‘leftover’ cores go to the inner?\n\nplan(list(tweak(multisession, workers = 2), multisession))\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = 2, envir = parent.frame())\n   - tweaked: TRUE\n   - call: plan(list(tweak(multisession, workers = 2), multisession))\n2. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(list(tweak(multisession, workers = 2), multisession))\n\n\nLet’s check an outer loop with 1, and inner with 10, and vice versa\n\ntest1_10_double &lt;- outer_par(outer_size = 1, \n                             innerfun = inner_par, inner_size = 10)\ntest10_1_double &lt;- outer_par(outer_size = 10, \n                             innerfun = inner_par, inner_size = 1)\n\n\nplot1_10_double &lt;- ggplot(test1_10_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot10_1_double &lt;- ggplot(test10_1_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot1_10_double + plot10_1_double\n\n\n\n\n\n\n\n\nThat restricted the outer, but didn’t give any to the inner. Can I get them there?\n\n\nTweak both plans\nNow, we’re explicitly telling each plan how many workers it gets.\n\nplan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 5)))\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = 2, envir = parent.frame())\n   - tweaked: TRUE\n   - call: plan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 5)))\n2. multisession:\n   - args: function (..., workers = 5, envir = parent.frame())\n   - tweaked: TRUE\n   - call: plan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 5)))\n\n\nLet’s check an outer loop with 1, and inner with 10, and vice versa\nThis worked previously but now is failing, parallelly won’t let me set up &gt; 3 localhost ‘workers’ on only one core. I thought that since parallelly::availableCores() returned 20, it would give the outer to 1, and then the inners to a different set of 10. But it seems to be trying to give the inner to 10 on the outer (note that this works just fine on a cluster, so it’s sort of esoteric here anyway).\n\ntest1_10_double &lt;- outer_par(outer_size = 1, \n                             innerfun = inner_par, inner_size = 10)\ntest10_1_double &lt;- outer_par(outer_size = 10, \n                             innerfun = inner_par, inner_size = 1)\n\n\nplot1_10_double &lt;- ggplot(test1_10_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot10_1_double &lt;- ggplot(test10_1_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot1_10_double + plot10_1_double\n\nThat worked (But not anymore)! So, if we explicitly give each plan workers, we can manage nestedness. (If we can figure out how to do it with the new parallelly).\nTurn the plan back to multisession\n\nplan(multisession)\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(multisession)",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Interrogating parallel resources"
    ]
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#inner-loop",
    "href": "parallelism/interrogating_parallel.html#inner-loop",
    "title": "Interrogating parallel functions",
    "section": "Inner loop",
    "text": "Inner loop\n\nParallel version\n\ninner_par &lt;- function(in_vec, size, outer_it) {\n  inner_out &lt;- foreach(j = in_vec,\n                       .combine = bind_rows) %dorng% {\n    d &lt;- rnorm(size, mean = j)\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    h &lt;- mean(g)\n    \n    thisproc &lt;- tibble(loop = \"inner\",\n                       outer_iteration = outer_it,\n                       inner_iteration = j, \n                       pid = Sys.getpid())\n    \n                       }\n  return(inner_out)\n}\n\n\n\nSequential version\n\ninner_seq &lt;- function(in_vec, size, outer_it) {\n  inner_out &lt;- foreach(j = in_vec,\n                       .combine = bind_rows) %do% {\n    d &lt;- rnorm(size, mean = j)\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    h &lt;- mean(g)\n    \n    thisproc &lt;- tibble(loop = \"inner\",\n                       outer_iteration = outer_it,\n                       inner_iteration = j, \n                       pid = Sys.getpid())\n    \n                       }\n  \n  return(inner_out)\n}\n\n\n\nUsing preallocated for\nThis is likely to be faster than the sequential. Preallocate both the vector and the new tibble output.\n\ninner_for &lt;- function(in_vec, size, outer_it) {\n  inner_out &lt;- vector(mode = 'numeric', length = size)\n  \n  thisproc &lt;- tibble(loop = \"inner\",\n                       outer_iteration = outer_it,\n                       inner_iteration = 1:length(in_vec), \n                       pid = Sys.getpid())\n  \n  for(j in 1:length(in_vec)) {\n    d &lt;- rnorm(size, mean = in_vec[j])\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    inner_out[j] &lt;- mean(g)\n    \n    thisproc$pid[j] &lt;- Sys.getpid()\n    thisproc$inner_iteration[j] &lt;- j\n    \n  }\n  \n  return(thisproc)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Interrogating parallel resources"
    ]
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#outer-loop",
    "href": "parallelism/interrogating_parallel.html#outer-loop",
    "title": "Interrogating parallel functions",
    "section": "Outer loop",
    "text": "Outer loop\nI cant divide by inner_out now that it’s not a matrix, so just get a cv.\n\nparallel\n\nouter_par &lt;- function(size, innerfun) {\n  outer_out &lt;- foreach(i = 1:size,\n                       .combine = bind_rows) %dorng% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a &lt;- rnorm(size, mean = i)\n                         \n                         b &lt;- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec &lt;- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(in_vec = cvec, \n                                               size = size, \n                                               outer_it = i)\n                         \n                         h &lt;- sd(cvec)/mean(cvec)\n                         \n                         inner_out\n                         \n                       }\n  \n  return(outer_out)\n}\n\n\n\nsequential\n\nouter_seq &lt;- function(size, innerfun) {\n  outer_out &lt;- foreach(i = 1:size,\n                       .combine = bind_rows) %do% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a &lt;- rnorm(size, mean = i)\n                         \n                         b &lt;- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec &lt;- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(in_vec = cvec, \n                                               size = size, outer_it = i)\n                         \n                         h &lt;- sd(cvec)/mean(cvec)\n                         \n                         inner_out\n                       }\n  \n  return(outer_out)\n}\n\n\n\nUn-preallocated for\nBecause this would need to replace chnks in the tibble, it’s hard to preallocate. Just don’t bother- the point isn’t speed, it’s testing pids.\n\nouter_for &lt;- function(size, innerfun) {\n  outer_out &lt;- matrix(nrow = size, ncol = size)\n  \n  thisproc &lt;- tibble(loop = \"inner\",\n                       outer_iteration = 1,\n                       inner_iteration = 1, \n                       pid = Sys.getpid(),\n                     .rows = 0)\n  \n  for(i in 1:size) {\n    \n    # Do a matrix mult on a vector specified with i\n    a &lt;- rnorm(size, mean = i)\n    \n    b &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    cvec &lt;- a %*% b\n    \n    # Now iterate over the values in c to do somethign else\n    inner_out &lt;- innerfun(in_vec = cvec, size = size, outer_it = i)\n    \n    outer_out[, i] &lt;- sd(cvec)/mean(cvec)\n    \n    thisproc &lt;- bind_rows(thisproc, inner_out)\n    \n    \n  }\n  outer_out &lt;- c(outer_out) \n  \n  \n  return(thisproc)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Interrogating parallel resources"
    ]
  },
  {
    "objectID": "parallelism/hpc_ephemera.html",
    "href": "parallelism/hpc_ephemera.html",
    "title": "HPC ephemera",
    "section": "",
    "text": "To see characteristics of the system (nodes, CPUs, memory, etc) and state:\nsinfo --Node --long",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "HPC notes"
    ]
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#slurm-commands",
    "href": "parallelism/hpc_ephemera.html#slurm-commands",
    "title": "HPC ephemera",
    "section": "",
    "text": "To see characteristics of the system (nodes, CPUs, memory, etc) and state:\nsinfo --Node --long",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "HPC notes"
    ]
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#common-workflow",
    "href": "parallelism/hpc_ephemera.html#common-workflow",
    "title": "HPC ephemera",
    "section": "Common workflow:",
    "text": "Common workflow:\ngit clone repo, cd into it\nset up environment with renv - usually just module load R then R for an interactive session\nwhen things break because the R version is too old, it sometimes works to force it to update using {rig}\ndo most dev on local computer, push\ngit pull to HPC- if on branch\ngit pull origin BRANCHNAME\nrun code with sbatch\nsomething breaks\nfix locally, push, pull, re-run\nuse scripts to copy data down",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "HPC notes"
    ]
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#transferring-files",
    "href": "parallelism/hpc_ephemera.html#transferring-files",
    "title": "HPC ephemera",
    "section": "Transferring files",
    "text": "Transferring files\n\nThe best solution- WinSCP\nUnless we want to batch-transfer a lot of stuff automatically, USE WINSCP- it’s way easier, and we can open docs with notepad++, etc.\n\n\nScripting\nI have better ones, but simply, if we are on a local terminal in a directory we want to put the file (or maybe we want it in a subdir)\nscp user@remote.address:~/path/to/file/filename.txt /subdir/filename.txt\nThat’s annoying because we need to start local, and so have an scp terminal running alongside the one that’s sshed. Otherwise we have to treat local as remote from inside the ssh session.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "HPC notes"
    ]
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#running-any-file",
    "href": "parallelism/hpc_ephemera.html#running-any-file",
    "title": "HPC ephemera",
    "section": "Running any file",
    "text": "Running any file\nThe how-tos for using SLURM all have a line that is Rscript filename.R to run that file. But that means we have to have a different shell script for each R script we want to run. THat’s really annoying, especially when prototyping or with a lot of similar R scripts. Instead, we want to build a shell script that takes the R script name as input in the sbatch call. E.g.\nsbatch shellname.sh\nWith the R script hardcoded in sh.\nInstead, we want\nsbatch shellname.sh rname.R\nthat can take an arbitrary R script.\nThis then will get to other arguments, but this is the first step and super useful.\nIt’s relatively easy- just use Rscript $1 in the shell script, and then the command above works.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "HPC notes"
    ]
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#arguments-in-sbatch",
    "href": "parallelism/hpc_ephemera.html#arguments-in-sbatch",
    "title": "HPC ephemera",
    "section": "Arguments in sbatch",
    "text": "Arguments in sbatch\nSometimes we want to pass arguments to R scripts in the sbatch call so we don’t need fully-new Rscripts just to change a parameter value. In that case, we can pass the args, and they’re available in R with commandArgs(). There are a couple things to be aware of to use this. Primarily, they boil down to The shell SLURM script must match the argument extraction in the R script- ie they need to know the argument order and meaning.\n\nSlots for them MUST be available in the slurm script. e.g.\nRscript $1 $2\nwill work to pass sbatch any_R.sh rscriptname.R argument1 in, with rscriptname.R being $1(the first argument to the slurm script) and argument1 being $2. (The first additional argument, intended for R). If you send sbatch any_R.sh rscriptname.R argument1 argument2 in to a slurm script as above, argument2 will just disappear into the ether.\nAlternatively, the slurm script itself can define arguments-\nRscript $1 \"argument1\" \"argument2\"\nmakes whatever $1 is coming in from command line, as well as \"argument1\" and \"argument2\" available in R via commandArgs()\nWe can use arbitrary numbers of arguments as well with $* , which accesses all the arguments.\nRscript $*\nNote that now we’ve dropped the $1- it’s included as the first item in the $* . This is a bit dangerous- we need to make sure we use the right order, but it is flexible.\nAccessing the arguments is tricky- they are numbered in order, but there are some initial invisible ones. It seems, but may not always be true, that the first four are set, with the fourth being --file=filename.R for the file called by Rscript, then 5 is --args and the subsequent args are 6-n.\nNumbers come through as characters\n${SLURM_ARRAY_TASK_ID} is a particularly useful variable to include, as it lets us manually divide tasks among a slurm array.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "HPC notes"
    ]
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#naming-jobs",
    "href": "parallelism/hpc_ephemera.html#naming-jobs",
    "title": "HPC ephemera",
    "section": "Naming jobs",
    "text": "Naming jobs\nAnd especially putting the name on the stdout and err. Creating a new dir for them would be even nicer, but I’ll leave that for later.\nthe produced stdout and err get hard to find after a lot of jobs. If we use %x in their names in the slurm script, it appends the jobname\nThe jobname by default is the name of the shell script, e.g. sbatch shellscript.sh has jobname “shellscript.sh”. That’s actually pretty useful if we have individual shell scripts. But if we’re using a script that takes R script names as arguments, it’s not. In that case, we can set the job with --job-name or -J flags, e.g. sbatch -J test_job shellscript.sh.\nNote that the jobname flag has to happen before the script, and does NOT affect the args returned by commandArgs() (thankfully).",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "HPC notes"
    ]
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#testing-future-plans",
    "href": "parallelism/hpc_ephemera.html#testing-future-plans",
    "title": "HPC ephemera",
    "section": "Testing future plans",
    "text": "Testing future plans\nplan(\"list\") tells us what the plan is. This is super helpful for checking what’s going on.\n\nlibrary(future)\nplan(multisession)\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(multisession)",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "HPC notes"
    ]
  },
  {
    "objectID": "parallelism/conditional_plans.html",
    "href": "parallelism/conditional_plans.html",
    "title": "Conditional futures",
    "section": "",
    "text": "library(future)\n\nIt’s likely that we want to write code that just runs either locally or on an HPC without having to change a bunch of things inside the relevant scripts. This is useful for local prototyping, as well as just sometimes needing to run things locally.\nThe portability of {futures} is a major benefit for this reason- futures should all work the same, no matter what plan is used to resolve them. So, we can write code that runs both locally and on the HPC by changing the plan. And we can automate this process by making the plan conditional.\nDifferent HPCs define themselves differently, but in the simple case where the code only runs on Linux on an HPC, something like\n\nif (grepl('^Windows', Sys.info()[\"sysname\"])) {\n  inpath &lt;- file.path('path', 'to', 'local', 'inputs')\n  outpath &lt;- file.path('path', 'to', 'local', 'outputs')\n  plan(multisession)\n}\n\nif (grepl('^Linux', Sys.info()[\"sysname\"])) {\n  inpath &lt;- file.path('path', 'to', 'HPC', 'inputs')\n  outpath &lt;- file.path('path', 'to', 'HPC', 'outputs')\n  plan(list(tweak(batchtools_slurm,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 5,\n                                 ntasks.per.node = 12, \n                                 mem = 1000)),\n          multicore))\n}\n\nObviously if you run on Linux that’s NOT an HPC, that second if needs to ask about something else. Sys.info()$user and login are often the same, but I’ve had success with something like\n\nif (grepl('^nameofcluster', Sys.info()[\"nodename\"]) | \n    grepl('^c', Sys.info()[\"nodename\"])) {\n  inpath &lt;- file.path('path', 'to', 'HPC', 'inputs')\n  outpath &lt;- file.path('path', 'to', 'HPC', 'outputs')\n  plan(list(tweak(batchtools_slurm,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 5,\n                                 ntasks.per.node = 12, \n                                 mem = 1000)),\n          multicore))\n}\n\nwhere that second grepl is because the working nodes have a different nodename than the login node, which tends to have the same name as the whole HPC. But I have encountered HPCs where there a many different formats for that nodename, so there’s just some trial and error involved, often involving some jobs like those in slurm_r_tests that query and return HPC resources used, including the names of the nodes.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Conditional plans for local and HPC"
    ]
  },
  {
    "objectID": "package/rlang_data.html",
    "href": "package/rlang_data.html",
    "title": "No visible binding for global variable",
    "section": "",
    "text": "When we check packages (usually with devtools::check(), but presumably R CMD CHECK as well, we get a lot of notes about “no visible binding for global variable” if we use tidyverse code. This is because of the data masking dplyr et al do to let us use bare names.\nHowever, there’s a different fix for select (and tidyselect generally) than for other verbs like mutate and summarise. It’s hard to find, because we can fix all of the ‘global variable’ errors with .data, but that then causes deprecation warnings for select and friends while testing.\nWhile styler does not find this problem, lintr does. So it’s very helpful to install lintr and lint files, rather than running a full check. It doesn’t pick up the issues with tidyselect deprecating .data though. At least test works on single files and picks that issue up.\nNote- use devtools::load_all() before linting, or it won’t pick up .data itself and will throw that as a non-visible global.\n\n\nThe answer for non-select verbs is to use the .data[['variable_name']] or .data$variable_name convention everywhere and usethis::use_import_from('rlang', '.data'). That works to get rid of the errors, but now we’ve lost one of the really nice things about writing dplyr code- the simplicity of bare data variable names.\nThen, we need to actually find all of those places we need to add .data$ in front of names. The notes check produces help, and then we just have to look through the files. In general, it’s everything in tidyverse that uses a bare name, with a few exceptions. I put together a repo to minimally check which functions need it.\n\n\n\nThe tidyselect approach has deprecated .data after 1.2.0. It still checks fine, and gets rid of the global variable issue, but we get lots of warnings Use of .data in tidyselect expressions was deprecated in tidyselect 1.2.0. The reasoning is described on the tidyselect blog, where the suggested solution is to use all_of (or just to use characters). That seems OK, but is again clunky- we now need to stuff at minimum “” around the terms, and at worst, tidyselect::all_of all over the place. At some point it’s just easier to use [. Where it’s likely going to be most needed is where the tidyselect is a helper to other verbs, e.g. in the .by argument of mutate and summarise.\nThe example repo is set up to not throw checks or errors, and so we can see what works for different verbs.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Handling the rlang no visible binding issue"
    ]
  },
  {
    "objectID": "package/rlang_data.html#no-visible-binding-issue",
    "href": "package/rlang_data.html#no-visible-binding-issue",
    "title": "No visible binding for global variable",
    "section": "",
    "text": "When we check packages (usually with devtools::check(), but presumably R CMD CHECK as well, we get a lot of notes about “no visible binding for global variable” if we use tidyverse code. This is because of the data masking dplyr et al do to let us use bare names.\nHowever, there’s a different fix for select (and tidyselect generally) than for other verbs like mutate and summarise. It’s hard to find, because we can fix all of the ‘global variable’ errors with .data, but that then causes deprecation warnings for select and friends while testing.\nWhile styler does not find this problem, lintr does. So it’s very helpful to install lintr and lint files, rather than running a full check. It doesn’t pick up the issues with tidyselect deprecating .data though. At least test works on single files and picks that issue up.\nNote- use devtools::load_all() before linting, or it won’t pick up .data itself and will throw that as a non-visible global.\n\n\nThe answer for non-select verbs is to use the .data[['variable_name']] or .data$variable_name convention everywhere and usethis::use_import_from('rlang', '.data'). That works to get rid of the errors, but now we’ve lost one of the really nice things about writing dplyr code- the simplicity of bare data variable names.\nThen, we need to actually find all of those places we need to add .data$ in front of names. The notes check produces help, and then we just have to look through the files. In general, it’s everything in tidyverse that uses a bare name, with a few exceptions. I put together a repo to minimally check which functions need it.\n\n\n\nThe tidyselect approach has deprecated .data after 1.2.0. It still checks fine, and gets rid of the global variable issue, but we get lots of warnings Use of .data in tidyselect expressions was deprecated in tidyselect 1.2.0. The reasoning is described on the tidyselect blog, where the suggested solution is to use all_of (or just to use characters). That seems OK, but is again clunky- we now need to stuff at minimum “” around the terms, and at worst, tidyselect::all_of all over the place. At some point it’s just easier to use [. Where it’s likely going to be most needed is where the tidyselect is a helper to other verbs, e.g. in the .by argument of mutate and summarise.\nThe example repo is set up to not throw checks or errors, and so we can see what works for different verbs.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Handling the rlang no visible binding issue"
    ]
  },
  {
    "objectID": "package/rlang_data.html#foreach",
    "href": "package/rlang_data.html#foreach",
    "title": "No visible binding for global variable",
    "section": "foreach",
    "text": "foreach\nThe ‘no visible binding of global variable’ also shows up for foreach indices. The solution there is to initialise the variable first. From foreach github issues.\n```{r}\n## To please 'R CMD check'\nx &lt;- NULL\n\ny &lt;- foreach::foreach(x = 1:3) %do% {\n  sqrt(x)\n}\n```",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Handling the rlang no visible binding issue"
    ]
  },
  {
    "objectID": "package/rlang_data.html#cheat-sheet",
    "href": "package/rlang_data.html#cheat-sheet",
    "title": "No visible binding for global variable",
    "section": "Cheat sheet",
    "text": "Cheat sheet\nIn general, the key is if the help says an argument uses ‘data masking’, use .data, and if it says it uses tidy-select, use tidyselect. The catch is, it can be annoying to check, and some functions aren’t very clear. A cheatsheet of what to use where is in Table 1. Also, as far as I can tell, anywhere where this says tidyselect, if we just have a set of variable names, we can use either tidyselect::all_of(c(\"v1\", 'v2')) or just c('v1', 'v2') (or, obviously, more complex tidyselecting).\n\ncheat_tibble |&gt; \n  knitr::kable()\n\n\n\nTable 1\n\n\n\n\n\n\nverb\nargument\nfix\n\n\n\n\nmutate\n…\n.data\n\n\nmutate\n.by\ntidyselect\n\n\nsummarise\n…\n.data\n\n\nsummarise\n.by\ntidyselect\n\n\ngroup_by\n…\n.data\n\n\nacross\n.cols\ntidyselect\n\n\nselect\n…\ntidyselect\n\n\nrename\n…\ntidyselect\n\n\njoin_by\n…\ncharacter?\n\n\nunnest\ncols\ntidyselect\n\n\nunnest_longer\ncol\ntidyselect\n\n\nunnest_wider\ncol\ntidyselect\n\n\ncase_when (in mutate)\n…\n.data\n\n\nfilter\n…\n.data\n\n\nfilter\n.by\ntidyselect\n\n\nggplot2::aes\nx,y,…\n.data\n\n\nforeach\nindex\npreassign NULL\n\n\ntidyr::separate\ncol\ntidyselect\n\n\npivot_wider\nid_cols\ntidyselect\n\n\npivot_wider\nnames_from\ntidyselect\n\n\npivot_wider\nvalues_from\ntidyselect\n\n\ndistinct\n…\n.data\n\n\nDiagrammeR::add_nodes_from_table\nlabel_col\ntidyselect\n\n\nDiagrammeR::add_nodes_from_table\nset_type\ntidyselect\n\n\nDiagrammeR::add_nodes_from_table\ndrop_cols\ntidyselect\n\n\nDiagrammeR::add_nodes_from_table\ntype_col\ntidyselect\n\n\narrange\n…\n.data",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Handling the rlang no visible binding issue"
    ]
  },
  {
    "objectID": "package/indexing_version.html",
    "href": "package/indexing_version.html",
    "title": "Indexing package versions",
    "section": "",
    "text": "I’ve started automatically indexing package dev versions (e.g. major.minor.patch.dev, with dev starting at 9000) with every commit, as it makes tracking easier and makes installing the current (or any specific) version much easier with remotes or renv (and I assume install.packages or pak). I do this with a pre-commit hook, developed largely from this stackoverflow.\nThis approach depends on tags, and the trick is the tags already have to exist and there needs to be at least one commit past them (cannot start the hook at the same time as you start the first tag). This is an issue not just for the first commit. So, for example, you can’t add a tag at the current commit and then push (get an error about syntax error invalid arithmetic operator). Basically, it sees the tag as the commit number and tries to do math with it. Instead, tag the previous commit and it should work.\nSo, I started a tag v0.0.1, made a commit. Then added the last answer there in the .git/hooks/pre-commit file:\n#!/bin/sh\n#\n# Pre-commit hooks. Simple dev version incrementation, assuming R and usethis are installed\n\n# From https://stackoverflow.com/questions/24209336/automating-version-increase-of-r-packages\nsed -i -- \"s/^Date: .*/Date: $(date '+%Y-%m-%d')/\" DESCRIPTION\n# get latest tags\ngit pull --tags --quiet\ncurrent_tag=`git describe --tags --abbrev=0 | sed 's/v//'`\ncurrent_commit=`git describe --tags | sed 's/.*-\\(.*\\)-.*/\\1/'`\n# combine tag (e.g. 0.1.0) and commit number (like 40) increased by 9000 to indicate beta version\nnew_version=\"$current_tag.$((current_commit + 9000))\" # results in 0.1.0.9040\nsed -i -- \"s/^Version: .*/Version: ${new_version}/\" DESCRIPTION\necho \"First 3 lines of DESCRIPTION:\"\nhead -3 DESCRIPTION\necho\nAnd then when I commit, it updates DESCRIPTION.\nThere is a bit of funny behaviour here because these changes are not part of the commit. So, it essentially is pre-updating the version- immediately after committing, there is a changed DESCRIPTION file with the new number, which should be committed with the next commit. This is just the nature of doing it with commit hooks; post-commit would do the same thing.\nA couple notes:\nIf you don’t commit the changed description file, it will still iterate the dev version with each commit, since it is counting commits since the tag.\nI tried to use Rscript -e \"usethis::use_version('dev')\", but it requires user input with no obvious way to bypass. And was a bit tricky to get Rscript in a place bash could find it on windows, see https://github.com/r-lib/rig/issues/189.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Automatically indexing package versions with git"
    ]
  },
  {
    "objectID": "observable/trying_observable.html",
    "href": "observable/trying_observable.html",
    "title": "Trying observablejs chunks",
    "section": "",
    "text": "Quarto has the option of using Observable JS for code chunks. This gives the ability to add interactivity in-browser, without needing to do server-side calcs, as we do with Shiny. The catch is with Shiny, I can serve R objects (e.g. ggplots) that I’m familiar with. Using observable means I need to figure out how that system works. I’m also a bit unclear how much processing can happen. My understanding is that any actual processing that happens needs to happen in the ojs, not R chunks, so we can’t interactively change a setting in the observable chunk and have that kick off some R. Though I might be wrong.\nI have quite a few use cases in mind if I can get this to work- serving maps, as an interface to {hydrogauge}, some drone settings, playing with population models, etc.",
    "crumbs": [
      "Code Demos",
      "Observable",
      "Figuring out how to use ojs chunks"
    ]
  },
  {
    "objectID": "observable/trying_observable.html#issues",
    "href": "observable/trying_observable.html#issues",
    "title": "Trying observablejs chunks",
    "section": "ISSUES",
    "text": "ISSUES\n\nInteractive notebooks\nThe ojs chunks dont work in interactive mode, and throw errors like “Error in ojs_define(iris = iris) : could not find function”ojs_define”“. So to work on anything past the first ojs chunk requires rendering. But that brings us to the next issue:\n\n\nNo output\nObservable chunks don’t have output unless you use quarto preview, not just quarto render. And the ‘Render’ button in Rstudio renders and previews, making this more confusing. JUST RUNNING quarto render doc.qmd at the terminal yields a document with code chunks but no output. This is expected behavior, but is super counter intuitive, espcially given the Render button’s name.\nUnfortunately, there is no per-document quarto preview at the terminal like there is for quarto render. So if you’re working in a quarto project (website, book, etc), you have to preview the whole thing just to check a document.\nThat means you’ll almost certainly want to turn caching on for the project (probably do anyway if it’s big), but if caching is on for the quarto project, it won’t render because the ojs_defined object can’t be cached. So the chunk with ojs_define needs to have #| cache: false added to it. Or perhaps just turn caching off in the yaml headers for pages using ojs. Depends on how much pre-processing happens in R, probably.\n\n\nChunk options\nPython and R both use #| option: value for setting chunk options. ojs cells use //| option: value.\n\n\nColumn names\nObservable uses object.thing notation like python, but it also uses the . to reference columns in arquero. That means Sepal_Length is confusing, because it gets referenced as d.Sepal_Length. So change the names.\n\nnames(iris) &lt;- stringr::str_replace_all(names(iris), '\\\\.', '_')\n\n\n\nCode changes\nI’m running into issues where I change some R code, and it works when I run it interactively, but then when I go to render, the new R code just doesn’t happen. E.g. I’ll add code that makes a dataframe with more values, and I can see them interactively in R, but they don’t appear in the render. I think it has something to do with the cache not resetting with changes, but I’m not positive.",
    "crumbs": [
      "Code Demos",
      "Observable",
      "Figuring out how to use ojs chunks"
    ]
  },
  {
    "objectID": "observable/trying_observable.html#r-setup",
    "href": "observable/trying_observable.html#r-setup",
    "title": "Trying observablejs chunks",
    "section": "R setup",
    "text": "R setup\n\nlibrary(ggplot2)\n\nI know the cool thing to do is {palmerpenguins}, but I’m just going to use {iris}.\nI have a feeling ojs is likely just as happy plotting vectors, but I’ll tend to have dataframes from analyses, so let’s stick with that.\nTo start, can we reproduce a simple ggplot?\n\nggplot(iris, aes(x = Sepal_Length, y = Sepal_Width, color = Species)) + geom_point()\n\n\n\n\n\n\n\n\nLet’s try that without reactivity to start.",
    "crumbs": [
      "Code Demos",
      "Observable",
      "Figuring out how to use ojs chunks"
    ]
  },
  {
    "objectID": "observable/trying_observable.html#data-to-ojs",
    "href": "observable/trying_observable.html#data-to-ojs",
    "title": "Trying observablejs chunks",
    "section": "Data to ojs",
    "text": "Data to ojs\nIt seems like Arquero makes a lot of sense, since it’s dplyr-like. But the example (and all other examples I can find) use it to read data in from an external file (csv, json, etc). That’s almost never what I’m going to want to do. So, how do I get a dataframe into Arquero? I’m guessing I can’t just grab it. The data sources documentation says we need to use ojs_define in R to make things available. Let’s see if we can do that and then make it an arquero object?\nNOTE I’ve not seen this mentioned anywhere, but ojs_define cannot be found in an interactive session- it’s only available on render. So interactively it throws “Error in ojs_define(iris = iris) : could not find function”ojs_define”“. AND, if caching is on for the quarto project, it won’t render because the ojs_defined object can’t be cached.\n\nojs_define(iris = iris)\n\nCan I see that in ojs? I thought .view made tables? Maybe not if we haven’t imported arquero? But I also thought order didn’t matter for ojs.\n\niris.view()\n\n\n\n\n\n\nAnyway, we can see it as an object (once we preview instead of render). I’m still grumpy about that.\n\niris\n\n\n\n\n\n\nI think we usually need to transpose according to various stackexchanges.\n\ntiris = transpose(iris)\ntiris\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, the arquero docs seem to suggest I might be able to use from to make it arquero?\n\nimport { aq, op } from '@uwdata/arquero'\nirtab = aq.from(iris)\n\nirtab.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat does look like it’s the wrong dims, let’s used the transposed.\n\ntirtab = aq.from(tiris)\ntirtab.view()",
    "crumbs": [
      "Code Demos",
      "Observable",
      "Figuring out how to use ojs chunks"
    ]
  },
  {
    "objectID": "observable/trying_observable.html#plots",
    "href": "observable/trying_observable.html#plots",
    "title": "Trying observablejs chunks",
    "section": "Plots",
    "text": "Plots\nTo make a first plot, do something I’m pretty sure should work, stolen directly from the penguins example that starts with a dataframe and just modifying the name and removing a facet level.\n\nPlot.rectY(tiris,\n  Plot.binX(\n    {y: \"count\"},\n    {x: \"Sepal_Width\", fill: \"Species\", thresholds: 10}\n  ))\n  .plot({\n    facet: {\n      data: tiris,\n      y: \"Species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\nScatter\nNow, can we make a scatter?\n\nPlot.dot(tiris, {x: \"Sepal_Length\", y: \"Sepal_Width\", fill: \"Species\"}).plot()\n\n\n\n\n\n\nThere’s lots of cleanup we could do to make that look different, but let’s go with that for now.\nCan I make a line? It’ll be jumbled, but whatever. Maybe I can sort it at least with arquero.\nRemember to use the arquero version of the data- this barfs with tiris.\n\ntirtab\n  .orderby('Sepal_Length')\n  .view()\n\n\n\n\n\n\nNow, how to plot that? Does the chunk above order tirtab permanently? Doesn’t seem to\n\ntirtab.view()\n\n\n\n\n\n\n\n\nLine\nLet’s try the line with the orderby\n\nPlot.line(tirtab.orderby('Sepal_Length'), {x: \"Sepal_Length\", y: \"Sepal_Width\", fill: \"Species\"}).plot()\n\n\n\n\n\n\nThat seems to have worked, but it sure is goofy looking. Oh. Is it because i’m using fill? Use stroke (not color- this isn’t ggplot).\n\nPlot.line(tirtab.orderby('Sepal_Length'), {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}).plot()\n\n\n\n\n\n\nWould be good to not do the data processing inside the plot call.\nI assume that’s as easy as\n\nirorder = tirtab.orderby('Sepal_Length')\n\nPlot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}).plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoint and line\nNeed to figure this out. Can I just do both? I think the answer might be to use a Plot.plot with mutliple marks?\nFirst, how does that syntax work? This should just recreate the above, right?\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"})\n  ]\n})\n\n\n\n\n\n\ncan we just add more Plot.marktypes?\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nCool. Would be nice if there was a ggplot-esque way to use the same x,y,color and just change the marks. Maybe there is? Look for that later.\nMoving toward reactivity, let’s say I only want dots where Sepal_Length &gt; 5 and &lt; 6\nI don’t quite seem to know the filter syntax. Not entirely sure what the d=&gt; means. Seems to be an internal reference to the data, but that feels weird and extra. I can get it to work, but doing anything complicated will require more thinking I think. Note that almost all the examples I can find use op.operation and so confused me for a bit thinking I needed op. The op access mathematical operations like abs, round, etc, and here I just need a simple &gt;&lt;.\n\nsl_filter = irorder\n  .filter(d =&gt; (d.Sepal_Length &lt; 6 & d.Sepal_Length &gt; 5))\n  \nsl_filter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow use that in a plot\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(sl_filter, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nThat seems to work",
    "crumbs": [
      "Code Demos",
      "Observable",
      "Figuring out how to use ojs chunks"
    ]
  },
  {
    "objectID": "observable/trying_observable.html#reactivity",
    "href": "observable/trying_observable.html#reactivity",
    "title": "Trying observablejs chunks",
    "section": "Reactivity",
    "text": "Reactivity\nThe thing here is to use viewof and Inputs.typeofinput. But what are those types? The observable docs seem to have a good overview.\nLet’s replicate the above, but also allow selecting the species. Basically following the quarto docs, but with a couple modifications. There’s got to be a way to obtain the ranges, species names, etc in code and not hardcode them in.\n\nviewof min_sl = Inputs.range(\n  [4, 8],\n  {value: 5, step: 0.1, label: \"Min Sepal Length:\"}\n)\n\nviewof max_sl = Inputs.range(\n  [4, 8],\n  {value: 6, step: 0.1, label: \"Max Sepal Length:\"}\n)\n\nviewof sp = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI like the tabset thing they do in the help, but to keep it simple just make the plot.\nI’m going to filter the data in its own chunk to try to aid figuring this out.\nThe .params here is needed to use the reactive values, and then gets referenced as $, while the data is d.\n\nspfilter = irorder\n  .params({\n  spf: sp\n})\n  .filter((d, $) =&gt; op.includes($.spf, d.Species))\n  \nspfilter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsizefilter = spfilter\n  .params({\n  minsl: min_sl,\n  maxsl: max_sl\n})\n  .filter((d, $) =&gt; d.Sepal_Length &gt; $.minsl && d.Sepal_Length &lt; $.maxsl)\n  \nsizefilter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd plot\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(sizefilter, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nThat seems to work. Can I package it up pretty like in the example?",
    "crumbs": [
      "Code Demos",
      "Observable",
      "Figuring out how to use ojs chunks"
    ]
  },
  {
    "objectID": "observable/trying_observable.html#making-better-ux",
    "href": "observable/trying_observable.html#making-better-ux",
    "title": "Trying observablejs chunks",
    "section": "Making better UX",
    "text": "Making better UX\nLet’s build that same thing, but at least kill off displaying code. We need to use different names here because ojs is reactive and so you can’t define variables in two places. Maybe I’ll just use petals instead of sepals.\n\nviewof min_pl = Inputs.range(\n  [1, 7],\n  {value: 5, step: 0.1, label: \"Min Petal Length:\"}\n)\n\nviewof max_pl = Inputs.range(\n  [1, 7],\n  {value: 6, step: 0.1, label: \"Max Petal Length:\"}\n)\n\nviewof sp2 = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [\"versicolor\"],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspfilter2 = irorder\n  .orderby('Petal_Length')\n  .params({\n  spf: sp2\n})\n  .filter((d, $) =&gt; op.includes($.spf, d.Species))\n\npetfilter = spfilter2\n  .params({\n  minpl: min_pl,\n  maxpl: max_pl\n})\n  .filter((d, $) =&gt; d.Petal_Length &gt; $.minpl && d.Petal_Length &lt; $.maxpl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter2, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\nFancy layouts\nSee the quarto layouts docs for help here, I’ll only try a couple things.\n\nTabset\nMake a tabset with the plot and data\n\nPlotData\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter2, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\npetfilter.view()\n\n\n\n\n\n\n\n\n\n\n\nSidebar panel\nNeed to rename the inputs to avoid double-naming\n\n\nviewof min_pl3 = Inputs.range(\n  [1, 7],\n  {value: 5, step: 0.1, label: \"Min Petal Length:\"}\n)\n\nviewof max_pl3 = Inputs.range(\n  [1, 7],\n  {value: 6, step: 0.1, label: \"Max Petal Length:\"}\n)\n\nviewof sp3 = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [\"versicolor\"],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter3, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter3, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\nspfilter3 = irorder\n  .orderby('Petal_Length')\n  .params({\n  spf: sp3\n})\n  .filter((d, $) =&gt; op.includes($.spf, d.Species))\n\npetfilter3 = spfilter3\n  .params({\n  minpl: min_pl3,\n  maxpl: max_pl3\n})\n  .filter((d, $) =&gt; d.Petal_Length &gt; $.minpl && d.Petal_Length &lt; $.maxpl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI think I’ll stop there. There’s lots more I could do to get better at using observable, and lots more I could do to integrate that with Quarto layouts, but my goal here was to figure out how to get it to work, and those other things will make more sense with specific use cases or their own quartos or something.",
    "crumbs": [
      "Code Demos",
      "Observable",
      "Figuring out how to use ojs chunks"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html",
    "href": "hydrogauge/hydrogauge_testing.html",
    "title": "Testing hydrogauge API",
    "section": "",
    "text": "# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html#section",
    "href": "hydrogauge/hydrogauge_testing.html#section",
    "title": "Testing hydrogauge API",
    "section": "",
    "text": "# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html#access-victoria-water-data-through-api",
    "href": "hydrogauge/hydrogauge_testing.html#access-victoria-water-data-through-api",
    "title": "Testing hydrogauge API",
    "section": "Access Victoria water data through API",
    "text": "Access Victoria water data through API\nThis document is my testing and development of functions to include in the {hydrogauge} package. Basically, it’s where I interactively sorted through how to hit the API functions, the formats of the lists, and how to unpack the returned lists. It is a work in progress, since that package is under development.\nWe want to access victorian water data for a set of sites. That requires using the api at https://data.water.vic.gov.au/cgi/webservice.exe?[JSON_request] , but it’s poorly documented. I think I got it mostly figured out in a testing document, but there’s a lot of extra testing in there that needs to be skipped over and cleaned up. My plan is to make this a package, but it needs more development. I’m moving further testing here so I can get to the point a bit quicker.\nLibraries. Do I still need jsonlite now that I’ve moved ot httr2?\n\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\nlibrary(httr2)\nlibrary(ggplot2)",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html#set-up-params",
    "href": "hydrogauge/hydrogauge_testing.html#set-up-params",
    "title": "Testing hydrogauge API",
    "section": "Set up params",
    "text": "Set up params\n\nURL\nThe base url that everything gets attached to is: and we use httr2::request to start building the request.\n\nvicurl &lt;- \"https://data.water.vic.gov.au/WMIS/cgi/webservice.exe?\"\nreqvic &lt;- request(vicurl)\n\n\n\nSite lists\nI want to test with one, two, and several sites in a site list. I had tried to do \"sitelist\" = c('site', 'site') , and that failed. But it works to have \"site, site\"\nThe upper steavenson is 405328, Barwon is 233217 (and has Temp), Taggerty 405331 only ran 2010-2013, And the Marysville golf course 405837 (only rainfall). That hits some things we want to make sure we pick up- no longer running gauges, gauges with only rain, gauges with lots of variables, etc.\nI only make one site_list here with multiple, but can do the str_c inside the calls usually.\n\nbarwon &lt;- '233217'\nsteavenson &lt;- '405328'\ntaggerty &lt;- '405331'\ngolf &lt;- '405837'\n\nallsites &lt;- str_c(barwon, steavenson, taggerty, golf, sep = \", \")",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html#api-functions",
    "href": "hydrogauge/hydrogauge_testing.html#api-functions",
    "title": "Testing hydrogauge API",
    "section": "API functions",
    "text": "API functions\nAnd how to call each- including multiple values.\nI finally found a couple sources of documentation that will hopefully be helpful: https://kisters.com.au/doco/hydllp.htm and https://water-monitoring.information.qld.gov.au/wini/Documents/RDMW_API_doco.pdf.\nThe first thing to do is to figure out what basic information is there, so we can ask for it. What we really want is get_ts_traces, but it has a lot of parameters (see Kisters docs). Some are relatively straightforward to meaning, though how to get them to be correct JSON can be tricky (e.g. site_list, while others are opaque, e.g. varfrom, varto, datasource, either to their meaning or what the options are we can ask for. We can try to figure that out with some querying of the other functions.\n\nDatasources\nCan we figure out what datasource means by asking for some by site?\nversion has to be 1.\n\nds_s_params &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites))\n\nreqvic %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 108\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\"}}\n\nresp_ds_s &lt;- reqvic %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_perform()\n\nrbody_ds_s &lt;- resp_ds_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 4\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405328\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405331\"\n  .. .. ..$ datasources:List of 1\n  .. .. .. ..$ : chr \"A\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405837\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\nI’ll need to sort out how to unpack that list later, but for now, let’s just look at it and see that all of them have options A and TELEM, and a couple have TELEMCOPY.\nAccording to the QLD pdf, the datasource distinguishes things like Archive and Telemetry. That’s similar in Vic, though QLD also had codes for back-filled holes, which don’t seem to be here (at least at these sites).\nSeems like it will be safest to ask for ‘A’ or ‘TELEM’.\nAnd the different variables can be in a var_list or varto and varfrom (though not always- see below). The numbers are for different variables, but again, no guarantee they’re the same in Vic.\n\nunpacking the list\nI might as well do this and build the function. Should be able to do it with unnest, and then maybe drop dumb columns? Nope, some of the lists unpack into lists of mixed type. But unnest_wider and unnest_longer might be the trick. Will need to test with single sites in case the structure changes.\nFor the function, we probably want to just print the error value or something, and not return it in the df. Or if it errors, return that, if it doesn’t, just give df. That’s probably best.\n\na &lt;- as_tibble(rbody_ds_s[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # error and a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # error, site, and a `datasources` list\n  unnest_longer(col = where(is.list)) # fully unpacked into a long df\na\n\n# A tibble: 10 × 2\n   site   datasources\n   &lt;chr&gt;  &lt;chr&gt;      \n 1 233217 A          \n 2 233217 TELEM      \n 3 233217 TELEMCOPY  \n 4 405328 A          \n 5 405328 TELEM      \n 6 405328 TELEMCOPY  \n 7 405331 A          \n 8 405837 A          \n 9 405837 TELEM      \n10 405837 TELEMCOPY  \n\n\nMight actually be better as a table or pivot_wider? Depends what the point is? Pivot wider is kind of a pain, use table? But table actually unpacks longer when I as_tibble or as.data.frame it. Which is annoying.\n\nb &lt;- table(a$site, a$datasources)\nb\n\n        \n         A TELEM TELEMCOPY\n  233217 1     1         1\n  405328 1     1         1\n  405331 1     0         0\n  405837 1     1         1\n\n\nI think just return the long tibble, and do the table as a plot or explicitly a table or something. Which orientation makes most sense? Not sure. Probably gauges on x? But plots of actual data will have gauges on y, time on x, so maybe stay consistent.\n\nc &lt;- b %&gt;% \n  as_tibble(.name_repair = 'unique') %&gt;% \n  rename(gauge = `...1`, datasource = `...2`)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n\nc %&gt;% \n  mutate(n = as.logical(n)) %&gt;% \nggplot2::ggplot(ggplot2::aes(x = datasource, y = gauge, fill = n)) + \n  ggplot2::geom_tile(colour=\"white\", size=0.25) +\n  ggplot2::scale_fill_discrete(type = c('firebrick', 'dodgerblue')) +\n  ggplot2::labs(fill = NULL) +\n  ggplot2::coord_equal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nCan I just build that plot from a? Yes, but would be just as annoying. maybe.\n\nallopts &lt;- a %&gt;%\n  expand(site, datasources)\n\na2 &lt;- a %&gt;% \n  mutate(indata = TRUE) %&gt;% \n  dplyr::right_join(allopts) %&gt;% \n  mutate(indata = ifelse(is.na(indata), FALSE, indata))\n\nJoining with `by = join_by(site, datasources)`\n\n  ggplot(a2, aes(x = datasources, y = site, fill = indata)) + \n  ggplot2::geom_tile(colour=\"white\", linewidth=0.25) +\n  ggplot2::scale_fill_discrete(type = c('firebrick', 'dodgerblue')) +\n  ggplot2::labs(fill = NULL) +\n  ggplot2::coord_equal()\n\n\n\n\n\n\n\n\nGood enough for this one- turn that into a function in a package.\n\n\n\nTest from the package\nI’m using devtools::load_all() to load the package repo in here for interactive testing and poking.\nObviously, hard paths are a bad idea, but I’m going to do it here since I typically do relative within repos, and these are across repos. Still, temporary and hacky.\n\ndevtools::load_all('C:/Users/Galen/Documents/code/hydrogauge')\n\nℹ Loading hydrogauge\n\n\n\nreturntib &lt;- get_datasources_by_site(vicurl, allsites)\n\n\nreturntib\n\n# A tibble: 10 × 2\n   site   datasource\n   &lt;chr&gt;  &lt;chr&gt;     \n 1 233217 A         \n 2 233217 TELEM     \n 3 233217 TELEMCOPY \n 4 405328 A         \n 5 405328 TELEM     \n 6 405328 TELEMCOPY \n 7 405331 A         \n 8 405837 A         \n 9 405837 TELEM     \n10 405837 TELEMCOPY \n\nplot_datasources_by_site(returntib)\n\nJoining with `by = join_by(site, datasource)`\n\n\n\n\n\n\n\n\n\n\n\nSites by datasource\nHaven’t written this one before, might blow things up. But if we want a list of sites, it might be better than the way I did this before of just asking for everything in the db, lots of which had no data.\nAnd now we know the datasource options. I think? I suppose it’s possible there’s another type I’m not aware of.\nFor some weird reason, sitelists should be \"site, site, site\", while the datasources should be c('source', 'source'). The latter yields JSON array ['source', 'source'], while the former yields JSON 'site', 'site'\nThis works. The list truncates, but it did work.\n\nds_wanted &lt;- c('A', 'TELEM')\ns_ds_params &lt;- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = ds_wanted))\n\nreqvic %&gt;% \n  req_body_json(s_ds_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}\n\nresp_s_ds &lt;- reqvic %&gt;% \n  req_body_json(s_ds_params) %&gt;% \n  req_perform()\n\nrbody_s_ds &lt;- resp_s_ds %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_s_ds)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ datasources:List of 2\n  .. ..$ :List of 2\n  .. .. ..$ datasource: chr \"A\"\n  .. .. ..$ sites     :List of 4177\n  .. .. .. ..$ : chr \"100023\"\n  .. .. .. ..$ : chr \"100500\"\n  .. .. .. ..$ : chr \"100503\"\n  .. .. .. ..$ : chr \"100504\"\n  .. .. .. ..$ : chr \"101708\"\n  .. .. .. ..$ : chr \"102621\"\n  .. .. .. ..$ : chr \"102827\"\n  .. .. .. ..$ : chr \"102828\"\n  .. .. .. ..$ : chr \"102829\"\n  .. .. .. ..$ : chr \"102830\"\n  .. .. .. ..$ : chr \"102831\"\n  .. .. .. ..$ : chr \"103811\"\n  .. .. .. ..$ : chr \"104104\"\n  .. .. .. ..$ : chr \"104929\"\n  .. .. .. ..$ : chr \"104930\"\n  .. .. .. ..$ : chr \"105134\"\n  .. .. .. ..$ : chr \"105222\"\n  .. .. .. ..$ : chr \"105287\"\n  .. .. .. ..$ : chr \"105317\"\n  .. .. .. ..$ : chr \"105480\"\n  .. .. .. ..$ : chr \"105484\"\n  .. .. .. ..$ : chr \"105936\"\n  .. .. .. ..$ : chr \"107631\"\n  .. .. .. ..$ : chr \"107970\"\n  .. .. .. ..$ : chr \"107971\"\n  .. .. .. ..$ : chr \"108201\"\n  .. .. .. ..$ : chr \"108202\"\n  .. .. .. ..$ : chr \"108203\"\n  .. .. .. ..$ : chr \"108319\"\n  .. .. .. ..$ : chr \"108320\"\n  .. .. .. ..$ : chr \"108321\"\n  .. .. .. ..$ : chr \"108898\"\n  .. .. .. ..$ : chr \"108899\"\n  .. .. .. ..$ : chr \"108917\"\n  .. .. .. ..$ : chr \"108944\"\n  .. .. .. ..$ : chr \"109133\"\n  .. .. .. ..$ : chr \"109461\"\n  .. .. .. ..$ : chr \"109462\"\n  .. .. .. ..$ : chr \"109644\"\n  .. .. .. ..$ : chr \"109645\"\n  .. .. .. ..$ : chr \"109769\"\n  .. .. .. ..$ : chr \"109770\"\n  .. .. .. ..$ : chr \"109778\"\n  .. .. .. ..$ : chr \"109779\"\n  .. .. .. ..$ : chr \"109781\"\n  .. .. .. ..$ : chr \"110151\"\n  .. .. .. ..$ : chr \"110152\"\n  .. .. .. ..$ : chr \"110153\"\n  .. .. .. ..$ : chr \"110171\"\n  .. .. .. ..$ : chr \"110186\"\n  .. .. .. ..$ : chr \"110464\"\n  .. .. .. ..$ : chr \"110721\"\n  .. .. .. ..$ : chr \"110724\"\n  .. .. .. ..$ : chr \"110731\"\n  .. .. .. ..$ : chr \"110739\"\n  .. .. .. ..$ : chr \"110745\"\n  .. .. .. ..$ : chr \"110943\"\n  .. .. .. ..$ : chr \"110978\"\n  .. .. .. ..$ : chr \"111543\"\n  .. .. .. ..$ : chr \"111549\"\n  .. .. .. ..$ : chr \"111551\"\n  .. .. .. ..$ : chr \"111691\"\n  .. .. .. ..$ : chr \"111692\"\n  .. .. .. ..$ : chr \"112182\"\n  .. .. .. ..$ : chr \"112185\"\n  .. .. .. ..$ : chr \"112235\"\n  .. .. .. ..$ : chr \"112236\"\n  .. .. .. ..$ : chr \"112237\"\n  .. .. .. ..$ : chr \"112459\"\n  .. .. .. ..$ : chr \"112708\"\n  .. .. .. ..$ : chr \"112803\"\n  .. .. .. ..$ : chr \"112804\"\n  .. .. .. ..$ : chr \"113004\"\n  .. .. .. ..$ : chr \"113124\"\n  .. .. .. ..$ : chr \"113125\"\n  .. .. .. ..$ : chr \"113467\"\n  .. .. .. ..$ : chr \"113694\"\n  .. .. .. ..$ : chr \"113695\"\n  .. .. .. ..$ : chr \"113705\"\n  .. .. .. ..$ : chr \"113706\"\n  .. .. .. ..$ : chr \"114129\"\n  .. .. .. ..$ : chr \"114158\"\n  .. .. .. ..$ : chr \"114169\"\n  .. .. .. ..$ : chr \"115732\"\n  .. .. .. ..$ : chr \"115872\"\n  .. .. .. ..$ : chr \"116382\"\n  .. .. .. ..$ : chr \"116459\"\n  .. .. .. ..$ : chr \"116460\"\n  .. .. .. ..$ : chr \"116802\"\n  .. .. .. ..$ : chr \"116803\"\n  .. .. .. ..$ : chr \"119329\"\n  .. .. .. ..$ : chr \"119330\"\n  .. .. .. ..$ : chr \"119337\"\n  .. .. .. ..$ : chr \"119338\"\n  .. .. .. ..$ : chr \"119339\"\n  .. .. .. ..$ : chr \"119340\"\n  .. .. .. ..$ : chr \"119341\"\n  .. .. .. ..$ : chr \"119342\"\n  .. .. .. ..$ : chr \"119347\"\n  .. .. .. .. [list output truncated]\n  .. ..$ :List of 2\n  .. .. ..$ datasource: chr \"TELEM\"\n  .. .. ..$ sites     :List of 1232\n  .. .. .. ..$ : chr \"100023\"\n  .. .. .. ..$ : chr \"100500\"\n  .. .. .. ..$ : chr \"100503\"\n  .. .. .. ..$ : chr \"100504\"\n  .. .. .. ..$ : chr \"100731\"\n  .. .. .. ..$ : chr \"101708\"\n  .. .. .. ..$ : chr \"102621\"\n  .. .. .. ..$ : chr \"102827\"\n  .. .. .. ..$ : chr \"102828\"\n  .. .. .. ..$ : chr \"102829\"\n  .. .. .. ..$ : chr \"102830\"\n  .. .. .. ..$ : chr \"102831\"\n  .. .. .. ..$ : chr \"103811\"\n  .. .. .. ..$ : chr \"104929\"\n  .. .. .. ..$ : chr \"104930\"\n  .. .. .. ..$ : chr \"105134\"\n  .. .. .. ..$ : chr \"105222\"\n  .. .. .. ..$ : chr \"105484\"\n  .. .. .. ..$ : chr \"105936\"\n  .. .. .. ..$ : chr \"107631\"\n  .. .. .. ..$ : chr \"107717\"\n  .. .. .. ..$ : chr \"107970\"\n  .. .. .. ..$ : chr \"107971\"\n  .. .. .. ..$ : chr \"108201\"\n  .. .. .. ..$ : chr \"108202\"\n  .. .. .. ..$ : chr \"108203\"\n  .. .. .. ..$ : chr \"108319\"\n  .. .. .. ..$ : chr \"108320\"\n  .. .. .. ..$ : chr \"108321\"\n  .. .. .. ..$ : chr \"108944\"\n  .. .. .. ..$ : chr \"109133\"\n  .. .. .. ..$ : chr \"109462\"\n  .. .. .. ..$ : chr \"109644\"\n  .. .. .. ..$ : chr \"109645\"\n  .. .. .. ..$ : chr \"109769\"\n  .. .. .. ..$ : chr \"109770\"\n  .. .. .. ..$ : chr \"109781\"\n  .. .. .. ..$ : chr \"110151\"\n  .. .. .. ..$ : chr \"110152\"\n  .. .. .. ..$ : chr \"110153\"\n  .. .. .. ..$ : chr \"110171\"\n  .. .. .. ..$ : chr \"110186\"\n  .. .. .. ..$ : chr \"110464\"\n  .. .. .. ..$ : chr \"110721\"\n  .. .. .. ..$ : chr \"110724\"\n  .. .. .. ..$ : chr \"110731\"\n  .. .. .. ..$ : chr \"110739\"\n  .. .. .. ..$ : chr \"110745\"\n  .. .. .. ..$ : chr \"110943\"\n  .. .. .. ..$ : chr \"110978\"\n  .. .. .. ..$ : chr \"111543\"\n  .. .. .. ..$ : chr \"111549\"\n  .. .. .. ..$ : chr \"111551\"\n  .. .. .. ..$ : chr \"111691\"\n  .. .. .. ..$ : chr \"111692\"\n  .. .. .. ..$ : chr \"112182\"\n  .. .. .. ..$ : chr \"112185\"\n  .. .. .. ..$ : chr \"112235\"\n  .. .. .. ..$ : chr \"112236\"\n  .. .. .. ..$ : chr \"112237\"\n  .. .. .. ..$ : chr \"112459\"\n  .. .. .. ..$ : chr \"112708\"\n  .. .. .. ..$ : chr \"112803\"\n  .. .. .. ..$ : chr \"112804\"\n  .. .. .. ..$ : chr \"113004\"\n  .. .. .. ..$ : chr \"113124\"\n  .. .. .. ..$ : chr \"113125\"\n  .. .. .. ..$ : chr \"113467\"\n  .. .. .. ..$ : chr \"113694\"\n  .. .. .. ..$ : chr \"113695\"\n  .. .. .. ..$ : chr \"113705\"\n  .. .. .. ..$ : chr \"113706\"\n  .. .. .. ..$ : chr \"114129\"\n  .. .. .. ..$ : chr \"114158\"\n  .. .. .. ..$ : chr \"114169\"\n  .. .. .. ..$ : chr \"115732\"\n  .. .. .. ..$ : chr \"116382\"\n  .. .. .. ..$ : chr \"116802\"\n  .. .. .. ..$ : chr \"116803\"\n  .. .. .. ..$ : chr \"119329\"\n  .. .. .. ..$ : chr \"119330\"\n  .. .. .. ..$ : chr \"119337\"\n  .. .. .. ..$ : chr \"119339\"\n  .. .. .. ..$ : chr \"119340\"\n  .. .. .. ..$ : chr \"119341\"\n  .. .. .. ..$ : chr \"119342\"\n  .. .. .. ..$ : chr \"119347\"\n  .. .. .. ..$ : chr \"119348\"\n  .. .. .. ..$ : chr \"119365\"\n  .. .. .. ..$ : chr \"119366\"\n  .. .. .. ..$ : chr \"119367\"\n  .. .. .. ..$ : chr \"119377\"\n  .. .. .. ..$ : chr \"120248\"\n  .. .. .. ..$ : chr \"121019\"\n  .. .. .. ..$ : chr \"122152\"\n  .. .. .. ..$ : chr \"123140\"\n  .. .. .. ..$ : chr \"126975\"\n  .. .. .. ..$ : chr \"127479\"\n  .. .. .. ..$ : chr \"127480\"\n  .. .. .. .. [list output truncated]\n\n\nCan I tibble that up?\n\ns &lt;- as_tibble(rbody_s_ds[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # sites, and a `datasource` list\n  unnest_longer(col = where(is.list)) # fully unpacked into a long df\ns\n\n# A tibble: 5,409 × 2\n   datasource sites \n   &lt;chr&gt;      &lt;chr&gt; \n 1 A          100023\n 2 A          100500\n 3 A          100503\n 4 A          100504\n 5 A          101708\n 6 A          102621\n 7 A          102827\n 8 A          102828\n 9 A          102829\n10 A          102830\n# ℹ 5,399 more rows\n\n\nWhat are the number of sites in each?\n\ns %&gt;% group_by(datasource) %&gt;% summarise(n = n())\n\n# A tibble: 2 × 2\n  datasource     n\n  &lt;chr&gt;      &lt;int&gt;\n1 A           4177\n2 TELEM       1232\n\n\nWay more in Archive. Are there any in Telem that aren’t in A?\n\ntsites &lt;- s %&gt;% \n  filter(datasource == 'TELEM') %&gt;% \n  select(sites) %&gt;% \n  pull()\n\nasites &lt;- s %&gt;% \n  filter(datasource == 'A') %&gt;% \n  select(sites) %&gt;% \n  pull()\n\nall(tsites %in% asites)\n\n[1] FALSE\n\nsum(!(tsites %in% asites))\n\n[1] 59\n\n\nsee if I can blow up ggplot. Oof the plurals\n\ns &lt;- s %&gt;% \n  rename(site = sites, datasources = datasource)\n\nUnreadable. Not surprisingly.\n\nplot_datasources_by_site(s) + coord_flip()\n\nyikes.\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/code/hydrogauge')\n\nℹ Loading hydrogauge\n\nsxd &lt;- get_sites_by_datasource(portal = 'vic', datasources = c('A', 'TELEM'))\n\n\nsxd\n\n# A tibble: 5,409 × 2\n   datasource site  \n   &lt;chr&gt;      &lt;chr&gt; \n 1 A          100023\n 2 A          100500\n 3 A          100503\n 4 A          100504\n 5 A          101708\n 6 A          102621\n 7 A          102827\n 8 A          102828\n 9 A          102829\n10 A          102830\n# ℹ 5,399 more rows\n\n\nPlot still needs work.\n\nplot_datasources_by_site(sxd) + coord_flip()\n\nJoining with `by = join_by(datasource, site)`",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html#variables",
    "href": "hydrogauge/hydrogauge_testing.html#variables",
    "title": "Testing hydrogauge API",
    "section": "Variables",
    "text": "Variables\nor do I go straight for get_ts_ and then back this back out? Or get_db_info?\nI think I’m going to want to use get_variable_list both in get_ts_traces and to generate a set of possible variables.\nI’d like to get all sites, then make a master list of datasources and variables. But I need to get this usable for get_ts_traces, I think.\n\nGet_variable_list\nFeeding this a c(datasource, datasource) makes it return only some of the results, but throws no errors. So do one at a time.\n\nv_s_params &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites,\n                               \"datasource\" = \"A\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 119\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\",\"datasource\":\"A\"}}\n\nresp_v_s &lt;- req %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_perform()\n\nrbody_v_s &lt;- resp_v_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 4\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ variables   :List of 6\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20240903074500\"\n  .. .. .. .. ..$ period_start: chr \"19610306171500\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20240903074500\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"210.00\"\n  .. .. .. .. ..$ units       : chr \"pH\"\n  .. .. .. .. ..$ name        : chr \"Acidity/Alkalinity (pH)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20240903074500\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"215.00\"\n  .. .. .. .. ..$ units       : chr \"ppm\"\n  .. .. .. .. ..$ name        : chr \"Dissolved Oxygen (ppm)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20240903074500\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20240903074500\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20240903074500\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"233217\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20240826102700\"\n  .. .. .. .. ..$ period_start: chr \"20091119170800\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. ..$ site        : chr \"405328\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"TAGGERTY R LADY TALT\"\n  .. .. .. ..$ name      : chr \"TAGGERTY RV @ LADY TALBOT DRIVE NEAR MARYSVILLE\"\n  .. .. ..$ variables   :List of 4\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"405331\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"R.G. MARYSVILLE\"\n  .. .. .. ..$ name      : chr \"RAINGAUGE @ MARYSVILLE GOLF CLUB\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20240506120600\"\n  .. .. .. .. ..$ period_start: chr \"20010621142700\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"10.00\"\n  .. .. .. .. ..$ units       : chr \"mm\"\n  .. .. .. .. ..$ name        : chr \"Rainfall (mm)\"\n  .. .. ..$ site        : chr \"405837\"\n\n\nUnpack that\n\ns &lt;- as_tibble(rbody_v_s[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # sites, and a `datasource` list\n  unnest_wider(col = site_details) %&gt;% # site details in new cols\n  unnest_longer(col = variables) %&gt;% # one line per variable, details of variables in a list\n  rename(long_name = name) %&gt;% # variables have names too, avoid conflicts\n  unnest_wider(col = variables) %&gt;% # columns for each attribute of the variables\n  rename(var_name = name)\ns\n\n# A tibble: 12 × 10\n   timezone short_name  long_name period_end period_start subdesc variable units\n   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;\n 1 10.0     BARWON @ G… BARWON R… 202409030… 19610306171… Availa… 100.00   metr…\n 2 10.0     BARWON @ G… BARWON R… 202409030… 20100706123… Availa… 210.00   pH   \n 3 10.0     BARWON @ G… BARWON R… 202409030… 20100706123… Availa… 215.00   ppm  \n 4 10.0     BARWON @ G… BARWON R… 202409030… 20100706123… Availa… 450.00   Degr…\n 5 10.0     BARWON @ G… BARWON R… 202409030… 20100706123… Availa… 810.00   NTU  \n 6 10.0     BARWON @ G… BARWON R… 202409030… 20100706123… Availa… 820.00   µS/c…\n 7 10.0     STEAVENSON… STEAVENS… 202408261… 20091119170… Availa… 100.00   metr…\n 8 10.0     TAGGERTY R… TAGGERTY… 201302111… 20100729122… Availa… 100.00   metr…\n 9 10.0     TAGGERTY R… TAGGERTY… 201302111… 20100729122… Availa… 450.00   Degr…\n10 10.0     TAGGERTY R… TAGGERTY… 201302111… 20100729122… Availa… 810.00   NTU  \n11 10.0     TAGGERTY R… TAGGERTY… 201302111… 20100729122… Availa… 820.00   µS/c…\n12 10.0     R.G. MARYS… RAINGAUG… 202405061… 20010621142… Availa… 10.00    mm   \n# ℹ 2 more variables: var_name &lt;chr&gt;, site &lt;chr&gt;\n\n\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/code/hydrogauge')\n\nℹ Loading hydrogauge\n\nvl &lt;- get_variable_list(portal = 'vic', site_list = allsites, datasource = 'A')\n\n\nvl\n\n# A tibble: 12 × 11\n   site   short_name       long_name variable units var_name period_start       \n   &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;dttm&gt;             \n 1 233217 BARWON @ GEELONG BARWON R… 100.00   metr… Stream … 1961-03-06 07:15:00\n 2 233217 BARWON @ GEELONG BARWON R… 210.00   pH    Acidity… 2010-07-06 02:31:00\n 3 233217 BARWON @ GEELONG BARWON R… 215.00   ppm   Dissolv… 2010-07-06 02:31:00\n 4 233217 BARWON @ GEELONG BARWON R… 450.00   Degr… Water T… 2010-07-06 02:31:00\n 5 233217 BARWON @ GEELONG BARWON R… 810.00   NTU   Turbidi… 2010-07-06 02:31:00\n 6 233217 BARWON @ GEELONG BARWON R… 820.00   µS/c… Conduct… 2010-07-06 02:31:00\n 7 405328 STEAVENSON R @ … STEAVENS… 100.00   metr… Stream … 2009-11-19 07:08:00\n 8 405331 TAGGERTY R LADY… TAGGERTY… 100.00   metr… Stream … 2010-07-29 02:20:00\n 9 405331 TAGGERTY R LADY… TAGGERTY… 450.00   Degr… Water T… 2010-07-29 02:20:00\n10 405331 TAGGERTY R LADY… TAGGERTY… 810.00   NTU   Turbidi… 2010-07-29 02:20:00\n11 405331 TAGGERTY R LADY… TAGGERTY… 820.00   µS/c… Conduct… 2010-07-29 02:20:00\n12 405837 R.G. MARYSVILLE  RAINGAUG… 10.00    mm    Rainfal… 2001-06-21 04:27:00\n# ℹ 4 more variables: period_end &lt;dttm&gt;, subdesc &lt;chr&gt;, datasource &lt;chr&gt;,\n#   database_timezone &lt;chr&gt;\n\n\nTry with two datasources\n\nv2 &lt;- get_variable_list(portal = 'vic', site_list = allsites, datasource = c('A', 'TELEM'))\n\n\nv2\n\n# A tibble: 24 × 11\n   site   short_name       long_name variable units var_name period_start       \n   &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;dttm&gt;             \n 1 233217 BARWON @ GEELONG BARWON R… 100.00   metr… Stream … 1961-03-06 07:15:00\n 2 233217 BARWON @ GEELONG BARWON R… 210.00   pH    Acidity… 2010-07-06 02:31:00\n 3 233217 BARWON @ GEELONG BARWON R… 215.00   ppm   Dissolv… 2010-07-06 02:31:00\n 4 233217 BARWON @ GEELONG BARWON R… 450.00   Degr… Water T… 2010-07-06 02:31:00\n 5 233217 BARWON @ GEELONG BARWON R… 810.00   NTU   Turbidi… 2010-07-06 02:31:00\n 6 233217 BARWON @ GEELONG BARWON R… 820.00   µS/c… Conduct… 2010-07-06 02:31:00\n 7 405328 STEAVENSON R @ … STEAVENS… 100.00   metr… Stream … 2009-11-19 07:08:00\n 8 405331 TAGGERTY R LADY… TAGGERTY… 100.00   metr… Stream … 2010-07-29 02:20:00\n 9 405331 TAGGERTY R LADY… TAGGERTY… 450.00   Degr… Water T… 2010-07-29 02:20:00\n10 405331 TAGGERTY R LADY… TAGGERTY… 810.00   NTU   Turbidi… 2010-07-29 02:20:00\n# ℹ 14 more rows\n# ℹ 4 more variables: period_end &lt;dttm&gt;, subdesc &lt;chr&gt;, datasource &lt;chr&gt;,\n#   database_timezone &lt;chr&gt;\n\n\nI think now go to get_ts_traces, and then go back and write some helpers that can call get_datasources and get_variable and geo-locate, and use them to allow passing things like variables = ‘all’",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html#get-traces",
    "href": "hydrogauge/hydrogauge_testing.html#get-traces",
    "title": "Testing hydrogauge API",
    "section": "get traces",
    "text": "get traces\nThe basic format is this, will need to do some testing.\n\nb1params &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(b1params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 219\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb1 &lt;- req %&gt;% \n  req_body_json(b1params) %&gt;% \n  req_perform()\n\nrbodyb1 &lt;- respb1 %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb1)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892200\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm-10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/- 11mm-20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nand with two params\n\nb2params &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(b2params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb2 &lt;- req %&gt;% \n  req_body_json(b2params) %&gt;% \n  req_perform()\n\nrbodyb2 &lt;- respb2 %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb2)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892200\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm-10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/- 11mm-20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892200\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm-10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nTwo params, two sites (one site only has one var, but not an error, I hope)\n\nb22params &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = str_c(barwon, steavenson, sep = \", \"),\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(b22params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 231\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217, 405328\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb22 &lt;- req %&gt;% \n  req_body_json(b22params) %&gt;% \n  req_perform()\n\nrbodyb22 &lt;- respb22 %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb22)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 3\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892200\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm-10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/- 11mm-20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892200\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm-10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ longitude : chr \"145.773503100\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. .. ..$ latitude  : chr \"-37.525797590\"\n  .. .. .. ..$ org_name  : chr \"Victorian Rural Water Corporation\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm-10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.738\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.736\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.734\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.755\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.739\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.735\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.759\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.742\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.757\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"405328\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nI wasn’t able to get discharge (140, 141) from a varlist. Double check\n\nbparamsd &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,140\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparamsd) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,140\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbd &lt;- req %&gt;% \n  req_body_json(bparamsd) %&gt;% \n  req_perform()\n\nrbodybd &lt;- respbd %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodybd)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892200\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm-10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/- 11mm-20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nKisters has an example of asking for 140.01? Try that? No, just do the varfrom/varto method for discharge and stage.\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,140.01\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 226\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,140.01\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892200\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm-10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/- 11mm-20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nvarfrom-varto check. Does it give us both, or do we have to ask for the from separately`?\n\nbparamsft &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"100\",\n                               \"varto\" = \"140\", \n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparamsft) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"100\",\"varto\":\"140\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbft &lt;- req %&gt;% \n  req_body_json(bparamsft) %&gt;% \n  req_perform()\n\nrbodybft &lt;- respbft %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodybft)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892200\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 150: chr \"Rating extrapolated above 1.5x maximum flow gauged.\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.184\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.163\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.130\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.102\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.087\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.078\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.070\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.048\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.029\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.080\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.146\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.114\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.097\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.075\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.113\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Discharge (m3/sec)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"140.00\"\n  .. .. .. ..$ units     : chr \"cubic metres/second\"\n  .. .. .. ..$ name      : chr \"Stream Discharge (m3/s)\"\n\n\nThat looks like it might have only given us varto.\nWhat happens if we ask for a varto/from for a site that doesn’t have it? Earlier we saw that the varlist just skips, but does this?\n\nbparamse &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = golf,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparamse) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"405837\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbe &lt;- req %&gt;% \n  req_body_json(bparamse) %&gt;% \n  req_perform()\n\nrbodybe &lt;- respbe %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodybe)\n\nList of 2\n $ error_num: int 125\n $ error_msg: chr \"No data for specified variable in file\"\n\n\nOk, that errors. So I’ll need to capture and skip errors.\n\nunpack different formats\n(varlist with multiple, varfrom/to, etc). b1params (one param, one site), b2params (two params, varlist), b22params (two params, two sites- only one param at one site) bparamsft (varfrom/varto), bparamse (error),\nstart simple- grab errors. Should have been doing this all along.\n\ner1 &lt;- rbodyb1[1]\nere &lt;- rbodybe[1]\ner1\n\n$error_num\n[1] 0\n\nere\n\n$error_num\n[1] 125\n\n\nUnpack the single. Yeesh\nI’m ignoring quality codes for the moment. They’re a definition list, and so maybe should be unpacked separately, matched to varto (I think not varfrom), and then joined? But I want to see how they work with more complex structures first.\n\ns &lt;- as_tibble(rbodyb1[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %&gt;% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\nTry two variables. still works. still not sure why I need both varfrom and varto when they match.\n\ns &lt;- as_tibble(rbodyb2[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %&gt;% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\nTwo variables, two sites\n\ns &lt;- as_tibble(rbodyb22[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %&gt;% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\nAnd finally, the one where we do have a varto\n\ns &lt;- as_tibble(rbodybft[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %&gt;% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\nFinally, some new q values. and pretty clear we don’t need the varfroms.\nNeed to change the time column to dates\nDo we want to split up or return stacked or return wide? Make an option.\nDo we want to return NA days as NA or skip them?\n\n\nCleaning that up\nsafest is code x site x varto. though I think don’t need site, we’ll have expanded there by the time we get varto.\n\ns &lt;- as_tibble(rbodyb22[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. \n  # and we can drop varfrom\n  select(-varfrom_details) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('variable_', .)), \n              c(short_name, name)) \n\n# break in here to get the quality codes to match\nqc &lt;- s %&gt;% \n  select(quality_codes, site, variable) %&gt;% \n  unnest_longer(col = quality_codes) %&gt;% \n  mutate(quality_codes_id = as.integer(quality_codes_id))\n\n# finish unpacking\ns &lt;- s %&gt;%\n  select(-quality_codes) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\n# clean up\ns &lt;- s %&gt;% \n  rename(value = v, time = t, quality_codes_id = q) %&gt;% \n  mutate(time = lubridate::ymd_hms(time)) %&gt;% \n  left_join(qc, by = c('quality_codes_id', 'site', 'variable')) %&gt;% \n  mutate(across(c(longitude, latitude, value), as.numeric)) # leaving some others because they either are names (gauges, variable) or display better (precision)\n\nCan I make that wide? Works without using id_cols but messy because too many info cols. Would end up being better to cut and join the info back on. But then I lose the quality codes, because they apply to each variable differently. Just return like this for now with a warning.\n\nsw &lt;- s %&gt;% pivot_wider(names_from = variable, values_from = value, id_cols = c(time, site))\n\nWhat I could do though is break it up into a list, potentially by sites and/or variables.\n\nslist &lt;- split(s, s$site)\nvlist &lt;- split(s, s$variable)\nsvlist &lt;- split(s, interaction(s$site, s$variable))\n\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/code/hydrogauge')\n\nℹ Loading hydrogauge\n\n\nOne site, one variable\n\nbs &lt;- get_ts_traces(portal = 'vic', site_list = barwon, datasource = 'A', var_list = '100', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nCan I pass decimals? it’s how they come out of get_variable_list\n\nbsdec &lt;- get_ts_traces(portal = 'vic', site_list = barwon, datasource = 'A', var_list = '100.00', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOne site, derived variables\n\nbsd &lt;- get_ts_traces(portal = 'vic', site_list = barwon, datasource = 'A', var_list = c('100', '140'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOnly derived\n\nbsod &lt;- get_ts_traces(portal = 'vic', site_list = barwon, datasource = 'A', var_list = '140', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nSome more variables, derived and not\n\nbsdv &lt;- get_ts_traces(portal = 'vic', site_list = barwon, datasource = 'A', var_list = c('100', '140', '210', '450'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nAnd multi-sites too- does it correctly collapse the vector?\n\nbsdvs &lt;- get_ts_traces(portal = 'vic', site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nAnd finally everything including rain at golf. Careful though- does mean make sense for that? Probably better as a sum? Tried that and threw an error but told me the options:\nMean/Max/Min/Start/End/First/Last/Tot/MaxMin/Point/Cum\nDefinitely need two calls if need two different values at least for now- total temp is nonsense.\n\nbsdvs &lt;- get_ts_traces(portal = 'vic', site_list = allsites, \n                       datasource = 'A', \n                       var_list = c('10', '100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nand return a list\n\nbsdvsl &lt;- get_ts_traces(portal = 'vic', site_list = allsites, \n                       datasource = 'A', \n                       var_list = c('10', '100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'sxvlist')",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html#a-variable-and-time-aware-version",
    "href": "hydrogauge/hydrogauge_testing.html#a-variable-and-time-aware-version",
    "title": "Testing hydrogauge API",
    "section": "A variable and time-aware version",
    "text": "A variable and time-aware version\n\npossibles &lt;- get_variable_list(portal = 'vic', site_list = allsites, datasource = 'A') %&gt;% \n  dplyr::select(site, datasource, variable, period_start, period_end)\n\n\nposs140 &lt;- possibles[possibles$variable == '100.00', ] \n\nposs141 &lt;- poss140\nposs140$variable &lt;- '140.00'\nposs141$variable &lt;- '141.00'\n\npossibles &lt;- bind_rows(possibles, poss140, poss141)\n\nall the tests above should run\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/code/hydrogauge')\n\nℹ Loading hydrogauge\n\n\nWe no longer support a working version of get_ts_traces2; a general version is in development.\n\nbs &lt;- get_ts_traces2(portal = 'vic', site_list = barwon, datasource = 'A', var_list = '100', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nCan I pass decimals? it’s how they come out of get_variable_list\n\nbsdec &lt;- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '100.00', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOne site, derived variables\n\nbsd &lt;- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = c('100', '140', '141'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOnly derived\n\nbsod &lt;- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '140', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nSome more variables, derived and not\n\nbsdv &lt;- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = c('100', '140', '210', '450'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nAnd multi-sites too- does it correctly collapse the vector?\n\nbsdvs &lt;- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nDo the ‘all’ settings work? Let’s bump to year so I don’t have so much data\n\nbsdvs &lt;- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = \"all\", \n                       start_time = \"all\", \n                       end_time = \"all\", \n                       interval = 'year', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nCan I throw something wrong to interval to see if it tells me what it can do? Kisters says\nyear, month, day, hour, minute, second,\nperiod,\ndefault\n\nbiw &lt;- get_ts_traces2(site_list = barwon, \n                       datasource = 'A', \n                       var_list = \"100\", \n                       start_time = \"20200101\", \n                       end_time = \"20211231\", \n                       interval = 'eon', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nerror is Invalid interval, must be YEAR, MONTH, DAY, HOUR, MINUTE or SECOND",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html#benchmark",
    "href": "hydrogauge/hydrogauge_testing.html#benchmark",
    "title": "Testing hydrogauge API",
    "section": "Benchmark",
    "text": "Benchmark\nThis likely varies a lot depending on what I’m asking for. Should be done more systematically, and use microbenchmark.\nThey should be roughly the same for a single?\n\nsystem.time(b1 &lt;- get_ts_traces(portal = 'vic', site_list = barwon, \n                       datasource = 'A', \n                       var_list = '100', \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\nsystem.time(b2 &lt;- get_ts_traces2(site_list = barwon, \n                       datasource = 'A', \n                       var_list = '100', \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\ninteresting. so the second is faster locally, but higher network, I think.\n\nsystem.time(bsdvs1 &lt;- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\nsystem.time(bsdvs2 &lt;- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\nOof. That’s pretty bad. Can I speed it up? probably.\nHow about parallel?\n\nlibrary(doFuture)\nregisterDoFuture()\nplan(multisession)\n\nsystem.time(bsdvs1p &lt;- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\nsystem.time(bsdvs2p &lt;- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_testing.html#get-db-info",
    "href": "hydrogauge/hydrogauge_testing.html#get-db-info",
    "title": "Testing hydrogauge API",
    "section": "Get db info",
    "text": "Get db info\n\nGeofiltering\nThe get_db_info API function can do a LOT. I’m primarily looking for using it for geofiltering, but it may turn out that that’s better done in other ways anyway. The filter_values sitelist_filter, etc are likely also useful, and will be worth adding as we go.\nThe area from Teesdale (-38, 144 at top left to Leopold (-38.2, 144.5) should contain 5 sites.\nOh no it doesn’t. It actually contains a bazillion, because there’s a bunch of stntype: \"GW\". Turning eval: false to we don’t do that again.\n\n# need a matrix to get the double brackets.\ntopleft &lt;- c('-38', '144')\nbottomright &lt;- c('-38.2', '144.5')\n\nrectbox &lt;- rbind(topleft, bottomright)\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nSo, that is WAAY too much, so we need to filter by surface water station types. Assuming the structure is the same, let’s figure out how to parse that into a table, and then get the stntype that matches surface and develop that capacity. And document how to choose which.\nAnd use the field_list to not return all the ‘category’ nonsense.\n\ndb &lt;- as_tibble(rbody_geo[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% \n  select(station, stntype, stname, everything(), -starts_with('category'))\n\nSo, what are the possible stntypes?\n\ntable(db$stntype)\n\nWhat are those? GW has got to be groundwater. HYD is Hydrology, presumably??? SOB I think is the State Observation Bore Network. VIR???.\nLet’s knock off the GW and see if we can figure the others out.\n\n##| rows.print: 30\ndbNoGW &lt;- db %&gt;% \n  filter(stntype != 'GW') %&gt;% \n  select(station, stntype, stname, shortname, parent, commence, cease, everything())\n\ndbNoGW\n\nSo, the SOB don’t have any info, the HYD seem to be A and B versions of the VIR. Do they have different data??? If we get traces for 233217A and B, do they differ from 233217?\nLet’s clean that up a bit first before figuring that out.\n\n##| rows.print = 30\ndbNoGW &lt;- dbNoGW %&gt;% \n  filter(stntype != 'SOB') %&gt;% \n  arrange(station)\ndbNoGW\n\nInteresting.\nSo, some things to do\nTODO-\n\nsort out the filters (filter_values I think?) to only get HYD and VIR (or whatever the user wants)\n\nand tell the user what the options are- apparently, ‘GW’, ‘SOB’, ‘HYD’, and ‘VIR’\n\nsort out field_list to only get a subset of useful fields\n\nAnd alert the user about what fields are being dropped\n\nfigure out whether the data differs between HYD and VIR (should be able to just pass the A,B, numbers to the get_ts_traces)?\nOther geo filters\n\n\n\nOther filtering\nTry filter_values and field_list. \"filter_values\" = list(\"stntype\" = 'HYD, VIR') does not work- figure out why.\n\ntopleft &lt;- c('-38', '144')\nbottomright &lt;- c('-38.2', '144.5')\n\nrectbox &lt;- rbind(topleft, bottomright)\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"stntype\" = 'VIR'),\n                               \"field_list\" = \"station, stntype, stname, shortname, commence, cease, active, northing, easting, longitude, latitude, lldatum\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 313\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"stntype\":\"VIR\"},\"field_list\":\"station, stntype, stname, shortname, commence, cease, active, northing, easting, longitude, latitude, lldatum\",\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 6\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5780764.372\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"MOORABOOL @ BATESFOR\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"MOORABOOL RIVER @ BATESFORD\"\n  .. .. ..$ category9 : chr \"YES\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"0\"\n  .. .. ..$ category4 : chr \"50\"\n  .. .. ..$ category3 : chr \"S/U\"\n  .. .. ..$ category2 : chr \"V_93F3\"\n  .. .. ..$ category1 : chr \"0\"\n  .. .. ..$ elevacc   : chr \"DEM10\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19080101\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"03 5276 1201\"\n  .. .. ..$ spare1    : chr \"Derwent Hotel\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1442\n  .. .. ..$ region    : chr \"232\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.278650000\"\n  .. .. ..$ comment   : chr \"\\r\\n\\r\\n\\r\\nDerwent Hotel BatesfordFrom Geelong West, head west along Ballarat Highway to Batesford. Just befor\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"232202\"\n  .. .. ..$ datemod   : int 20240131\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Batesford\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"23.325\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"232202A\"\n  .. .. ..$ latitude  : chr \"-38.089300000\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"261340.248\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"\"\n  .. .. ..$ active    : logi FALSE\n  .. .. ..$ northing  : chr \"5779117.670\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"WASTEWATER DR @ FORD\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ category9 : chr \"\"\n  .. .. ..$ category8 : chr \"\"\n  .. .. ..$ category7 : chr \"\"\n  .. .. ..$ category6 : chr \"\"\n  .. .. ..$ category5 : chr \"\"\n  .. .. ..$ category4 : chr \"\"\n  .. .. ..$ category3 : chr \"\"\n  .. .. ..$ category2 : chr \"\"\n  .. .. ..$ category1 : chr \"\"\n  .. .. ..$ elevacc   : chr \"DEM10\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"N\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 20060630\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEELONG\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"\"\n  .. .. ..$ posacc    : chr \"1\"\n  .. .. ..$ timemod   : int 1442\n  .. .. ..$ region    : chr \"232\"\n  .. .. ..$ grdatum   : chr \"ANG\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.355800000\"\n  .. .. ..$ comment   : chr \"\\r\\nStation operated for Ford Aus. Project No 1563532011/07/17 Virtual site entry generated by DSEVIRTUALSITE.H\"| __truncated__\n  .. .. ..$ lldatum   : chr \"AGD66\"\n  .. .. ..$ station   : chr \"232711\"\n  .. .. ..$ datemod   : int 20240131\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"VRW\"\n  .. .. ..$ barcode   : chr \"\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"4.455\"\n  .. .. ..$ cease     : int 20071029\n  .. .. ..$ local_map : chr \"232711A\"\n  .. .. ..$ latitude  : chr \"-38.105900000\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"N\"\n  .. .. ..$ easting   : chr \"268159.899\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5774520.999\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON POLLOCKSFORD\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"B\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"4WD\"\n  .. .. ..$ category5 : chr \"50\"\n  .. .. ..$ category4 : chr \"150\"\n  .. .. ..$ category3 : chr \"GRAVEL\"\n  .. .. ..$ category2 : chr \"V_93D4\"\n  .. .. ..$ category1 : chr \"18\"\n  .. .. ..$ elevacc   : chr \"DEM10\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19060701\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"N/A\"\n  .. .. ..$ spare1    : chr \"N/A\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1442\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.187100400\"\n  .. .. ..$ comment   : chr \"\\r\\nFrom Inverliegh, travel east along Hamilton Highway. Turn south down Pollocksford Road and travel 3.1 km to\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233200\"\n  .. .. ..$ datemod   : int 20240131\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Moriac\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"28.681\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"233200B\"\n  .. .. ..$ latitude  : chr \"-38.143395970\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"253491.999\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5779017.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"LEIGH @ INVERLEIGH\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"20\"\n  .. .. ..$ category4 : chr \"100\"\n  .. .. ..$ category3 : chr \"SEALED\"\n  .. .. ..$ category2 : chr \"V_93B3\"\n  .. .. ..$ category1 : chr \"1\"\n  .. .. ..$ elevacc   : chr \"DEM10\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"N\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19460315\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"N/A\"\n  .. .. ..$ spare1    : chr \"N/A\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1442\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.062212200\"\n  .. .. ..$ comment   : chr \"\\r\\nIt is underneath the Hamilton Highway Bridge that passes over the Leigh River.2011/07/17 Virtual site entry\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233209\"\n  .. .. ..$ datemod   : int 20240131\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Inverleigh\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"55.983\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"233209A\"\n  .. .. ..$ latitude  : chr \"-38.099828090\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"n\"\n  .. .. ..$ easting   : chr \"242392.001\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5772691.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON @ GEELONG\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"0\"\n  .. .. ..$ category4 : chr \"150\"\n  .. .. ..$ category3 : chr \"SEALED\"\n  .. .. ..$ category2 : chr \"V_93G4\"\n  .. .. ..$ category1 : chr \"0\"\n  .. .. ..$ elevacc   : chr \"DEM10\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19601118\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"BW\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1442\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.346892200\"\n  .. .. ..$ comment   : chr \"\\r\\n\\r\\n\\r\\nBarwon Water flood monitoring stationFrom the intersection of the Fyans St and La Trobe Terrace, he\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233217\"\n  .. .. ..$ datemod   : int 20240131\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Geelong\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"1.992\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"233217D\"\n  .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"267562.001\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"\"\n  .. .. ..$ active    : logi FALSE\n  .. .. ..$ northing  : chr \"5777388.289\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON RIVER\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ category9 : chr \"\"\n  .. .. ..$ category8 : chr \"\"\n  .. .. ..$ category7 : chr \"\"\n  .. .. ..$ category6 : chr \"\"\n  .. .. ..$ category5 : chr \"\"\n  .. .. ..$ category4 : chr \"\"\n  .. .. ..$ category3 : chr \"\"\n  .. .. ..$ category2 : chr \"\"\n  .. .. ..$ category1 : chr \"\"\n  .. .. ..$ elevacc   : chr \"DEM10\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"N\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19680208\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEELONG\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"\"\n  .. .. ..$ posacc    : chr \"1\"\n  .. .. ..$ timemod   : int 1442\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.150000000\"\n  .. .. ..$ comment   : chr \"2011/07/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 233219A\\r\\n\"\n  .. .. ..$ lldatum   : chr \"\"\n  .. .. ..$ station   : chr \"233219\"\n  .. .. ..$ datemod   : int 20240131\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"VRW\"\n  .. .. ..$ barcode   : chr \"\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"36.129\"\n  .. .. ..$ cease     : int 19690911\n  .. .. ..$ local_map : chr \"233219A\"\n  .. .. ..$ latitude  : chr \"-38.116666670\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"N\"\n  .. .. ..$ easting   : chr \"250148.885\"\n  .. .. ..$ stntype   : chr \"VIR\"\n\n\ncleanup\n\ndb &lt;- as_tibble(rbody_geo[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% \n  select(station, stntype, stname, everything(), -starts_with('category'))\ndb\n\n# A tibble: 6 × 42\n  station stntype stname active northing timezone shortname datecreate elevdatum\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;lgl&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;    \n1 232202  VIR     MOORA… TRUE   5780764… 10.0     MOORABOO…   18991230 \"\"       \n2 232711  VIR     WASTE… FALSE  5779117… 10.0     WASTEWAT…   18991230 \"\"       \n3 233200  VIR     BARWO… TRUE   5774520… 10.0     BARWON P…   18991230 \"\"       \n4 233209  VIR     LEIGH… TRUE   5779017… 10.0     LEIGH @ …   18991230 \"\"       \n5 233217  VIR     BARWO… TRUE   5772691… 10.0     BARWON @…   18991230 \"\"       \n6 233219  VIR     BARWO… FALSE  5777388… 10.0     BARWON R…   18991230 \"\"       \n# ℹ 33 more variables: elevacc &lt;chr&gt;, dbver47 &lt;lgl&gt;, quarter &lt;chr&gt;,\n#   section &lt;int&gt;, commence &lt;int&gt;, parent &lt;chr&gt;, mapname &lt;chr&gt;, meridian &lt;chr&gt;,\n#   spare5 &lt;chr&gt;, spare4 &lt;chr&gt;, spare3 &lt;chr&gt;, spare2 &lt;chr&gt;, spare1 &lt;chr&gt;,\n#   posacc &lt;chr&gt;, timemod &lt;int&gt;, region &lt;chr&gt;, grdatum &lt;chr&gt;, township &lt;chr&gt;,\n#   longitude &lt;chr&gt;, comment &lt;chr&gt;, lldatum &lt;chr&gt;, datemod &lt;int&gt;,\n#   timecreate &lt;int&gt;, orgcode &lt;chr&gt;, barcode &lt;chr&gt;, zone &lt;int&gt;, elev &lt;chr&gt;,\n#   cease &lt;int&gt;, local_map &lt;chr&gt;, latitude &lt;chr&gt;, range &lt;chr&gt;, …\n\n\n\nnames(db)\n\n [1] \"station\"    \"stntype\"    \"stname\"     \"active\"     \"northing\"  \n [6] \"timezone\"   \"shortname\"  \"datecreate\" \"elevdatum\"  \"elevacc\"   \n[11] \"dbver47\"    \"quarter\"    \"section\"    \"commence\"   \"parent\"    \n[16] \"mapname\"    \"meridian\"   \"spare5\"     \"spare4\"     \"spare3\"    \n[21] \"spare2\"     \"spare1\"     \"posacc\"     \"timemod\"    \"region\"    \n[26] \"grdatum\"    \"township\"   \"longitude\"  \"comment\"    \"lldatum\"   \n[31] \"datemod\"    \"timecreate\" \"orgcode\"    \"barcode\"    \"zone\"      \n[36] \"elev\"       \"cease\"      \"local_map\"  \"latitude\"   \"range\"     \n[41] \"qquarter\"   \"easting\"   \n\n\nThat seems to have worked for filter vaues but not field list. and it doesn’t work for filtering to multiple values. Figure both those things out.\nHow about c() on both. Shorten the field list for the moment. Runs but no data.\nc() on the field list works for a single stntype. Now to figure out the stntypes\n\ntopleft &lt;- c('-38', '144')\nbottomright &lt;- c('-38.2', '144.5')\n\nrectbox &lt;- rbind(topleft, bottomright)\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"stntype\" = 'HYD'),\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"stntype\":\"HYD\"},\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 12\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"MOORABOOL RIVER AT BATESFORD\"\n  .. .. ..$ station: chr \"232202A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ station: chr \"232711A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ station: chr \"233209A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217C\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217D\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ station: chr \"233219A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Waurn Ponds Creek @ Brearly Reserve\"\n  .. .. ..$ station: chr \"233221A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"NATIVE HUT CREEK @ TURTLE BEND PARK TEESDALE\"\n  .. .. ..$ station: chr \"233242A\"\n  .. .. ..$ stntype: chr \"HYD\"\n\n\n\ndb &lt;- as_tibble(rbody_geo[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% \n  select(station, stntype, stname, everything(), -starts_with('category'))\ndb\n\n# A tibble: 12 × 3\n   station stntype stname                                      \n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                                       \n 1 232202A HYD     MOORABOOL RIVER AT BATESFORD                \n 2 232711A HYD     WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG \n 3 233200A HYD     BARWON RIVER @ POLLOCKSFORD                 \n 4 233200B HYD     BARWON RIVER @ POLLOCKSFORD                 \n 5 233209A HYD     LEIGH RIVER @ INVERLEIGH                    \n 6 233217A HYD     BARWON RIVER @ GEELONG                      \n 7 233217B HYD     BARWON RIVER @ GEELONG                      \n 8 233217C HYD     BARWON RIVER @ GEELONG                      \n 9 233217D HYD     BARWON RIVER @ GEELONG                      \n10 233219A HYD     BARWON RIVER @ MURGHEBOLUC                  \n11 233221A HYD     Waurn Ponds Creek @ Brearly Reserve         \n12 233242A HYD     NATIVE HUT CREEK @ TURTLE BEND PARK TEESDALE\n\n\n\n\nTrying for the stntype\nTried to use sitelist_filter = \"GROUP(PLUVIO_STATIONS), which qld made look like would work, but nothing. maybe get_groups is a way to figure out what the groups are?\n\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"sitelist_filter\" = \"GROUP(PLUVIO_STATIONS)\",\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 241\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"sitelist_filter\":\"GROUP(PLUVIO_STATIONS)\",\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows: list()\n\n\n\n\nAside- get_groups\nCan we use hydstra groupings, as defined in the db? What are those groupings?\n\ngrp_params &lt;- list(\"function\" = 'get_groups',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites))\n\nreqvic %&gt;% \n  req_body_json(grp_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 95\n\n{\"function\":\"get_groups\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\"}}\n\nresp_grp &lt;- reqvic %&gt;% \n  req_body_json(grp_params) %&gt;% \n  req_perform()\n\nrbody_grp &lt;- resp_grp %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_grp)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 34\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"to be at the top of the list\"\n  .. ..$ group       : chr \"AB\"\n  .. ..$ value       : chr \"CTS\"\n  .. ..$ value_decode: chr \"list of site A files that have changed in the last 24hrs\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"to be at the top of the list\"\n  .. ..$ group       : chr \"AB\"\n  .. ..$ value       : chr \"FLOOD\"\n  .. ..$ value_decode: chr \"All VIR Sites for Flood Zoom\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"10PLUS\"\n  .. ..$ value_decode: chr \"sites with records longer than 10 years\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"30PLUS\"\n  .. ..$ value_decode: chr \"sites with records longer than 30 years\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"ARCHIVE\"\n  .. ..$ value_decode: chr \"Archive files\"\n  .. ..$ stations    :List of 4\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"FLOWA\"\n  .. ..$ value_decode: chr \"All sites with flow (Vir and Hyd)\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"LEVEL\"\n  .. ..$ value_decode: chr \"All sites with level (100.00 and 130)\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"VFLOW\"\n  .. ..$ value_decode: chr \"Virtual sites with flow\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Goups for the CMA areas\"\n  .. ..$ group       : chr \"CMA\"\n  .. ..$ value       : chr \"CCMA\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"daily on change generated site lists\"\n  .. ..$ group       : chr \"HYVIRTUAL\"\n  .. ..$ value       : chr \"TS\"\n  .. ..$ value_decode: chr \"list of site A files that have changed in the last 24hrs\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Proposed WMIS 2 Groups by me\"\n  .. ..$ group       : chr \"PROP\"\n  .. ..$ value       : chr \"RF\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Proposed WMIS 2 Groups by me\"\n  .. ..$ group       : chr \"PROP\"\n  .. ..$ value       : chr \"SW\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Proposed WMIS 2 Groups by me\"\n  .. ..$ group       : chr \"PROP\"\n  .. ..$ value       : chr \"WQ\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Telemetry Sites\"\n  .. ..$ group       : chr \"TELEMETRY\"\n  .. ..$ value       : chr \"RAWDATA\"\n  .. ..$ stations    :List of 4\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Yukkie\"\n  .. ..$ group       : chr \"WDTF\"\n  .. ..$ value       : chr \"LEVEL\"\n  .. ..$ value_decode: chr \"All sites with level (100.00 and 130)\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Yukkie\"\n  .. ..$ group       : chr \"WDTF\"\n  .. ..$ value       : chr \"RAIN\"\n  .. ..$ value_decode: chr \"All rain monitoring sites\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Yukkie\"\n  .. ..$ group       : chr \"WDTF\"\n  .. ..$ value       : chr \"WQSITES\"\n  .. ..$ value_decode: chr \"all sites that have WQ in sample table\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"CMA\"\n  .. ..$ group       : chr \"WEB_GW_CMA\"\n  .. ..$ value       : chr \"CMA01\"\n  .. ..$ value_decode: chr \"Corangamite CMA\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"CMA\"\n  .. ..$ group       : chr \"WEB_GW_CMA\"\n  .. ..$ value       : chr \"CMA04\"\n  .. ..$ value_decode: chr \"Goulburn Broken CMA\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Registered Aboriginal Party site lists\"\n  .. ..$ group       : chr \"WEB_RAP\"\n  .. ..$ value       : chr \"TLWCAC\"\n  .. ..$ value_decode: chr \"Taungurung Land and Waters Council Aboriginal Corporation\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Registered Aboriginal Party site lists\"\n  .. ..$ group       : chr \"WEB_RAP\"\n  .. ..$ value       : chr \"WTOAC\"\n  .. ..$ value_decode: chr \"Wadawurrung Traditional Owners Aboriginal Corporation\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites\"\n  .. ..$ group       : chr \"WEB_SW\"\n  .. ..$ value       : chr \"B_233_BARWON\"\n  .. ..$ value_decode: chr \"233-Barwon Basin\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites\"\n  .. ..$ group       : chr \"WEB_SW\"\n  .. ..$ value       : chr \"B_405_GOULBURN\"\n  .. ..$ value_decode: chr \"405-Goulburn Basin\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites - active telem only\"\n  .. ..$ group       : chr \"WEB_SW_TELEM\"\n  .. ..$ value       : chr \"B_233_BARWON\"\n  .. ..$ value_decode: chr \"233-Barwon Basin\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites - active telem only\"\n  .. ..$ group       : chr \"WEB_SW_TELEM\"\n  .. ..$ value       : chr \"B_405_GOULBURN\"\n  .. ..$ value_decode: chr \"405-Goulburn Basin\"\n  .. ..$ stations    :List of 2\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Test Sites\"\n  .. ..$ group       : chr \"WEB_TEST\"\n  .. ..$ value       : chr \"WEB_TEST\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"WMIS2 Sites\"\n  .. ..$ group       : chr \"WMIS2\"\n  .. ..$ value       : chr \"RF\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"WMIS2 Sites\"\n  .. ..$ group       : chr \"WMIS2\"\n  .. ..$ value       : chr \"SW\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"WMIS2 Sites\"\n  .. ..$ group       : chr \"WMIS2\"\n  .. ..$ value       : chr \"WQ\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"WMIS2 Topics Sites\"\n  .. ..$ group       : chr \"WMIS2TOPICS\"\n  .. ..$ value       : chr \"RF\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"WMIS2 Topics Sites\"\n  .. ..$ group       : chr \"WMIS2TOPICS\"\n  .. ..$ value       : chr \"SW\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"WMIS2 Topics Sites\"\n  .. ..$ group       : chr \"WMIS2TOPICS\"\n  .. ..$ value       : chr \"WQ\"\n  .. ..$ stations    :List of 2\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 CMAs\"\n  .. ..$ group       : chr \"WMIS2_CMA\"\n  .. ..$ value       : chr \"CMA01\"\n  .. ..$ value_decode: chr \"Corangamite CMA\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 CMAs\"\n  .. ..$ group       : chr \"WMIS2_CMA\"\n  .. ..$ value       : chr \"CMA04\"\n  .. ..$ value_decode: chr \"Goulburn Broken CMA\"\n  .. ..$ stations    :List of 2\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n\n\nUnpack\n\ndb &lt;- as_tibble(rbody_grp[2]) %&gt;% # the [2] drops the error column\n  unnest_wider(col = where(is.list)) %&gt;% # a `return` list\n  unnest_longer(col = where(is.list))\n\nDrop the stations- what are the groups?\n\ndb %&gt;% select(-stations) %&gt;% distinct()\n\n# A tibble: 34 × 4\n   group_decode                         group     value   value_decode          \n   &lt;chr&gt;                                &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                 \n 1 to be at the top of the list         AB        CTS     list of site A files …\n 2 to be at the top of the list         AB        FLOOD   All VIR Sites for Flo…\n 3 All sites in archive                 ALLA      10PLUS  sites with records lo…\n 4 All sites in archive                 ALLA      30PLUS  sites with records lo…\n 5 All sites in archive                 ALLA      ARCHIVE Archive files         \n 6 All sites in archive                 ALLA      FLOWA   All sites with flow (…\n 7 All sites in archive                 ALLA      LEVEL   All sites with level …\n 8 All sites in archive                 ALLA      VFLOW   Virtual sites with fl…\n 9 Goups for the CMA areas              CMA       CCMA    &lt;NA&gt;                  \n10 daily on change generated site lists HYVIRTUAL TS      list of site A files …\n# ℹ 24 more rows\n\n\nNo obvious groupings there to select on except maybe surface water sites, but that drops rainfall.\n\n\nHow about sitelist_filter?\nwith one site\n\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"sitelist_filter\" = \"233217\",\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 225\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"sitelist_filter\":\"233217\",\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 1\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217\"\n  .. .. ..$ stntype: chr \"VIR\"\n\n\nwith multiple sites, it just returns the first one. And using c(\"site1\", \"site2\") errors out.\n\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"sitelist_filter\" = \"233217, 405331\",\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 233\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"sitelist_filter\":\"233217, 405331\",\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 1\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217\"\n  .. .. ..$ stntype: chr \"VIR\"\n\n\n\n\nusing complex_filter\nis this easier or harder than just running it twice (or however many times)\nIt’s more flexible, but pretty terrible to sort out. And absurdly slow.\n\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"complex_filter\" = list(list('fieldname' = 'stntype',\n                                                         'value' = \"HYD\",\n                                                         'operator' = 'EQ'),\n                                                    list('combine' = 'OR',\n                                                         'fieldname' = 'stntype',\n                                                         'value' = \"VIR\",\n                                                         'operator' = 'EQ')),\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /WMIS/cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 340\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"complex_filter\":[{\"fieldname\":\"stntype\",\"value\":\"HYD\",\"operator\":\"EQ\"},{\"combine\":\"OR\",\"fieldname\":\"stntype\",\"value\":\"VIR\",\"operator\":\"EQ\"}],\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 18\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"MOORABOOL RIVER @ BATESFORD\"\n  .. .. ..$ station: chr \"232202\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"MOORABOOL RIVER AT BATESFORD\"\n  .. .. ..$ station: chr \"232202A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ station: chr \"232711\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ station: chr \"232711A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ station: chr \"233209\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ station: chr \"233209A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217C\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217D\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ station: chr \"233219\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ station: chr \"233219A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Waurn Ponds Creek @ Brearly Reserve\"\n  .. .. ..$ station: chr \"233221A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"NATIVE HUT CREEK @ TURTLE BEND PARK TEESDALE\"\n  .. .. ..$ station: chr \"233242A\"\n  .. .. ..$ stntype: chr \"HYD\"\n\n\nthat seems to have worked. Unpack- remember we’ve used a field_list here so it’s simpler and we don’t need to have the select we had above. In the package we should probably allow doing that both ways.\n\ndb &lt;- as_tibble(rbody_geo[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list))\n\nTODO clean that up into a function and include. And test where we know there’s rain gauges- am I losing those with the stntypes I’ve gone with?\nallow field_list\nand geo_filter- all three kinds\nhow to do stntype- could have one option that allows actually directly passing lists to complex filter (or filter_values).\nOR (and?) could have an option that builds that list for the user somehow. would need to limit it to a few common things like stntype. *could I do this by saying if filter_values gets a c() of things for any of its keys, build an OR complex_filter. ie filter_values can only have one stntype. So if it gets c(stntype1, stntype2), just short-circuit and use complex instead. Would need to be careful if user passes more than one filtering thing in. In that case, just make them build the complex filter themselves. maybe provide a test_complex_filter function that just returns req_dry_run?",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing hydrogauge API"
    ]
  },
  {
    "objectID": "drones/overlaps_reactive.html",
    "href": "drones/overlaps_reactive.html",
    "title": "Drone flight calculations",
    "section": "",
    "text": "This is an interactive page based on more detailed exploration of drone overlaps.\nThis gives us the opportunity to plug in flight parameters and back-calculate others. For example, give us the size of the photos on the ground given height, the needed drone flight distance for a desired overlap at a given height, or the speed we need to fly at a given height to yield the right overlap given the photo interval.\nOverlap is fixed at 80% for the moment.\n\n\n\n\n\nWarning: Using `by = character()` to perform a cross join was deprecated in dplyr 1.1.0.\nℹ Please use `cross_join()` instead.\n\n\n\n\n\nimport { aq, op } from '@uwdata/arquero'\n\ndroned_aq = aq.from(transpose(droned))\ndronev_aq = aq.from(transpose(dronev))\ndronei_aq = aq.from(transpose(dronei))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof dronetype = Inputs.checkbox(\n  [\"Mini 2\", \"Phantom 4 Pro V2\"],\n  {value: [\"Mini 2\"],\n    label: \"Drone\"\n  }\n)\n\nviewof aspect = Inputs.checkbox(\n  [\"3:2\", \"4:3\", \"16:9\"],\n  {value: [\"4:3\"],\n    label: \"Aspect Ratio\"\n  }\n)\n\nviewof overlap_p = Inputs.range(\n  [0.7, 0.95],\n  {value: 0.8, step: 0.05, label: \"Overlap:\"}\n)\n\nviewof height = Inputs.range(\n  [1, 50],\n  {value: 5, step: 0.1, label: \"Altitude (m):\"}\n)\n\nviewof velocity = Inputs.range(\n  [0.1, 5],\n  {value: 1, step: 0.1, label: \"Velocity (m/s):\"}\n)\n\nviewof interval = Inputs.range(\n  [0.1, 5],\n  {value: 2, step: 0.1, label: \"Photo interval (s):\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the selected values\nGround distances (photo footprints) and needed photo distances to achieve overlap are\n\ndist_h\n  .select('altitude_m', 'overlap', 'direction', 'photo_distance_m', 'ground_distance_m')\n  .view()\n\n\n\n\n\n\nFor the selected interval and height, the needed velocity is\n\nvel_h\n  .select('altitude_m', 'overlap', 'intervals', 'velocity_ms')\n  .view()\n\n\n\n\n\n\nFor the selected velocity, the needed photo interval is\n\ni_h\n  .select('altitude_m', 'overlap', 'velocity_ms', 'photo_interval')\n  .view()\n\n\n\n\n\n\n\n\n\n\n\ndistfilter = droned_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect\n})\n  .filter((d, $) =&gt; op.includes(d.drone, $.dr))\n  .filter((d, $) =&gt; op.equal(d.overlap, $.ov))\n  .filter((d, $) =&gt; op.includes(d.aspect, $.asp))\n\ndist_h = distfilter\n  .params({\n  h: height\n})\n  .filter((d, $) =&gt; op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvfilter = dronev_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect,\n  inter: interval\n})\n  .filter((d, $) =&gt; op.includes(d.drone, $.dr))\n  .filter((d, $) =&gt; op.equal(d.overlap, $.ov))\n  .filter((d, $) =&gt; op.includes(d.aspect, $.asp))\n  .filter((d, $) =&gt; op.equal(d.intervals, $.inter))\n\nvel_h = vfilter\n  .params({\n  h: height\n})\n  .filter((d, $) =&gt; op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nifilter = dronei_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect,\n  v: velocity\n})\n  .filter((d, $) =&gt; op.includes(d.drone, $.dr))\n  .filter((d, $) =&gt; op.equal(d.overlap, $.ov))\n  .filter((d, $) =&gt; op.includes(d.aspect, $.asp))\n  .filter((d, $) =&gt; op.equal(d.velocity_ms, $.v))\n\ni_h = ifilter\n  .params({\n  h: height\n})\n  .filter((d, $) =&gt; op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGround distancePhoto distanceVelocityPhoto interval\n\n\n\nPlot.plot({\n  grid: false,\n  color: {\n    legend: true\n  },\n  marks: [\n    Plot.line(distfilter, {x: \"altitude_m\", y: \"ground_distance_m\", stroke: \"direction\"}),\n    Plot.dot(dist_h, {x: \"altitude_m\", y: \"ground_distance_m\", stroke: \"drone\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  color: {\n    legend: true\n  },\n  marks: [\n    Plot.line(distfilter, {x: \"altitude_m\", y: \"photo_distance_m\", stroke: \"direction\"}),\n    Plot.dot(dist_h, {x: \"altitude_m\", y: \"photo_distance_m\", stroke: \"drone\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(vfilter, {x: \"altitude_m\", y: \"velocity_ms\"}),\n    Plot.dot(vel_h, {x: \"altitude_m\", y: \"velocity_ms\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(ifilter, {x: \"altitude_m\", y: \"photo_interval\"}),\n    Plot.dot(i_h, {x: \"altitude_m\", y: \"photo_interval\"})\n  ]\n})",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Calculations for flight planning"
    ]
  },
  {
    "objectID": "data_acquisition/read_geofabric.html",
    "href": "data_acquisition/read_geofabric.html",
    "title": "Read australia geofabric",
    "section": "",
    "text": "library(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWe downloaded and saved the geofabric. Now we want to open and plot some of it.\nThe folder structure is too complex inside the zip, so have to extract. I’m most interested right now in the river lines.\n\ngdbpath &lt;- file.path('data', 'geofabric', 'SH_Network_GDB', 'SH_Network.gdb')\nif (!file.exists(gdbpath))\n  \nunzip(file.path('data', 'geofabric', 'SH_Network_GDB_V3_3.zip'),\n      exdir = file.path('data', 'geofabric'))\n\nThere are a lot of layers, so can’t just st_read\n\nst_layers(gdbpath)\n\nDriver: OpenFileGDB \nAvailable layers:\n                    layer_name     geometry_type features fields crs_name\n1    AHGFNetworkConnectivityUp                NA  3694332      3     &lt;NA&gt;\n2  AHGFNetworkConnectivityDown                NA  2642087      3     &lt;NA&gt;\n3         AHGFNetworkStream_FS                NA   120614      2     &lt;NA&gt;\n4           AHGFNetworkNodeLUT                NA     6370     14     &lt;NA&gt;\n5              AHGFNetworkNode             Point  2532980     29    GDA94\n6                AHGFCatchment     Multi Polygon  7047524     27    GDA94\n7            AHGFNetworkStream Multi Line String  2578471     31    GDA94\n8                AHGFWaterbody     Multi Polygon   159198     27    GDA94\n9     SH_Network_Net_Junctions             Point        0      1    GDA94\n10                    N_1_Desc                NA  5117450      5     &lt;NA&gt;\n11                   N_1_EDesc                NA     1893      2     &lt;NA&gt;\n12                 N_1_EStatus                NA       40      2     &lt;NA&gt;\n13                   N_1_ETopo                NA      631      2     &lt;NA&gt;\n14                  N_1_FloDir                NA       40      2     &lt;NA&gt;\n15                   N_1_JDesc                NA     1856      2     &lt;NA&gt;\n16                 N_1_JStatus                NA       39      2     &lt;NA&gt;\n17                   N_1_JTopo                NA     4948      2     &lt;NA&gt;\n18                  N_1_JTopo2                NA        2      2     &lt;NA&gt;\n19                   N_1_Props                NA        9      2     &lt;NA&gt;\n\n\nThe stream lines are big, but just reading doesn’t take too long.\n\nrivers &lt;- st_read(gdbpath, layer = 'AHGFNetworkStream')\n\nReading layer `AHGFNetworkStream' from data source \n  `C:\\Users\\galen\\Documents\\code\\web_testing\\galen_website\\data\\geofabric\\SH_Network_GDB\\SH_Network.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 2578471 features and 31 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 113.3753 ymin: -43.62889 xmax: 153.6317 ymax: -10.06028\nGeodetic CRS:  GDA94\n\n\nLet’s subset to the major rivers to not crash?\n\nmajor_rivers &lt;- rivers |&gt; \n  dplyr::filter(Hierarchy == 'Major')\n\nStill 182557, but we can try to plot it?\n\nplot(major_rivers[,'Name'])\n\n\n\n\n\n\n\n\n\nriverlen_plot &lt;- ggplot(major_rivers, aes(color = log(UpstrGeoLn))) +\n  geom_sf() +\n  theme_void()\n\n\nriverlen_plot + theme(legend.position = 'none')",
    "crumbs": [
      "Code Demos",
      "Data",
      "Using Australian rivers geofabric"
    ]
  },
  {
    "objectID": "data_acquisition/get_geofabric.html",
    "href": "data_acquisition/get_geofabric.html",
    "title": "Get australia geofabric",
    "section": "",
    "text": "To get the australia river geofabric gdb. The links are here, the mapservice links are all annoyingly interactive, though I imagine there’s a way to httr2 them.\n\nurl &lt;- \"ftp://ftp.bom.gov.au/anon/home/geofabric/\"\n\nSee what’s there\n\nfiles &lt;- RCurl::getURL(url) |&gt; \n  strsplit(\"\\r*\\n\") |&gt; \n  unlist()\n\n\nfilenames &lt;- stringr::str_extract(files, '\\\\s([^ ]+)$') |&gt; \n  stringr::str_remove_all(' ')\nfilenames\n\n [1] \"GW_Cartography_GDB_V3_3.zip\"               \n [2] \"Geofabric_Metadata_GDB_V3_3.zip\"           \n [3] \"Geofabric_National_V3_3_PRODUCT_README.txt\"\n [4] \"Geofabric_Sample_Toolset_v1_6_1.zip\"       \n [5] \"Geofabric_V3x_FTP_README.txt\"              \n [6] \"Geofabric_V3x_Feedback_Form.docx\"          \n [7] \"HR_Catchments_GDB_V3_3.zip\"                \n [8] \"HR_Regions_GDB_V3_3.zip\"                   \n [9] \"SH_Cartography_GDB_V3_3.zip\"               \n[10] \"SH_Catchments_GDB_V3_3.zip\"                \n[11] \"SH_Network_GDB_V3_3.zip\"                   \n[12] \"version2\"                                  \n\n\nwe don’t want ‘version2’, but otherwise, let’s download all of that.\nIt’s a lot of data, so it’ll take a while. Turn the eval off unless we actually want to run this.\n\nif (!dir.exists('data/geofabric')) {\n  dir.create('data/geofabric')\n}\n\nfilestodl &lt;- filenames[filenames != 'version2']\n\nsystem.time(\n  purrr::map(filestodl, \n             \\(x) curl::curl_download(paste0(url, x), \n                                      destfile = paste0('data/geofabric/', x)))\n)",
    "crumbs": [
      "Code Demos",
      "Data",
      "Downloading Australian rivers geofabric"
    ]
  },
  {
    "objectID": "data_acquisition/bom_water.html",
    "href": "data_acquisition/bom_water.html",
    "title": "Waterdata from BOM",
    "section": "",
    "text": "I wrote {hydrogauge} to get information about water gauges in Victoria, and then discovered it also works in NSW and Queensland. It does not seem to work in South Australia though.\nCan we just use bomWater? Or use it to figure out how to call BOM myself?\nFirst question is whether it works. I’ve heard rumors BOM has gotten harder to call, but mdba-gauge-getter still manages.\nrenv::install('buzacott/bomWater')\nlibrary(bomWater)\nIt doesn’t obviously have a lot of the query tools from hydrogauge, but let’s see if it works with the example\ncotter_river &lt;- get_daily(parameter_type = 'Water Course Discharge',\n                          station_number = '410730',\n                          start_date     = '2020-01-01',\n                          end_date       = '2020-01-31')\nSeems to. Let’s try a couple I know I need\nmr97 &lt;- get_daily(parameter_type = 'Water Course Discharge',\n                  station_number = 'A4260505',\n                  start_date = '2000-01-01',\n                  end_date = '2000-05-30')\nThat basically looks like it works. It’s missing some functionality I want, but much better than nothing.\nget_station_list(station_number = 'A4260505')\n\n# A tibble: 1 × 5\n  station_name          station_no station_id station_latitude station_longitude\n  &lt;chr&gt;                 &lt;chr&gt;           &lt;int&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n1 River Murray at Lock… A4260505      1617110            -34.2              142.\nget_parameter_list(station_number = 'A4260505')\n\n# A tibble: 2 × 7\n  station_no station_id station_name         parametertype_id parametertype_name\n  &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;                           &lt;int&gt; &lt;chr&gt;             \n1 A4260505      1617110 River Murray at Loc…            11762 Water Course Disc…\n2 A4260505      1617110 River Murray at Loc…            11763 Water Course Level\n# ℹ 2 more variables: parametertype_unitname &lt;chr&gt;,\n#   parametertype_shortunitname &lt;chr&gt;\nIt looks like there is a getDataAvailabiltiy option in the API, but bomWater doesn’t query it. The requests in bomWater don’t obviously map to the docs, so this would take some tweaking.\nIn searching for how the ‘request’ in bomWater turns into the getSomethingSomething in the API (unsuccessfully), I found another package to try. It’s canadian, but seems to hit Kisters WISKI generally.\nlibrary('kiwisR')\nCheck it works.\nki_timeseries_list(hub = 'https://www.swmc.mnr.gov.on.ca/KiWIS/KiWIS?', station_id = '144659')\n\n# A tibble: 223 × 6\n   station_name station_id ts_id ts_name from                to                 \n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;             \n 1 Jackson Cre… 144659     9489… Precip… 2007-06-18 20:15:00 2024-09-10 22:15:00\n 2 Jackson Cre… 144659     1143… Precip… 2007-07-01 05:00:00 2020-09-01 05:00:00\n 3 Jackson Cre… 144659     1143… Precip… 2007-06-18 05:00:00 2024-09-11 05:00:00\n 4 Jackson Cre… 144659     9489… TAir.1… 2007-06-18 20:15:00 2024-09-10 22:15:00\n 5 Jackson Cre… 144659     9489… TAir.D… 2007-06-18 05:00:00 2024-09-09 05:00:00\n 6 Jackson Cre… 144659     9489… TAir.D… 2007-06-18 05:00:00 2024-09-09 05:00:00\n 7 Jackson Cre… 144659     1129… TAir.6… 2007-06-19 00:00:00 2024-09-10 18:00:00\n 8 Jackson Cre… 144659     1326… TAir.D… 2007-06-18 05:00:00 2024-09-11 05:00:00\n 9 Jackson Cre… 144659     1326… TAir.D… 2007-06-18 05:00:00 2024-09-11 05:00:00\n10 Jackson Cre… 144659     9490… TWater… 2007-06-18 05:00:00 2024-09-09 05:00:00\n# ℹ 213 more rows\nDoes it work for BOM? No, the url is almost certainly wrong. This expects a KiWIS API, which it looks like BoM doesnt use (at least just shoving Kiwis on the end doesn’t work.\nki_timeseries_list(hub = \"http://www.bom.gov.au/waterdata/services\", station_id = 'A4260505')\n\nError in if (nrow(json_content) == 2) {: argument is of length zero\n\nki_timeseries_list(hub = \"http://www.bom.gov.au/waterdata/services/KiWIS/KiWIS?\", station_id = 'A4260505')\n\nError: lexical error: invalid char in json text.\n                                       &lt;html&gt;&lt;head&gt;&lt;title&gt;Apache Tomca\n                     (right here) ------^\nInteresting. If I go to http://www.bom.gov.au/waterdata/services, I get the message “KISTERS KiWIS QueryServices - add parameter ‘request’ to execute a query.” So it is a KiWIS, but maybe doesn’t take request in the same way as kiwisR expects? bomWater does use request, so maybe this will help figure out how to specify new ones. Looking at code, kiwisR and bomWater look like they’re constructing the requests the same, so it’s a bit odd the kiwis doesn’t work with the bomWater url.\nAm I just calling something incorrectly? Can kiwisR hit that URL for other things? I can get a list of all stations. So that implies the URL does work. This is just very long, so I’m not rendering it.\nki_station_list(hub = \"http://www.bom.gov.au/waterdata/services\")\nIt doesn’t seem to work to search for stations by id though.\nki_station_list(hub = \"http://www.bom.gov.au/waterdata/services\", search_term = \"A4260505\")\n\n# A tibble: 0 × 5\n# ℹ 5 variables: station_name &lt;chr&gt;, station_no &lt;chr&gt;, station_id &lt;chr&gt;,\n#   station_latitude &lt;dbl&gt;, station_longitude &lt;dbl&gt;\nAh! It hits the station_name, not the gauge number in station_no\nki_station_list(hub = \"http://www.bom.gov.au/waterdata/services\", search_term = \"A*\")\n\n# A tibble: 1,951 × 5\n   station_name station_no station_id station_latitude station_longitude\n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;             &lt;dbl&gt;\n 1 A            61700620   400630                -30.3              115.\n 2 A            60210202   11517225              -35.1              118.\n 3 A            60210203   11517229              -35.1              118.\n 4 A            60210201   11517221              -35.1              118.\n 5 A            60210298   11520185              -35.1              118.\n 6 A            61311025   11457669              -32.7              116.\n 7 A            120310078  11287358              -25.5              129.\n 8 A            60110485   11465949              -33.7              121.\n 9 A            60110497   11465977              -33.7              121.\n10 A            60110584   11466057              -33.6              122.\n# ℹ 1,941 more rows\nSo, can I get it with\nki_station_list(hub = \"http://www.bom.gov.au/waterdata/services\",\n                search_term = \"River Murray at Lock 9 Downstream*\")\n\n# A tibble: 1 × 5\n  station_name          station_no station_id station_latitude station_longitude\n  &lt;chr&gt;                 &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;             &lt;dbl&gt;\n1 River Murray at Lock… A4260505   1617110               -34.2              142.\nThe ki_timeseries_list uses station_id. But I’ve been feeding it gauge numbers, which are station_no. It works with the ID.\ntl &lt;- ki_timeseries_list(hub = \"http://www.bom.gov.au/waterdata/services\", station_id = '1617110')\n\ntl\n\n# A tibble: 55 × 6\n   station_name station_id ts_id ts_name from                to                 \n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;             \n 1 River Murra… 1617110    2086… Receiv… NA                  NA                 \n 2 River Murra… 1617110    2086… Harmon… NA                  NA                 \n 3 River Murra… 1617110    2086… DMQaQc… 1949-07-01 23:29:59 2024-09-08 22:30:01\n 4 River Murra… 1617110    2086… DMQaQc… 2008-11-26 04:34:59 2024-09-09 22:15:01\n 5 River Murra… 1617110    2086… DMQaQc… 2008-11-26 03:30:00 2024-09-09 21:30:00\n 6 River Murra… 1617110    3293… Derive… NA                  NA                 \n 7 River Murra… 1617110    3293… PR01Ma… NA                  NA                 \n 8 River Murra… 1617110    3293… PR01Ma… NA                  NA                 \n 9 River Murra… 1617110    3293… PR01Qa… 1949-07-01 23:29:59 2024-09-08 22:30:01\n10 River Murra… 1617110    3293… PR01Qa… 1949-07-01 23:29:59 2024-09-08 22:30:01\n# ℹ 45 more rows\nThen, I should be able to use ki_timeseries_values if I know the ts_id I want.There’s lots of cryptic ts_name in there, but daily is “DMQaQc.Merged.DailyMean.24HR”. There are two versions here, for different date ranges.\ntl[tl$ts_name == \"DMQaQc.Merged.DailyMean.24HR\", ]\n\n# A tibble: 2 × 6\n  station_name  station_id ts_id ts_name from                to                 \n  &lt;chr&gt;         &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;             \n1 River Murray… 1617110    2086… DMQaQc… 2008-11-25 14:30:00 2024-09-09 14:30:00\n2 River Murray… 1617110    2086… DMQaQc… 1949-07-01 14:30:00 2024-09-08 14:30:00\nIs that why ki_timeseries_values doesn’t have a station argument? are the ts_ids unique across gauges? Look at two gauges. COtter river (from way above) is id 13360.\nNo idea why Cotter has so many ts_ids with identical ranges, but they are unique.\ntl2 &lt;- ki_timeseries_list(hub = \"http://www.bom.gov.au/waterdata/services\", \n                         station_id = c('1617110', '13360'))\ntl2 |&gt; \n  dplyr::filter(ts_name == \"DMQaQc.Merged.DailyMean.24HR\")\n\n# A tibble: 8 × 6\n  station_name  station_id ts_id ts_name from                to                 \n  &lt;chr&gt;         &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;             \n1 River Murray… 1617110    2086… DMQaQc… 2008-11-25 14:30:00 2024-09-09 14:30:00\n2 River Murray… 1617110    2086… DMQaQc… 1949-07-01 14:30:00 2024-09-08 14:30:00\n3 Cotter R. at… 13360      1573… DMQaQc… 1963-07-02 14:00:00 2024-09-08 14:00:00\n4 Cotter R. at… 13360      1598… DMQaQc… 1963-07-02 14:00:00 2024-09-09 14:00:00\n5 Cotter R. at… 13360      3801… DMQaQc… 2003-02-23 14:00:00 2024-09-09 14:00:00\n6 Cotter R. at… 13360      3801… DMQaQc… 2003-02-23 14:00:00 2024-09-09 14:00:00\n7 Cotter R. at… 13360      3801… DMQaQc… 2003-02-23 14:00:00 2024-09-08 14:00:00\n8 Cotter R. at… 13360      3801… DMQaQc… 1999-09-23 14:00:00 2024-09-09 14:00:00\nbomWater must be dealing with duplication somehow, because\nany(duplicated(cotter_river$Timestamp))\n\n[1] FALSE\nah. bomwater just uses ts_id[1]. That’s likely not the best move. What’s better? not sure. Would be good to assess them somehow. Could give options of ‘longest’, ‘all’, ‘first’ (with longest possibly still needing a ‘first’ or ‘all’ if there are multiple.)\nSo, all that boils down to that I should be able to choose one of those ts_ids and pull data. Choosing one from the Murray and one from Cotter\ntest_timeseries &lt;- ki_timeseries_values(hub = \"http://www.bom.gov.au/waterdata/services\",\n                                        ts_id = c(\"208669010\", \"380185010\"), \n                                        start_date = '2010-01-01', end_date = '2010-02-28')\nlibrary(ggplot2)\nggplot(test_timeseries, aes(x = Timestamp, y = Value, color = station_name)) +\n  geom_line()\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls from BOM"
    ]
  },
  {
    "objectID": "data_acquisition/bom_water.html#fitting-into-a-workflow",
    "href": "data_acquisition/bom_water.html#fitting-into-a-workflow",
    "title": "Waterdata from BOM",
    "section": "Fitting into a workflow",
    "text": "Fitting into a workflow\nI typically have a gauge number, want to get the period of record, and then pull data. I can do that here, but it’s a bit roundabout because the filters keep changing what they filter. And I’d like to not have to depend on both bomWater and kiwisR.\nAbove, I had to go from all stations, find the name and id that matched the no, and then could get the other things. But there’s got to be a way to just search with any of those, rather than different ones for different functions, right? bomWater seems to do it.\nIs there a way to search for the gauge? Not obviously, weirdly.\nSo, as it stands, a kiwisR based workflow looks something like this:\nGet the cross-referencing info for the gauges\n\ngauge_numbers &lt;- c('410730', 'A4260505')\n\nall_stations &lt;- ki_station_list(hub = \"http://www.bom.gov.au/waterdata/services\")\n\nintended_stations &lt;- all_stations |&gt; \n  dplyr::filter(station_no %in% gauge_numbers)\n\nIf we want to see what info is available (including date ranges)\n\navailable_info &lt;- ki_timeseries_list(hub = \"http://www.bom.gov.au/waterdata/services\",\n                                     station_id = intended_stations$station_id)\n\nIf we want to get the info, choose a var, but then we also need a ts_id.\n\nvar_to_get &lt;- \"DMQaQc.Merged.DailyMean.24HR\"\nstart_time &lt;- \"2010-01-01\"\nend_time &lt;- \"2010-02-28\"\nchoose_ids &lt;- 'first'\n\nall_var_to_get &lt;- available_info |&gt; \n  dplyr::filter(ts_name == var_to_get)\n\nif (choose_ids == 'first') {\n  ids_to_get &lt;- all_var_to_get |&gt; \n    dplyr::group_by(station_id) |&gt; \n    dplyr::summarise(ts_id = dplyr::first(ts_id),\n                     from = dplyr::first(from), \n                     to = dplyr::first(to)) # not sure worth returning\n}\n\nids &lt;- ids_to_get$ts_id\n\n\npulled_ts &lt;- ki_timeseries_values(hub = \"http://www.bom.gov.au/waterdata/services\",\n                                        ts_id = ids, \n                                        start_date = start_time, end_date = end_time)\n\nThat works. And then we’d likely want to conver to ML/d instead of cm^3s-1\n\nggplot(pulled_ts, aes(x = Timestamp, y = Value, color = station_name)) +\n  geom_line()\n\n\n\n\n\n\n\n\nSo, that is roundabout, but works. I guess I’ll do that until it gets too slow to pull the whole thing and then fork and add code.",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls from BOM"
    ]
  },
  {
    "objectID": "data_acquisition/bom_water.html#some-checking-of-the-available-sites",
    "href": "data_acquisition/bom_water.html#some-checking-of-the-available-sites",
    "title": "Waterdata from BOM",
    "section": "Some checking of the available sites",
    "text": "Some checking of the available sites\nWhat are the groups?\n\nki_group_list(hub = \"http://www.bom.gov.au/waterdata/services\")\n\n# A tibble: 8 × 3\n  group_id group_name             group_type\n  &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;     \n1 20017539 MDB_WIP_Storages       station   \n2 20017550 MDB_WIP_Watercourse    station   \n3 19792386 Rainfall daily 24      timeseries\n4 19792387 Rainfall monthly       timeseries\n5 19792388 Rainfall yearly        timeseries\n6 19792389 Rainfall daily 9       timeseries\n7 20017540 TS_MDB_WIP_Storages    timeseries\n8 20017541 TS_MDB_WIP_Watercourse timeseries\n\n\nWhy are those all prefaced by MDB? Shouldn’t this be australia-wide?\n\nall_watercourse_stations &lt;- ki_station_list(hub = \"http://www.bom.gov.au/waterdata/services\",\n                                            group_id = '20017550')\n\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nall_ws &lt;- all_watercourse_stations |&gt; \n  st_as_sf(coords = c('station_longitude', 'station_latitude'))\n\nThat is quite obviously just the Murray-Darling Basin. Where are the rest of the BOM sites?\n\nggplot(all_ws) + geom_sf()\n\n\n\n\n\n\n\n\n\nall_stations &lt;- ki_station_list(hub = \"http://www.bom.gov.au/waterdata/services\")\n\nThere’s a lot of NA in there, so delete them and make sf\n\nall_s &lt;- all_stations |&gt; \n  dplyr::filter(!is.na(station_longitude) & !is.na(station_latitude)) |&gt; \n  st_as_sf(coords = c('station_longitude', 'station_latitude'))\n\nClearly nationwide. Plus some that are clearly wrong. Some I’m sure are boreholes and such, but there must be flow gauges that just don’t end up in any group_id.\n\nggplot(all_s) + geom_sf()\n\n\n\n\n\n\n\n\nTo confirm, look for a river definitely not in the MDB- the Gordon (at least some of these around -42 latitude) are in Tassie.\n\nki_station_list(hub = \"http://www.bom.gov.au/waterdata/services\",\n                search_term = \"Gordon*\")\n\n# A tibble: 29 × 5\n   station_name         station_no station_id station_latitude station_longitude\n   &lt;chr&gt;                &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;             &lt;dbl&gt;\n 1 GORDON BK@FINEFLOWER 204067     586091                -29.4              153.\n 2 GORDON LAKE - AT IN… 646.1      3162538               -42.7              146.\n 3 GORDON RIVER - A/B … 187.1      3295551               -42.6              146.\n 4 GORDON RIVER - ABOV… 2491.1     3297416               -42.7              146.\n 5 Gordon               61310704   11447065              -33.0              116.\n 6 Gordon               509568     383787                -32.6              116.\n 7 Gordon Bore Rainfall 570823     14015                 -35.5              149.\n 8 Gordon Catchment     614060     392486                -32.6              116.\n 9 Gordon Clim17M Tower 509582     383835                -32.6              116.\n10 Gordon Clim33 Tower  509581     383828                -32.6              116.\n# ℹ 19 more rows",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls from BOM"
    ]
  },
  {
    "objectID": "data_acquisition/bom_water.html#add-to-hydrogauge",
    "href": "data_acquisition/bom_water.html#add-to-hydrogauge",
    "title": "Waterdata from BOM",
    "section": "Add to hydrogauge?",
    "text": "Add to hydrogauge?\nCan I get this to work?\nI’m having issues with the requests, seemingly because I’m using httr2 instead of httr\nFor example, if i handbuild the call to the API for getStationList from kiwisR,\n\napi_url &lt;- \"http://www.bom.gov.au/waterdata/services\"\n\nreturn_fields &lt;- \"station_name,station_no,station_id,station_latitude,station_longitude\"\n\nsearch_term &lt;- \"River Murray at Lock*\"\n\n# Query\n  api_query &lt;- list(\n    service = \"kisters\",\n    datasource = 0,\n    type = \"queryServices\",\n    request = \"getStationList\",\n    format = \"json\",\n    kvp = \"true\",\n    returnfields = paste(\n      return_fields,\n      collapse = \",\"\n    )\n  )\n  \n  api_query[[\"station_name\"]] &lt;- search_term\n\nRun with httr::GET, as they do\n\nraw &lt;- httr::GET(\n        url = api_url,\n        query = api_query,\n        httr::timeout(15)\n      )\n\nraw\n\nResponse [http://www.bom.gov.au/waterdata/services?service=kisters&datasource=0&type=queryServices&request=getStationList&format=json&kvp=true&returnfields=station_name%2Cstation_no%2Cstation_id%2Cstation_latitude%2Cstation_longitude&station_name=River%20Murray%20at%20Lock%2A]\n  Date: 2024-09-10 23:24\n  Status: 200\n  Content-Type: application/json;charset=UTF-8\n  Size: 1.95 kB\n\n\nParse\n\nraw_content &lt;- httr::content(raw, \"text\")\n\n  # Parse text\n  json_content &lt;- jsonlite::fromJSON(raw_content)\n  \n    # Convert to tibble\n  content_dat &lt;- tibble::as_tibble(\n    x = json_content,\n    .name_repair = \"minimal\"\n  )[-1, ]\n\nBut if I use httr2, it doesn’t return anything in the body\n\n  response_body &lt;- httr2::request(api_url) |&gt;\n    httr2::req_body_json(api_query) |&gt;\n    httr2::req_perform()\n\n# Cannot retrieve empty body\n response_body &lt;- response_body |&gt;\n    httr2::resp_body_json(check_type = FALSE)\n\nError in `resp_body_raw()`:\n! Can't retrieve empty body.\n\n\nI think the issue is that the request format actually shouldn’t be json– this doesn’t look like what HTTR says its request looks like\n\n  httr2::request(api_url) |&gt;\n    httr2::req_body_json(api_query) |&gt;\n    httr2::req_dry_run()\n\nPOST /waterdata/services HTTP/1.1\nHost: www.bom.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 241\n\n{\"service\":\"kisters\",\"datasource\":0,\"type\":\"queryServices\",\"request\":\"getStationList\",\"format\":\"json\",\"kvp\":\"true\",\"returnfields\":\"station_name,station_no,station_id,station_latitude,station_longitude\",\"station_name\":\"River Murray at Lock*\"}\n\n\nIs it that i need to just use headers? instead of a json body?\n\n  httr2::request(api_url) |&gt;\n    httr2::req_headers(!!!api_query) |&gt;\n    httr2::req_dry_run()\n\nGET /waterdata/services HTTP/1.1\nHost: www.bom.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nservice: kisters\ndatasource: 0\ntype: queryServices\nrequest: getStationList\nformat: json\nkvp: true\nreturnfields: station_name,station_no,station_id,station_latitude,station_longitude\nstation_name: River Murray at Lock*\n\n\n\ntest_resp &lt;-   httr2::request(api_url) |&gt;\n    httr2::req_headers(!!!api_query) |&gt;\n    httr2::req_perform()\n\nLooks like that didn’t work…\n\nhttr2::resp_body_string(test_resp)\n\n[1] \"KISTERS KiWIS QueryServices - add parameter 'request' to execute a query.\"\n\n\nHow about req_url_query? That looks right\n\n  httr2::request(api_url) |&gt;\n    httr2::req_url_query(!!!api_query) |&gt;\n    httr2::req_dry_run()\n\nGET /waterdata/services?service=kisters&datasource=0&type=queryServices&request=getStationList&format=json&kvp=true&returnfields=station_name%2Cstation_no%2Cstation_id%2Cstation_latitude%2Cstation_longitude&station_name=River%20Murray%20at%20Lock%2A HTTP/1.1\nHost: www.bom.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\n\n\n\ntest_out &lt;-  httr2::request(api_url) |&gt;\n    httr2::req_url_query(!!!api_query) |&gt;\n    httr2::req_perform()\n\n\njsonout &lt;- httr2::resp_body_json(test_out)\n\n# after some flipping and checking;\ntibnames &lt;- unlist(jsonout[1])\n\ntibout &lt;- jsonout[-1] |&gt; \n  tibble::tibble() |&gt; \n  tidyr::unnest_wider(col = 1, names_sep = '_') |&gt; \n  setNames(tibnames)\n\nThat seems to work. So, do I want to integrate this with hydrogauge? Can I use the same basic code? Not really, since the states need json bodies, and this needs a list-query. BUT, can I do some background parsing? If it works to send NULL in for the query and the body, can write the request to do both, but only actually do one or the other.\nDoes it work to do this?\n\ntest_out_bom &lt;-  httr2::request(api_url) |&gt;\n    httr2::req_url_query(!!!api_query) |&gt;\n  httr2::req_body_json(NULL) |&gt; \n    httr2::req_perform()\n\nparamlist &lt;- list(\"function\" = 'get_variable_list',\n                     \"version\" = \"1\",\n                     \"params\" = list(\"site_list\" = '233217',\n                                     \"datasource\" = \"A\"))\n# The query requires somethign named.\ntest_out_state &lt;- httr2::request(\"https://data.water.vic.gov.au/cgi/webservice.exe?\") |&gt;\n    httr2::req_url_query(fake = NULL) |&gt;\n  httr2::req_body_json(paramlist) |&gt; \n    httr2::req_perform()\n\n# it can be a list of null\nnullist &lt;- list(fake = NULL)\ntest_out_state &lt;- httr2::request(\"https://data.water.vic.gov.au/cgi/webservice.exe?\") |&gt;\n    httr2::req_url_query(!!!nullist) |&gt;\n  httr2::req_body_json(paramlist) |&gt; \n    httr2::req_perform()\n\nThat leaves aside the question of do we want to do that. It would be nice to unify the experience, I think, if that’s all we have to change in the main getResponse function. And then I can write separate bom and state versions of the functions accessible separately or through common wrappers that standardize syntax and outputs. Potentially just get_ts_traces_2.",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls from BOM"
    ]
  },
  {
    "objectID": "data_acquisition/ALA_data_pull.html",
    "href": "data_acquisition/ALA_data_pull.html",
    "title": "Atlas of living australia",
    "section": "",
    "text": "I need to pull some records from the Atlas of living australia. There’s an API. Can I use it so I don’t have to do this manually (and can re-do it easily?).\nWas going to set up to hit the API directly, but they have an R package (that hits a LOT of these sorts of atlases, which is nice).\n# library(httr2)\nlibrary(galah)\nHave to register at their website first- if you’re at an Australian university, it’s likely you can login with those credentials- click the AAF button.\ngalah_config(email = \"g.holt@deakin.edu.au\", atlas = \"Australia\")",
    "crumbs": [
      "Code Demos",
      "Data",
      "Pulling data from Atlas of Living Australia"
    ]
  },
  {
    "objectID": "data_acquisition/ALA_data_pull.html#try-it",
    "href": "data_acquisition/ALA_data_pull.html#try-it",
    "title": "Atlas of living australia",
    "section": "Try it",
    "text": "Try it\nI’m just following the vignette, but not filtering or selecting\n\nlippia &lt;- galah_call() |&gt;\n  galah_identify(\"lippia\") |&gt;\n  # galah_select(institutionID, group = \"basic\") |&gt;\n  atlas_occurrences()\n\nRequest for 111 occurrences placed in queue\nCurrent queue length: 1\n\n\n-\n\n\nDownloading\n\nlippia\n\n# A tibble: 111 × 8\n   recordID       scientificName taxonConceptID decimalLatitude decimalLongitude\n   &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;                    &lt;dbl&gt;            &lt;dbl&gt;\n 1 009f0e72-30db… Lippia         https://id.bi…            NA               NA  \n 2 00a48aef-c1f7… Lippia         https://id.bi…            NA               NA  \n 3 00a7d2db-634f… Lippia alba v… https://id.bi…           -23.4            150. \n 4 03feac6c-4e8c… Lippia         https://id.bi…           -20.3            -50.5\n 5 04992d8b-d1d1… Lippia         https://id.bi…            NA               NA  \n 6 04e286bb-efd0… Lippia         https://id.bi…           -22.2            -47.1\n 7 04f27e61-cfa5… Lippia alba    https://id.bi…            NA               NA  \n 8 0f687a47-e2f8… Lippia alba v… https://id.bi…           -23.6            149. \n 9 101fb056-4704… Lippia alba v… https://id.bi…           -23.6            149. \n10 10a0dc77-4c4d… Lippia alba v… https://id.bi…           -15.4            132. \n# ℹ 101 more rows\n# ℹ 3 more variables: eventDate &lt;dttm&gt;, occurrenceStatus &lt;chr&gt;,\n#   dataResourceName &lt;chr&gt;\n\n\nFrom the website though, I know that Phyla nodiflora comes up as a synonym with way more records. Can I get that sort of info from the API before pulling?\n\nall_lippia &lt;- search_taxa('lippia')\nall_lippia\n\n# A tibble: 1 × 13\n  search_term scientific_name scientific_name_authorship taxon_concept_id  rank \n  &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;                      &lt;chr&gt;             &lt;chr&gt;\n1 lippia      Lippia          L.                         https://id.biodi… genus\n# ℹ 8 more variables: match_type &lt;chr&gt;, kingdom &lt;chr&gt;, phylum &lt;chr&gt;,\n#   class &lt;chr&gt;, order &lt;chr&gt;, family &lt;chr&gt;, genus &lt;chr&gt;, issues &lt;chr&gt;\n\n\nLooks like no- these come up in the search bar on the website, but not in the above. Oh well, I guess be aware of that.\n\npnode &lt;- search_taxa('phyla nodiflora')",
    "crumbs": [
      "Code Demos",
      "Data",
      "Pulling data from Atlas of Living Australia"
    ]
  },
  {
    "objectID": "data_acquisition/ALA_data_pull.html#can-i-ask-for-both-in-one-call",
    "href": "data_acquisition/ALA_data_pull.html#can-i-ask-for-both-in-one-call",
    "title": "Atlas of living australia",
    "section": "Can I ask for both in one call?",
    "text": "Can I ask for both in one call?\nYeszs\n\nlippia_phyla &lt;- galah_call() |&gt;\n  galah_identify(c(\"lippia\", 'phyla nodiflora')) |&gt;\n  # galah_select(institutionID, group = \"basic\") |&gt;\n  atlas_occurrences()\n\nRequest for 1824 occurrences placed in queue\nCurrent queue length: 1\n\n\n--\n\n\nDownloading\n\nlippia_phyla\n\n# A tibble: 1,824 × 8\n   recordID       scientificName taxonConceptID decimalLatitude decimalLongitude\n   &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;                    &lt;dbl&gt;            &lt;dbl&gt;\n 1 00268357-9fec… Phyla nodiflo… https://id.bi…           -12.6             133.\n 2 0031342a-24c0… Phyla nodiflo… https://id.bi…           -11.9             131.\n 3 00792596-1929… Phyla nodiflo… https://id.bi…           -12.9             130.\n 4 009f0e72-30db… Lippia         https://id.bi…            NA                NA \n 5 00a48aef-c1f7… Lippia         https://id.bi…            NA                NA \n 6 00a7d2db-634f… Lippia alba v… https://id.bi…           -23.4             150.\n 7 00b4c6fe-2c52… Phyla nodiflo… https://id.bi…           -12.8             130.\n 8 00bd4543-36ab… Phyla nodiflo… https://id.bi…           -33.2             145.\n 9 00ce5430-d3da… Phyla nodiflo… https://id.bi…           -24.4             151.\n10 00d35f73-85c4… Phyla nodiflo… https://id.bi…           -34.2             140.\n# ℹ 1,814 more rows\n# ℹ 3 more variables: eventDate &lt;dttm&gt;, occurrenceStatus &lt;chr&gt;,\n#   dataResourceName &lt;chr&gt;\n\n\nAnd it returns the names, so I can filter.",
    "crumbs": [
      "Code Demos",
      "Data",
      "Pulling data from Atlas of Living Australia"
    ]
  },
  {
    "objectID": "betabinomial/random_uneven_groups_dev.html",
    "href": "betabinomial/random_uneven_groups_dev.html",
    "title": "Random effects with uneven groups",
    "section": "",
    "text": "library(tidyverse)\nlibrary(withr)\nlibrary(glmmTMB)\ndevtools::load_all()\nThis is essentially preamble to understanding the beta-binomial issues we’re having. But we should be able to do a better job sharpening our intuition of what we expect if we start off with gaussian.\nThis builds on the outline I developed and the work Sarah did (students/Sarah_Taig/Random Effects Simulations) (though I think the emphasis will be different; we’ll see), as well as some beta-binom errorbar checks in caddis/Testing/Error_bars.qmd.\nI think I’ll likely just do gaussian here. Then bb in a parallel doc. And likely will do a model comparison thing too- i.e. spaMM vs glmTMB vs lme4::glmer as in caddis/Analyses/Testing/df_z_t_for_sarah/ (and add {brms}).\nAnd will likely need to incorporate some assessments of the se being calculated at various scales, as explored in caddis/Analyses/Testing/Error_bars.qmd.\nWe’ll need to bring in real data at some point, but I think not in this doc (hence why it’s here, and some of the other testing is in caddis.\nAre we just recapitulating this? Kind of. Slightly different emphasis and we’ll end up taking it further, but should be careful.\nThe data generation function lets us have random slopes. These are super relevant in some cases, e.g. following people or riffles through time, where observations might have different x-values within the cluster. I think I’ll largely ignore them here, because the situation we’re trying to address doesn’t (each cluster has a single x), and so the in-cluster slopes are irrelevant. They could certainly be dealt with in this general exploration, but it would just make everything factorially complicated."
  },
  {
    "objectID": "betabinomial/random_uneven_groups_dev.html#fit-models",
    "href": "betabinomial/random_uneven_groups_dev.html#fit-models",
    "title": "Random effects with uneven groups",
    "section": "Fit models",
    "text": "Fit models\nNow, we can fit the models. What do we want to use here? Build in the model comparison now, or deal with that later? Later, I think (though likely that will mean coming back up here).\n\n# sddata &lt;- sddata |&gt; \n#   mutate(cluster_rand_x = map(simdata, \\(x) glmmTMB(y ~ x + (1|cluster), data = x)),\n#          # This one ends up rank-deficient for the last cluster usually\n#          cluster_fixed_x = map(simdata, \\(x) glmmTMB(y ~ x + cluster, data = x)),\n#          cluster_fixed = map(simdata, \\(x) glmmTMB(y ~ cluster, data = x)),\n#          cluster_rand = map(simdata, \\(x) glmmTMB(y ~ 1|cluster, data = x)))\n\nI want to extract the cluster estimates for each model skipping the intercept estimates here, though we might want to revisit that (see below)\nFirst, set up a dataset we’re going to want to predict on for the continuous fits\n\n# clusterdata &lt;- sddata$simdata[[3]] |&gt; \n#   select(x, cluster) |&gt; \n#   distinct()\n\nxdata &lt;- tibble(x = seq(from = min(xrange), to = max(xrange), by = diff(xrange)/100))\n\nStart setting this up for a function.\nFor the models with cluster random, we could get the values straight out of the main fit. But I think we want to actually get them from ranef and coef, because those are more directly what it’s estimating for the point estimates of the clusters. It ends up being the same value, but not the same sd.\n\n# I'm not sure if I actually want this one or not. I think I might, in conjunction with a basic summarise() for the fixefs. Or a clever extraction from summary(model)$coefficients, but the catch is sorting out the x's and intercepts.\n# cluster_estimates_rand &lt;- function(model, alldata) {\n#   clusterdata &lt;- make_clusterdata(alldata)\n#   clustib &lt;- tibble(cluster = row.names(ranef(model)$cond$cluster),\n#   intercept = fixef(model)$cond[\"(Intercept)\"], \n#        slope = fixef(model)$cond['x'], \n#        cluster_deviation = ranef(model)$cond$cluster[[1]], \n#        cluster_sd = sqrt(c(attributes(ranef(model)$cond$cluster)$condVar))) |&gt; \n#   mutate(slope = ifelse(is.na(slope), 0, slope)) |&gt; \n#   left_join(clusterdata, by = 'cluster') |&gt; \n#   mutate(cluster_estimate = intercept + slope*x + cluster_deviation)\n#   \n#   return(dplyr::select(clustib, cluster, x, cluster_estimate, cluster_sd))\n# }\n\nTry to get that for the non-random. There’s an issue with the se for the intercept level\n\n# cluster_estimates_fixed &lt;- function(model, alldata) {\n#   clusterdata &lt;- make_clusterdata(alldata)\n#   sumvals &lt;- summary(model)$coefficients$cond\n#   unique(clusterdata$cluster)\n#   row.names(sumvals) &lt;- stringr::str_remove_all(row.names(sumvals), 'cluster')\n#   missingc &lt;- setdiff(unique(clusterdata$cluster), row.names(sumvals))\n#   # We get the intercept elsewhere\n#   sumvals[row.names(sumvals) == '(Intercept)', 'Estimate'] &lt;- 0\n#   row.names(sumvals)[row.names(sumvals) == '(Intercept)'] &lt;- missingc\n#   \n#   \n#   clustib &lt;- as_tibble(sumvals, rownames = 'cluster') |&gt; \n#     select(cluster, estimate = Estimate, se = `Std. Error`) |&gt; \n#     mutate(intercept = fixef(model)$cond['(Intercept)'],\n#            slope = fixef(model)$cond['x'],\n#            slope = ifelse(is.na(slope), 0, slope)) |&gt; \n#     filter(cluster != 'x') |&gt; \n#     left_join(clusterdata) |&gt; \n#     mutate(cluster_estimate = intercept + estimate + slope*x)\n#   \n#   return(dplyr::select(clustib, cluster, x, cluster_estimate, cluster_sd = se))\n#   \n# }\n\n\n# extract_cluster_terms &lt;- function(model, alldata) {\n#   clusterdata &lt;- make_clusterdata(alldata)\n#   if (length(ranef(test_fixed)$cond) == 0) {\n#     clustib &lt;- cluster_estimates_fixed(model, clusterdata)\n#   } else {\n#     clustib &lt;- cluster_estimates_rand(model, clusterdata)\n#   }\n#   \n#   return(clustib)\n# }\n\nThis gets the cluster estimates from the fits at the cluster values. Because it accounts for the full model, the se in particular are not the same.\n\n# predict_at_clusters &lt;- function(model, alldata) {\n#   clusterdata &lt;- make_clusterdata(alldata)\n#   fitcluster &lt;- clusterdata |&gt; \n#   add_predictions(model, se.fit = TRUE) |&gt; \n#   rename(predicted = fit, sd = se.fit)\n#   return(fitcluster)\n# }\n\n\n# naive_clusters &lt;- function(alldata) {\n#   alldata |&gt; \n#     summarise(cluster_mean = mean(y),\n#            cluster_se = sd(y)/sqrt(n()), .by = cluster)\n# }\n\nSo, the above should let us get the cluster points.\nDo we really want to stay in this tibble framework, or move on to something else?\nBreak it down to a single thing to make sure we have what we want. Should be able to re-purr if we want. But I think we probably will want to skip some intermediate steps\nOne set of data\n\nsimdata &lt;- sddata$simdata[[3]]\n\nfit the models\n\ncluster_rand_x &lt;- glmmTMB(y ~ x + (1|cluster), data = simdata)\n# This one ends up rank-deficient for the last cluster usually\ncluster_fixed_x &lt;- glmmTMB(y ~ x + cluster, data = simdata)\ncluster_fixed &lt;- glmmTMB(y ~ cluster, data = simdata)\ncluster_rand &lt;- glmmTMB(y ~ 1|cluster, data = simdata)\n\nno_cluster &lt;- glmmTMB(y ~ x, data = simdata)\n\nGet estimates for each cluster\n\n# Extraction as best I can of *just* the cluster estimates and se\nclusters_from_rand_x &lt;- extract_cluster_terms(cluster_rand_x, simdata)\nclusters_from_fixed_x &lt;- extract_cluster_terms(cluster_fixed_x, simdata)\nclusters_from_rand &lt;- extract_cluster_terms(cluster_rand, simdata)\nclusters_from_fixed &lt;- extract_cluster_terms(cluster_fixed, simdata)\n\n# From the data\nclusters_from_raw &lt;- naive_clusters(simdata)\n\n# Extraction of estimates from the full model for the clusters at their x values (this works here with no in-cluster x variation)\nclusters_full_rand_x &lt;- predict_at_clusters(cluster_rand_x, simdata)\nclusters_full_fixed_x &lt;- predict_at_clusters(cluster_fixed_x, simdata)\nclusters_full_rand &lt;- predict_at_clusters(cluster_rand, simdata)\nclusters_full_fixed &lt;- predict_at_clusters(cluster_fixed, simdata)\n\nno_cluster_full &lt;- predict_at_clusters(no_cluster, simdata)\n\nThere are a LOT of potential comparisons there; let’s try to clarify what they are.\nPreliminary (sort of sideways, but checking):\n\nHow do the cluster estimates differ from those from a full model?\n\nThe extracted cluster term has the same estimate and marginally larger se, as we would expect.\n\nest_compare &lt;- \n  bind_rows(clusters_from_fixed, clusters_full_fixed,\n            clusters_from_fixed_x, clusters_full_fixed_x, \n            clusters_from_rand_x, clusters_full_rand_x, \n            clusters_from_rand, clusters_full_rand) |&gt; \n  ggplot(aes(y = estimate, x = x, color = estimate_type, ymin = estimate-se, ymax = estimate+se)) + \n  geom_pointrange(position = position_dodge(width = 0.1)) +\n  facet_wrap('model')\n\nest_compare\n\n\nHow does the inclusion of x affect cluster estimates?\n\nIf clusters are a fixed effect, nothing changes. If clusters are random, we do see some deviations between the estimates, which don’t look much different if we just have the cluster term or the full predictions.\n\nx_compare &lt;- \n  bind_rows(clusters_from_fixed, clusters_full_fixed,\n            clusters_from_fixed_x, clusters_full_fixed_x, \n            clusters_from_rand_x, clusters_full_rand_x, \n            clusters_from_rand, clusters_full_rand) |&gt; \n  mutate(hasx = grepl('_x$', model),\n         clusterrand = grepl('_rand', model)) |&gt; \n  ggplot(aes(y = estimate, x = x, color = hasx, ymin = estimate-se, ymax = estimate+se)) + \n  geom_pointrange(position = position_dodge(width = 0.1)) +\n  facet_grid(estimate_type ~ clusterrand, labeller = 'label_both')\n\nx_compare\n\n\nDoes the ‘separate’ estimation of each cluster match the fixeds?\n\nAt the mean, yes. It has a smaller se than the cluster terms themselves, but larger than the full model (usually).\n\nsep_fixed &lt;- \n  bind_rows(clusters_from_fixed,\n            clusters_from_fixed_x,\n            clusters_full_fixed, \n            clusters_full_fixed_x,\n            # so I can include on both facets\n            clusters_from_raw |&gt; mutate(estimate_type = 'cluster_term'),\n            clusters_from_raw |&gt; mutate(estimate_type = 'full_model_predict')) |&gt; \n  \n  ggplot(aes(y = estimate, x = x, color = model, ymin = estimate-se, ymax = estimate+se)) + \n  geom_pointrange(position = position_dodge(width = 0.3)) +\n  facet_wrap('estimate_type')\n\nsep_fixed\n\nHere’s what I really want to know- how do predictions change between fixed effect estimates (and the naive, but since that’s the same, not including here), and the random? I think I’ll use the cluster term fits (not full model), and I suppose with and without x in the random models. I’ll also include the ‘no cluster’ model as a reference.\nSo, the rands (particularly rand_x) tend to pull closer to the points without a cluster term (blue, ‘no_cluster’)\n\ncluster_compare &lt;- \n  bind_rows(clusters_from_fixed,\n            clusters_from_rand_x, \n            clusters_from_rand,\n            no_cluster_full) |&gt; \n  ggplot(aes(y = estimate, x = x, color = model, ymin = estimate-se, ymax = estimate+se)) + \n  geom_pointrange(position = position_dodge(width = 0.3))\n\ncluster_compare\n\nOn to lines- how does any of this change the FITS?\n\nFor the models themselves? (rand_x, fixed_x, no_cluster)\nNew fits through the cluster estimates (IE how we try to mentally fit the the data in a plot) (rand_x- do I want the full model or just the cluster ests?-, fixed_x, naive)\n\nGet the fits that our eyes will try to draw:\n\ncluster_model_rand_x &lt;- glmmTMB(y ~ x, data = clusters_from_rand_x |&gt; rename(y = estimate))\ncluster_model_fixed_x &lt;- glmmTMB(y ~ x, data = clusters_from_fixed_x |&gt; rename(y = estimate))\ncluster_model_raw &lt;- glmmTMB(y ~ x, data = clusters_from_raw |&gt; rename(y = estimate))\n# This is what we're doing in the plots, though there it's internally nested as well\ncluster_model_fixed &lt;- glmmTMB(y ~ x, data = clusters_from_fixed |&gt; rename(y = estimate))\n\nFit the lines for each model At the mean random level when relevant\n\n# The full models\nlines_rand_x &lt;- predict_fit(cluster_rand_x, xdata)\nlines_fixed_x &lt;- predict_fit(cluster_fixed_x, xdata)\nlines_no_cluster &lt;- predict_fit(no_cluster, xdata)\n\n# Fits to the cluster estimates\nlines_rand_x_clusters &lt;- predict_fit(cluster_model_rand_x, xdata)\nlines_fixed_x_clusters &lt;- predict_fit(cluster_model_fixed_x, xdata)\nlines_raw_clusters &lt;- predict_fit(cluster_model_raw, xdata)\nlines_fixed_clusters &lt;- predict_fit(cluster_model_fixed, xdata)\n\nNow set up some comparisons\nAgain, some prelim to know what we’re dealing with\n\nDo the full models differ?\n\nThe no cluster points are annoying, but this basically works. The fit through the data matches the fit of the random clusters, and is slightly different than the fit you’d get if it were possible to fit the x from the model with fixed clusters.\n\nbind_rows(lines_rand_x,\n          lines_fixed_x,\n          lines_no_cluster) |&gt; \nggplot(aes(x = x, y = estimate, color = model, linetype = model)) +\n  geom_line() +\n  geom_point(data = bind_rows(clusters_from_rand_x, \n                              clusters_from_fixed_x, \n                              simdata |&gt; mutate(estimate = y, model = 'no_cluster')))\n\n\nHow do the full fits differ from fits through the summary points for each set? The last two are a funny comparison (no cluster fit and the raw clusters).\n\n\nbind_rows(lines_rand_x,\n          lines_rand_x_clusters,\n          lines_fixed_x, \n          lines_fixed_x_clusters,\n          lines_no_cluster,\n          lines_raw_clusters) |&gt; \n  mutate(fittype = ifelse(grepl('model', model), 'clusters', 'data'),\n         model_group = stringr::str_remove_all(model, 'model_'),\n         model_group = case_when(model_group %in% c('no_cluster', 'cluster_raw') ~ 'data',\n                                 .default = model_group)) |&gt; \n  ggplot(aes(x = x, y = estimate, color = fittype, linetype = fittype)) +\n  geom_line() +\n  facet_wrap('model_group') +\n  geom_point(data = bind_rows(clusters_from_rand_x, clusters_from_fixed_x, clusters_from_raw) |&gt; \n               mutate(model_group = ifelse(model == 'separate', 'data', model),\n                      fittype = 'clusters'))\n\nNow, let’s figure out the situation we have, and what we actually want.\nWe’re plotting the model fits (lines_rand_x), and the cluster estimates from a cluster model\n\nlines_rand_x |&gt; \n  ggplot(aes(x = x, y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n    geom_ribbon(alpha = 0.25) +\n  geom_line() +\n  geom_point(data = clusters_from_fixed) +\n  geom_errorbar(data = clusters_from_fixed)\n\nWhat makes more sense as a target is the random estimates and the fit\n\nlines_rand_x |&gt; \n  ggplot(aes(x = x, y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n    geom_ribbon(alpha = 0.25) +\n  geom_line() +\n  geom_point(data = clusters_from_rand_x) +\n  geom_errorbar(data = clusters_from_rand_x)\n\nFor error testing as we go, there’s a few more things I want to plot- the naive cluster estimates, their fit, and the fit to all the data. We basically have that above, but this gets more directly at it. As we move to nested randoms, we will likely end up wanting the thing we currently do in the paper (the outer-level as fixed with inner random). But let’s cross that bridge when we get to it.\n\nlines_rand_x |&gt; \n  ggplot(aes(x = x, y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n    geom_ribbon(alpha = 0.25) +\n  geom_line() +\n  geom_point(data = clusters_from_rand_x) +\n  geom_errorbar(data = clusters_from_rand_x) +\n  geom_point(data = clusters_from_raw, color = 'seagreen1') +\n  geom_errorbar(data = clusters_from_raw, color = 'seagreen1') +\n  geom_line(data = lines_raw_clusters, color = 'seagreen1', linetype = 'dashed') +\n  geom_line(data = lines_no_cluster, color = 'orchid', linetype = 'dotted')\n\nCan I build that in a tidier way? Yes. Need to come up with a better set of colors, but whatever.\n\nlinedf &lt;- bind_rows(lines_rand_x, lines_raw_clusters, lines_no_cluster) |&gt; \n  mutate(model2 = case_when(model == 'cluster_rand_x' ~ 'Random cluster estimates',\n                            model == 'cluster_model_raw' ~ 'Naive cluster estimates',\n                            model == 'no_cluster' ~ 'No cluster term'))\n\npointdf &lt;- bind_rows(clusters_from_rand_x, clusters_from_raw) |&gt; \n  mutate(model2 = case_when(model == 'cluster_rand_x' ~ 'Random cluster estimates',\n                            model == 'separate' ~ 'Naive cluster estimates'))\n\nggplot(linedf, \n       aes(x = x, y = estimate, ymin = estimate-se, ymax = estimate + se, \n           color = model2, linetype = model2)) +\n    geom_ribbon(data = linedf |&gt; filter(model2 == 'Random cluster estimates'),\n                alpha = 0.25, linetype = 0) +\n  geom_line() +\n  geom_point(data = pointdf) +\n  geom_errorbar(data = pointdf) +\n  geom_text(data = pointdf |&gt; filter(model == 'cluster_rand_x'), aes(label = cluster), nudge_x = 0.2)\n\nWhere are the fixeds? I end up needing them below (maybe?)\n\nggplot(lines_fixed_x, \n       aes(x = x, y = estimate, ymin = estimate-se, ymax = estimate + se, \n           color = model, linetype = model)) +\n      geom_line() +\n  geom_point(data = clusters_from_fixed_x) +\n  geom_errorbar(data = clusters_from_fixed_x) +\n  geom_text(data = clusters_from_fixed_x, aes(label = cluster), nudge_x = 0.2)\n\nTHEN, package all this up so we can do it for different sds. And unbalanced. And beta-binom. And nested.\nDo I want to make the ‘Intercept’ plots below to show shrinkage directly? Or is it obvious enough here?\n\n# These top two are comparable\nrand_x_coefs &lt;- as_tibble(coef(cluster_rand_x)$cond$cluster, \n                          rownames = 'cluster') |&gt;\n  # this adds the condVar- are these sd or se?\n  bind_cols(se = \n              sqrt(c(attributes(ranef(cluster_rand_x)$cond$cluster)$condVar))) |&gt;\n  rename(intercept = `(Intercept)`) |&gt; \n  mutate(grand_intercept = fixef(cluster_rand_x)$cond['(Intercept)']) |&gt; \n  mutate(model = 'cluster_rand_x', hasx = TRUE, ismm = TRUE)\n\n# I *think* this is what I want- the effect of cluster at x = 0 in the fixed model. That's not the same as the cluster estimates in the cluster model, which don't account for the x (or the purely naive estimates).\n# Or is it unique(simdata$x[simdata$cluster == 1])\nfixed_x_coefs &lt;- tibble(cluster = unique(simdata$cluster), \n                        x = 0) |&gt; \n add_predictions(cluster_fixed_x, se.fit = TRUE) |&gt; \n  rename(intercept = fit, se= se.fit) |&gt;  # for this specific case at x = 0\n    # mutate(grand_intercept = fixef(cluster_fixed_x)$cond['(Intercept)']) |&gt; \n      mutate(grand_intercept = fixef(cluster_fixed_x)$cond['(Intercept)'] + fixef(cluster_fixed_x)$cond['x']*unique(simdata$x[simdata$cluster == 1])) |&gt; \n\n  mutate(model = 'cluster_fixed_x', hasx = TRUE, ismm = FALSE)\n\n# These are comparable and already created, but don't have the slope taken out.\n# clusters_from_rand\n# clusters_from_raw \n\n\nbind_rows(rand_x_coefs, fixed_x_coefs) |&gt; \nggplot(aes(x = cluster, y = intercept, \n           color = model)) +  \n  geom_point(position = position_dodge(width = 0.1)) +\n  geom_linerange(aes(ymax = intercept+se, ymin = intercept-se),\n                  position = position_dodge(width = 0.1)) +\n  geom_hline(aes(yintercept = grand_intercept, color = model, linetype = model)) \n\nThe way I do it below is better with the ranefs.\n\nbind_rows(clusters_from_rand |&gt; \n            mutate(grand_intercept = fixef(cluster_rand)$cond['(Intercept)']),\n          clusters_from_raw |&gt; \n            # Should this be overall data mean or mean of the estimates? It doesn't matter here, but will when we have uneven data.\n            mutate(grand_intercept = mean(estimate))) |&gt; \n            # mutate(grand_intercept = mean(simdata$y))) |&gt; \nggplot(aes(x = x, y = estimate, \n           color = model)) +  \n  geom_point(position = position_dodge(width = 0.1)) +\n  geom_linerange(aes(ymax = estimate+se, ymin = estimate-se),\n                  position = position_dodge(width = 0.1))  +\n  geom_hline(aes(yintercept = grand_intercept, color = model, linetype = model)) \n\nI really need to set up a standard set of colors, this is really hard when the colors keep changing.\nWhat about the way m-clark does it? He fits lmList, which splits the dataset into clusters and fits them as random. The catch is because I have a true fixed x and no in-cluster x variation, his method doesn’t work the same, and so I end up with the full cluster estimates\n\nll &lt;- lme4::lmList(y ~ 1|cluster, simdata)\n\nThere’s no exact equivalent in glmmTMB, but easy enough to loop, for ex\n\ntl &lt;- glmmTMB(y ~ 1|cluster, data = simdata |&gt; filter(cluster == 9))\n\nBut we see that that has the full value (as doe the lmList version), and adding x in doesn’t help\n\ntl &lt;- glmmTMB(y ~ x|cluster, data = simdata |&gt; filter(cluster == 9))\ntl &lt;- glmmTMB(y ~ x + 1|cluster, data = simdata |&gt; filter(cluster == 9))\n\nCan I look at the full model, extract the difference from the fixed line at that x value and compare to the ranef? Do we need to? These coef values from the clusters random but separate are exactly clusters_from_fixed and clusters_full_fixed_x . SO. REALLY, I need to make sure I’m doing the proper comparisons to the fixed values. It’s clear from the above that the ‘intercept’ isn’t working, at least how I’m extracting it.\nso yeah, coef is giving me the ranef + fixef. and there’s no obviously great way to do that for the fixed clusters. BUT, I should be able to extract them as the difference between their value and a ‘pure’ fit of the line, and compare that to ranef. TRY THAT.\nSo basically, what I want is the cluster residuals. But again, how do I get them for the fixed model? They’re also (at least for this case) the same as cluster_from_raw\n\n# The full models\nnc_rand_x &lt;- predict_fit(cluster_rand_x, make_clusterdata(simdata)) |&gt; \n  rename(line_est = estimate, line_se = se) |&gt; \n  left_join(clusters_from_rand_x) |&gt; \n  mutate(cluster_resid = estimate-line_est) |&gt; \n  # As a check that this is correct,\n  left_join(as_tibble(ranef(cluster_rand_x)$cond$cluster, rownames = 'cluster')) |&gt; \n  rename(extract_ranef = `(Intercept)`) |&gt; \n  mutate(check = cluster_resid - extract_ranef)\n\n# Need to figure this one out eventually, probably, but it's not working.\n# nc_fixed_x &lt;- predict_fit(cluster_fixed_x, make_clusterdata(simdata)) |&gt; \n#   rename(line_est = estimate, line_se = se) |&gt;\n#   left_join(clusters_from_fixed_x) |&gt; \n#   mutate(cluster_resid = estimate-line_est)\n\nnc_raw_cluster &lt;- predict_fit(no_cluster, make_clusterdata(simdata)) |&gt; \n  rename(line_est = estimate, line_se = se) |&gt; \n  select(-model) |&gt; \n  left_join(clusters_from_raw) |&gt; \n  mutate(cluster_resid = estimate-line_est)\n\n# Fits to the cluster estimates\n# I don't think I actually need this\n# nc_rand_x_clusters &lt;- predict_fit(cluster_model_rand_x, make_clusterdata(simdata))\n# nc_fixed_x_clusters &lt;- predict_fit(cluster_model_fixed_x, make_clusterdata(simdata))\n# nc_raw_clusters &lt;- predict_fit(cluster_model_raw, make_clusterdata(simdata))\n# nc_fixed_clusters &lt;- predict_fit(cluster_model_fixed, make_clusterdata(simdata))\n\nThis will work to show shrinkage.\n\nggplot(bind_rows(nc_rand_x, nc_raw_cluster), aes(x = x, y = cluster_resid, \n                                                 ymin = cluster_resid-se, \n                                                 ymax = cluster_resid + se, \n                                                 color = model)) + \n  geom_point() +\n  geom_linerange() +\n  geom_hline(yintercept = 0)\n\nCan we get the full fixed model to give up its values? Following https://stackoverflow.com/questions/72820236/comparing-all-factor-levels-to-the-grand-mean-can-i-tweak-contrasts-in-linear-m\nThis might work, but the NA in cluster 9 kills me every time.\n\nContrSumMat &lt;- function (fctr, sparse = FALSE) {\n  if (!is.factor(fctr)) stop(\"'fctr' is not a factor variable!\")\n  N &lt;- nlevels(fctr)\n  Cmat &lt;- contr.sum(N, sparse = sparse)\n  dimnames(Cmat) &lt;- list(levels(fctr), seq_len(N - 1))\n  Cmat\n}\n\n\nCmat &lt;- ContrSumMat(as.factor(simdata$cluster))\n\nExtract\n\ncoef_ac &lt;- summary(cluster_fixed_x)$coef$cond[3:nrow(summary(cluster_fixed_x)$coef$cond), 'Estimate']\n\nLevel-specific\n\ncoef_bc &lt;- (Cmat %*% coef_ac)[, 1]"
  },
  {
    "objectID": "betabinomial/random_uneven_groups_dev.html#testing",
    "href": "betabinomial/random_uneven_groups_dev.html#testing",
    "title": "Random effects with uneven groups",
    "section": "Testing",
    "text": "Testing\nSo, this is working. Let’s choose one of those and sort out everything we want to do and then do the purrrring.\n\ntest_full &lt;- sddata$cluster_rand_x[[3]]\ntest_fixed &lt;- sddata$cluster_fixed_x[[3]]\ntest_cluster &lt;- sddata$cluster_fixed[[3]]\ntest_rand &lt;- sddata$cluster_rand[[3]]\n\nCan I get shrinkage from these? I’m now far enough from what the website is doing, I’m a bit lost. Because he still seems to be building a mixed model at this point.\n\n# 'For coef.glmmTMB: a similar list, but containing the overall coefficient value for each level, i.e., the sum of the fixed effect estimate and the random effect value for that level'\ncoef(test_full)\n\n\nranef(test_full)\n\nI can extract the conditional variance too, though it’s a hassle to get. Will be very useful though, I think.\n\nattributes(ranef(test_full)$cond$cluster)$condVar |&gt; c()\n\n\nfixef(test_full)\n\n\ncoef(test_rand)\n\n\nranef(test_rand)\n\n\nfixef(test_rand)\n\n\nfixef(test_cluster)\n\nCan I plot all of those? Will move on to predicting the whole line in a bit. I just really want to understand what I’m dealing with.\n\n# add_predictions &lt;- function(df, model, se.fit = TRUE, ...) {\n#   preds &lt;- predict(object = model,\n#                    newdata = df,\n#                    se.fit = TRUE, ...) |&gt; \n#     as_tibble()\n#   \n#   df &lt;- bind_cols(df, preds)\n#   return(df)\n# }\n\n\nfull_coefs &lt;- as_tibble(coef(test_full)$cond$cluster, rownames = 'cluster') |&gt; \n  # this adds the condVar- are these sd or se?\n  bind_cols(sd = sqrt(c(attributes(ranef(test_full)$cond$cluster)$condVar))) |&gt;\n  rename(intercept = `(Intercept)`) |&gt; \n  mutate(model = 'full_mm', hasx = TRUE, ismm = TRUE)\n\nrand_coefs &lt;- as_tibble(coef(test_rand)$cond$cluster, rownames = 'cluster') |&gt; \n    # this adds the condVar- are these sd or se?\n  bind_cols(sd = sqrt(c(attributes(ranef(test_rand)$cond$cluster)$condVar))) |&gt;\n  rename(intercept = `(Intercept)`) |&gt; \n  mutate(model = 'rand', hasx = FALSE, ismm = TRUE)\n\n# I *think* this is what I want- the effect of cluster at x = 0 in the fixed model. That's not the same as the cluster estimates in the cluster model, which don't account for the x. They'll come in when we look at the actual fit line.\nfixed_coefs &lt;- tibble(cluster = unique(full_coefs$cluster), x = 0) |&gt; \n add_predictions(test_fixed, se.fit = TRUE) |&gt; \n  rename(intercept = fit, sd = se.fit) |&gt;  # for this specific case at x = 0\n  mutate(model = 'fixed', hasx = TRUE, ismm = FALSE)\n\n# cluster coefs- these are comparable to 'rand', I think- there's no x term to that gets rolled into cluster.\ncluster_coefs &lt;- tibble(cluster = unique(full_coefs$cluster), x = 0) |&gt; \n  add_predictions(test_cluster) |&gt; \n  rename(intercept = fit, sd = se.fit) |&gt;  # these don't even have an x, so doesn't matter\n  mutate(model = 'cluster', hasx = FALSE, ismm = FALSE)\n\n\n\nintercept_compare &lt;- bind_rows(full_coefs, rand_coefs, \n                               fixed_coefs, cluster_coefs) |&gt; mutate(model = forcats::fct_reorder(model, ismm))\n\nWe can see the ‘shrinkage’ here in two ways- first in the models with an x term assessed at the origin, where the clusters are estimating different random intercepts and we just extract the intercepts from a fixed model. The blue points move closer to the intercept (horizontal line). Though in some cases they cross it.\n\n# Leaving here to remind myself why we dont' want rand.\nggplot(intercept_compare |&gt; filter(hasx), \n       aes(x = cluster, y = intercept, \n           color = model)) +\n  geom_pointrange(aes(ymax = intercept+sd, ymin = intercept-sd),\n                  position = position_dodge(width = 0.1)) +\n  geom_hline(yintercept = fixef(test_full)$cond[1])\n\nSimilarly, if we look at a model without an x term, and so just estimate the clusters themselves, the blue (random) version is closer to the intercept than the red (clusters as fixed effects).\n\nggplot(intercept_compare |&gt; filter(!hasx), \n       aes(x = cluster, y = intercept, color = model)) +\n    geom_pointrange(aes(ymax = intercept+sd, ymin = intercept-sd),\n                  position = position_dodge(width = 0.1)) +\n  geom_hline(yintercept = fixef(test_rand)$cond[1])\n\nSo now, can we plot the fits along x, and see if we can get the lines to make sense in terms of what they’re doing relative to those points above?\n\nfull_preds_cluster &lt;- as_tibble(coef(test_full)$cond$cluster, \n                                  rownames = 'cluster') |&gt; \n  bind_cols(sd = sqrt(c(attributes(ranef(test_full)$cond$cluster)$condVar))) |&gt;\n  rename(intercept = `(Intercept)`, slope = x)\n\nfull_clusterx &lt;- test_full$frame |&gt; select(x, cluster) |&gt; distinct()\n\nfull_preds_cluster &lt;- full_preds_cluster |&gt; \n  left_join(full_clusterx) |&gt; \n  mutate(predicted = intercept + slope*x) |&gt; \n  mutate(model = 'full_mm', hasx = TRUE, ismm = TRUE)\n\n# add some checks\n# The full model fit- fits match,se a bit smaller\nfull_preds_cluster &lt;- full_preds_cluster |&gt; \n  add_predictions(test_full) |&gt; \n  rename(full_model_fit = fit, \n         full_model_sd = se.fit)\n\n# can we build it from ranef?\nfull_preds_cluster &lt;- full_preds_cluster |&gt; \n  bind_cols(ranef(test_full)$cond$cluster) |&gt; \n  rename(randdev = `(Intercept)`) |&gt; \n  add_predictions(test_full, re.form = NA) |&gt; \n  rename(no_clust_fit = fit, no_clust_sd = se.fit) |&gt; \n  mutate(check_re = no_clust_fit + randdev)\n  \n\n# rand_preds &lt;- as_tibble(coef(test_rand)$cond$cluster, rownames = 'cluster') |&gt; \n#     # this adds the condVar- are these sd or se?\n#   bind_cols(sd = sqrt(c(attributes(ranef(test_rand)$cond$cluster)$condVar))) |&gt;\n#   rename(intercept = `(Intercept)`) |&gt; \n#   mutate(model = 'rand', hasx = FALSE, ismm = TRUE)\n\n# I *think* this is what I want- the effect of cluster at x = 0 in the fixed model. That's not the same as the cluster estimates in the cluster model, which don't account for the x. They'll come in when we look at the actual fit line.\nfixed_preds_cluster &lt;- test_fixed$frame |&gt;\n  select(x, cluster) |&gt; distinct() |&gt; \n  add_predictions(test_fixed, se.fit = TRUE) |&gt; \n  rename(predicted = fit, sd = se.fit) |&gt;  # for this specific case at x = 0\n  mutate(model = 'fixed', hasx = TRUE, ismm = FALSE)\n\npreds_compare &lt;- bind_rows(full_preds_cluster, \n                           fixed_preds_cluster) |&gt; \n  mutate(model = forcats::fct_reorder(model, ismm))\n\nA quick check of the various point fits for the full model- the estimates for the clusters are the same whether we use the full model or extract them in various ways from coef and ranef. The blue points here are the odd one out, they are the fits in the no-cluster predict (and the orchid are those with the cluster deviations added back on).\n\nggplot(full_preds_cluster) +\n  geom_point(aes(x = x, y = predicted), color = 'firebrick') +\n  geom_point(aes(x = x+0.1, y = full_model_fit), color = 'forestgreen') +\n  geom_point(aes(x = x+0.2, y = no_clust_fit), color = 'dodgerblue') +\n  geom_point(aes(x = x+0.3, y = check_re), color = 'orchid')\n\nNow, I want to fit some lines.\nFirst, let’s fit those models themselves.\n\nnewdata_nc &lt;- tibble(x = 0:10)\n# use re.form = NA to fit at population\nfit_full &lt;- newdata_nc |&gt; \n  add_predictions(test_full, se.fit = TRUE, re.form = NA) |&gt; \n  rename(predicted = fit, sd = se.fit) |&gt; \n  mutate(model = 'full_mm')\n\nnewdata_c &lt;- expand_grid(newdata_nc, cluster = unique(full_coefs$cluster))\nfit_fixed &lt;- newdata_c |&gt; \n  add_predictions(test_fixed, se.fit = TRUE) |&gt; \n  rename(predicted = fit, sd = se.fit) |&gt; \n  mutate(model = 'fixed')\n\n# That gives a line for each cluster. But what I want is the main effect of x. I can get that slope from fixef(test_fixed), but the intercept is cluster 1, not the true intercept. so I need to calculate that?\nfixed_intercept &lt;- test_fixed$frame |&gt; filter(cluster == 1) |&gt; \n  distinct(x) |&gt; pull()\n\nfixed_slope &lt;- fixef(test_fixed)$cond['x']\n\n# intercept error (note, I'm not trying to deal with slope error here yet)\nint_se &lt;- summary(test_fixed)$coefficients$cond[1, 'Std. Error']\n\nfit_fixed2 &lt;- newdata_nc |&gt; \n  mutate(predicted = fixed_intercept + fixed_slope*x,\n         sd = int_se,\n         model = 'fixed')\n\nLet’s also add lines that fit through the cluster estimates. Note that these will have much too high error, but we’re largely ignoring that anyway.\n\nfitrandclusts &lt;- glmmTMB(predicted ~ x, \n                         data = preds_compare |&gt; filter(model == 'full_mm'))\n\nfitfixedclusts &lt;- glmmTMB(predicted ~ x, \n                         data = preds_compare |&gt; filter(model == 'fixed'))\n\n# I think we can add the other two on here too, if we join to the relevant x-values\nxc &lt;- sddata$simdata[[3]] |&gt; \n  select(x, cluster) |&gt; \n  distinct()\n\n# Fit the cluster estimates\nrand_addx &lt;- rand_coefs |&gt; \n  left_join(xc) |&gt; \n  rename(predicted = intercept)\n\ncluster_addx &lt;- cluster_coefs |&gt; \n  select(-x) |&gt; \n  left_join(xc) |&gt; \n  rename(predicted = intercept)\n\nfitrandonly &lt;- glmmTMB(predicted ~ x, \n                         data = rand_addx)\nfitclustonly &lt;- glmmTMB(predicted ~ x, \n                         data = cluster_addx)\n\nfit_randonly &lt;- newdata_nc |&gt; \n  add_predictions(fitrandonly, se.fit = TRUE, re.form = NA) |&gt; \n  rename(predicted = fit, sd = se.fit) |&gt; \n  mutate(model = 'rand')\n\nfit_clusteronly &lt;- newdata_nc |&gt; \n  add_predictions(fitclustonly, se.fit = TRUE, re.form = NA) |&gt; \n  rename(predicted = fit, sd = se.fit) |&gt; \n  mutate(model = 'cluster')\n\n# Fit the points without cluster info\nmodel_nocluster &lt;- glmmTMB(y ~ x, data = sddata$simdata[[3]])\n\nfit_nocluster &lt;- newdata_nc |&gt; \n  add_predictions(model_nocluster, se.fit = TRUE, re.form = NA) |&gt; \n  rename(predicted = fit, sd = se.fit) |&gt; \n  mutate(model = 'nocluster')\n\npreds_withextra &lt;- bind_rows(preds_compare, rand_addx, cluster_addx)\n\nNeed a linetype here to see that the red (fixed effect for cluster, then fit x) and blue (cluster random with x) sit on top of each other. Which seems odd. I expected the random cluster, then fit x would match (i.e. the purple), but that doesn’t. Doesnt’ this imply zero shrinkage? I guess in the slope. So maybe that’s actually right??\n\nggplot(preds_withextra, aes(x = x, y = predicted, \n                          ymin = predicted-sd, \n                          ymax = predicted+sd, \n                          color = model,\n                          linetype = model)) +\n  geom_pointrange(position = position_dodge(width = 0.1)) +\n  geom_line(data = fit_full) +\n  geom_line(data = fit_fixed2) +\n  geom_line(data = fit_randonly) +\n  geom_line(data = fit_clusteronly) +\n  geom_line(data = fit_nocluster)\n\nWhat do I want to do now? Clean up the above so it makes more sense."
  },
  {
    "objectID": "betabinomial/random_uneven_groups_dev.html#cleanup",
    "href": "betabinomial/random_uneven_groups_dev.html#cleanup",
    "title": "Random effects with uneven groups",
    "section": "Cleanup",
    "text": "Cleanup\n##\n\n\nranef(test_full)\nPlot the fit and error\n\nAt mean random\nat each random\n\nThe other stuff above- just points, just cluster means, etc.\nWhat’s the ‘shrinkage’? Can we plot essentially the points they move to a la the website?\nPlot\nNested? Is that where the shrinkage ends up mattering?"
  },
  {
    "objectID": "betabinomial/random_uneven_groups_dev.html#what-do-random-effects-mean",
    "href": "betabinomial/random_uneven_groups_dev.html#what-do-random-effects-mean",
    "title": "Random effects with uneven groups",
    "section": "What do random effects mean?",
    "text": "What do random effects mean?\nSome plots here of how the data looks- points around clusters following the mean.\nFor the situation we have in our data, with each ‘cluster’ (random level) having the same x-value, we can see how the various terms work out to yield the overall distribution given some ‘true’ slope and intercept, random variance, and observation-level variance.\n\nsimdata &lt;- simulate_gaussian_mm(n_per_cluster = 5,\n                                    n_clusters = 10,\n                                    intercept = 1,\n                                    slope = 0.5,\n                                    sigma = 0.1,\n                                    sd_rand_intercept = 0.2,\n                                    sd_rand_slope = 0,\n                                    rand_si_cor = 0,\n                                    cluster_x = \\(x) runif(x, min = 0, max = 10),\n                                    obs_x_sd = 0)\n\nggplot(simdata, aes(x = x, y = y, color = factor(cluster))) + geom_point()\n\nIncreasing random variance\n\nsimdatarv &lt;- simulate_gaussian_mm(n_per_cluster = 5,\n                                    n_clusters = 10,\n                                    intercept = 1,\n                                    slope = 0.5,\n                                    sigma = 0.1,\n                                    sd_rand_intercept = 1,\n                                    sd_rand_slope = 0,\n                                    rand_si_cor = 0,\n                                    cluster_x = \\(x) runif(x, min = 0, max = 10),\n                                    obs_x_sd = 0)\n\nggplot(simdatarv, aes(x = x, y = y, color = factor(cluster))) + geom_point()\n\nIncreasing obs variance\n\nsimdataov &lt;- simulate_gaussian_mm(n_per_cluster = 5,\n                                    n_clusters = 10,\n                                    intercept = 1,\n                                    slope = 0.5,\n                                    sigma = 1,\n                                    sd_rand_intercept = 0.2,\n                                    sd_rand_slope = 0,\n                                    rand_si_cor = 0,\n                                    cluster_x = \\(x) runif(x, min = 0, max = 10),\n                                    obs_x_sd = 0)\n\nggplot(simdataov, aes(x = x, y = y, color = factor(cluster))) + geom_point()\n\nThis will make my life easier moving forward to figure this out:\n\nsimparams &lt;- expand_grid(n_per_cluster = 5, \n                         n_clusters = 10, \n                         intercept = 1, \n                         slope = 0.5,\n                         sigma = c(0.1, 1),\n                         sd_rand_intercept = c(0.5, 2),\n                         sd_rand_slope = 0, \n                         rand_si_cor = 0,\n                         obs_x_sd = 0)\n\n\n# Have to do cluster_x not in the expand_grid-it doesnt' fit in a row.\n# Tibbles *will* hold functions, but it's weirdly hard to get them in there if not handbuilding.\nsimresults &lt;- simparams |&gt; \n  bind_cols(tibble(simdata = purrr::pmap(simparams, simulate_gaussian_mm, \n                                         cluster_x = \\(x) runif(x, min = 0, max = 10))))\n\n\n# I thought there might be a way to ggplot straight out of the nested cols, but not obviously working\nsimresults |&gt; \n  unnest(cols = simdata) |&gt; \nggplot(aes(x = x, y = y, color = factor(cluster))) + \n  geom_point() +\n  facet_grid(sigma ~ sd_rand_intercept)"
  },
  {
    "objectID": "betabinomial/beta_binomial_error_outline.html",
    "href": "betabinomial/beta_binomial_error_outline.html",
    "title": "Beta-binomial model testing",
    "section": "",
    "text": "# I got as far as writing a short outline and loading some libraries about 6 months ago...\n# No sense actually loading these until there's any code.\n# library(tidyverse)\n# library(lme4)\n# library(lmerTest)\n# library(spaMM)"
  },
  {
    "objectID": "betabinomial/beta_binomial_error_outline.html#issues-q-and-approach",
    "href": "betabinomial/beta_binomial_error_outline.html#issues-q-and-approach",
    "title": "Beta-binomial model testing",
    "section": "Issues, Q, and approach",
    "text": "Issues, Q, and approach\nWhat are the questions and how can we set this up to find answers for Sarah and leave the door open for more?\nThe issue\nFundamentally, we have a situation where we’re using ‘observation-level random effects’, but not consistently. It’s inconsistent because the number of observations within a random level depends on natural variation in number of egg masses on rocks, but is unrelated to the question (in Sarah’s case), or it is in fact a feature of the experimental design (looking for DD). In the DD case, we have a situation where we enforce structure on the relationship between n within levels, and in the incidental case, there still might be structure either by chance or as a feature of the system (simply by chance- more low-n rocks end up in chamber 1, riffle 3 has more low-n rocks, despite the same overall riffle-scale N; feature of the system- Aps lays fewer egg masses per rock, and so will be more affected.\nBasic Q: What is the impact of low n within random levels, and how does that impact results, especially when the low n is structured in some way?\nBasic approach: Generate data with known relationships, and vary the way the random structure works to represent the different issues and understand how they affect the ability to recover the relationship- frequency of Type 1 & 2 errors, needed sample size, etc.\nBasic technical approach: Write a function to generate simulated data that allows varying some properties:\n\nThe strength of the x-y relationship\n\nEach random level will be given an x-value that may be independent of n or dependent on n\n\ne.g. rocks are random levels within temp treatments, y ~ temp\ne.g. rocks are random, and y ~ density\n\n\nVariance around the x-y relationship\nrandom variance- how variable are the deviations of the random effects?\n\ni.e. how much does the mean y on each rock deviate from the x expected from that rock’s temp or n or…\n\nNumber of random levels (i.e. how many rocks, not nestedness yet- deal with that later)\nN within random levels\n\nall the same\nfrom a distribution\ncorrelated or not with the x-y relationship (e.g. are we interested in DD)\n\nType of error distributions and random error\nNested random levels- let’s deal with this once we do all the rest\nMulti-x (e.g. Sarah’s situation of species and temp treatment). Like nested randoms, let’s sort out the simple case first.\n\nThen for a given question, loop over some variable of interest and see how it changes the outcome.\nAdditional considerations\nDoes this work differently for binomial responses than gaussian?\nHow do nested random variables modify the results, particularly if the low-n within levels is actually about low nested levels within outer levels (e.g. what if there’s one cluster on a rock, but it has 100 egg masses? what if there are 100 egg masses, but all singletons).\nSome questions we can ask\nWhat is the needed sample size- n within levels? number of random levels with n of some low number? amount of nestedness? We can address these with measurements of Type 1 and Type 2 error, ROI curves, and similar."
  },
  {
    "objectID": "betabinomial/beta_binomial_error_outline.html#implicationspapers",
    "href": "betabinomial/beta_binomial_error_outline.html#implicationspapers",
    "title": "Beta-binomial model testing",
    "section": "Implications/papers",
    "text": "Implications/papers\nComparison of small-n random level issues with gaussian vs logistic (beta-binom)\nIssues of variable-n random levels (with gaussian and logistic), possibly including structure\nUsing random effects models to estimate density-dependence: unavoidable confounding and how to deal with it. (for gaussian and logistic).\nIt will I think also help us understand e.g. Georgia’s paper, and allow analysis of the original experiment, which I’ve been sitting on until we deal with some of this because it’s so blow-uppy. And do a better job with all future work."
  },
  {
    "objectID": "betabinomial/beta_binomial_error_outline.html#some-references",
    "href": "betabinomial/beta_binomial_error_outline.html#some-references",
    "title": "Beta-binomial model testing",
    "section": "Some references",
    "text": "Some references\nsee caddis/Analyses/Testing/errorbarchecks.R for some poking I did on the experiments\nI know I did a bunch of checking different glm functions for Sarah’s data and the t-z issue that might be relevant at some point, in caddis/Analyses/Testing/df_z_t_for_sarah/\nThe visualisations here are really good, and get at a fair amount of this in a way that will help us think through things more clearly, though we’ll want to dig into it more. They also have a function that does some of what we need for data generation. Might as well start there or lightly re-write it. They largely do the first couple main questions, but I think we’ll want to focus things a bit differently.\nsee this discussion at stack exchange and citations therein. Suggests obs-level random effects are fine, you just need reasonable sample size. But we a) don’t have reasonable sample size, and b) our n per random level is variable and often unbalanced\nBen Bolker does a bit with logistic on stack exchange.\nThe ideas here show some useful sorts of diagnostics- we can plot the endmember situations of ignore random or treat random as fixed and then see how using mixed models changes things (e.g. shrinkage). And how that depends on the things we tweak.\nHarrison BB paper\nHarrison, Xavier A. “A Comparison of Observation-Level Random Effect and Beta-Binomial Models for Modelling Overdispersion in Binomial Data in Ecology & Evolution.” PeerJ 3 (July 21, 2015): e1114. https://doi.org/10.7717/peerj.1114.\nHarrison Obs level random effects\nHarrison, Xavier A. “Using Observation-Level Random Effects to Model Overdispersion in Count Data in Ecology and Evolution.” PeerJ 2 (2014): e616. https://doi.org/10.7717/peerj.616.\nThe spaMM repo has links to papers and in-depth user guides."
  },
  {
    "objectID": "betabinomial/beta_binomial_error_outline.html#general-understanding-of-n-within-levels-and-n-levels",
    "href": "betabinomial/beta_binomial_error_outline.html#general-understanding-of-n-within-levels-and-n-levels",
    "title": "Beta-binomial model testing",
    "section": "General understanding of n within levels and n levels",
    "text": "General understanding of n within levels and n levels\nGaussian to start\n- everything will be easier to interpret\nSimulate data of set relationship between x and y with gaussian variance, and random levels with gaussian deviations. Say y is something like growth and the random levels are assigned to temps, for example. Should we use x as factors (e.g. a couple qualitative levels) or continuous (e.g. temperature or density)? Probably both- the math is the same but some of the tests will be more natural to set up or interpret one way or the other, so if we just do everything both ways we’ll cover our bases.\n\nn within random levels and n random levels\nHold these constant within an experiment- i.e. each rock has the same number of egg masses in any given run, but we vary the egg masses per rock and n rocks between loops.\nMight as well do these two questions in one go, factorially.\nHold all random levels to the same n within them, but loop over both that n and the number of levels. So, e.g. assume some number of ‘rocks’ 1:1000, and some number of egg masses per rock, 1:1000. Each rock has the same number of egg masses in any given run.\nAlso will need to vary the usual probability dist parameters- fixed y ~ x relationship, variance, random variance.\n\nChanging random variance will be really important here and throughout- what happens when it is 0?\n\nWe could consider nested randoms here too in a similar way (see Nested Randoms section), though I’m not sure whether it makes more sense to do that for each of these sections as we go, or to address all these sections with simple single randoms first and then go back.\nThis is basically this post, but without the slope-randoms. Though maybe we should consider slope randoms.\n\n\nUnstructured but variable n’s\nThis is also in this post.\nSame as above, but instead of the same n within levels, let it vary. Control that variation with something like a Poisson, where we can loop over the \\(\\lambda\\) and see how the mean and low-inflation changes the outcome. If we want more control over mean and variance, maybe negative binomial and loop over r and p.\nWe will also still want to loop over number of random levels here- we expect the variableness of n within random level will matter less with more levels.\nThis will get very close to what Sarah needs- she essentially has this situation, where the probability just happened to give structure WRT x (where ‘x’ is both species and temp treatment). We can ask specifically about that structure- see “Accidental” structure section below. And I think Sarah’s in an interesting situation where the Species structure isn’t ‘accidental’ at all, but the structure WRT temp is. Either way, if we can understand structure, whether accidental or intrinsic, we’ll be a long way there.\n\n\nUnbalanced designs in terms of number of random levels\nVary the distribution of random levels along x. E.g. maybe rocks are clustered in such a way that more of them are at high temps than low, or they’re clustered at the ends of the distribution, or the middle, or… Basically, capture something like unbalanced designs like 20 rocks in a high treatment and 15 in low, but the n on the rocks is not structured. Likely start this with the same n on all rocks (and loop over that), and then proceed to variable but unstructured n on each rock.\n\n\n“Accidental” structure of n on within random levels\nVary n on rocks systematically with x, looping over how strong that relationship is (e.g. something like vary poisson \\(\\lambda\\) with x, though we might want something more tunable in terms of variance like negative binomial).\nThis is getting very close to what Sarah needs, I think. and getting close to the DD question too, even if Sarah’s structure is accidental (though Species is arguably intrinsic).\nI think this will be where we can get at questions about how correlations between random structure and fixed effects can yield blowups/large errors."
  },
  {
    "objectID": "betabinomial/beta_binomial_error_outline.html#biases-in-the-random-variance",
    "href": "betabinomial/beta_binomial_error_outline.html#biases-in-the-random-variance",
    "title": "Beta-binomial model testing",
    "section": "Biases in the random variance",
    "text": "Biases in the random variance\nI’m not sure if this goes here or earlier, but rather than loop over a range of random variances, with the variance fixed for a given experiment, allow the random variances to differ in a structured way\n\nStructure WRT n- maybe rock deviations are more variable at low n\nStructure WRT x- maybe rock deviations are higher at low or high temp?\n\nThis structure could come about due to simple chance or some underlying unmodelled process, but either way we should try to understand it. See some older thoughts below in the ‘older notes’ section."
  },
  {
    "objectID": "betabinomial/beta_binomial_error_outline.html#stop-here-and-do-the-above-with-beta-binomial",
    "href": "betabinomial/beta_binomial_error_outline.html#stop-here-and-do-the-above-with-beta-binomial",
    "title": "Beta-binomial model testing",
    "section": "Stop here and do the above with beta-binomial",
    "text": "Stop here and do the above with beta-binomial\nWill need to be careful about the probability distributions we use to model the data and randoms. Likely will be easiest to model the data on logit, transform to probability, and then run the models. But we could also just specify binomial probs for each unit (egg mass) and betas for the randoms. That’s clearer in a lot of ways, we’d just need to make sure we look across the range of 0-1, since things will behave differently between 0.98 and 0.999 than they do between 0.5 and 0.6. See Harrison’s beta-binom paper for the math (since he sets his up with Bayesian ideas, he’s actually really explicit about his prob dists).\nAre the answers different from Gaussian? How so? On the logit scale too?\n\nThere might be cases where the beta variance is biased by binomial p that wouldn’t happen with gaussian- see Harrison and my notes below.\n\nWhat are the implications for studies of events vs continuous outcomes?"
  },
  {
    "objectID": "betabinomial/beta_binomial_error_outline.html#density-dependence",
    "href": "betabinomial/beta_binomial_error_outline.html#density-dependence",
    "title": "Beta-binomial model testing",
    "section": "Density-dependence",
    "text": "Density-dependence\nThis was the original impetus for this project:\nIf we’re measuring dd, we necessarily have structure in the n within randoms with respect to x (density).\nWe now have explored how the n within randoms works when y is related to some other x that is unrelated to n within randoms. That serves as an excellent baseline.\nNow, we can do a very similar set of analyses, but define the relationship as being between n (in each random level) and y (this is very close to the ‘accidental structure’ case above) with some variance.\nAgain, start gaussian, then move to logistic\nE.g. say growth ~ density.\n\nVary the number of random levels uniformly\nLoop over the number of random levels, but let there be the same number for each density (or, more accurately, since density is continuous, the same probability distribution of n levels- though we could think of this as a manipulative experiment with some number of density levels and put the same number of ‘rocks’ in each). Likely do both approaches- it makes it more relevant for the different ways people will use this.\n\n\nVary the range of densities considered\ne.g. does starting at 2 do considerably better than 1? Loop over the density range (or set of treatments). Likely focus on the low end: 1 to 2 to 3 to 5 to ….. and not just the lowest value, but the spacing- if we space closely low, e.g. have 1 and 2 and 3 treatments before going to 7, 10, 20, is that better/worse than 1, 5, 10, 20 with the 1 having more reps?\n\n\nUnevenly vary the number of random levels\nCan we unbalance the random levels to efficiently gain power? e.g. if we have one rock with 100 at the high end, do we need 100 rocks with 1 to get the same power? Is it linear like that? Seems unlikely, but we can explore that surface.\n\n\nDo the same with beta-binomial\nAre the results the same as with gaussian? How do they differ, and what are the implications for things like dd infection, predator attack, etc?"
  },
  {
    "objectID": "RpyEnvs/python_updated_functions.html",
    "href": "RpyEnvs/python_updated_functions.html",
    "title": "Updating function defs",
    "section": "",
    "text": "As I develop, I often try a function, tweak it, try again, etc. In R, I can just run the function definition to have access, or source(filewithfunction.R). In python, I could tweak the function, but just trying to use them elsewhere (e.g. in a .qmd) wasn’t working, even if I re-ran import filename. Clearly, there are differences between import in python and source in R. After poking around a bit, it looks like python caches on first import, and so subsequent ones don’t refresh.\nWhat does seem to work is to run importlib.reload(filename). Obviously we wouldn’t put that in a script, but when using an interactive session, it’s really helpful. Not sure why this requires a whole separate package, but it works. See stackoverflow. It appears to be typical to just restart, but that is really prohibitive if the testing involves processing data that took a long time to create.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Small python bits"
    ]
  },
  {
    "objectID": "RpyEnvs/python_setup.html",
    "href": "RpyEnvs/python_setup.html",
    "title": "Python setup",
    "section": "",
    "text": "I’m working on a Python project, and trying to figure out how to set up and get started. I’m used to R, where most simply, all I have to do is download R, Rstudio, and then start coding. R doesn’t need any environment manager to get going, but I do tend to use renv to manage packages, but that’s pretty lightweight and straightforward. And I can start coding without it.\nPython, on the other hand, is more opaque. In part it’s because I’m new to it, but a bit of googling suggests I’m not the only one. It’s likely also because there’s no one dedicated IDE/workflow that almost everyone uses, a la Rstudio (maybe that will change with the Rstudio–&gt; Posit move?).\nSo, I’m going to work out getting setup to code in Python (I sorta did it before, but I’m trying a new way with fewer black boxes). And using this as a place to write notes/what I did as I go. That means this might end up being less a tutorial and more a series of pitfalls, but we’ll see how it goes.\n\n\nI’m trying to get set up to manage Python versions themselves with pyenv and packages with poetry. As far as I can tell, poetry does approximately similar things to renv (but more complicated because python). And I haven’t used something similar to pyenv to manage R versions themselves, but I am about to have to figure that out too because a lot of packages broke when I moved to R 4.2. Will probably try rig, and write another one of these. I’m assuming I’ll code primarily in VS Code, unless Posit suddenly runs python like R (without reticulate). Even then, remote work will all use VS Code for the time being. I’m loosely following https://briansunter.com/blog/python-setup-pyenv-poetry and https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/, and doing all the actual setup in VS Code, not Rstudio.\nRealising after I wrote this that I probably could have actually done all of this inside quarto- I think I can run powershell/system code in code blocks?",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Python environments with pyenv and poetry"
    ]
  },
  {
    "objectID": "RpyEnvs/python_setup.html#the-issue",
    "href": "RpyEnvs/python_setup.html#the-issue",
    "title": "Python setup",
    "section": "",
    "text": "I’m working on a Python project, and trying to figure out how to set up and get started. I’m used to R, where most simply, all I have to do is download R, Rstudio, and then start coding. R doesn’t need any environment manager to get going, but I do tend to use renv to manage packages, but that’s pretty lightweight and straightforward. And I can start coding without it.\nPython, on the other hand, is more opaque. In part it’s because I’m new to it, but a bit of googling suggests I’m not the only one. It’s likely also because there’s no one dedicated IDE/workflow that almost everyone uses, a la Rstudio (maybe that will change with the Rstudio–&gt; Posit move?).\nSo, I’m going to work out getting setup to code in Python (I sorta did it before, but I’m trying a new way with fewer black boxes). And using this as a place to write notes/what I did as I go. That means this might end up being less a tutorial and more a series of pitfalls, but we’ll see how it goes.\n\n\nI’m trying to get set up to manage Python versions themselves with pyenv and packages with poetry. As far as I can tell, poetry does approximately similar things to renv (but more complicated because python). And I haven’t used something similar to pyenv to manage R versions themselves, but I am about to have to figure that out too because a lot of packages broke when I moved to R 4.2. Will probably try rig, and write another one of these. I’m assuming I’ll code primarily in VS Code, unless Posit suddenly runs python like R (without reticulate). Even then, remote work will all use VS Code for the time being. I’m loosely following https://briansunter.com/blog/python-setup-pyenv-poetry and https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/, and doing all the actual setup in VS Code, not Rstudio.\nRealising after I wrote this that I probably could have actually done all of this inside quarto- I think I can run powershell/system code in code blocks?",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Python environments with pyenv and poetry"
    ]
  },
  {
    "objectID": "RpyEnvs/python_setup.html#getting-started--systemwide-installations",
    "href": "RpyEnvs/python_setup.html#getting-started--systemwide-installations",
    "title": "Python setup",
    "section": "Getting started- systemwide installations",
    "text": "Getting started- systemwide installations\nBoth those websites are working on Unix and Mac, so while step 1 is install pyenv, we aren’t going to apt-get or brew install. In fact, the pyenvgithub says we need to use a windows fork. Things already getting nonstandard. Should I just run everything in Windows Subsystem for Linux? Maybe, but I’d like to just use windows if possible, as much as I like WSL.\n\npyenv\n\nWindows\nGuess I’ll just start at Quick start. Going to use powershell directly rather than inside vs code here, because vs code likes to open in recent directories instead of globally, and I think I want pyenv system-wide.\nStep 1- install in powershell with Invoke-WebRequest -UseBasicParsing -Uri \"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\" -OutFile \"./install-pyenv-win.ps1\"; &\"./install-pyenv-win.ps1\"\nCan’t run scripts (new computer). Sends me to https:/go.microsoft.com/fwlink/?LinkID=135170.\n\nSetting the policy to just the running process. Will probably regret that when I next try to run a script, but for now I don’t really want universal unrestricted powershell scripts.\n\n\nPowershell script permissions\nAside- it starts to get really annoying because pyenv runs scripts, so will need to fix. Get an error when I try to change Scope to CurrentUser because of a group policy. Setting it to Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope LocalMachine seems to work, despite apparently being a larger set than my User.\nIt says it wasn’t installed successfully, but when I try again it says it’s there. I guess push on?\n\nStep 2 was just to shut down and reopen powershell\nStep 3- Run pyenv --version. If you haven’t changed policy to something larger than Process, this will fail because of the ExecutionPolicy. Guess I need to turn it back on. I kept doing one-offs for a while until I got annoyed and then set it to LocalMachine (see above).\nStep 4, pyenv install -l lists a million python versions. Seems like a good thing. I’m going to need older versions in projects, but for now, let’s install the latest.\nStep 5- the install of python. There’s a 3.12.a1, which I’m assuming means alpha, so I’ll go with pyenv install 3.11.0, which is the most recent without the .a1.\nThe install seems to have worked.\nStep 6- set global pyenv global 3.11.0\nStep 7- check it worked pyenv version \nStep 8- check python with python -c \"import sys; print(sys.executable)\"\nworks, gives me the path, \\.pyenv\\pyenv-win\\versions\\3.11.0\\python.exe\nNow it says to validate by closing and reopening- do that. Now pyenv --version gives the version, while pyenv alone tells us the commands. They’re also all at the github page I’m following.\nThen try in a vs code terminal, also works. Note that using the git bash terminal instead of powershell bypasses the script permissions issue if it hasn’t been set larger than Process.\n\n\nLinux\nThis is much easier. Install with curl https://pyenv.run | bash. That tells you to add somethign to .bashrc, do that. Then close and restart bash, and run pyenv install 3.11.0 or whatever version we’re using. I needed to sudo apt-get install libffi-dev to get it to compile some C bits.\nNow, we have Python 3.11 as the global python version, but should be able to install other versions and use them in local projects. Assuming I’ll get there once I set up poetry.\n\n\n\npoetry\n\nWindows\nOnce again, the websites I’m following have commands for mac/unix, so back to the main poetry page to sort this out on windows.\nAgain, powershell command on windows. Could I use the git bash in vs? Maybe? Just stick with powershell. (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -.\nThe instructions say that last bit should be py instead of python if python not installed from Microsoft Store, but had to use python anyway.\nseems to have worked- \nThe instructions then have an advanced section I’m skipping, but that install message above seems to match step 3, where we add Poetry to the PATH in order to run Poetry by just running poetry and not the full path.\nI could change it with powershell, but the instructions I found involved a bunch of regex. Instead, search for “Advanced System Settings”, then in the bottom right, click Environment Variables, then in the System Variables box, click on Path, then Edit button, then New. That creates a blank line, paste in the path from poetry install, or use the one from their website, %APPDATA%\\Python\\Scripts. OK out of all the system settings boxes.\nShut down powershell, then fire it back up and run poetry --version. If the path setting failed, it won’t be able to find poetry, if it worked, it’ll give the version number.\nThat worked for me once, but now isn’t working the next day. Actually, it works in powershell, but not vs code powershell. It wasn’t unpacking the %APPDATA% correctly - running (Get-ChildItem env:path).Value lists %APPDATA% instead of the expanded directory. I stuffed the full path as in the screenshot above in the PATH, restarted VS Code and it works. Interestingly though, I left the %APPDATA% version there too, and it’s now unpacked when I run (Get-ChildItem env:path).Value.\n\n\nLinux\nAs at the docs, install with curl -sSL https://install.python-poetry.org | python3 -\nadd export PATH=\"/home/azureuser/.local/bin:$PATH\" to ~/.bashrc, start a new shell and make sure it worked with poetry --version.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Python environments with pyenv and poetry"
    ]
  },
  {
    "objectID": "RpyEnvs/python_setup.html#setting-up-a-project",
    "href": "RpyEnvs/python_setup.html#setting-up-a-project",
    "title": "Python setup",
    "section": "Setting up a project",
    "text": "Setting up a project\nLet’s first say we’re going to use a different-than-standard version of python, so install that with pyenv. For this test, let’s just use 3.8.9. Not really any particular reason. So, run pyenv install 3.8.9\nNow, pyenv versions (NO FLAGS- the “--” flag will give the version of pyenv) shwos two versions with an asterisk by global.\n\n\nCreate the project\nJust need a directory and cd inside it, I think. I’ll make it inside the directory with this qmd. mkdir pytesting, cd pytesting. Actually, this yields too much nesting. poetry builds a directory for the project, and another directory in that, so this just yields annoying levels of nesting. Call poetry new (see below) from the directory you want to contain the main project directory.\n\n\nSet the python version\nI think just pyenv local 3.8.9. That creates a .python-version file in the directory, which seems to be the idea.\n\n\nSet poetry\nAm I going to completely screw up my R project having this inside it? Guess we’ll find out.\npoetry new pytesting then creates another directory and returns “Created package pytesting in pytesting”. It builds the outer directory, so don’t make one first or the nesting gets silly.\nThat directory seems to be establishing a standard package structure and the lockfiles etc. Opening the .toml looks like it didn’t pick up the python version though- it’s using 3.11. Hmmm. Tried killing and restarting powershell and it’s still doing that. Not sure why it’s not picking up the local python.\nIf I move up a directory, the pyenv versions returns back to 3.11. So pyenv seems to be working, but poetry’s not picking it up. I guess I can change in manually, but that’s annoying.\nSeems to be a long-running known issue- recent posts here https://github.com/python-poetry/poetry/issues/651. Ignore for now, maybe fix manually if it becomes an issue. I tried the solution in the last post (poetry config virtualenvs.prefer-active-python true), and it didn’t fix it. Tried completely starting over a few times. No luck. Worry about that later. I guess that means the pyenv stuff might be useless for the moment- will need to use the version poetry thinks it has in the directory. It does look like can reinstall poetry, but that seems like a pain. (No one else seems to have issues with the config above).\nSo, I’ve just set the pyenv to the global for now, and moving ahead with poetry for the project. I guess I could change pyenv global each time I switch projects as an annoying workaround if it becomes an issue. An answer might be poetry env use , see https://python-poetry.org/docs/managing-environments/.\n\n\nUsing poetry for dependencies\nMuch like renv can install all dependencies from info in renv.lock, we could build the project with dependencies from pyproject.toml. Or, also like renv, as we’re developing a project, we can add iteratively. Let’s do that, since that’s what we’re doing.\nThere seems to be an intermediate step here though, running poetry install to initialise a virtual environment from the .toml. I assume this would install whatever’s in the toml, but we don’t have anything at present. That apparently creates a virtual environment somewhere globally (.cache/, according to https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/).\nAnd now I have poetry.lock . According to the docs, this takes precedence over the .toml, though I doubt that’s true for python version itself. This is what gets committed to share the project.\nTyping poetry shell at the command line activates the environment. But how do we activate it for a VS code session?\nSeems to just be active once we open a .py file in that directory (e.g. if we open the file, then a powershell at that location, it appears with pyenv shell already going.\nTo test adding a dependency, i’ll try numpy. First, I can run simply python code- a = 1 etc. But import numpy as np fails (as expected)- ModuleNotFoundError: No module named ‘numpy’. So, click back to the powershell terminal, and try poetry add numpy. It resolves dependencies and writes a lock file.\nAnd yet, if I try import numpy as np, same error. The poetry show command lists it as installed.\nSo, it’s because VScode doesn’t know where to find the venvs. On a one-off basis, can use poetry env info --path to get the path, then in VS code select interpreter (ctrl-shift-p for the search thingy), then paste in that path. But that’s annoying.\nI’m trying getting to the search thing, then User settings, then adding C:\\Users\\galen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ to the venv Folders. That seems to work, but it may just be remembering from last time.\nThe most robust option might be to not keep the venvs in .cache but instead local to the project, as described here. Looks like that’s done with poetry config virtualenvs.in-project true at the very outset, and then rebuilding the project (and it should persist for future projects).\nAnd that’s what we’re doing in the project I’m doing this for. So let’s do that. First, run poetry env list to get the name, then poetry env remove NAME to delete. But it failed, seemingly partway. So also go to AppData\\Local\\pypoetry\\Cache\\virtualenvs and delete the folder. Restart vs and now poetry env list doesn’t return anything.\nTo set the config, poetry config virtualenvs.in-project true . I believe that’s global (I ran it outside the project).\nNow rebuild from .toml in the project with poetry install. The .venv is there now, but VS still can’t find it.\nOK, shut everything down, and instead of opening VS again as usual, I opened it and then opened a new window, and now it works. It was somehow setting the root based on where VS happened to open, rather than based on where files were. That probably makes sense for a git repo with just python, but broke here. I assume we need to add the venv directory to the gitignore.\nJust did poetry add pandas and it works and I immediately have access to it in a python script.\n\n\nComplications with add\nTo add a specific version of a package, use\npoetry add packagename==2.0.5\nThere are a number of other ways to specify version ranges, installing from git, etc.\nTo install from git, dpending on https or ssh or different branches\n# For the main branch\npoetry add git+https://github.com/sdispater/pendulum.git\npoetry add git+ssh://git@github.com/sdispater/pendulum.git\n\n# For other branches\npoetry add git+https://github.com/sdispater/pendulum.git#develop\npoetry add git+https://github.com/sdispater/pendulum.git#2.0.5\n\npoetry add git+ssh://github.com/sdispater/pendulum.git#develop\npoetry add git+ssh://github.com/sdispater/pendulum.git#2.0.5\n\n\nVS code note\nSometimes VS seems to find the poetry venv and use it, and other times (I think if it’s not at the head dir of the workspace?) it needs to be pointed at the python.exe. To do that, open the command palette, (ctrl-shift-p), select python interpreter, then .venv\\scripts\\python.exe wherever that venv is.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Python environments with pyenv and poetry"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html",
    "href": "RpyEnvs/py_r_project_overview.html",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "I’m working on a project that needs both Python and R, and they need to talk to each other. Most of the work is in R, but the first bit is Python, and more importantly, it has to call a python package (more in future, likely). I’m packaging everything up so the user doesn’t need to do everything through the repo, but can just library(package) and be off and running, but that’s complicated by the fact that any actual use of this project needs both languages.\nIt seems like there are a couple options to how I deal with the two languages.\n\nHave a python package and an R package, and make the user install both and manage the bilinguilism in their own scripts using the packages.\n\nSeems most cumbersome for user, but the users are fairly involved in the project, so could work\n\nSkip creating my own python package entirely, and just have the bits of python wrapper in inst/python\n\nLikely how I’ll start to figure out how to include python in package\n\nAnd see below- I think it’ll be easier to rapidly iterate the py code.\n\nAs python side grows, this will get very cumbersome\nIs it slower than the first option?\n\nMake my own python package, and wrap that in inst/python\n\nI think this is actually the best option, since I won’t have to maintain two copies of py code\nWill also allow a user to just use option 1 (or for the whole project to move that way in future, or shifting functions over to python).\nIt will make the R package more cumbersome than option 1, but I think it’s the best tradeoff.\nIt will also slow down initial dev- as I change the python side, I’ll have to rebuild the py package and re-install it into my environment. There’s no obvious analogue to devtools::load_all that can reach across and do all that. Whereas I think if I have my py code in inst it’ll get refreshed when I load_all.\n\n\nAll of these will follow {reticulate} docs, but those are fairly sketchy as to how to actually write code that works.\n\nhttps://rstudio.github.io/reticulate/articles/package.html (seems to conflict a bit with the next)\nhttps://rstudio.github.io/reticulate/articles/python_dependencies.html\nhttps://rstudio.github.io/reticulate/articles/calling_python.html\n\n\n\nhttps://stackoverflow.com/questions/72185273/reticulate-fails-automatic-configuration-in-r-package\nhttps://github.com/rstudio/reticulate/issues/997\nWill need to test how this works both when we already have a venv and when we don’t.\n\n\n\nas I’m developing\nThis works from console, after I put the file in the inst/python\nreticulate::source_python(file.path('inst', 'python', 'controller_functions.py'))\ntested with\nscene_namer(file.path('inst', 'extdata', 'testsmall'))\nAt first it didn’t work to have the Config/retictulate in description and the source_python in .onLoad. I didn’t get anything. Then I fixed it by adding it to the package’s global environment with the argument envir = globalenv(), and it started working. But only when I already have a python environment. If I try to get the Config/reticulate to build a python environment with the necessary dependencies in a bare project, it won’t even install the package.\nI think the issue there is that the way the config/reticulate and onload are set up, it doesn’t try to install dependencies until Python is used, and for some reason running that file in onLoad doesn’t trigger it.\nSo, somehow I need to be able to install the package without needing the py dependencies, and then install them the first time someone types library(packagename) or otherwise tries to use it.\nThat’s a whole new thing to figure out, I think.\nIf we assume the user has the python dependencies installed and accessible to reticulate, then the Config/reticulate works with the following .onLoad.\n.onLoad &lt;- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n\n  reticulate::source_python(system.file(\"python/py_functions.py\", package = 'packagename'), envir = globalenv())\n\n}\nSee some initial work I’ve done sorting this out.\n\n\n\nThe above method works to expose python functions, but the specific ones I have take dicts and lists as arguments. How do I pass those from R? We can’t just assign them to a variable in R, because those formats don’t work- e.g. we cannot create the lists and dicts in R to pass.\n\noutputType = ['summary', 'all']\n\nallowance ={'minThreshold': MINT, 'maxThreshold': MAXT, 'duration': DUR, 'drawdown': DRAW}\n\nI have sorted out how to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. See my testing of type-passing (well, if I could get it to render in quarto).\n\n\n\nIs there a way to auto-build the Config/reticulate part of DESCRIPTION? Maybe from pyproject.toml? Similar to the way usethis::use_package installs automatically add them to renv? That’s the other way though, so maybe it’s moot. Still, would be nice to automate.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Overview of building R-py package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html#other-useful-sites",
    "href": "RpyEnvs/py_r_project_overview.html#other-useful-sites",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "https://stackoverflow.com/questions/72185273/reticulate-fails-automatic-configuration-in-r-package\nhttps://github.com/rstudio/reticulate/issues/997\nWill need to test how this works both when we already have a venv and when we don’t.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Overview of building R-py package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html#notes",
    "href": "RpyEnvs/py_r_project_overview.html#notes",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "as I’m developing\nThis works from console, after I put the file in the inst/python\nreticulate::source_python(file.path('inst', 'python', 'controller_functions.py'))\ntested with\nscene_namer(file.path('inst', 'extdata', 'testsmall'))\nAt first it didn’t work to have the Config/retictulate in description and the source_python in .onLoad. I didn’t get anything. Then I fixed it by adding it to the package’s global environment with the argument envir = globalenv(), and it started working. But only when I already have a python environment. If I try to get the Config/reticulate to build a python environment with the necessary dependencies in a bare project, it won’t even install the package.\nI think the issue there is that the way the config/reticulate and onload are set up, it doesn’t try to install dependencies until Python is used, and for some reason running that file in onLoad doesn’t trigger it.\nSo, somehow I need to be able to install the package without needing the py dependencies, and then install them the first time someone types library(packagename) or otherwise tries to use it.\nThat’s a whole new thing to figure out, I think.\nIf we assume the user has the python dependencies installed and accessible to reticulate, then the Config/reticulate works with the following .onLoad.\n.onLoad &lt;- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n\n  reticulate::source_python(system.file(\"python/py_functions.py\", package = 'packagename'), envir = globalenv())\n\n}\nSee some initial work I’ve done sorting this out.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Overview of building R-py package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html#passing-types",
    "href": "RpyEnvs/py_r_project_overview.html#passing-types",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "The above method works to expose python functions, but the specific ones I have take dicts and lists as arguments. How do I pass those from R? We can’t just assign them to a variable in R, because those formats don’t work- e.g. we cannot create the lists and dicts in R to pass.\n\noutputType = ['summary', 'all']\n\nallowance ={'minThreshold': MINT, 'maxThreshold': MAXT, 'duration': DUR, 'drawdown': DRAW}\n\nI have sorted out how to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. See my testing of type-passing (well, if I could get it to render in quarto).",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Overview of building R-py package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html#question",
    "href": "RpyEnvs/py_r_project_overview.html#question",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "Is there a way to auto-build the Config/reticulate part of DESCRIPTION? Maybe from pyproject.toml? Similar to the way usethis::use_package installs automatically add them to renv? That’s the other way though, so maybe it’s moot. Still, would be nice to automate.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Overview of building R-py package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html",
    "href": "RpyEnvs/py_in_rpkg.html",
    "title": "Wrapping python in an R package",
    "section": "",
    "text": "I’ve been slowly sorting this out with trial and error, but haven’t really come up with a satisfactory solution. I have things that work, but they’re not ideal- often either make the user do too much, or clutter up the global environment with python objects as soon as we library in the package. So I’ve created a test repo to build a package and test different options.\nWhat do I want? There are potentially a few different things we might want to do, and I may or may not get to all of them.\n\nAccess functions in an existing python package\nAccess a small set of functions in inst/python in the R package\n\nOne option here is clearly to make these a py package and revert to (1)\n\nThe function reticulate::import_from_path just imports a module like reticulate::import, it just does so from a local path, and so these are functionally equivalent.\n\nThese may only use base python (simplest, but unlikely)\nThese may depend on python packages (most likely)\n\nAuto-install (or at least help the user install) necessary python dependencies",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#dev-setup",
    "href": "RpyEnvs/py_in_rpkg.html#dev-setup",
    "title": "Wrapping python in an R package",
    "section": "Dev setup",
    "text": "Dev setup\nWe need a python environment for the package (needed for dev, we’ll sort out how they get configured for users below). I’ve gone with just poetry init and poetry install in the git repo/ outer project directory, which doesn’t create a full python package structure but will at least create a .venv, which is all I really need.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#wrappers-to-keep-r-clean",
    "href": "RpyEnvs/py_in_rpkg.html#wrappers-to-keep-r-clean",
    "title": "Wrapping python in an R package",
    "section": "Wrappers to keep R clean",
    "text": "Wrappers to keep R clean\nOne option might be instead of import ing in .onLoad to import within R wrapper functions. Then the python never ends up in the user’s environment. And we can document the functions in R (via the wrappers). And, we can deprecate py functions through time by simply re-writing them into the R wrappers (this applies mostly to setup-type functions, not big things like all of scipy or something). But, does that yield weird things when the package is loaded?\nThis works- writing and exporting an R function that does the onload, e.g.\nadder_wrap &lt;- function(x,y) {\n  adder &lt;- reticulate::import_from_path(\"adder\",\n                                        path = system.file(\"python\",\n                                                           package = 'PyInRpkg'),\n                                        delay_load = TRUE)\n  adder$adder(x,y)\n}\nkeeps everything clean, and we can document and test the function in R. The downside is that the first time we run that function, it is really slow. Likely on the same order as the increased time library(package) takes when the import is on .onLoad). It does get much faster subsequently, so this may not be a huge issue, though it’s possible if we were calling it thousands of times there would be significant overhead (but then maybe that would suggest a different flow, writing whatever loop in python and import ing that loop.\nThe other advantage here is that in more complex situations, where we need to pass objects of more complex types (dates, lists, dicts), we can control that in the wrapper function, letting the user pass in standard R types and converting them, rather than make the user know that named lists become dicts, as they would need to when the adder module itself is exposed.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#functions-with-py-dependencies",
    "href": "RpyEnvs/py_in_rpkg.html#functions-with-py-dependencies",
    "title": "Wrapping python in an R package",
    "section": "Functions with py dependencies",
    "text": "Functions with py dependencies\nI’m not super interested in just wrapping a python package. But what I will need to do is have the R-package-specific python depend on other python packages; it won’t be as simple as adder.\nThat means we have to do two things- managing a python environment and checking to see if the methods above (both through .onLoad or in wrapper functions) work to import python modules with dependencies.\nIn any case, the python module that comes with the package should have functions that do all the python interaction; e..g. we don’t really want to be passing in and out of python a bunch. That makes things like method chaining work how they’re supposed to.\nI think I’ll use pandas to test since it’s common, and will be a bit more interesting/complex than numpy, for example.\nThat works just as before- if we have a function in /inst/python/pdsummary.py,\nimport pandas as pd\n\ndef pdsummary(df, group):\n  summarydf = df.groupby(group).mean()\n  return(summarydf)\nWe can use .onLoad to get it into the global environment and pass it R dataframes,\n.onLoad &lt;- function(libname, pkgname) {\n  pdsummary &lt;&lt;- reticulate::import_from_path(\"pdsummary\",\n                                         path = system.file(\"python\",\n                                                            package = 'PyInRpkg'),\n                                         delay_load = TRUE)\n}\nand it works as before with the module$method notation\npdsummary$pdsummary(iris, 'Species')\nIf I have a few functions in the module, can I import the module at the top of a .R with a bunch of wrappers, and then use it in each? Or does that load immediately?\nIf we make a few py functions in one file, some of which depend on pandas\nimport pandas as pd\n\ndef pdmean(df, group):\n  df = df.groupby(group).mean()\n  return(df)\n  \ndef pdvar(df, group):\n  df = df.groupby(group).var()\n  return(df)\n\ndef pdselect(df, cols):\n  df = df[cols]\n  return(df)\n  \ndef divide(x,y):\n  return(x/y)\nThen we can import in .onLoad and have access to all of them with multipy$NAME\n.onLoad &lt;- function(libname, pkgname) {\n  multipy &lt;&lt;- reticulate::import_from_path(\"multipython\",\n                                         path = system.file(\"python\",\n                                                            package = 'PyInRpkg'),\n                                         delay_load = TRUE)\n}\nAnd we can use them\nmultipy$pdmean(iris, 'Species')\nmultipy$pdvar(iris, 'Species')\nmultipy$pdselect(iris, c('Species', 'Sepal.Length'))\nmultipy$divide(10, 5)\n\nWrapping in various ways\nAnd, as before, we can do the import inside wrapper functions, and then it doesn’t clutter up the world and only gets imported on first use. We have to @export them, and we can cheat a bit and use the … as the argument. This is a bit dangerous- the user needs to know the arguments to the python function, since they won’t really end up documented in the R wrapper. It’s not best practice, but it does work.\n#' wrap pandas grouped mean\n#'\n#' @param df\n#' @param group\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_mean &lt;- function(df, group) {\n  multipy &lt;- reticulate::import_from_path(\"multipython\",\n                                           path = system.file(\"python\",\n                                                              package = 'PyInRpkg'),\n                                           delay_load = TRUE)\n  return(multipy$pdmean(df, group))\n}\n\n#' wrap pandas grouped var\n#'\n#' @param ... df, character group column\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_var &lt;- function(...) {\n  multipy &lt;- reticulate::import_from_path(\"multipython\",\n                                           path = system.file(\"python\",\n                                                              package = 'PyInRpkg'),\n                                           delay_load = TRUE)\n  return(multipy$pdvar(...))\n}\n\n#' wrap pandas column select\n#'\n#' @param ... df, columns to select as character vector\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_select &lt;- function(...) {\n  multipy &lt;- reticulate::import_from_path(\"multipython\",\n                                           path = system.file(\"python\",\n                                                              package = 'PyInRpkg'),\n                                           delay_load = TRUE)\n  return(multipy$pdselect(...))\n}\n\n#' Wrap python divide\n#'\n#' @param ... two values\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_divide &lt;- function(...) {\n  multipy &lt;- reticulate::import_from_path(\"multipython\",\n                                           path = system.file(\"python\",\n                                                              package = 'PyInRpkg'),\n                                           delay_load = TRUE)\n  return(multipy$divide(...))\n}\nIt still is slow on the first use and fast on subsequent, even when calling different functions.\nwrap_mean(iris, 'Species')\nwrap_var(iris, 'Species')\nwrap_select(iris, c('Species', 'Sepal.Length'))\nwrap_divide(10,5)\nIf we don’t want the import inside each function, we can put it at the head of the Rscript. In the demo package, I demo this with a new set of python functions in multi2.py\nimport pandas as pd\n\ndef pdmin(df, group):\n  df = df.groupby(group).min()\n  return(df)\n  \ndef pdsum(df, group):\n  df = df.groupby(group).sum()\n  return(df)\n  \ndef minus(x,y):\n  return(x-y)\nAnd then wrap it with the import outside the functions\nmulti2 &lt;- reticulate::import_from_path(\"multi2\",\n                                       path = system.file(\"python\",\n                                                          package = 'PyInRpkg'),\n                                       delay_load = TRUE)\n\n#' Wrap pandas group sd\n#'\n#' @param df\n#' @param group\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_min &lt;- function(df, group) {\n  return(multi2$pdmin(df, group))\n}\n\n#' Wrap pandas group sum\n#'\n#' @param df\n#' @param group\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_sum &lt;- function(df, group) {\n  return(multi2$pdsum(df, group))\n}\n\n#' wrap python subtraction\n#'\n#' @param df\n#' @param group\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_minus &lt;- function(df, group) {\n  return(multi2$minus(df, group))\n}\nAnd that still works\nwrap_min(iris, 'Species')\nwrap_sum(iris, 'Species')\nwrap_minus(5,4)\nJust like before, that is slow the first time, and fast subsequently. It does not put the imported module into the R environment, so as far as I can tell, it’s equivalent to when we put the module import in each function. It’s just a bit cleaner to only have the import line once.\nThat made me think maybe we could do the import with &lt;- instead of &lt;&lt;- in .onLoad, and then use that in functions, but it doesn’t work that way, unfortunately.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#setup",
    "href": "RpyEnvs/py_in_rpkg.html#setup",
    "title": "Wrapping python in an R package",
    "section": "setup",
    "text": "setup\nTo make this easier on myself, the commands to set this up after I create the project are\n\nrenv::install('devtools')\n\nThe following package(s) will be installed:\n- devtools [2.4.5]\nThese packages will be installed into \"~/AppData/Local/R/cache/R/renv/library/galen_website-f20f9fdb/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing devtools ...                       OK [linked from cache]\nSuccessfully installed 1 package in 31 milliseconds.\n\n# I have the package in /Documents, so.\ndevtools::install_local('~/py_in_rpkg')\n\nWarning in normalizePath(path.expand(path), winslash, mustWork):\npath[1]=\"C:/Users/galen/Documents/py_in_rpkg\": The system cannot find the file\nspecified\n\n\nError : Could not copy `C:\\Users\\galen\\Documents\\py_in_rpkg` to `C:\\Users\\galen\\AppData\\Local\\Temp\\RtmpOUTEck\\file7a70585350fa`\n\nlibrary(PyInRpkg)",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#no-configreticulate-no-preexisting-python",
    "href": "RpyEnvs/py_in_rpkg.html#no-configreticulate-no-preexisting-python",
    "title": "Wrapping python in an R package",
    "section": "No Config/reticulate , no preexisting python",
    "text": "No Config/reticulate , no preexisting python\nInstall works, library works.\nRunning adder_wrap(1,3) kicks off a conda install of a bunch of python stuff. It then insalls a bunch of python and packages, and while the original adder_wrap call gets lost, a new one works fine.\nWhen I try to use wrap_min, though, I get\nError: Error loading Python module multi2\nThis is a cryptic error, I think because the wrap_min function is in the R file that uses import_from_path at the head of the R file. If I use wrap_mean, which is in the R file that uses import_from_path inside each wrapper function, I get\nError in py_module_import(module, convert = convert) :    ModuleNotFoundError: No module named 'pandas'\nThat’s more helpful.\nI think before I move on to trying to manage the package dependencies with Config/reticulate or pre-loading, I want to try two things-\n\nJust closing and reopening- does the whole python install have to happen again?\n\nNo.\n\nStarting another clean directory- does it grab the python env we just built, or does it build a new one per-project?\n\nIt builds a new one, but MUCH faster- must be cross-linking packages and/or python itself",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#pre-setting-py-env",
    "href": "RpyEnvs/py_in_rpkg.html#pre-setting-py-env",
    "title": "Wrapping python in an R package",
    "section": "Pre-setting py env",
    "text": "Pre-setting py env\nInstead of starting blank, what if I manually control the python environment by initializing a poetry project? e.g. go create a poetry project poetry new prepoetry, then cd prepoetry, poetry add pandas to create a .venv with pandas installed. Then create the R project in prepoetry directory.\nNow, when I use adder_wrap(1,5), it works without building any new python envs. As usual, the first run is slow, later are fast.\nAnd both wrap_min(iris, 'Species') and wrap_mean(iris, 'Species') work (import_from_module external and internal to the functions, respectively). No additional python building has to happen. First run is slow (even when adder_wrap has been used), but later are fast, even across R functions in different files, as here. So it must have to do some behind the scenes python the first time we use pandas.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#using-configreticulate",
    "href": "RpyEnvs/py_in_rpkg.html#using-configreticulate",
    "title": "Wrapping python in an R package",
    "section": "Using Config/reticulate",
    "text": "Using Config/reticulate\nIn theory, including a Config/reticulate: entry in the DESCRIPTION should handle the dependencies. I think those docs also say that we will need a .onLoad with reticulate::configure_environment to get it to work.\nFirst, add\nConfig/reticulate:\n  list(\n    packages = list(\n      list(package = \"pandas\")\n    )\n  )\nTo the description. I’m ignoring the version and pip arguments for now.\nI’ll need to do a few tests here-\n\nBare directory, no py environments\nExisting py environments with the dependencies already\nExisting py environments without the right dependencies\n\nI’m starting without the reticulate::configure_environment in .onLoad so I can see why/when it’s necessary.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#bare-no-py",
    "href": "RpyEnvs/py_in_rpkg.html#bare-no-py",
    "title": "Wrapping python in an R package",
    "section": "Bare, no py",
    "text": "Bare, no py\nEverything works, even without the reticulate::configure_environment. It installs python and pandas on the first use of a function, and the adder_wrap, wrap_min, and wrap_mean all work.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#existing-py-env-with-pandas",
    "href": "RpyEnvs/py_in_rpkg.html#existing-py-env-with-pandas",
    "title": "Wrapping python in an R package",
    "section": "Existing py env with pandas",
    "text": "Existing py env with pandas\nAs above, set up a .venv with poetry , then put an R project in it. Everything works, but it worked before too, since there’s no python configuring that needs to happen.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#existing-py-env-without-pandas",
    "href": "RpyEnvs/py_in_rpkg.html#existing-py-env-without-pandas",
    "title": "Wrapping python in an R package",
    "section": "Existing py env without pandas",
    "text": "Existing py env without pandas\nI’ll do the same thing as above, but build a .venv with numpy but not pandas.\nIn this case, as soon as we use any function, it installs pandas into the .venv, just as it did before for both python and pandas. All the functions work.\nSo, what is the point of configure_environment? I thought it was to deal with this? Maybe it’s that I need it to alter python once I’m already using it in a session? E.g. we have a python version running with reticulate for something else, and we then want to use this package? I guess I’ll try that.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#active-in-session-python-without-pandas",
    "href": "RpyEnvs/py_in_rpkg.html#active-in-session-python-without-pandas",
    "title": "Wrapping python in an R package",
    "section": "Active in-session python without pandas",
    "text": "Active in-session python without pandas\nI’ll start a session in a project with numpy, and use reticulate to do some simple numpy to get python running,\nnp &lt;- reticulate::import('numpy')\nnp$array(list(c(1, 2, 3, 4), c(5, 6, 7, 8), c(9, 10, 11, 12)))\nThen I’ll install the package.\nWhen I try to use it, adder_wrap works, but the pandas-based functions don’t- same errors as earlier, and it hasn’t added pandas to the venv.\n\nusing .onLoad\nIf I add a .onLoad function to zzz.R, e.g.\n.onLoad &lt;- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n}\nThen it works. I can load up numpy as before, but as soon as I library(PyInRpkg), it installs pandas, and then the functions work.\nSo, that seems to be the way to avoid weird runtime pitfalls in interactive sessions. It’s a particular case, but likely a fairly common one, so seems like the way to go.\nDoes having that in .onLoad affect any of the other ways? e.g. bare projects or preexisting but inactive python environments? Seems to work everywhere.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#passing-types",
    "href": "RpyEnvs/py_in_rpkg.html#passing-types",
    "title": "Wrapping python in an R package",
    "section": "Passing types",
    "text": "Passing types\nWe might need to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. This is where wrapper functions can be very helpful. See my testing of type-passing.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#python-venv-locations",
    "href": "RpyEnvs/py_in_rpkg.html#python-venv-locations",
    "title": "Wrapping python in an R package",
    "section": "Python venv locations",
    "text": "Python venv locations\nI have run into issues with reticulate finding the right virtual env, especially if it’s not in the top-level of the package. In that case, we need to be careful and control more things manually with environment variables and .Rprofile.\n\nSys.setenv(RETICULATE_PYTHON = 'path/to/venv') (otherwise it will try to install conda and barf)\n\nI would have thought I’d have to do this before the library, but it seems to work.\nBetter is to have a path to venv in .Rprofile, anyway\nNOTE sometimes if the .venv is in the outer directory, I’m getting weird errors when I try to Sys.setenv or otherwise set the path to the venv (it’s either prepending ~/virtualenvs or C::/ unless I pass a full fixed path. Seems to work in that case to just not set the environment variable though, and {reticulate} sorts it out correctly.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Wrapping python in R package"
    ]
  },
  {
    "objectID": "RpyEnvs/end_run_renv.html",
    "href": "RpyEnvs/end_run_renv.html",
    "title": "End-running renv",
    "section": "",
    "text": "I use {renv} to manage packages, but it is not uncommon for renv::restore to fail if enough time has passed. Maybe a package no longer exists. Maybe the package versions don’t work with the current R version (most common). We can use rig to downgrade R for that project, but lately that is failing with new versions of Rstudio. And sometimes we just want to install all the packages in a project, but all-new versions. If everything worked right, renv::update() should do that, but not if the packages haven’t been successfully restore()d.\nWe can stil use renv to solve this. Get the dependencies, make them a character vector, and install them. That’s way less work than my old method of running renv::status(), seeing what it said was missing, and installing, especially since that method included dependencies, and so had a lot of needless installs that would have happened with the outer package.\n\ndeps &lt;- renv::dependencies()\nchardeps &lt;- unique(deps$Package)\n\n# I really don't want to run this here, and only partially trust eval: false\n# renv::install(chardeps)\n\nSome end up failing anyway. To find out where those are used, use\n\ndeps[which(deps$Package == 'FAILEDPACKAGENAME'), ]\n\nThe failures are annoying because they don’t just fail that one, they fail everything and then we have to start over. One dumb solution that fixes this would be to loop over each package in turn, and save its name if it fails. I have code to do that, but will need to find it.",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Dealing with renv failures"
    ]
  },
  {
    "objectID": "RpyEnvs/R_py_type_passing.html",
    "href": "RpyEnvs/R_py_type_passing.html",
    "title": "R-py type passing",
    "section": "",
    "text": "This runs fine interactively, but when I render, everything fails. The python chunks lose what was in the previous ones, and the references to py$ in R chunks all fail. For now I’m going to just skip errors and have this not render correctly, which is unsatisfying. Not much more I can do though until I sort out the NameError issue, which is tricky because it doesn’t replicate consistently.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Passing types between R and python"
    ]
  },
  {
    "objectID": "RpyEnvs/R_py_type_passing.html#note--quarto-renders-fail",
    "href": "RpyEnvs/R_py_type_passing.html#note--quarto-renders-fail",
    "title": "R-py type passing",
    "section": "",
    "text": "This runs fine interactively, but when I render, everything fails. The python chunks lose what was in the previous ones, and the references to py$ in R chunks all fail. For now I’m going to just skip errors and have this not render correctly, which is unsatisfying. Not much more I can do though until I sort out the NameError issue, which is tricky because it doesn’t replicate consistently.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Passing types between R and python"
    ]
  },
  {
    "objectID": "RpyEnvs/R_py_type_passing.html#passing-types",
    "href": "RpyEnvs/R_py_type_passing.html#passing-types",
    "title": "R-py type passing",
    "section": "Passing types",
    "text": "Passing types\nI have a python function that takes dicts and lists as arguments. How do I pass those from R? We can’t just assign them to a variable in R, because those formats don’t work- e.g. we cannot create the lists and dicts in R to pass.\n\n```{r}\n#| eval: false\noutputType = ['summary', 'all']\n\nallowance ={'minThreshold': MINT, 'maxThreshold': MAXT, 'duration': DUR, 'drawdown': DRAW}\n```\n\nSo, let’s write a function to tell me the type of what I’m passing and try a few things.\n\n```{python}\ndef test_type(testarg):\n  \n  return(type(testarg))\n```\n\nIs a named list a dict or a list? what about just a c()? Is that a list?\n\n```{r}\nrlist &lt;- list(dict1 = 100, dict2 = 'testing')\nrc &lt;- c(100, 50)\n```\n\nThe named list is a dict. Look at it in python and R.\n\n```{python}\ntest_type(r.rlist)\n```\n\n&lt;class 'dict'&gt;\n\n\n\n```{r}\npy$test_type(rlist)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\nThe c() is a list- but see below- this fails if it’s length-one\n\n```{r}\npy$test_type(rc)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\nAn unnamed list is a list\n\n```{r}\nrlistu = list(100, 'testunname')\npy$test_type(rlistu)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\nThat is useful, since creating a length-one list doesn’t work with single values or c() wrapping single values\n\n```{r}\nrone = 100\npy$test_type(rone)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\n\n```{r}\nronec = c('testingc')\npy$test_type(ronec)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\nWe do get a length-one list with an unnamed list of length 1.\n\n```{r}\nronel = list(100)\npy$test_type(ronel)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Passing types between R and python"
    ]
  },
  {
    "objectID": "RpyEnvs/R_py_shared_projects.html",
    "href": "RpyEnvs/R_py_shared_projects.html",
    "title": "R and python envs in same project",
    "section": "",
    "text": "Both renv and poetry want to set up project structures and work within them. And I want to do both in the same git repo because I’m using both for the same project. And have access to both everywhere within the project (ie I want to be able to use reticulate, but more importantly I want the different parts of the project to be able to have both py and R components.\nI’ve done a bit getting them to work in the same script, and setting up clean python environments with poetry. With the shared scripts, I did it with a subdirectory, and that sort of worked for testing, but won’t work for a project where they’re both used a fair amount and in both places.\n\n\nCan I just initiate them both in the base git directory? I know i can with renv, but does poetry let us stick a project in an existing repo? When I was sorting out poetry, I always made a new dir with poetry new dirname.\nIt looks like py code should be in the inner directory of the poetry structure. Let’s assume that. Which roughly matches R structure, where we’ll have code in an R/ dir if it’s a package or in some other dir structure. IE, if we can just get the environment management into the outer dir of the repo, and then all other code inside. I’m not sure though that I’ll want to split py from R at present. Think about that.\nSo, really, the question is whether I can poetry new and poetry install in a dir that already exists.\nMaybe poetry init instead of poetry new? Asks a bunch of questions.\nIt creates a pyproject.toml file, and then poetry install creates the poetry.lockand .venv, but the rest of the structure’s not there (tests dir, second level of the project dir). Will it work? Probably. Do we want that structure? Probably.\n\n\n\nSo, maybe better to poetry new somewhere else and drag over, then poetry install. Does that work? I copied over everything inside the outer dir, since I want the whole project to share the outer dir. It makes the lock and venv, but I get ‘dirname does not contain any element’. I’m guessing because I made a poetry env with a different name. Try using the same name, then again copying over the internals.\nThat seems to work. Now to build the env so everything actually works. But that’s about project details, so I’ll leave this here.\nThat seemed to have made vs code happy- it can find a venv in the workspace and use it. It didn’t do that automatically when the venv was in a subdir. (I had to command palette- select python interpreter).",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Shared R-py project structure"
    ]
  },
  {
    "objectID": "RpyEnvs/R_py_shared_projects.html#the-issue",
    "href": "RpyEnvs/R_py_shared_projects.html#the-issue",
    "title": "R and python envs in same project",
    "section": "",
    "text": "Both renv and poetry want to set up project structures and work within them. And I want to do both in the same git repo because I’m using both for the same project. And have access to both everywhere within the project (ie I want to be able to use reticulate, but more importantly I want the different parts of the project to be able to have both py and R components.\nI’ve done a bit getting them to work in the same script, and setting up clean python environments with poetry. With the shared scripts, I did it with a subdirectory, and that sort of worked for testing, but won’t work for a project where they’re both used a fair amount and in both places.\n\n\nCan I just initiate them both in the base git directory? I know i can with renv, but does poetry let us stick a project in an existing repo? When I was sorting out poetry, I always made a new dir with poetry new dirname.\nIt looks like py code should be in the inner directory of the poetry structure. Let’s assume that. Which roughly matches R structure, where we’ll have code in an R/ dir if it’s a package or in some other dir structure. IE, if we can just get the environment management into the outer dir of the repo, and then all other code inside. I’m not sure though that I’ll want to split py from R at present. Think about that.\nSo, really, the question is whether I can poetry new and poetry install in a dir that already exists.\nMaybe poetry init instead of poetry new? Asks a bunch of questions.\nIt creates a pyproject.toml file, and then poetry install creates the poetry.lockand .venv, but the rest of the structure’s not there (tests dir, second level of the project dir). Will it work? Probably. Do we want that structure? Probably.\n\n\n\nSo, maybe better to poetry new somewhere else and drag over, then poetry install. Does that work? I copied over everything inside the outer dir, since I want the whole project to share the outer dir. It makes the lock and venv, but I get ‘dirname does not contain any element’. I’m guessing because I made a poetry env with a different name. Try using the same name, then again copying over the internals.\nThat seems to work. Now to build the env so everything actually works. But that’s about project details, so I’ll leave this here.\nThat seemed to have made vs code happy- it can find a venv in the workspace and use it. It didn’t do that automatically when the venv was in a subdir. (I had to command palette- select python interpreter).",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Shared R-py project structure"
    ]
  },
  {
    "objectID": "RpyEnvs/RandPython.html",
    "href": "RpyEnvs/RandPython.html",
    "title": "Using R and python together",
    "section": "",
    "text": "```{r setup}\n#| warning: false\n#| message: false\n\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Using R and python together"
    ]
  },
  {
    "objectID": "RpyEnvs/RandPython.html#the-issue",
    "href": "RpyEnvs/RandPython.html#the-issue",
    "title": "Using R and python together",
    "section": "The issue",
    "text": "The issue\nI have a project primarily in R, but needs some python. For the big python work, I’ll have a directory with a poetry environment and python code. But I’ve run into the issue that I want to run just one or two lines of python from R. The specific case is that I have python code for extracting river gauge data, and I’ve filtered some river gauges in R for something else, and rather than do the finding of the gauges again in python, I’d rather just do the extraction in R. I think that means I have to sort out {reticulate}, but also how to point reticulate at my python environment. The situation I have is a poetry project inside a directory with an Rproj (which probably needs to be split up, but it’s what I have now).\nMy python_setup sets up a very similar situation, so let’s see if I can use it.\nI’m going to try to remember to always put\nexecute:\n  echo: fenced\nin the yaml headers of mixed r-python notebooks, so it’s clear which chunks are which language (though we can usually tell by the code).\n\nAdditional issues\nThis all worked, but then stopped- python chunks separated by R chunks couldn’t share objects. This particular notebook still ran, but others would run fine interactively but when rendered would throw errors about “name ‘pyobjname’ is not defined”. I tried making sure jupyter was in the python env and set engine: knitr in yaml headers, since that’s what the help suggested. And I set the QUARTO_PYTHON environment variable in an _environments file, since that helped me previously. It’s unclear why it worked previously.\nIDE NOTE The above may be due in part because Rstudio uses the reticulate python REPL for python chunks, and so they are available to R. Quarto itself does as well. But VScode does not, and so python and R chunks are run wholly independently of each other and we can’t just pass objects between them as we do here in interactive notebooks. Instead, we need to run the python through reticulate in R chunks., rather than interactive python in the REPL. This is usually fine, but becomes a pain when we want to stay in python for a while, e.g to run something, process it, run something else, and then get it back in R.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Using R and python together"
    ]
  },
  {
    "objectID": "RpyEnvs/RandPython.html#set-up-reticulate-from-r",
    "href": "RpyEnvs/RandPython.html#set-up-reticulate-from-r",
    "title": "Using R and python together",
    "section": "Set up reticulate from R",
    "text": "Set up reticulate from R\nPoint reticulate at the venv. See stackoverflow. This seems to not be necessary if the .venv is in the outer project directory. Or if we’ve set the RETICULATE_PYTHON environment variable elsewhere (like .Rprofile).\n``` {{r}}\nreticulate::use_virtualenv(file.path('RpyEnvs', 'pytesting', 'Scripts', '.venv'), required = TRUE)\n```\nIf this is more than a one-off and you’re using an R project, it’s usually better to set the RETICULATE_PYTHON environment variable in .Rprofile. Here, that means adding this line in .Rprofile . This has the added bonus of stopping Rstudio/R throwing warnings about conda at startup when it detects {reticulate} being used in the project.\n``` {{r}}\nSys.setenv(RETICULATE_PYTHON = file.path('RpyEnvs', 'pytesting', '.venv'))\n```\nSee Quarto notes for some similar issues for different python-related env variables.\nLoad the library. Interestingly, the python code chunks will run without loading the library, but I can’t access their values using py$pythonobject unless I load it.\n\n```{r}\nlibrary(reticulate)\n```",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Using R and python together"
    ]
  },
  {
    "objectID": "RpyEnvs/RandPython.html#r",
    "href": "RpyEnvs/RandPython.html#r",
    "title": "Using R and python together",
    "section": "R",
    "text": "R\nFirst, let’s create some things in R.\n\n```{r}\na &lt;- 1\nb &lt;- 2\n```",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Using R and python together"
    ]
  },
  {
    "objectID": "RpyEnvs/RandPython.html#python",
    "href": "RpyEnvs/RandPython.html#python",
    "title": "Using R and python together",
    "section": "Python",
    "text": "Python\nDoes not just inherit the values from R, but runs.\n\n```{python}\na = 1\nb = 2\na+b\n```\n\n3\n\n\nDo I have access to packages? Yes.\n\n```{python}\nimport numpy as np\n\nx = np.arange(15, dtype=np.int64).reshape(3, 5)\nx[1:, ::2] = -99\nx\n```\n\narray([[  0,   1,   2,   3,   4],\n       [-99,   6, -99,   8, -99],\n       [-99,  11, -99,  13, -99]], dtype=int64)\n\n\nDoes access to python objects persist? Yes\nThough in lot of other docs, this has proved to be super unstable, and fails intermittently\n\n```{python}\nx.max(axis=1)\n```\n\narray([ 4,  8, 13], dtype=int64)",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Using R and python together"
    ]
  },
  {
    "objectID": "RpyEnvs/RandPython.html#moving-data-back-and-forth",
    "href": "RpyEnvs/RandPython.html#moving-data-back-and-forth",
    "title": "Using R and python together",
    "section": "Moving data back and forth",
    "text": "Moving data back and forth\n\nPython to R\nCan I access objects with R? Yes, but not quite directly. Have to use the py$pythonObject notation. But only if I’ve loaded library(reticulate) or specified with reticulate::py. That’s a pain, so probably almost always better to load the library. Even though the python chunks run fine without explictly loading it, I can’t seem to access py without loading it.\n\n```{r}\n# x\nreticulate::py$x\n```\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    2    3    4\n[2,]  -99    6  -99    8  -99\n[3,]  -99   11  -99   13  -99\n\n\n\n\nR to python\nSimilar to python objects being in py, R objects are in r, and are accessed with . instead of $.\n\n```{r}\nc &lt;- 17\n```\n\nInterestingly, the r. notation to get R into python does not need reticulate:: on it. Which I guess makes some sense- this block is actually running in python and python doesn’t know what reticulate is. But it does know what r. is, somehow. Pretty cool.\n\n```{python}\nr.c + b\n```\n\n19.0\n\n\n\n\nPython-R-python and NameErrors\nElsewhere I’m running into a new issue of getting “NameError: name ‘pyobjectname’ is not defined” when I try to access an object defined in a previous python chunk in a later python chunk. It seems to be worse when there’s an R chunk in the middle. It doesn’t seem to be happening here, since the preceding chunk could get at b, which was defined way earlier.\nDoes the issue happen when R has touched it? YES. This all runs fine interactively, but when I try to render, I get “Error in py_call_impl(callable, dots$args, dots$keywords) : NameError: name ‘b’ is not defined” in the python chunk.\n\n```{python}\nb + a\n```\n\n3\n\n\n\n```{r}\nrb &lt;- py$b + c\nrb\n```\n\n[1] 19\n\n\n\n```{python}\nb + a\n```\n\n3",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Using R and python together"
    ]
  },
  {
    "objectID": "RpyEnvs/managingprivate.html",
    "href": "RpyEnvs/managingprivate.html",
    "title": "Private data and website",
    "section": "",
    "text": "I’m getting set up to use github pages to host a website. But some content I (might) host needs to be private. A clear option is to simply mock-up data matching that private data, and that’s the way I’ll go. But because a large part of the content here will be sorting through issues, the initial sort-through will likely depend on figuring out what it is about the private data that needs to be mocked-up, and some portion of the testing will depend on that data. And I want all of that to be version controlled, but not shared publicly. In short, I need a private development location, and then go through, make a clean version based on mocked-up data, and publish that. So, how?\nThe first thing that came to mind is to just have a local-only branch that I keep private. I could have a private/ folder, where I do dev, with that folder ignored in the master .gitignore. And have a different .gitignore in another branch so development would be tracked in that other branch. Then, as things were ready to make public, I could just drop them over. However, because github requires the whole github pages repo to be public, I would never be able to push this branch. Sure, people would be unlikely to poke around in it, but it would all be there. And if I ever forgot the process, I would expose things. And I don’t want to lose a cloud-hosted version- only storing locally isn’t so great, even if it is backed up or dropboxed.\nI’m now leaning towards having a second, private repo for development, and then drag and drop into the github pages repo once the doc I’m working on is clean. That’s basically the same idea as the internal private/ folder, but as a whole different repo, and so could actually be held as a private repo on github. There are two main catches that I can see with this approach at the outset-\n\nThe actual development history of files won’t be available on the public repo. That’s kind of the point, but it is a bit annoying\nKeeping the two repos synced will be a pain. If I make a small change to a file that’s already public in the public repo, I’d need to get it back into the other. The obvious solution is to do everything in the private, and then move things over. But if I make a small change to a file that’s already moved to the public repo in the private repo, I need to make sure it moves.\n\nI think I’ve dealt with this issue before, but can’t remember the details. I had a repo as a fork of another that was upstream or something. Will need to sort that out. It’s essentially a repo-diff, but needing to check what should be diff (still private), vs. what shouldn’t be a diff (a change that needs to move over).\n\n\nIs there a better solution that allows building from somewhere other than github pages, and so could use a private repo? Netlify would work. And might be better anyway. But if the whole point is to make messy code public, then we want it on a public repo, right? And just hold the private stuff back/ do it elsewhere.",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Private data for website repo"
    ]
  },
  {
    "objectID": "RpyEnvs/managingprivate.html#the-issue",
    "href": "RpyEnvs/managingprivate.html#the-issue",
    "title": "Private data and website",
    "section": "",
    "text": "I’m getting set up to use github pages to host a website. But some content I (might) host needs to be private. A clear option is to simply mock-up data matching that private data, and that’s the way I’ll go. But because a large part of the content here will be sorting through issues, the initial sort-through will likely depend on figuring out what it is about the private data that needs to be mocked-up, and some portion of the testing will depend on that data. And I want all of that to be version controlled, but not shared publicly. In short, I need a private development location, and then go through, make a clean version based on mocked-up data, and publish that. So, how?\nThe first thing that came to mind is to just have a local-only branch that I keep private. I could have a private/ folder, where I do dev, with that folder ignored in the master .gitignore. And have a different .gitignore in another branch so development would be tracked in that other branch. Then, as things were ready to make public, I could just drop them over. However, because github requires the whole github pages repo to be public, I would never be able to push this branch. Sure, people would be unlikely to poke around in it, but it would all be there. And if I ever forgot the process, I would expose things. And I don’t want to lose a cloud-hosted version- only storing locally isn’t so great, even if it is backed up or dropboxed.\nI’m now leaning towards having a second, private repo for development, and then drag and drop into the github pages repo once the doc I’m working on is clean. That’s basically the same idea as the internal private/ folder, but as a whole different repo, and so could actually be held as a private repo on github. There are two main catches that I can see with this approach at the outset-\n\nThe actual development history of files won’t be available on the public repo. That’s kind of the point, but it is a bit annoying\nKeeping the two repos synced will be a pain. If I make a small change to a file that’s already public in the public repo, I’d need to get it back into the other. The obvious solution is to do everything in the private, and then move things over. But if I make a small change to a file that’s already moved to the public repo in the private repo, I need to make sure it moves.\n\nI think I’ve dealt with this issue before, but can’t remember the details. I had a repo as a fork of another that was upstream or something. Will need to sort that out. It’s essentially a repo-diff, but needing to check what should be diff (still private), vs. what shouldn’t be a diff (a change that needs to move over).\n\n\nIs there a better solution that allows building from somewhere other than github pages, and so could use a private repo? Netlify would work. And might be better anyway. But if the whole point is to make messy code public, then we want it on a public repo, right? And just hold the private stuff back/ do it elsewhere.",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Private data for website repo"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html",
    "href": "RpyEnvs/py_r_dates.html",
    "title": "Complex passing py-R",
    "section": "",
    "text": "```{r setup}\n#| warning: false\n#| message: false\n\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\nOld way of giving path to venv, new way is to set it in .Rprofile\n```{r}\n# reticulate::use_virtualenv(file.path('RpyEnvs', 'pytesting', '.venv'), required = TRUE)\n```\n```{r}\nlibrary(reticulate)\n```",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Passing complex objects py-R"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#general-problem",
    "href": "RpyEnvs/py_r_dates.html#general-problem",
    "title": "Complex passing py-R",
    "section": "General problem",
    "text": "General problem\nThe real issue here is when we bring a df into R from python and it has a list column with ‘environment’ in it. Then the conversion hasn’t really happened, and doing that conversion post-hoc has to convert every single environment, which is all rows. And that takes forever. It’s not necessarily dates (and as we can see below, sometimes they do work just fine). So, if that happens, rather than taking an eternity to purrr or otherwise go through the column to translate, just put it in something easier in py, bring it over, and put it back.\nI do wish I had a better idea about why it happened. I’m sure as it happens more I’ll start to figure it out.\nIt’s possible this has been fixed in more recent {reticulate}, and that’s why I can’t reproduce? https://github.com/rstudio/reticulate/pull/1266. No, I’m still getting the error where I was before. Still struggling to replicate it, but it does happen in the original case.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Passing complex objects py-R"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#a-demonstration-sort-of",
    "href": "RpyEnvs/py_r_dates.html#a-demonstration-sort-of",
    "title": "Complex passing py-R",
    "section": "A demonstration (sort of)",
    "text": "A demonstration (sort of)\nSay we have a pandas dataframe with a few columns of simple types (numeric, character) and 1000 rows\n\n```{python}\nimport pandas as pd\nimport random\nimport string\n\nrandnums = [random.gauss(0, 1) for _ in range(1000)]\nallchars = list(string.ascii_lowercase) + list(string.ascii_uppercase)\n\nrandchars = [random.choice(allchars) for _ in range(1000)]\n\nsimpledf = pd.DataFrame({'rand_nums': randnums, 'rand_chars': randchars}, columns=['rand_nums', 'rand_chars'], index=range(1000))\n\nsimpledf\n```\n\n     rand_nums rand_chars\n0     0.133291          H\n1    -0.614250          w\n2    -0.620462          N\n3    -2.493694          N\n4     0.805436          n\n..         ...        ...\n995  -1.994659          S\n996   0.460565          r\n997   0.701467          y\n998   1.222810          K\n999   1.423616          l\n\n[1000 rows x 2 columns]\n\n\nNow, we can get that into R without too much fuss using py$.\n\n```{r}\nrsimple &lt;- py$simpledf\nrsimple\n```\n\n        rand_nums rand_chars\n1     0.133290969          H\n2    -0.614249581          w\n3    -0.620462491          N\n4    -2.493693901          N\n5     0.805435503          n\n6     2.956532749          d\n7    -0.269791645          d\n8     0.623760030          I\n9     0.434931601          j\n10   -0.196023029          U\n11    2.160265597          s\n12   -0.337976341          i\n13   -0.074536375          t\n14    0.222722866          j\n15    0.293960431          g\n16    1.098460288          V\n17    1.130655504          F\n18    0.942662301          L\n19    0.038805967          p\n20    0.768540675          H\n21   -0.738591424          q\n22   -0.422585201          A\n23    0.945387050          u\n24   -0.711961711          S\n25    0.663938931          A\n26   -0.892279703          D\n27   -0.153834699          k\n28   -1.009738832          F\n29   -0.456634303          J\n30    0.325540164          n\n31   -1.081435122          P\n32   -0.879610895          h\n33   -0.107386570          j\n34    1.718037769          a\n35    1.552609666          f\n36    0.522724384          M\n37   -0.081958625          h\n38    1.321904381          v\n39    2.198275363          i\n40    1.714438261          O\n41    0.048707125          m\n42   -0.198145202          A\n43   -0.803614480          f\n44   -0.055894093          M\n45    0.362882326          P\n46   -1.846538635          I\n47   -1.286636258          j\n48   -1.839522128          e\n49    0.503038215          X\n50    0.887794938          O\n51   -0.621214728          D\n52    0.625089781          s\n53    0.598281571          U\n54    0.588028803          E\n55   -1.115054378          I\n56    1.599606587          P\n57   -0.441912383          j\n58   -1.301741393          d\n59   -0.213642691          g\n60   -1.327684241          z\n61    0.545201577          W\n62   -0.536702669          U\n63    1.846664427          P\n64    0.761506287          L\n65    0.248476075          M\n66    0.052190133          z\n67   -0.138237629          v\n68    0.827792601          t\n69    0.005779370          M\n70    0.626125878          a\n71    0.707466575          t\n72   -0.545022812          W\n73    1.005888056          d\n74    1.022683770          a\n75    0.042368768          R\n76   -0.890763798          F\n77    0.384980444          y\n78   -0.467631535          X\n79    0.658856630          n\n80   -1.044855472          a\n81    2.068693297          j\n82   -0.552501530          U\n83    1.016135162          I\n84   -1.131892101          K\n85    0.073397538          N\n86    0.573699992          f\n87   -0.365487317          v\n88   -0.028719842          o\n89   -0.516597286          G\n90    1.257439274          n\n91   -0.842566975          x\n92   -0.552633968          x\n93   -0.419609772          J\n94    0.737165147          R\n95    0.926204507          R\n96   -1.703984053          B\n97    0.193253834          o\n98   -0.483670319          l\n99   -0.220242500          j\n100   0.162833763          F\n101  -0.821256855          u\n102  -0.981962735          C\n103   1.502564938          M\n104   0.956338467          V\n105   1.923728670          b\n106  -0.774926316          L\n107   1.355735905          p\n108   0.220879712          S\n109  -0.045146286          t\n110  -0.239706416          r\n111   1.116304978          F\n112  -0.911247570          B\n113  -0.377103048          x\n114   0.752145352          y\n115  -1.121208842          N\n116  -0.628543841          r\n117   0.684114233          M\n118  -0.091952033          G\n119  -1.801924421          g\n120   0.579427843          e\n121  -0.674565416          b\n122  -0.661157970          p\n123  -0.083969362          L\n124  -1.626475583          P\n125  -0.818741031          R\n126   0.380291716          C\n127  -0.129153769          T\n128  -2.117605781          C\n129  -1.181478925          b\n130   0.835466899          L\n131   0.972300450          n\n132  -0.746167929          C\n133  -2.474815876          P\n134   0.449600179          M\n135   0.388629020          c\n136   1.609919724          L\n137  -0.996391203          F\n138  -0.546480822          P\n139   0.053716663          R\n140   1.099594386          A\n141  -0.552606278          i\n142   2.090444549          f\n143   1.072774169          j\n144   1.508252297          F\n145   1.391979642          G\n146  -0.205902083          U\n147  -0.215596129          A\n148   0.121290402          C\n149  -0.033819258          u\n150   1.258824146          e\n151   0.941587394          z\n152  -0.506882475          n\n153  -0.307031047          S\n154  -0.842593360          O\n155  -1.045992029          N\n156   0.100648368          T\n157  -0.620450835          L\n158   1.800641216          F\n159  -1.277398279          F\n160  -0.842739207          r\n161  -0.945952392          j\n162  -0.018004535          X\n163   1.020687016          t\n164   0.251196202          m\n165  -2.494604787          V\n166  -1.006140104          I\n167  -0.704367279          a\n168  -0.172325666          i\n169   0.750710668          s\n170  -1.215626673          x\n171   0.436325530          A\n172   0.234159106          I\n173  -0.269909635          V\n174  -1.196639509          d\n175   0.477298458          G\n176  -0.511461692          s\n177   0.522874180          R\n178  -0.054055863          E\n179  -0.372422070          S\n180   1.180761839          X\n181   0.379987227          D\n182  -0.102005828          u\n183   0.782634674          t\n184  -1.358987326          q\n185   2.417957180          v\n186  -0.121614405          V\n187  -0.523882906          D\n188   1.557816440          H\n189  -1.086155215          c\n190  -1.431025239          e\n191   0.472836128          I\n192  -0.317541462          x\n193  -0.582328887          B\n194   2.312490390          i\n195  -0.541847947          Y\n196  -0.344131888          m\n197  -0.913184109          s\n198  -1.477406998          z\n199   1.961895996          y\n200   0.907502683          s\n201  -0.051012503          b\n202  -0.195251315          F\n203  -1.358216826          Z\n204   1.789420101          R\n205   0.313445902          P\n206  -0.799589288          u\n207  -0.485112747          W\n208  -0.688425153          B\n209   0.067223022          l\n210  -0.522933397          X\n211  -0.929661696          R\n212  -0.007888543          B\n213  -0.480354971          N\n214   2.179086603          a\n215   0.672427749          E\n216  -0.172343886          A\n217   0.686980601          j\n218  -0.108323679          k\n219   0.304086289          C\n220  -0.306608409          K\n221  -0.331369481          f\n222   1.547642181          E\n223  -1.651367646          L\n224  -0.798884909          E\n225  -0.254334830          p\n226   0.500720453          W\n227  -0.134019815          D\n228   1.125804197          h\n229   2.582999116          o\n230   0.149131981          Y\n231   0.378214574          J\n232   1.005898181          Y\n233  -1.284547386          l\n234  -0.845913636          C\n235  -0.428644899          x\n236   1.149137048          v\n237  -0.509882524          k\n238   0.987589439          v\n239   0.754700185          w\n240   0.140052524          b\n241   0.843921087          B\n242  -0.065536895          K\n243   1.379837637          t\n244  -1.399404832          e\n245  -1.089223769          n\n246  -1.687449580          Y\n247  -0.414218580          F\n248   0.727049082          d\n249  -0.624120630          V\n250   0.900054050          P\n251  -0.913343944          I\n252   0.720818674          l\n253  -0.145136897          W\n254  -0.117257773          f\n255   1.436712608          G\n256   0.655844310          w\n257  -0.223566120          i\n258  -0.205955794          Q\n259  -0.293414937          c\n260   0.669391459          d\n261  -0.544510124          V\n262  -0.305366427          K\n263   0.060177280          d\n264   0.797992895          V\n265  -1.894621117          s\n266   1.056077782          l\n267  -0.335771332          W\n268   1.625757230          A\n269   0.151245252          f\n270  -0.800585955          b\n271   0.466719815          a\n272   0.620069634          R\n273  -2.015213534          X\n274   0.396395023          t\n275  -1.161510612          j\n276  -0.155535506          E\n277   0.536285651          v\n278   1.212486004          g\n279   0.499729205          M\n280   0.364111686          o\n281  -0.585659414          L\n282   0.507073862          b\n283   1.080988323          v\n284  -0.587718031          d\n285   0.573178333          L\n286  -0.448026109          s\n287   0.018277121          d\n288  -0.653194709          J\n289   1.984109534          R\n290  -0.120536238          s\n291  -0.009666630          h\n292  -0.270368526          X\n293  -0.158613825          D\n294  -0.820627611          e\n295   0.821071310          z\n296   1.798171340          R\n297   0.815738298          n\n298  -0.517867131          h\n299   0.597862211          I\n300   0.111636782          A\n301   0.726193542          q\n302   0.693289890          Q\n303   0.798415248          X\n304   0.935584710          W\n305   0.089466460          N\n306  -0.952748507          i\n307   0.067181781          f\n308   0.384723396          H\n309  -0.787840230          j\n310   0.918874737          d\n311  -1.551456590          n\n312  -0.105096848          y\n313  -0.995997658          P\n314  -1.142917658          H\n315  -2.084868616          K\n316  -0.562559045          g\n317  -0.367899620          t\n318   0.958188641          N\n319  -0.414075729          n\n320   0.247033226          S\n321  -0.408690635          N\n322   0.355066414          b\n323   1.181881647          B\n324  -1.942317961          p\n325   0.687417940          J\n326  -0.139886373          M\n327   1.721754523          w\n328  -0.676887487          q\n329   0.022655467          y\n330   2.063708971          k\n331   0.527333879          X\n332   2.033538617          W\n333  -0.371122612          f\n334  -1.165702215          F\n335   1.273530467          Q\n336   0.793559692          l\n337   0.748991909          k\n338  -0.633145751          G\n339  -0.784559910          g\n340   1.476111045          X\n341   0.354139275          X\n342  -0.674359454          K\n343  -1.365197168          k\n344  -0.507931845          a\n345   0.225120017          h\n346   0.129927247          Y\n347   0.173364552          q\n348   0.559597543          H\n349  -0.490257888          c\n350  -1.284180535          B\n351   0.619761307          D\n352   0.500147736          f\n353  -0.369340328          i\n354   0.803197104          j\n355   1.397572924          b\n356  -0.953040286          p\n357  -0.816560532          s\n358  -1.051015931          j\n359  -0.181634598          s\n360  -0.147096216          d\n361   0.027221709          U\n362   0.620745888          F\n363   0.262687984          v\n364   1.380483335          U\n365   0.067567313          X\n366   1.751570602          W\n367   0.438825748          M\n368   1.808340312          a\n369   0.605527586          m\n370  -0.483390631          X\n371   0.211558655          I\n372  -0.310306157          m\n373   1.929228659          q\n374  -1.617544923          P\n375   0.832937253          C\n376  -1.354096351          v\n377   0.864418238          y\n378  -0.726033869          b\n379  -0.081856841          m\n380  -1.334423081          Q\n381   1.349231266          d\n382  -0.222002196          Q\n383   0.096457879          D\n384  -0.002604509          T\n385  -0.071723054          s\n386  -0.165322615          j\n387   0.354317703          y\n388  -2.132777022          E\n389   0.746357122          R\n390  -0.573555847          j\n391  -0.985549065          Y\n392   1.048377586          H\n393   0.460067177          Q\n394  -0.622628970          U\n395   0.219134248          N\n396   0.910607981          z\n397  -1.419319633          e\n398   0.797456230          K\n399  -0.915363689          T\n400   0.781248226          x\n401  -0.465741769          b\n402   0.347827955          t\n403  -0.484938541          i\n404  -2.615634813          k\n405   0.636529569          P\n406   0.822472348          C\n407  -0.743673255          p\n408  -0.471838816          v\n409   1.125488678          i\n410  -0.584376931          F\n411  -0.045750751          E\n412   0.375471677          r\n413   0.439682384          B\n414  -0.499024458          l\n415  -0.462578656          Q\n416   0.227108042          W\n417   0.214445204          D\n418   0.677033616          r\n419   0.594437079          F\n420   0.766851411          d\n421  -0.366306336          O\n422  -1.624552019          u\n423   0.156704168          N\n424  -0.178724694          P\n425  -1.219080521          q\n426   0.813037877          P\n427  -0.040579485          I\n428   0.899585211          s\n429  -0.469361921          K\n430   1.165576753          B\n431   0.655512633          X\n432   1.269338020          U\n433   0.061847941          m\n434   0.043032287          W\n435  -1.205534717          Y\n436  -1.669729379          q\n437   0.472410435          C\n438  -0.680110898          q\n439   0.099689207          q\n440  -1.517089491          j\n441  -0.473095178          Q\n442  -1.341674480          j\n443  -0.547533492          m\n444  -1.075333395          B\n445  -0.516450797          W\n446  -0.089872719          P\n447  -0.116574984          Q\n448   0.999104890          h\n449   0.283193034          d\n450   0.209168887          e\n451  -0.244943703          W\n452   0.369033996          h\n453  -0.137591890          X\n454   0.402650717          g\n455  -1.643721269          q\n456   0.552094263          L\n457  -0.170055723          k\n458  -0.085523014          c\n459   0.161168458          d\n460   0.026135651          d\n461   0.006249210          n\n462   0.418695142          V\n463   0.619619112          k\n464   0.732148800          V\n465  -0.612258867          a\n466  -1.484476305          G\n467   0.106063420          P\n468   0.890715422          R\n469   0.737146085          N\n470   1.337442914          k\n471  -0.011583379          J\n472   0.473525830          u\n473   0.934309141          B\n474   0.848466438          n\n475   0.191603208          k\n476   0.074001182          W\n477  -0.774560825          R\n478   0.194674439          D\n479  -0.042914952          W\n480   0.061507791          k\n481  -0.339130690          S\n482   1.216940233          b\n483  -0.801135515          b\n484   0.248687120          A\n485  -0.090106558          g\n486   0.056776855          G\n487   0.756716250          n\n488  -1.189236326          s\n489   0.852729173          h\n490   0.790296790          A\n491   0.166906623          m\n492  -1.485032283          B\n493   0.473288640          U\n494  -0.337701505          x\n495   0.400640444          G\n496  -0.549024347          r\n497  -0.707923360          E\n498  -0.363968437          o\n499  -0.227005034          z\n500  -0.507420238          d\n501   1.090735900          M\n502   0.487110952          Q\n503   1.598110625          l\n504  -0.414871492          g\n505   0.509773619          H\n506  -1.547784889          H\n507  -0.267903308          A\n508   0.454836389          h\n509  -1.294239517          i\n510   1.427798866          P\n511   0.626832625          x\n512  -0.270171961          v\n513   0.683703705          e\n514  -0.242178967          e\n515   1.113898200          I\n516  -1.106326958          L\n517  -1.829972604          t\n518  -0.957891628          n\n519   1.150266879          D\n520  -1.020600052          G\n521   0.418048137          S\n522  -0.488215952          L\n523  -1.239629812          r\n524   0.512532540          m\n525  -0.847790739          p\n526   1.140368473          g\n527  -1.147748779          w\n528  -1.605929073          V\n529   0.938544348          s\n530   0.725422473          b\n531  -0.649194418          b\n532   1.508653353          W\n533  -1.092310046          k\n534   1.274452030          Y\n535  -1.157686536          S\n536   0.168140225          k\n537   0.674805533          o\n538   1.018426747          h\n539   1.637443236          D\n540   0.347698826          X\n541  -0.232420640          H\n542  -0.549612574          Y\n543  -2.293790187          t\n544  -0.248125746          h\n545  -1.131133447          x\n546   1.035848342          a\n547  -0.915169225          F\n548   1.472984395          C\n549  -0.222447120          e\n550  -1.853596798          E\n551   1.366062746          R\n552  -0.089091978          U\n553  -1.374126277          h\n554   0.803198350          Y\n555  -2.331352954          J\n556   1.095309539          n\n557   0.667086890          U\n558   0.246743847          v\n559  -0.116318663          k\n560   0.375461995          W\n561   0.656705800          Q\n562   0.795290592          Y\n563   0.708626783          m\n564  -1.271059359          s\n565  -0.007916019          V\n566  -0.150617692          y\n567   0.711100695          h\n568   0.408747297          p\n569  -0.744456680          r\n570   1.231652156          A\n571   1.305571610          T\n572   0.188627294          R\n573   0.088461302          I\n574  -0.468062191          j\n575  -1.270152051          M\n576   2.710786854          u\n577  -0.212354187          s\n578   0.856251542          S\n579   0.954718887          f\n580  -1.205852934          R\n581  -2.389677976          i\n582  -0.011964941          f\n583  -1.951361060          k\n584  -0.978222506          x\n585   1.621375845          u\n586   0.534849442          E\n587   1.065981885          L\n588  -1.864112996          r\n589  -1.189043591          p\n590   1.495348409          A\n591   1.298833153          D\n592   0.109554846          N\n593   0.896960776          G\n594  -0.384848805          F\n595  -0.437187705          c\n596   0.650794390          f\n597   0.762920514          k\n598   0.652238676          E\n599   0.214962766          r\n600   1.314182573          s\n601  -0.989364217          Y\n602  -0.456532942          N\n603   2.698417145          g\n604   0.664847464          A\n605   1.318593908          i\n606   0.523739859          r\n607  -0.546917164          o\n608  -0.689931290          B\n609   1.279365610          C\n610  -0.924985618          b\n611  -0.387369247          k\n612  -0.143435021          s\n613  -1.440571799          F\n614   0.555061899          a\n615  -0.849209172          l\n616  -0.645695105          a\n617  -0.385508184          f\n618   1.135516767          x\n619   0.221361985          p\n620  -0.676484388          a\n621  -0.582881481          i\n622   1.143699449          q\n623   1.385825299          j\n624   1.178294757          p\n625  -1.644661538          K\n626   1.292093937          t\n627  -1.368298339          Y\n628  -0.687719956          h\n629  -0.352599811          l\n630  -0.483822982          H\n631  -1.012401042          n\n632  -0.103346946          q\n633  -0.298277633          x\n634  -0.529234919          H\n635   0.735607515          p\n636  -0.953478972          q\n637   0.049777009          s\n638   1.794968256          K\n639  -0.602398767          u\n640   1.348821784          u\n641   0.074252797          G\n642   2.428886010          l\n643  -0.858069520          A\n644   0.996619446          d\n645   0.132223805          K\n646  -0.402397931          i\n647  -0.404545474          z\n648  -0.482684533          T\n649   0.301099634          B\n650   0.092504753          L\n651  -1.041226447          j\n652   0.275762503          R\n653  -0.481135546          M\n654   1.521959621          z\n655   0.013970784          P\n656  -1.164906574          K\n657  -0.769764197          R\n658  -0.407098360          U\n659   0.692137313          i\n660  -1.458167515          Q\n661  -0.270806113          F\n662   0.485273148          J\n663   0.334282077          J\n664   0.637046039          g\n665  -0.706806028          i\n666  -0.159734655          T\n667   0.019349640          T\n668   0.921840266          s\n669   1.161080626          X\n670  -0.841260681          m\n671   0.863261918          c\n672  -0.669842304          G\n673   1.064663135          m\n674  -0.541715453          F\n675   0.385597180          T\n676   0.832636697          x\n677  -0.816978156          j\n678  -0.439431160          F\n679   1.182698479          P\n680  -2.641130197          h\n681  -3.425794969          a\n682  -0.264003151          N\n683   0.814116714          O\n684  -0.518768072          s\n685  -0.813826981          o\n686   1.621309943          Q\n687   0.046368720          Q\n688   0.831397987          J\n689  -0.359076716          L\n690   0.176302230          t\n691  -0.280269713          T\n692  -0.875091222          I\n693  -0.184881146          a\n694  -0.187893100          r\n695  -0.010546753          V\n696  -0.811666823          T\n697  -0.725640883          K\n698  -0.895899393          v\n699   0.302912213          T\n700  -0.074020441          j\n701   1.636457126          P\n702  -0.788059980          q\n703  -0.772041302          Q\n704  -0.471964463          y\n705  -0.714632594          l\n706   0.253616276          X\n707   1.548813503          E\n708  -0.206721030          H\n709   0.174694728          A\n710  -0.408169545          k\n711   1.822220987          d\n712  -0.548483289          V\n713  -0.958250202          x\n714   0.863268800          B\n715  -0.435127058          i\n716   0.708791473          f\n717  -0.274634384          p\n718   0.641979800          k\n719  -0.854322096          l\n720  -0.844374342          P\n721  -0.736718257          u\n722  -0.085917149          L\n723   1.016965933          z\n724   0.210994190          g\n725  -0.675013360          u\n726   1.036072152          E\n727   0.116366153          H\n728  -1.259644944          C\n729  -1.123268429          K\n730  -0.921403895          M\n731  -1.237759010          W\n732  -0.249523600          c\n733   0.835099880          S\n734   1.767077902          p\n735   0.542571429          o\n736   0.037910229          R\n737   1.247946937          Y\n738   0.375565640          z\n739  -0.287372340          D\n740   1.354784408          I\n741   0.654531640          p\n742   0.852835790          X\n743  -1.267447689          S\n744  -0.412240791          f\n745   1.784815075          j\n746   0.643970200          L\n747  -0.265997939          O\n748   1.085963717          L\n749   0.608321456          c\n750   1.129974073          n\n751   0.808468735          i\n752   0.946144988          v\n753   0.216468574          v\n754  -0.193555197          Q\n755  -0.905116741          S\n756  -0.290976421          p\n757   0.209083481          k\n758  -1.262641657          G\n759  -0.156325792          V\n760  -0.073991477          c\n761  -0.268585679          G\n762   0.388661026          W\n763   0.101443427          p\n764  -0.718004416          X\n765   0.454448868          a\n766   0.211720060          S\n767  -0.595319876          G\n768  -0.785499707          Y\n769   0.487139831          n\n770   0.851797480          A\n771  -0.702159642          y\n772   0.646773584          w\n773  -0.266816670          Z\n774   1.918694607          k\n775  -0.282033738          a\n776   0.419856842          r\n777   0.521949165          y\n778   0.931677135          f\n779   0.615630688          D\n780   2.026088678          C\n781  -0.632827039          d\n782   0.282809000          Z\n783   1.052449643          M\n784   0.759557384          y\n785   1.132104018          g\n786  -1.118075823          A\n787  -1.155696808          F\n788   1.110308124          z\n789  -0.332546703          E\n790  -1.993530463          B\n791  -0.323993855          Q\n792   1.003071443          T\n793   0.530573694          r\n794   0.484398124          e\n795  -2.946079952          d\n796  -1.565539231          U\n797  -1.066983422          Y\n798  -0.015828641          T\n799  -0.088380813          H\n800   0.281055063          i\n801   0.931979885          L\n802   0.318145439          E\n803   0.548225298          k\n804  -1.206661828          L\n805   0.375261135          q\n806   0.246320102          n\n807   1.077560684          L\n808  -2.005814138          K\n809   1.437339941          S\n810  -0.999633906          S\n811   1.354633971          a\n812  -0.023363317          j\n813   1.783377354          D\n814   1.034842029          a\n815  -0.407835639          U\n816   0.669910625          o\n817  -1.578196504          C\n818   0.556915498          y\n819  -1.520008319          B\n820  -0.941989039          b\n821  -0.028672079          X\n822   1.136414136          q\n823   1.373862918          x\n824  -0.174444736          O\n825  -0.951346908          T\n826  -0.032294895          h\n827   1.187572347          o\n828   1.454372142          K\n829  -1.073661686          T\n830  -1.509977819          p\n831  -0.808428189          r\n832  -0.206626698          X\n833   0.401802019          U\n834  -0.149867197          N\n835   0.603452317          r\n836  -0.418550389          K\n837   0.069911879          u\n838   0.168385991          x\n839  -1.033971713          d\n840   0.666071218          H\n841  -0.443262447          H\n842   1.314743287          W\n843   1.088574490          Y\n844   0.090566315          b\n845  -0.689411097          i\n846  -0.560099886          f\n847  -0.095166423          Y\n848   1.659329989          b\n849   2.115119517          b\n850  -0.095349503          z\n851   0.151420138          f\n852  -0.599216235          Z\n853  -1.044039888          X\n854  -0.977129068          S\n855  -0.836369219          t\n856  -0.138905155          T\n857  -0.320798364          x\n858  -0.178019909          A\n859  -0.385612679          i\n860  -1.130974941          Z\n861  -1.516166603          S\n862  -1.188933655          A\n863  -0.591099108          O\n864  -0.704681440          C\n865  -2.100540282          L\n866  -1.083926348          p\n867   0.924310269          b\n868  -0.744559010          S\n869   1.341053320          m\n870  -0.066515980          t\n871  -0.185075317          e\n872   0.122222539          R\n873   0.110523846          J\n874  -0.624955176          O\n875  -0.226141564          p\n876  -0.150442113          T\n877  -1.554956472          T\n878   0.817585727          z\n879  -1.093428024          C\n880   0.083669041          a\n881  -0.363984705          H\n882   0.857606314          h\n883  -0.578666637          P\n884   0.695382031          M\n885   0.223144558          a\n886  -0.910722471          t\n887   0.588875844          u\n888   0.030301076          u\n889  -0.887624024          F\n890  -1.451915427          i\n891  -0.325353926          g\n892   0.574979467          T\n893   0.577286339          X\n894   0.942858001          s\n895   0.687924774          V\n896   0.268485424          O\n897  -0.586009515          z\n898   0.097277897          L\n899  -2.161857030          V\n900  -0.344354570          z\n901  -1.987384490          l\n902   0.651788848          p\n903  -0.135703179          a\n904  -0.350016712          S\n905   1.019608983          B\n906  -0.058168663          Q\n907  -1.412936402          z\n908   0.202399205          Y\n909   0.238798161          A\n910   1.707516900          S\n911   1.018748162          I\n912  -0.233390927          R\n913  -0.054426120          c\n914  -1.167133086          q\n915  -0.961944317          m\n916   0.145154890          W\n917  -1.329591799          M\n918   0.685257271          j\n919  -2.165137353          n\n920   1.537077395          D\n921   3.170655395          o\n922  -0.583085953          A\n923  -0.200506685          D\n924  -0.482447627          f\n925   0.799978501          b\n926   0.269259107          Y\n927   0.905755656          t\n928   0.210174750          h\n929   0.267648982          E\n930  -0.381440151          D\n931  -0.288765707          r\n932   0.464681588          O\n933  -0.486026781          i\n934  -1.680386937          w\n935  -0.287732489          k\n936   2.566538763          j\n937  -1.182550359          R\n938   0.517304612          S\n939   2.661175755          Z\n940  -0.836474378          k\n941   1.498772143          v\n942   0.331586499          E\n943  -0.625229890          B\n944  -0.855253562          e\n945   1.359865865          t\n946   0.414675823          H\n947  -1.055197971          R\n948   0.075003209          t\n949  -0.100969184          f\n950   1.106992628          B\n951   0.827849241          J\n952  -0.298887474          S\n953  -0.843170414          D\n954  -0.668100724          p\n955  -1.271991797          O\n956   0.262855822          A\n957  -0.431714283          g\n958  -0.120455116          U\n959  -0.048794095          h\n960   0.135342470          D\n961   1.295291676          o\n962  -0.752691282          o\n963   0.071833685          h\n964  -0.496364345          y\n965  -0.184916864          b\n966   1.717031332          q\n967  -0.685297174          I\n968  -0.739211749          Y\n969  -1.204921890          X\n970   0.511363389          h\n971  -1.459805693          L\n972  -0.990872745          B\n973   2.458072252          Z\n974   1.431963877          w\n975   1.880584857          z\n976   0.218424403          i\n977   0.743657086          c\n978  -0.517339639          o\n979   0.194354308          l\n980   0.779488329          w\n981  -0.142717299          R\n982  -0.776088791          h\n983  -0.751613809          w\n984  -0.984560457          W\n985  -0.169688936          G\n986   0.862051110          u\n987  -0.459938069          l\n988   0.487569343          t\n989   0.500037106          q\n990   0.373956329          N\n991  -0.622625525          c\n992  -0.892980978          r\n993  -0.169332737          x\n994  -1.336198951          j\n995  -1.938961635          y\n996  -1.994658725          S\n997   0.460565354          r\n998   0.701467161          y\n999   1.222810195          K\n1000  1.423615900          l\n\n\nQuick and easy. I think py_to_r is supposed to do some of this, but I can never get it to work. I think maybe it would make more sense in a script where we’re moving back and forth than here where we have separate code chunks?",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Passing complex objects py-R"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#now-with-time",
    "href": "RpyEnvs/py_r_dates.html#now-with-time",
    "title": "Complex passing py-R",
    "section": "Now, with time",
    "text": "Now, with time\nLet’s add a column of dates to simpledf. First, create the dates, then add to simpledf.\n\n```{python}\nimport datetime\n\ndates = []\n\nd1 = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n\n# Because i starts at 0, the first loop is the start date\nfor i in range(1000):\n    # Add i days to the start date\n    day_new = d1 + datetime.timedelta(days=i)\n    # Append the current date string to the list of dates\n    dates.append(day_new)\n\ntimedf = simpledf.assign(date = dates)\n\n# Try another way too\ntimedf['date2'] = pd.to_datetime(timedf['date'])\n```\n\nNow, when we bring it into R, it just works? That’s not at all what happened to me when I had the original issue.\n\n```{r}\ntimedfR &lt;- py$timedf\n# tibble::as_tibble(timedfR)\n```",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Passing complex objects py-R"
    ]
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#for-future-reference",
    "href": "RpyEnvs/py_r_dates.html#for-future-reference",
    "title": "Complex passing py-R",
    "section": "For future reference",
    "text": "For future reference\nSo, I can’t seem to replicate the issue. Previously, the datetime col came in as a list-col into a tibble, and was wrapped in a python environment. It was possible to parse it with purrr (or lapply, but purrr was much faster (and weirdly, furrr was slower). Not running, because this isn’t in a python env, and so this doesn’t actually work.\n\n```{r}\n#| eval: false\ndemodates &lt;- timedfR$date\nrdates &lt;- purrr::map(demodates, py_to_r) %&gt;%\n  tibble(.name_repair = ~'Date') %&gt;%\n  unnest(cols = Date)\n```\n\nBut what was much faster was to convert the column to strings in python with\n\n```{python}\ntimedf['date_str'] = timedf['date'].astype(str)\ntimedf.info()\n```\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   rand_nums   1000 non-null   float64       \n 1   rand_chars  1000 non-null   object        \n 2   date        1000 non-null   datetime64[ns]\n 3   date2       1000 non-null   datetime64[ns]\n 4   date_str    1000 non-null   object        \ndtypes: datetime64[ns](2), float64(1), object(2)\nmemory usage: 39.2+ KB\n\n\nAnd then lubridate back to dates in R\n\n```{r}\ntimedfRstr &lt;- py$timedf\ntimedfRstr &lt;- dplyr::select(timedfRstr, -date) |&gt; \n  dplyr::mutate(date = lubridate::ymd(date_str))\nstr(timedfRstr)\n```\n\n'data.frame':   1000 obs. of  5 variables:\n $ rand_nums : num  0.133 -0.614 -0.62 -2.494 0.805 ...\n $ rand_chars: chr  \"H\" \"w\" \"N\" \"N\" ...\n $ date2     : POSIXct, format: \"2000-01-01 11:00:00\" \"2000-01-02 11:00:00\" ...\n $ date_str  : chr  \"2000-01-01\" \"2000-01-02\" \"2000-01-03\" \"2000-01-04\" ...\n $ date      : Date, format: \"2000-01-01\" \"2000-01-02\" ...\n - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=1000, step=1)",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Passing complex objects py-R"
    ]
  },
  {
    "objectID": "RpyEnvs/python_nameerror.html",
    "href": "RpyEnvs/python_nameerror.html",
    "title": "Python NameErrors",
    "section": "",
    "text": "Issue\nI keep getting NameErrors and weird behavior with mixed R and python chunks. Usually it seems to be that python can’t find objects if there’s an R chunk in between. It only happens when I render in quarto- running this interactively in Rstudio always works fine.\nI’ve set the RETICULATE_PYTHON and QUARTO_PYTHON environment variables, and put engine: knitr in the yaml, and it doesn’t seem to have helped.\nThe issue seems to be worst when R is somehow involved with the objects.\nThis doc is to try to systematically and simply reproduce the issue. I hope that works.\nIt was working, but the more often I rendered, the more intermittent the problem became. I’ve noted something about the frequency of errors for the chunks- some error more than others.\n\n```{r}\nlibrary(reticulate)\n```\n\nAn R chunk to make sure knitr kicks off\n\n```{r}\na &lt;- 1\n```\n\nDefine a bunch of python variables to do different things with.\n\n```{python}\nb = 2\nc = 3\nd = 4\ne = 5\nf = 6\ng = 7\n```\n\nCan we see those immediately?\n\n```{python}\nb\n```\n\n2\n\n\nWhat if there’s an R chunk in the middle?\n\n```{r}\nrdummy &lt;- 1\n```\n\nCan we still see the python? Yes\n\n```{python}\nc\n```\n\n3\n\n\nWhat if the R touches the python?\n\n```{r}\nrb &lt;- py$d + 1\nrb\n```\n\n[1] 5\n\n\nThis now fails almost every time\n\n```{python}\nd\n```\n\n4\n\n\nAre the other variables unscathed? No, this also fails consistently (but not 100% of the time).\n\n```{python}\nc\n```\n\n3\n\n\nWhat about those that haven’t been used since declared? Fails again most of the time.\n\n```{python}\ne\n```\n\n5\n\n\nCan we declare more python?\n\n```{python}\nf = 6\n```\n\nagain, R in the middle\n\n```{r}\nrtest &lt;- 5\n```\n\nDoes the new python persist?\n\n```{python}\nf\n```\n\n6\n\n\nWhat if there’s a python chunk that accesses R?\n\n```{python}\ng = r.a + 1\ng\n```\n\n2.0\n\n\nCan we access the previous python variables? Sometimes. I intermittently get an error here, but sometimes it runs.\n\n```{python}\nf\n```\n\n6\n\n\nCan we access that new python variable? Also only sometimes.\n\n```{python}\ng\n```\n\n2.0\n\n\nWhat if we declare python in two chunks? Do they all get annihilated after crossing the language boundary, or only on the basis of their chunk declaration?\n\n```{python}\ng = 7\nh = 8\n```\n\n\n```{python}\nj = 9\n```\n\nContaminate with R\n\n```{r}\nrgh &lt;- py$g + 9\nrgh\n```\n\n[1] 16\n\n\nCan we see the python that was defined with g? This works sometimes. So whatever is going on is unstable. It’s strange that this and the following two work sometimes- they seem like the same thing that consistently fails above (though that’s now working too).\n\n```{python}\nh\n```\n\n8\n\n\nI wasn’t expecting that to work (and it only does sometimes). Can we see g itself? Sometimes\n\n```{python}\ng\n```\n\n7\n\n\nCan we get to the python that was defined in a different chunk? Sometimes.\n\n```{python}\nj\n```\n\n9\n\n\nIs the issue that we didn’t ask for the touched variable first? Do the same thing, but this time ask for the contaminated variable\n\n```{python}\nk = 10\nl = 11\n```\n\n\n```{python}\nm = 12\n```\n\nAccess in R\n\n```{r}\nr_m &lt;- py$k + 9\n```\n\nIs that variable there in python?\n\n```{python}\nk\n```\n\n10\n\n\nIs the other one that’s defined with it?\n\n```{python}\nl\n```\n\n11\n\n\nHow about the one in the other code chunk?\n\n```{python}\nm\n```\n\n12\n\n\nI’m not really sure what to do with this. Anything defined before any interaction across languages (either py$pyvar or r.rvar) would die in python when I rendered this the first few times, but now it’s all working after about 10 update renders. I’m throwing execute: error: true in all the R-py yaml headers, but that doesn’t actually help when I actually want them to run and there are errors intermittently.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Intermittent NameError py-R"
    ]
  },
  {
    "objectID": "RpyEnvs/python_testing.html",
    "href": "RpyEnvs/python_testing.html",
    "title": "Python testing",
    "section": "",
    "text": "This is very sketchy as I figure out some python testing and debug. Mostly notes for myself. In theory, I should be able to test the package by just calling pytest at a terminal, but it frequently fails. I get an error\nModuleNotFoundError: No module named 'module_trying_to_test'\nThe project I’m working in already has pytest set up, but when i call pytest test_script.py, it can’t find the module I’m developing). The solution seems to be to use\npython -m pytest tests/test_script.py\nFrom the project root directory.\nIt also seems to often work to just run\npython -m pytest\nNote if you have a file with ‘test’ in the name, pytest will try to run it. So don’t have e.g. figuring_stuff_out_test.py in there.",
    "crumbs": [
      "Code Demos",
      "R and python",
      "Testing python"
    ]
  },
  {
    "objectID": "RpyEnvs/rig.html",
    "href": "RpyEnvs/rig.html",
    "title": "Managing R versions",
    "section": "",
    "text": "I’ve updated to R 4.2, but have projects that were built with 4.0 and even 3.x. Most new versions of packages for R 4.x don’t work in 3.x, and 4.2 seems to have broken quite a bit compared to 4.0. So I can’t just renv::restore(), and would need to update packages one at a time, but I know doing that will break things, and I don’t have time to do a full update of the project.\nI use renv to manage the packages, but not currently anything to switch/manage R versions itself. In python, there’s pyenv to manage python versions. I’ve run across rig (https://github.com/r-lib/rig).",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Managing old R versions with rig"
    ]
  },
  {
    "objectID": "RpyEnvs/rig.html#issue",
    "href": "RpyEnvs/rig.html#issue",
    "title": "Managing R versions",
    "section": "",
    "text": "I’ve updated to R 4.2, but have projects that were built with 4.0 and even 3.x. Most new versions of packages for R 4.x don’t work in 3.x, and 4.2 seems to have broken quite a bit compared to 4.0. So I can’t just renv::restore(), and would need to update packages one at a time, but I know doing that will break things, and I don’t have time to do a full update of the project.\nI use renv to manage the packages, but not currently anything to switch/manage R versions itself. In python, there’s pyenv to manage python versions. I’ve run across rig (https://github.com/r-lib/rig).",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Managing old R versions with rig"
    ]
  },
  {
    "objectID": "RpyEnvs/rig.html#install",
    "href": "RpyEnvs/rig.html#install",
    "title": "Managing R versions",
    "section": "Install",
    "text": "Install\nclick on windows installer. Restart terminal. Type rig list to see what R is available.",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Managing old R versions with rig"
    ]
  },
  {
    "objectID": "RpyEnvs/rig.html#using-it",
    "href": "RpyEnvs/rig.html#using-it",
    "title": "Managing R versions",
    "section": "Using it",
    "text": "Using it\nGo to the project I want to run, and figure out what version of R it was using. The one I’m testing on uses 4.0.2, which still is different enough from 4.2 that a lot of the packages fail when I renv::restore() in a session with 4.2. To avoid any issues, I will downgrade to that to run the project because I really just need things to work. Then, once I know it works, I can start updating and testing.\nSo, try rig add 4.0.2. Seems to have worked. Set the default to current, though.\nrig default 4.2.1.\n\nChoosing for a project\nWhat if I just open the project file by double clicking? There’s no obvious way to change the R version just by opening Rstudio- it uses the default.\nI think there’s probably a way to use the CLI to change the R version and then double click, but what seems to be easiest is cd path/to/repo and then rig rstudio renv.lock to open with the version in the lockfile.\nNote: this does not work with the new version of Rstudio (2022.12.0 Build 353 and others): https://github.com/r-lib/rig/issues/134 and https://github.com/rstudio/rstudio/issues/12545\nAnd do I keep using other R versions elsewhere? Seem to. For now, this should do what I need.",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Managing old R versions with rig"
    ]
  },
  {
    "objectID": "RpyEnvs/rig.html#installing-rtools",
    "href": "RpyEnvs/rig.html#installing-rtools",
    "title": "Managing R versions",
    "section": "Installing rtools",
    "text": "Installing rtools\nWe need rtools to install packages with compiled components. R 4.2 has updated to Rtools 42 (from 40), and so using previous versions of R need older Rtools. The telltale is when trying to install a package, we get errors about ‘make’ not being found. The rig documents imply that rig system update-rtools40 should work, but I get “Error: the system cannot find the path specified”. I’m not sure what path that is, so hard to fix. So, I seem to be OK until I need something that needs ‘make’, and then I’m out of luck.\n\nThe solution\nTo install Rtools40, needed for R 4.0- 4.1, run rig add rtools40. Seems to be all it took, now I can compile. I assume there’s a similar command for even older Rtools if need to downgrade to R 3.x, but haven’t tried.\n\n\nMaybe not the solution\nAs of R4.4, using rig system update-rtools40 seems to work for installs with install.packages, renv, and remotes, but when I install with pak I get pkgbuild errors. And yet, when I try the suggested pkgbuild::check_build_tools() it says ‘Your system is ready to build packages!’. Installing rtools directly from the website (not rig) seems to fix it.",
    "crumbs": [
      "Code Demos",
      "Setting up projects",
      "Managing old R versions with rig"
    ]
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html",
    "href": "betabinomial/random_uneven_groups.html",
    "title": "Random effects with uneven groups",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(withr)\nlibrary(glmmTMB)\nlibrary(patchwork)\nlibrary(latex2exp)\n\ndevtools::load_all()\n\nℹ Loading galenR\n# need consistent plot colors\nmod_types &lt;- c(\"cluster_rand_x\", \"cluster_fixed_x\",\n               \"cluster_rand\", \"cluster_fixed\",\n               \"cluster_raw\", 'no_cluster') \nmod_pal &lt;- make_pal(mod_types, palette = 'ggsci::default_uchicago')\nxrange = c(0, 10)\n# we'll want this for predicting the fits\nxdata &lt;- tibble(x = seq(from = min(xrange), to = max(xrange), by = diff(xrange)/100))\nThis is essentially preamble to understanding the beta-binomial issues we’re having. But we should be able to do a better job sharpening our intuition of what we expect if we start off fully gaussian.\nThis builds on the outline I developed and the work Sarah did (students/Sarah_Taig/Random Effects Simulations) (though I think the emphasis will be different; we’ll see), as well as some beta-binom errorbar checks in caddis/Testing/Error_bars.qmd.\nI think I’ll likely just do gaussian here. Then bb in a parallel doc. And likely will do a model comparison thing too- i.e. spaMM vs glmTMB vs lme4::glmer as in caddis/Analyses/Testing/df_z_t_for_sarah/ (and add {brms}).\nAnd will likely need to incorporate some assessments of the se being calculated at various scales, as explored in caddis/Analyses/Testing/Error_bars.qmd.\nWe’ll need to bring in real data at some point, but I think not in this doc (hence why it’s here, and some of the other testing is in caddis.\nI think we’ll want to do some actual math here somewhere too, to show exactly how the random coefficients relate to the estimates of the fit and the clusters.\nAnd extract the coefficients as Sarah did and see if they match what they should.\nAre we just recapitulating this? Kind of. Slightly different emphasis and we’ll end up taking it further, but should be careful.\nThe data generation function lets us have random slopes. These are super relevant in some cases, e.g. following people or riffles through time, where observations might have different x-values within the cluster. I think I’ll largely ignore them here, because the situation we’re trying to address doesn’t (each cluster has a single x), and so the in-cluster slopes are irrelevant. They could certainly be dealt with in this general exploration, but it would just make everything factorially complicated."
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html#balanced-n-per-cluster-vary-resid-and-random-variance",
    "href": "betabinomial/random_uneven_groups.html#balanced-n-per-cluster-vary-resid-and-random-variance",
    "title": "Random effects with uneven groups",
    "section": "Balanced N per cluster, vary resid and random variance",
    "text": "Balanced N per cluster, vary resid and random variance\n\nGenerate the data\n\nwith_seed(2,\n          sdparams &lt;- expand_grid(\n            N = 50, \n            n_clusters = 10,\n            cluster_N = 'fixed',\n            intercept = 1, \n            slope = 0.5,\n            obs_sigma = c(0.1, 1),\n            sd_rand_intercept = c(0.5, 2),\n            sd_rand_slope = 0, \n            rand_si_cor = 0,\n            # putting this in a list lets me send in vectors that are all the same\n            cluster_x = list(runif(n_clusters,\n                                   min = min(xrange), max = max(xrange))),\n            obs_x_sd = 0\n          )\n)\n\nGenerate the data\n\nsddata &lt;- with_seed(2,\n                    make_analysed_tibble(sdparams, mod_pal)\n)\n\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\n\n\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\n\n\nWarning: There were 4 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `fitted_x_lines = map(full_models, function(x) fit_x_full(x,\n  xvals = xdata))`.\nCaused by warning:\n! SE for fits with fixed clusters are not correct\nℹ Run `dplyr::last_dplyr_warnings()` to see the 3 remaining warnings.\n\n\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\n\nsdstacks &lt;- extract_unnest(sddata, sdparams)\n\nJoining with `by = join_by(group)`\n\n\nThe resulting data; each dot is an observation, the ‘cluster’ (random units) are colors.\n\nsdstacks$points |&gt; \n  ggplot(aes(x = x, y = y, color = factor(cluster, levels = c(1:max(as.numeric(cluster)))))) + \n  geom_point() +\n  facet_grid(obs_sigma ~ sd_rand_intercept) +\n  labs(color = 'Cluster ID')\n\n\n\n\n\n\n\n\n\n\nResults\n\nMain result\nThis is the model fit for the full model (line +- se), along with the estimates of the clusters +- se extracted from the full model fit. Gray points are the underlying observations.\n\nsd_fit &lt;- \n  # The estimates +- se out of the model.\n  sdstacks$fitlines |&gt;\n  filter(model == 'cluster_rand_x') |&gt; \n  ggplot(aes(x = x, color = model)) +\n  # The raw observations\n  geom_point(data = sdstacks$points, aes(y = y), color = 'grey20', alpha = 0.2) +\n  geom_ribbon(aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(aes(y = estimate)) +\n  # The cluster estimates with se errorbars out of the model\n  geom_point(data = sdstacks$fitclusters  |&gt;\n               filter(model == 'cluster_rand_x'),\n             aes(y = estimate)) +\n  geom_errorbar(data = sdstacks$fitclusters |&gt;\n                  filter(model == 'cluster_rand_x'),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  # facet by random residual sigmas \n  facet_grid(obs_sigma ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(obs $\\sigma$)'),\n                                         breaks = NULL, labels = NULL))\n\nsd_fit + theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nHow did we do on the estimates? The error bars here are 95% CI of the estimates. Observation sd does not have an se, it’s the leftovers, and so does not have error bars.\n\nsdstacks$set_v_est |&gt; \n  ggplot(aes(x = term, shape = type)) + \n  geom_point(mapping = aes(y = estimate)) +\n  geom_errorbar(mapping = aes(ymin = cil, ymax = ciu, width = 0.1)) +\n  geom_point(mapping = aes(y = set_value), color = 'firebrick')  +\n  facet_grid(obs_sigma ~ sd_rand_intercept, labeller = 'label_both') +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(obs $\\sigma$)'),\n                                         breaks = NULL, labels = NULL))\n\n\n\n\n\n\n\n# The plot is nicer than the table\n# knitr::kable(set_v_est |&gt; \n#                select(-group, `2.5%` = cil, `97.5%` = ciu) |&gt; \n#                mutate(across(where(is.numeric), \\(x) round(x, digits = 3))))\n\n\n\n\nMethod comparison\nNow, how does that compare with the raw cluster estimates or the fit through the data with no random structure? The dashed blue line is the fit through the raw cluster estimates for visualisation.\n\nsd_method_comparison &lt;-\n  ggplot(mapping = aes(x = x, color = model)) +\n  geom_ribbon(data = sdstacks$fitlines |&gt;\n                filter(model %in% c('cluster_rand_x')),\n              aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(data = sdstacks$fitlines |&gt;\n              filter(model %in% c('cluster_rand_x', 'cluster_raw', 'no_cluster')),\n            aes(y = estimate)) +\n  geom_line(data = sdstacks$fitlinesc |&gt;\n              filter(model %in% c('cluster_raw')),\n            aes(y = estimate), linetype = 'dashed') +\n  geom_point(data = sdstacks$fitclusters  |&gt;\n               filter(model %in% c('cluster_rand_x', \"cluster_raw\")),\n             aes(y = estimate)) +\n  geom_errorbar(data = sdstacks$fitclusters |&gt;\n                  filter(model %in% c('cluster_rand_x', \"cluster_raw\")),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal)  +\n  facet_grid(obs_sigma ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(obs $\\sigma$)'),\n                                         breaks = NULL, labels = NULL))\n\nsd_method_comparison\n\n\n\n\n\n\n\n\n\n\nShrinkage\nAnd here we can see the very different shrinkage. I guess what we really need to do is show how unbalanced shrinkage yields different sloped lines.\n\nsd_shrink &lt;- sdstacks$shrink |&gt; \n  \n  ggplot(\n    aes(x = x, y = cluster_resid,\n        ymin = cluster_resid-se,\n        ymax = cluster_resid + se,\n        color = model)\n  ) +\n  geom_point(position = position_dodge(width = 0.1)) +\n  geom_linerange(position = position_dodge(width = 0.1)) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(obs_sigma ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(obs $\\sigma$)'),\n                                         breaks = NULL, labels = NULL))\n\nsd_shrink\n\nWarning: `position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals."
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html#unbalanced-groups",
    "href": "betabinomial/random_uneven_groups.html#unbalanced-groups",
    "title": "Random effects with uneven groups",
    "section": "Unbalanced groups",
    "text": "Unbalanced groups\nFor now, we’ll just use two of the options from above, the sd_rand of 0.5 and 2 and sigma of 1, so random variance is half or double residual.\nI’m tempted to include an ‘even’ option, but we can either just refer back to the previous or bind_rows the relevant rows.\n\nwith_seed(2,\n          unbalparams &lt;- expand_grid(\n            N = 50, \n            n_clusters = 10,\n            cluster_N = 'uneven',\n            nobs_mean = 'fixed', # keep simple for now\n            force_nclusters = TRUE,\n            force_N = TRUE,\n            # Definitely want 0, the others are essentailly arbitrary\n            nbsize = c(1, 5, 50),\n            intercept = 1, \n            slope = 0.5,\n            obs_sigma = c(1),\n            sd_rand_intercept = c(0.5, 2),\n            sd_rand_slope = 0, \n            rand_si_cor = 0,\n            # putting this in a list lets me send in vectors that are all the same\n            cluster_x = list(runif(n_clusters,\n                                   min = min(xrange), max = max(xrange))),\n            obs_x_sd = 0\n          )\n)\n\n\nunbaldata &lt;- with_seed(2, \n                       make_analysed_tibble(unbalparams, mod_pal)\n)\n\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\n\n\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\n\n\nWarning: There were 6 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `fitted_x_lines = map(full_models, function(x) fit_x_full(x,\n  xvals = xdata))`.\nCaused by warning:\n! SE for fits with fixed clusters are not correct\nℹ Run `dplyr::last_dplyr_warnings()` to see the 5 remaining warnings.\n\n\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\n\n# this can be more useful\nunbal &lt;- extract_unnest(unbaldata, unbalparams)\n\nJoining with `by = join_by(group)`"
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html#results-1",
    "href": "betabinomial/random_uneven_groups.html#results-1",
    "title": "Random effects with uneven groups",
    "section": "Results",
    "text": "Results\n\nData\nAgain, we look at the data, and now can see there are different numbers of points.\n\nunbal$points |&gt; \n  ggplot(aes(x = x, y = y, color = factor(cluster, levels = c(1:max(as.numeric(cluster)))))) + \n  geom_point() +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  labs(color = 'Cluster ID')\n\n\n\n\n\n\n\n\nProbably worth being explicit here about the unevenness. I really do think I’m going to want to rethink this to use nbinom. Then once we get to our data, could parameterise based on what we see.\n\nunbal$points |&gt; \n  ggplot(aes(x = cluster, color = factor(cluster, levels = c(1:max(as.numeric(cluster)))))) + \n  geom_bar() +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\n\nMain result\n\nunbal_fit &lt;- unbal$fitlines |&gt;\n  filter(model == 'cluster_rand_x') |&gt; \n  ggplot(aes(x = x, color = model)) +\n  geom_point(data = unbal$points, aes(y = y), color = 'grey20', alpha = 0.2) +\n  geom_ribbon(aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(aes(y = estimate)) +\n  geom_point(data = unbal$fitclusters  |&gt;\n               filter(model == 'cluster_rand_x'),\n             aes(y = estimate)) +\n  geom_errorbar(data = unbal$fitclusters |&gt;\n                  filter(model == 'cluster_rand_x'),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nunbal_fit + theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nHow did we do on the estimates? The error bars here are 95% CI of the estimates. Observation sd does not have an se, it’s the leftovers, and so does not have error bars.\n\nunbal$set_v_est |&gt; \n  ggplot(aes(x = term, shape = type)) + \n  geom_point(mapping = aes(y = estimate)) +\n  geom_errorbar(mapping = aes(ymin = cil, ymax = ciu, width = 0.1)) +\n  geom_point(mapping = aes(y = set_value), color = 'firebrick') +\n  facet_grid(nbsize ~ sd_rand_intercept, labeller = 'label_both')\n\n\n\n\n\n\n\n\n\n\nMethod comparison\nNow we have some meaningful shrinkage and it can cause the lines to deviate. The dashed blue line is the fit through the raw cluster estimates for visualisation.\n\nunbal_method_comparison &lt;-\n  ggplot(mapping = aes(x = x, color = model)) +\n  geom_ribbon(data = unbal$fitlines |&gt;\n                filter(model %in% c('cluster_rand_x')),\n              aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(data = unbal$fitlines |&gt;\n              filter(model %in% c('cluster_rand_x', 'cluster_raw', 'no_cluster')),\n            aes(y = estimate)) +\n  geom_line(data = unbal$fitlinesc |&gt;\n              filter(model %in% c('cluster_raw')),\n            aes(y = estimate), linetype = 'dashed') +\n  geom_point(data = unbal$fitclusters  |&gt;\n               filter(model %in% c('cluster_rand_x', \"cluster_raw\")),\n             aes(y = estimate)) +\n  geom_errorbar(data = unbal$fitclusters |&gt;\n                  filter(model %in% c('cluster_rand_x', \"cluster_raw\")),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nunbal_method_comparison\n\n\n\n\n\n\n\n\nIt can help to look at one of those more closely just to see what’s happening, and show the fit through the raw clusters as well.\n\nunbal_method_comparison_single &lt;-\n  ggplot(mapping = aes(x = x, color = model)) +\n  geom_ribbon(data = unbal$fitlines |&gt;\n                filter(model %in% c('cluster_rand_x') &\n                         nbsize == 1 & sd_rand_intercept == 2),\n              aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(data = unbal$fitlines |&gt;\n              filter(model %in% c('cluster_rand_x', 'cluster_raw', 'no_cluster') &\n                       nbsize == 1 & sd_rand_intercept == 2),\n            aes(y = estimate)) +\n  geom_line(data = unbal$fitlinesc |&gt;\n              filter(model %in% c('cluster_raw') &\n                       nbsize == 1 & sd_rand_intercept == 2),\n            aes(y = estimate), linetype = 'dashed') +\n  geom_point(data = unbal$fitclusters  |&gt;\n               filter(model %in% c('cluster_rand_x', \"cluster_raw\") &\n                        nbsize == 1 & sd_rand_intercept == 2),\n             aes(y = estimate)) +\n  geom_errorbar(data = unbal$fitclusters |&gt;\n                  filter(model %in% c('cluster_rand_x', \"cluster_raw\") &\n                           nbsize == 1 & sd_rand_intercept == 2),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nunbal_method_comparison_single\n\n\n\n\n\n\n\n\n\n\nShrinkage\nAnd here we can see the very different shrinkage. I guess what we really need to do is show how unbalanced shrinkage yields different sloped lines.\n\nunbal_shrink &lt;- unbal$shrink |&gt; \n  \n  ggplot(\n    aes(x = x, y = cluster_resid,\n        ymin = cluster_resid-se,\n        ymax = cluster_resid + se,\n        color = model)) +\n  geom_point(position = position_dodge(width = 0.1)) +\n  geom_linerange(position = position_dodge(width = 0.1)) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nunbal_shrink\n\nWarning: `position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nDo the clusters with few observations shrink more? Yes, but not a 1:1 relationship\n\nunbal_shrinksize &lt;- unbal$shrink |&gt; \n  \n  ggplot(\n    aes(x = x, y = cluster_resid,\n        ymin = cluster_resid-se,\n        ymax = cluster_resid + se,\n        color = model)) +\n  geom_point(aes(size = n), position = position_dodge(width = 0.1), alpha = 0.5) +\n  # geom_linerange(position = position_dodge(width = 0.1)) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nunbal_shrinksize\n\nWarning: `position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals.\n\n\n\n\n\n\n\n\n\n\n\nFull vs through estimate diagnostic\nAnd finally, do the fits through the random cluster estimates match the line from the random model?\n\nunbal_fit_with_clusters_compare &lt;- unbal$fitlines |&gt; \n  filter(model == 'cluster_rand_x') |&gt;\n  ggplot(aes(x = x, color = model)) +\n  geom_ribbon(aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(aes(y = estimate)) +\n  geom_line(data = unbal$fitlinesc |&gt; \n              filter(model == 'cluster_rand_x'), \n            aes(y = estimate), color = 'black', linetype = 'dashed') +\n  geom_point(data = unbal$fitclusters |&gt; \n               filter(model == 'cluster_rand_x'), \n             aes(y = estimate)) +\n  geom_errorbar(data = unbal$fitclusters |&gt; \n                  filter(model == 'cluster_rand_x'),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nunbal_fit_with_clusters_compare + theme(legend.position = 'none')"
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html#x-is-density",
    "href": "betabinomial/random_uneven_groups.html#x-is-density",
    "title": "Random effects with uneven groups",
    "section": "X is density",
    "text": "X is density\nFor now, we’ll just use two of the options from above, the sd_rand of 0.5 and 2 and sigma of 1, so random variance is half or double residual. By using x_is_density = TRUE, we generate the obs among clusters as before, and then calculate density from that. So this should have the same distribution of cluster sizes, but now it’s arranged on x.\n\nwith_seed(2,\n          xdensparams &lt;- expand_grid(\n            N = 50, \n              n_clusters = 10,\n              cluster_N = 'uneven',\n              nobs_mean = 'fixed', # keep simple for now\n              force_nclusters = TRUE,\n              force_N = TRUE,\n              # Definitely want 0, the others are essentailly arbitrary\n              nbsize = c(1, 5, 50),\n              intercept = 1, \n              slope = 0.5,\n              obs_sigma = c(1),\n              sd_rand_intercept = c(0.5, 2),\n              sd_rand_slope = 0, \n              rand_si_cor = 0,\n              # putting this in a list lets me send in vectors that are all the same\n              cluster_x = list(runif(n_clusters,\n                                     min = min(xrange), max = max(xrange))),\n              obs_x_sd = 0,\n            x_is_density = TRUE\n          )\n)\n\n\nxdensdata &lt;- with_seed(2, \n                       make_analysed_tibble(xdensparams, mod_pal)\n)\n\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\n\n\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\n\n\nWarning: There were 6 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `fitted_x_lines = map(full_models, function(x) fit_x_full(x,\n  xvals = xdata))`.\nCaused by warning:\n! SE for fits with fixed clusters are not correct\nℹ Run `dplyr::last_dplyr_warnings()` to see the 5 remaining warnings.\n\n\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\n\n# this can be more useful\nxdens &lt;- extract_unnest(xdensdata, xdensparams)\n\nJoining with `by = join_by(group)`"
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html#results-2",
    "href": "betabinomial/random_uneven_groups.html#results-2",
    "title": "Random effects with uneven groups",
    "section": "Results",
    "text": "Results\n\nData\nAgain, we look at the data, same as above, but now arranged on x.\n\nxdens$points |&gt; \n  ggplot(aes(x = x, y = y, color = factor(cluster, levels = c(1:max(as.numeric(cluster)))))) + \n  geom_point() +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  labs(color = 'Cluster ID')\n\n\n\n\n\n\n\n\nThis is the same distribution as above.\n\nxdens$points |&gt; \n  ggplot(aes(x = cluster, color = factor(cluster, levels = c(1:max(as.numeric(cluster)))))) + \n  geom_bar() +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\n\nMain result\n\nxdens_fit &lt;- xdens$fitlines |&gt;\n  filter(model == 'cluster_rand_x') |&gt; \n  ggplot(aes(x = x, color = model)) +\n  geom_point(data = xdens$points, aes(y = y), color = 'grey20', alpha = 0.2) +\n  geom_ribbon(aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(aes(y = estimate)) +\n  geom_point(data = xdens$fitclusters  |&gt;\n               filter(model == 'cluster_rand_x'),\n             aes(y = estimate)) +\n  geom_errorbar(data = xdens$fitclusters |&gt;\n                  filter(model == 'cluster_rand_x'),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nxdens_fit + theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nHow did we do on the estimates? The error bars here are 95% CI of the estimates. Observation sd does not have an se, it’s the leftovers, and so does not have error bars.\n\nxdens$set_v_est |&gt; \n  ggplot(aes(x = term, shape = type)) + \n  geom_point(mapping = aes(y = estimate)) +\n  geom_errorbar(mapping = aes(ymin = cil, ymax = ciu, width = 0.1)) +\n  geom_point(mapping = aes(y = set_value), color = 'firebrick') +\n  facet_grid(nbsize ~ sd_rand_intercept, labeller = 'label_both')\n\n\n\n\n\n\n\n\n\n\nMethod comparison\nNow we have some meaningful shrinkage and it can cause the lines to deviate. The dashed blue line is the fit through the raw cluster estimates for visualisation.\n\nxdens_method_comparison &lt;-\n  ggplot(mapping = aes(x = x, color = model)) +\n  geom_ribbon(data = xdens$fitlines |&gt;\n                filter(model %in% c('cluster_rand_x')),\n              aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(data = xdens$fitlines |&gt;\n              filter(model %in% c('cluster_rand_x', 'cluster_raw', 'no_cluster')),\n            aes(y = estimate)) +\n  geom_line(data = xdens$fitlinesc |&gt;\n              filter(model %in% c('cluster_raw')),\n            aes(y = estimate), linetype = 'dashed') +\n  geom_point(data = xdens$fitclusters  |&gt;\n               filter(model %in% c('cluster_rand_x', \"cluster_raw\")),\n             aes(y = estimate)) +\n  geom_errorbar(data = xdens$fitclusters |&gt;\n                  filter(model %in% c('cluster_rand_x', \"cluster_raw\")),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nxdens_method_comparison\n\n\n\n\n\n\n\n\nIt can help to look at one of those more closely just to see what’s happening, and show the fit through the raw clusters as well.\n\nxdens_method_comparison_single &lt;-\n  ggplot(mapping = aes(x = x, color = model)) +\n  geom_ribbon(data = xdens$fitlines |&gt;\n                filter(model %in% c('cluster_rand_x') &\n                         nbsize == 1 & sd_rand_intercept == 2),\n              aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(data = xdens$fitlines |&gt;\n              filter(model %in% c('cluster_rand_x', 'cluster_raw', 'no_cluster') &\n                       nbsize == 1 & sd_rand_intercept == 2),\n            aes(y = estimate)) +\n  geom_line(data = xdens$fitlinesc |&gt;\n              filter(model %in% c('cluster_raw') &\n                       nbsize == 1 & sd_rand_intercept == 2),\n            aes(y = estimate), linetype = 'dashed') +\n  geom_point(data = xdens$fitclusters  |&gt;\n               filter(model %in% c('cluster_rand_x', \"cluster_raw\") &\n                        nbsize == 1 & sd_rand_intercept == 2),\n             aes(y = estimate)) +\n  geom_errorbar(data = xdens$fitclusters |&gt;\n                  filter(model %in% c('cluster_rand_x', \"cluster_raw\") &\n                           nbsize == 1 & sd_rand_intercept == 2),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n                width = 0.2) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nxdens_method_comparison_single\n\n\n\n\n\n\n\n\n\n\nShrinkage\nAnd here we can see the very different shrinkage. I guess what we really need to do is show how xdensanced shrinkage yields different sloped lines.\n\nxdens_shrink &lt;- xdens$shrink |&gt; \n  \n  ggplot(\n    aes(x = x, y = cluster_resid,\n        ymin = cluster_resid-se,\n        ymax = cluster_resid + se,\n        color = model)) +\n  geom_point(position = position_dodge(width = 0.1)) +\n  geom_linerange(position = position_dodge(width = 0.1)) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nxdens_shrink\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nDo the clusters with few observations shrink more? Yes, but not a 1:1 relationship\n\nxdens_shrinksize &lt;- xdens$shrink |&gt; \n  \n  ggplot(\n    aes(x = x, y = cluster_resid,\n        ymin = cluster_resid-se,\n        ymax = cluster_resid + se,\n        color = model)) +\n  geom_point(aes(size = n), position = position_dodge(width = 0.1), alpha = 0.5) +\n  # geom_linerange(position = position_dodge(width = 0.1)) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nxdens_shrinksize\n\n\n\n\n\n\n\n\n\n\nFull vs through estimate diagnostic\nAnd finally, do the fits through the random cluster estimates match the line from the random model?\n\nxdens_fit_with_clusters_compare &lt;- xdens$fitlines |&gt; \n  filter(model == 'cluster_rand_x') |&gt;\n  ggplot(aes(x = x, color = model)) +\n  geom_ribbon(aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(aes(y = estimate)) +\n  geom_line(data = xdens$fitlinesc |&gt; \n              filter(model == 'cluster_rand_x'), \n            aes(y = estimate), color = 'black', linetype = 'dashed') +\n  geom_point(data = xdens$fitclusters |&gt; \n               filter(model == 'cluster_rand_x'), \n             aes(y = estimate)) +\n  geom_errorbar(data = xdens$fitclusters |&gt; \n                  filter(model == 'cluster_rand_x'),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nxdens_fit_with_clusters_compare + theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nSo far, there’s not an obvious issue with biased shrinkage. Everything differs from a naive fit without randoms, but that’s clearly wrong anyway."
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html#larger-numbers-and-freer-obs-per-cluster-distribution",
    "href": "betabinomial/random_uneven_groups.html#larger-numbers-and-freer-obs-per-cluster-distribution",
    "title": "Random effects with uneven groups",
    "section": "Larger numbers and freer obs per cluster distribution",
    "text": "Larger numbers and freer obs per cluster distribution\nI’ll adjust the above to have larger obs and more variance between clusters and also change the nbin mu. Basically crank up the variation and biases. Should I crank up n_clusters? Maybe not (or just a bit), if they’re ‘riffles’.\n\nwith_seed(2,\n          bigdensparams &lt;- expand_grid(\n            N = 1000, \n              n_clusters = 15,\n              cluster_N = 'uneven',\n              nobs_mean = 'x', \n              force_nclusters = FALSE,\n              force_N = FALSE,\n              # Definitely want 0, the others are essentailly arbitrary\n              nbsize = c(0.5, 1, 10),\n              intercept = 1, \n              slope = 0.5,\n              obs_sigma = c(1),\n              sd_rand_intercept = c(0.5, 2),\n              sd_rand_slope = 0, \n              rand_si_cor = 0,\n              # putting this in a list lets me send in vectors that are all the same\n              cluster_x = list(runif(n_clusters,\n                                     min = min(xrange), max = max(xrange))),\n              obs_x_sd = 0,\n            x_is_density = TRUE\n          )\n)\n\n\nbigdensdata &lt;- with_seed(2, \n                       make_analysed_tibble(bigdensparams, mod_pal)\n)\n\ndropping columns from rank-deficient conditional model: cluster9\n\n\ndropping columns from rank-deficient conditional model: cluster8\n\n\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\ndropping columns from rank-deficient conditional model: cluster9\n\n\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\nJoining with `by = join_by(cluster)`\n\n\nWarning: There were 6 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `fitted_x_lines = map(full_models, function(x) fit_x_full(x,\n  xvals = xdata))`.\nCaused by warning:\n! SE for fits with fixed clusters are not correct\nℹ Run `dplyr::last_dplyr_warnings()` to see the 5 remaining warnings.\n\n\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\nJoining with `by = join_by(x, cluster)`\n\n# this can be more useful\nbigdens &lt;- extract_unnest(bigdensdata,bigdensparams)\n\nJoining with `by = join_by(group)`"
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html#results-3",
    "href": "betabinomial/random_uneven_groups.html#results-3",
    "title": "Random effects with uneven groups",
    "section": "Results",
    "text": "Results\n\nData\nAgain, we look at the data, same as above.\n\nbigdens$points |&gt; \n  ggplot(aes(x = x, y = y, color = factor(cluster, levels = c(1:max(as.numeric(cluster)))))) + \n  geom_point() +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  labs(color = 'Cluster ID')\n\n\n\n\n\n\n\n\nThis is the same distribution as above.\n\nbigdens$points |&gt; \n  ggplot(aes(x = cluster, color = factor(cluster, levels = c(1:max(as.numeric(cluster)))))) + \n  geom_bar() +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\n\nMain result\n\nbigdens_fit &lt;-bigdens$fitlines |&gt;\n  filter(model == 'cluster_rand_x') |&gt; \n  ggplot(aes(x = x, color = model)) +\n  geom_point(data =bigdens$points, aes(y = y), color = 'grey20', alpha = 0.2) +\n  geom_ribbon(aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(aes(y = estimate)) +\n  geom_point(data =bigdens$fitclusters  |&gt;\n               filter(model == 'cluster_rand_x'),\n             aes(y = estimate)) +\n  geom_errorbar(data =bigdens$fitclusters |&gt;\n                  filter(model == 'cluster_rand_x'),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nbigdens_fit + theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nHow did we do on the estimates? The error bars here are 95% CI of the estimates. Observation sd does not have an se, it’s the leftovers, and so does not have error bars.\n\nbigdens$set_v_est |&gt; \n  ggplot(aes(x = term, shape = type)) + \n  geom_point(mapping = aes(y = estimate)) +\n  geom_errorbar(mapping = aes(ymin = cil, ymax = ciu, width = 0.1)) +\n  geom_point(mapping = aes(y = set_value), color = 'firebrick') +\n  facet_grid(nbsize ~ sd_rand_intercept, labeller = 'label_both')\n\n\n\n\n\n\n\n\n\n\nMethod comparison\nNow we have some meaningful shrinkage and it can cause the lines to deviate. The dashed blue line is the fit through the raw cluster estimates for visualisation. Where these are deviating from the no cluster fit, it seems appropriate- they’re equalising the weight between clusters, and not letting the super dense ones pull the whole fit around. Which is right, provided we have enough data to estimate the low ones.\n\nbigdens_method_comparison &lt;-\n  ggplot(mapping = aes(x = x, color = model)) +\n  geom_ribbon(data =bigdens$fitlines |&gt;\n                filter(model %in% c('cluster_rand_x')),\n              aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(data =bigdens$fitlines |&gt;\n              filter(model %in% c('cluster_rand_x', 'cluster_raw', 'no_cluster')),\n            aes(y = estimate)) +\n  geom_line(data =bigdens$fitlinesc |&gt;\n              filter(model %in% c('cluster_raw')),\n            aes(y = estimate), linetype = 'dashed') +\n  geom_point(data =bigdens$fitclusters  |&gt;\n               filter(model %in% c('cluster_rand_x', \"cluster_raw\")),\n             aes(y = estimate)) +\n  geom_errorbar(data =bigdens$fitclusters |&gt;\n                  filter(model %in% c('cluster_rand_x', \"cluster_raw\")),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nbigdens_method_comparison\n\n\n\n\n\n\n\n\nIt can help to look at one of those more closely just to see what’s happening, and show the fit through the raw clusters as well.\n\nbigdens_method_comparison_single &lt;-\n  ggplot(mapping = aes(x = x, color = model)) +\n  geom_ribbon(data =bigdens$fitlines |&gt;\n                filter(model %in% c('cluster_rand_x') &\n                         nbsize == 1 & sd_rand_intercept == 2),\n              aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(data =bigdens$fitlines |&gt;\n              filter(model %in% c('cluster_rand_x', 'cluster_raw', 'no_cluster') &\n                       nbsize == 1 & sd_rand_intercept == 2),\n            aes(y = estimate)) +\n  geom_line(data =bigdens$fitlinesc |&gt;\n              filter(model %in% c('cluster_raw') &\n                       nbsize == 1 & sd_rand_intercept == 2),\n            aes(y = estimate), linetype = 'dashed') +\n  geom_point(data =bigdens$fitclusters  |&gt;\n               filter(model %in% c('cluster_rand_x', \"cluster_raw\") &\n                        nbsize == 1 & sd_rand_intercept == 2),\n             aes(y = estimate)) +\n  geom_errorbar(data =bigdens$fitclusters |&gt;\n                  filter(model %in% c('cluster_rand_x', \"cluster_raw\") &\n                           nbsize == 1 & sd_rand_intercept == 2),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n                width = 0.2) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nbigdens_method_comparison_single\n\n\n\n\n\n\n\n\n\n\nShrinkage\nAnd here we can see the very different shrinkage. I guess what we really need to do is show how bigdensanced shrinkage yields different sloped lines.\n\nbigdens_shrink &lt;-bigdens$shrink |&gt; \n  \n  ggplot(\n    aes(x = x, y = cluster_resid,\n        ymin = cluster_resid-se,\n        ymax = cluster_resid + se,\n        color = model)) +\n  geom_point(position = position_dodge(width = 0.1)) +\n  geom_linerange(position = position_dodge(width = 0.1)) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nbigdens_shrink\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nDo the clusters with few observations shrink more? Yes, but not a 1:1 relationship\n\nbigdens_shrinksize &lt;-bigdens$shrink |&gt; \n  \n  ggplot(\n    aes(x = x, y = cluster_resid,\n        ymin = cluster_resid-se,\n        ymax = cluster_resid + se,\n        color = model)) +\n  geom_point(aes(size = n), position = position_dodge(width = 0.1), alpha = 0.5) +\n  # geom_linerange(position = position_dodge(width = 0.1)) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nbigdens_shrinksize\n\n\n\n\n\n\n\n\n\n\nFull vs through estimate diagnostic\nAnd finally, do the fits through the random cluster estimates match the line from the random model?\n\nbigdens_fit_with_clusters_compare &lt;-bigdens$fitlines |&gt; \n  filter(model == 'cluster_rand_x') |&gt;\n  ggplot(aes(x = x, color = model)) +\n  geom_ribbon(aes(y = estimate, ymin = estimate-se, ymax = estimate + se),\n              alpha = 0.25, linetype = 0) +\n  geom_line(aes(y = estimate)) +\n  geom_line(data =bigdens$fitlinesc |&gt; \n              filter(model == 'cluster_rand_x'), \n            aes(y = estimate), color = 'black', linetype = 'dashed') +\n  geom_point(data =bigdens$fitclusters |&gt; \n               filter(model == 'cluster_rand_x'), \n             aes(y = estimate)) +\n  geom_errorbar(data =bigdens$fitclusters |&gt; \n                  filter(model == 'cluster_rand_x'),\n                aes(y = estimate, ymin = estimate-se, ymax = estimate + se)) +\n  scale_color_manual(values = mod_pal) +\n  facet_grid(nbsize ~ sd_rand_intercept) +\n  scale_x_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(random $\\sigma$)'),\n                                         breaks = NULL, labels = NULL)) +\n  scale_y_continuous(sec.axis = sec_axis(~ . , name = TeX(r'(N successes (\"size\"))'),\n                                         breaks = NULL, labels = NULL))\n\nbigdens_fit_with_clusters_compare + theme(legend.position = 'none')"
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html#subclusters",
    "href": "betabinomial/random_uneven_groups.html#subclusters",
    "title": "Random effects with uneven groups",
    "section": "Subclusters",
    "text": "Subclusters\ntesting for now\n\nwith_seed(2,\n          subparams &lt;- expand_grid(\n            N = 10000, \n            n_clusters = 10, # 'riffles'\n            n_subclusters = list(c(100, 1000)), # 'rocks' (absolute, not per-riffle I think?)\n            cluster_N = 'uneven',\n            nobs_mean = 'fixed', # keep simple for now\n            force_nclusters = TRUE,\n            force_N = TRUE,\n            # Definitely want 0, the others are essentailly arbitrary\n            nbsize = c(1, 5, 50),\n            intercept = 1, \n            slope = 0.5,\n            obs_sigma = c(1),\n            sd_rand_intercept = list(c(0.5, 2, 1)),\n            sd_rand_slope = list(c(0, 0, 0)), \n            rand_si_cor = 0,\n            # putting this in a list lets me send in vectors that are all the same\n            cluster_x = list(runif(n_clusters,\n                                   min = min(xrange), max = max(xrange))),\n            obs_x_sd = 0\n          )\n)\n\n\n# This is throwing an error, need to fix.\nsubdata &lt;- with_seed(2, \n                       make_analysed_tibble(subparams, mod_pal)\n)\n\n# this can be more useful\nsubout &lt;- extract_unnest(subdata, subparams)"
  },
  {
    "objectID": "betabinomial/random_uneven_groups.html#next",
    "href": "betabinomial/random_uneven_groups.html#next",
    "title": "Random effects with uneven groups",
    "section": "Next",
    "text": "Next\nAnd how does nesting work, especially if it is again uneven?\nExplicitly look at singletons? Scaling- if we have 50 singletons at an x, is it as ‘good’ as 1 50?\nThen, does the beta introduce additional issues?\nAND, finally, what should we plot? What should we DO. It’s one thing to find out the stats are working right and understand why they’re counterintuitive, it’s another to decide how to present them or whether we need to do something else."
  },
  {
    "objectID": "code_demos.html",
    "href": "code_demos.html",
    "title": "Code Demos",
    "section": "",
    "text": "This section has code and demos that cover many topics and serves several purposes. The pages here are organised thematically, though it will likely take me some iterating on the quarto website yaml to get there.\nThe goal in many of these examples and demos is NOT clean, efficient coding, but exploring HOW the code works and how to accomplish something. That often means creating LOTS of extra variables, copy-paste, and being extremely verbose.",
    "crumbs": [
      "Code Demos"
    ]
  },
  {
    "objectID": "code_demos.html#clarify-thinking-and-testing",
    "href": "code_demos.html#clarify-thinking-and-testing",
    "title": "Code Demos",
    "section": "Clarify thinking and testing",
    "text": "Clarify thinking and testing\nClarify what I’m actually trying to do, and what the expected outcomes are. Then figuring out a) how do get those, and b) why I sometimes don’t, which can be just as important. Doing this sort of testing here instead of in-project can be very helpful as using minimal examples forces me to isolate the issue I’m trying to solve from all the particulars of a given dataset or project structure.",
    "crumbs": [
      "Code Demos"
    ]
  },
  {
    "objectID": "code_demos.html#central-location-for-useful-bits",
    "href": "code_demos.html#central-location-for-useful-bits",
    "title": "Code Demos",
    "section": "Central location for useful bits",
    "text": "Central location for useful bits\nA central point for (relatively) clean, complete things that I want to be able to use across many projects (e.g. 2d autocorrelation, the Johnson distribution, how to use certain packages, fonts, colours and other plotting things etc). Having one central reference point keeps me from having to either reinvent the wheel or remember which project I put the wheel in, and having many slightly different variations. And improvements/extensions can then be accessed across projects.",
    "crumbs": [
      "Code Demos"
    ]
  },
  {
    "objectID": "code_demos.html#understanding-code-testing-beyond-standard-uses",
    "href": "code_demos.html#understanding-code-testing-beyond-standard-uses",
    "title": "Code Demos",
    "section": "Understanding code, testing beyond standard uses",
    "text": "Understanding code, testing beyond standard uses\nI spend quite a lot of time figuring out how to do things in code, understanding how code works, and double-checking everything is working correctly. There are a lot of good demos and tutorials out there (e.g. stackoverflow, some package vignettes and websites), but I often end up needing to figure out weird edge cases. And I often end up doing something similar later, but needing not the final answer, but some intermediate step along the way.",
    "crumbs": [
      "Code Demos"
    ]
  },
  {
    "objectID": "code_demos.html#the-process-of-coding",
    "href": "code_demos.html#the-process-of-coding",
    "title": "Code Demos",
    "section": "The process of coding",
    "text": "The process of coding\nI also think there can be value in seeing how I’ve solved a problem and tested the various avenues, both for my own future reference and others. For one, if I do later have a need for one of those side avenues, they’re available. For another, it exposes the actual process of coding a bit more than the usual tutorial that has cleaned everything up start to finish. And it gives a better starting point for additional development potentially much later if I can see what I’ve already tried. Maybe most importantly, there are few tutorials/walkthroughs I’ve followed that don’t end up with some sort of error, especially as soon as I try to modify them for my purposes. Seeing where I’ve hit errors, what caused them, and how I solved it can be incredibly helpful, rather than only seeing what worked.",
    "crumbs": [
      "Code Demos"
    ]
  },
  {
    "objectID": "data_acquisition/bom_gauges.html",
    "href": "data_acquisition/bom_gauges.html",
    "title": "Bom reference stations",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\n\nlibrary(ggplot2)\nlibrary(sf)\n\nTrying to find BOM gauge locations. Found reference stations. It’s a simple link, but have to use httr2 to download because there’s an error with the user_agent if we try to just download.file.\nMostly including this here as an example of changing user_agent.\nhttp://www.bom.gov.au/waterdata/ has a clickable link to what I want, but the data is buried in a frame so can’t scrape.\nThe below is because I found a link to reference stations and wanted to see what they were.\n\nIs there a url for BOM?\nIt’s just a csv, but have to faff about with httr2 and deparsing back to csv because need to pass a user agent or get a 403 error.\n\nbom2 &lt;- httr2::request(\"http://www.bom.gov.au/water/hrs/content/hrs_station_details.csv\") |&gt;\n  httr2::req_user_agent(\"md-werp\") |&gt; \n  httr2::req_perform() |&gt; \n  httr2::resp_body_string() |&gt; \n  readr::read_csv(skip = 11) |&gt; \n  dplyr::select(site = `Station Name`, \n                gauge = `AWRC Station Number`,  \n                owner = `Data Owner Name`, \n                Latitude, Longitude)\n\nRows: 467 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): AWRC Station Number, Station Name, Jurisdiction, Data Owner Name, D...\ndbl (3): Latitude, Longitude, Catchment Area (km2)\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nbasin &lt;- read_sf(file.path('data','mdb_boundary', 'mdb_boundary.shp'))\n\n\nbom2 &lt;- bom2 |&gt;\n    # lat an long come in as chr because there is a line for 'undefined'\n    dplyr::filter(site != 'undefined') |&gt;\n  st_as_sf(coords = c('Longitude', 'Latitude'), crs = 4326) |&gt; \n  st_transform(crs = st_crs(basin))\n\nThey’re not the gauges I’m looking for. Only 457, instead of 6500, and around the edges of the basin.\n\nggplot() + \n  geom_sf(data = basin) +\n  geom_sf(data = bom2)",
    "crumbs": [
      "Code Demos",
      "Data",
      "BOM gauge locations with httr2"
    ]
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html",
    "href": "data_acquisition/gauge_data_pre_gauge.html",
    "title": "Gauge data and gauged period",
    "section": "",
    "text": "library(foreach)\nlibrary(dplyr)\nlibrary(hydrogauge)\nlibrary(reticulate)\nimport mdba_gauge_getter as gg\nWe often end up needing to pull gauge data for lots of projects. If we just want flow data, and especially if we’re working in python, there’s the mdba_gauge_getter, and I’ve written {hydrogauge} for R and expanded it to cover Victoria, NSW, and Qld. A primary advantage of {hydrogauge} is that it can access more of the API calls and return any desired variables.\nOne issue we often end up having is wanting to grab gauge data for a range of dates that may or may not go earlier than a gauge was put in place. There’s actually an issue with the Kisters API for that, where it silently returns values of 0 for those dates, which is not good. Because it comes from the API itself, it affects both packages.\nFor example, we can go to the website and find the period of record for NSW gauge 410007 (gauges_to_pull[138]) is 10/01/1979 - present. We’ll show first what happens for three situations with both mdba_gauge_getter and hydrogauge\nand then we’ll show how to handle it in {hydrogauge}.",
    "crumbs": [
      "Code Demos",
      "Data",
      "Data from pre-gauged periods"
    ]
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html#all-dates-pre-gauge",
    "href": "data_acquisition/gauge_data_pre_gauge.html#all-dates-pre-gauge",
    "title": "Gauge data and gauged period",
    "section": "All dates pre-gauge",
    "text": "All dates pre-gauge\nIf we ask for the period before the gauge is operational {hydrogauge} passes the API error through.\n\nget_ts_traces(portal = 'NSW', \n                site_list = gaugenum, \n                var_list = '141',\n                start_time = weekbefore,\n                end_time = daybefore,\n                interval = 'day',\n                data_type = 'mean')\n\nError in {: task 1 failed - \"API error number 126. Message: No data within specified period\"\n\n\nWe get an empty dataframe from mdba_gauge_getter\n\ndemo_levs_pre = gg.gauge_pull(r.gaugenum, start_time_user = r.weekbefore, end_time_user = r.daybefore)\n\nC:\\Users\\galen\\DOCUME~1\\code\\WEB_TE~1\\GALEN_~1\\RpyEnvs\\PYTEST~1\\VENV~1\\Lib\\site-packages\\mdba_gauge_getter\\gauge_getter.py:82: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n  gauges['State'] = gauges['gauge_owner'].str.strip().str.split(' ', 1).str[0]\n\ndemo_levs_pre\n\nEmpty DataFrame\nColumns: [DATASOURCEID, SITEID, SUBJECTID, DATETIME, VALUE, QUALITYCODE]\nIndex: []\n\n\nSo, that’s slightly different behavior, but neither is returning misleading data.",
    "crumbs": [
      "Code Demos",
      "Data",
      "Data from pre-gauged periods"
    ]
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html#all-dates-with-gauge",
    "href": "data_acquisition/gauge_data_pre_gauge.html#all-dates-with-gauge",
    "title": "Gauge data and gauged period",
    "section": "All dates with gauge",
    "text": "All dates with gauge\nNow {hydrogauge} gives a dataframe.\n\nget_ts_traces(portal = 'NSW', \n                site_list = gaugenum, \n                var_list = '141',\n                start_time = gaugestart,\n                end_time = weeklater,\n                interval = 'day',\n                data_type = 'mean')\n\n# A tibble: 8 × 20\n  error_num compressed site_short_name    longitude site_name  latitude org_name\n      &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   \n1         0 0          YANCO CK @ OFFTAKE      146. YANCO CRE…    -34.7 WaterNSW\n2         0 0          YANCO CK @ OFFTAKE      146. YANCO CRE…    -34.7 WaterNSW\n3         0 0          YANCO CK @ OFFTAKE      146. YANCO CRE…    -34.7 WaterNSW\n4         0 0          YANCO CK @ OFFTAKE      146. YANCO CRE…    -34.7 WaterNSW\n5         0 0          YANCO CK @ OFFTAKE      146. YANCO CRE…    -34.7 WaterNSW\n6         0 0          YANCO CK @ OFFTAKE      146. YANCO CRE…    -34.7 WaterNSW\n7         0 0          YANCO CK @ OFFTAKE      146. YANCO CRE…    -34.7 WaterNSW\n8         0 0          YANCO CK @ OFFTAKE      146. YANCO CRE…    -34.7 WaterNSW\n# ℹ 13 more variables: value &lt;dbl&gt;, time &lt;dttm&gt;, quality_codes_id &lt;int&gt;,\n#   site &lt;chr&gt;, variable_short_name &lt;chr&gt;, precision &lt;chr&gt;, subdesc &lt;chr&gt;,\n#   variable &lt;chr&gt;, units &lt;chr&gt;, variable_name &lt;chr&gt;, database_timezone &lt;chr&gt;,\n#   quality_codes &lt;chr&gt;, data_type &lt;chr&gt;\n\n\nAs does mdba_gauge_getter\n\ndemo_levs_exists = gg.gauge_pull(r.gaugenum, start_time_user = r.gaugestart, end_time_user = r.weeklater)\ndemo_levs_exists\n\n  DATASOURCEID  SITEID SUBJECTID    DATETIME    VALUE  QUALITYCODE\n0          NSW  410007     WATER  1979-01-10  510.939          255\n1          NSW  410007     WATER  1979-01-11  551.848          130\n2          NSW  410007     WATER  1979-01-12  535.988          130\n3          NSW  410007     WATER  1979-01-13  515.685          130\n4          NSW  410007     WATER  1979-01-14  512.529          130\n5          NSW  410007     WATER  1979-01-15  486.422          130\n6          NSW  410007     WATER  1979-01-16  491.710          130\n7          NSW  410007     WATER  1979-01-17  532.730          130\n\n\nThat again returns what it should. All the dates have data.",
    "crumbs": [
      "Code Demos",
      "Data",
      "Data from pre-gauged periods"
    ]
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html#dates-spanning-gauge-start",
    "href": "data_acquisition/gauge_data_pre_gauge.html#dates-spanning-gauge-start",
    "title": "Gauge data and gauged period",
    "section": "Dates spanning gauge start",
    "text": "Dates spanning gauge start\nNow {hydrogauge} gives a dataframe, but that initial period has value = 0, which is wrong. It should be NA, but the API returns 0 silently.\n\nget_ts_traces(portal = 'NSW', \n                site_list = gaugenum, \n                var_list = '141',\n                start_time = weekbefore,\n                end_time = weeklater,\n                interval = 'day',\n                data_type = 'mean')\n\n# A tibble: 15 × 20\n   error_num compressed site_short_name    longitude site_name latitude org_name\n       &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;   \n 1         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n 2         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n 3         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n 4         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n 5         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n 6         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n 7         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n 8         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n 9         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n10         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n11         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n12         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n13         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n14         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n15         0 0          YANCO CK @ OFFTAKE      146. YANCO CR…    -34.7 WaterNSW\n# ℹ 13 more variables: value &lt;dbl&gt;, time &lt;dttm&gt;, quality_codes_id &lt;int&gt;,\n#   site &lt;chr&gt;, variable_short_name &lt;chr&gt;, precision &lt;chr&gt;, subdesc &lt;chr&gt;,\n#   variable &lt;chr&gt;, units &lt;chr&gt;, variable_name &lt;chr&gt;, database_timezone &lt;chr&gt;,\n#   quality_codes &lt;chr&gt;, data_type &lt;chr&gt;\n\n\nThe same thing happens with mdba_gauge_getter\n\ndemo_levs_span = gg.gauge_pull(r.gaugenum, start_time_user = r.weekbefore, end_time_user = r.weeklater)\ndemo_levs_span\n\n   DATASOURCEID  SITEID SUBJECTID    DATETIME    VALUE  QUALITYCODE\n0           NSW  410007     WATER  1979-01-03    0.000          255\n1           NSW  410007     WATER  1979-01-04    0.000          255\n2           NSW  410007     WATER  1979-01-05    0.000          255\n3           NSW  410007     WATER  1979-01-06    0.000          255\n4           NSW  410007     WATER  1979-01-07    0.000          255\n5           NSW  410007     WATER  1979-01-08    0.000          255\n6           NSW  410007     WATER  1979-01-09    0.000          255\n7           NSW  410007     WATER  1979-01-10  510.939          255\n8           NSW  410007     WATER  1979-01-11  551.848          130\n9           NSW  410007     WATER  1979-01-12  535.988          130\n10          NSW  410007     WATER  1979-01-13  515.685          130\n11          NSW  410007     WATER  1979-01-14  512.529          130\n12          NSW  410007     WATER  1979-01-15  486.422          130\n13          NSW  410007     WATER  1979-01-16  491.710          130\n14          NSW  410007     WATER  1979-01-17  532.730          130\n\n\nSo, that’s not good. Especially the silent part",
    "crumbs": [
      "Code Demos",
      "Data",
      "Data from pre-gauged periods"
    ]
  },
  {
    "objectID": "data_acquisition/hydro_sheds_rivers_atlas.html",
    "href": "data_acquisition/hydro_sheds_rivers_atlas.html",
    "title": "HydroSHEDS/RIVERS/ATLAS",
    "section": "",
    "text": "library(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\ndevtools::load_all()\n\nℹ Loading galenR\n\n\nHydroSHEDS has BASINS, RIVERS, LAKES, and ATLAS, with the atlas being versions of the others with covariates.\nI’m mostly interested in the rivers and sheds atlases. For now, get rivers.\n\nratlas &lt;- \"https://figshare.com/ndownloader/files/20087321\"\n\n\nzipdownload('RiverATLAS_Data_v10.gdb', 'data', ratlas)\n\n[1] TRUE\n\n\nWhat layers are in that gdb?\n\nst_layers(file.path('data',\n                    'RiverATLAS_Data_v10', 'RiverATLAS_v10.gdb'))\n\nDriver: OpenFileGDB \nAvailable layers:\n      layer_name     geometry_type features fields crs_name\n1 RiverATLAS_v10 Multi Line String  8477883    296   WGS 84\n\n\nOnly one layer, but pretty big (it is the whole world), probably worth using the query argument.\nThe docs have a pdf of available columns. The real trick is going to be figuring out their values so we can filter on them.\nThose are in HydroATLAS_v10_Legends.xlsx.\nCan I get the available countries (global administrative areas)?\n\ngad_ids &lt;- st_read(file.path('data',\n                    'RiverATLAS_Data_v10', 'RiverATLAS_v10.gdb'),\n                   query = sprintf(\"SELECT DISTINCT gad_id_cmj FROM \\\"%s\\\"\", 'RiverATLAS_v10'))\n\nReading query `SELECT DISTINCT gad_id_cmj FROM \"RiverATLAS_v10\"'\nfrom data source `C:\\Users\\galen\\Documents\\code\\web_testing\\galen_website\\data\\RiverATLAS_Data_v10\\RiverATLAS_v10.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n\nThose aren’t particularly useful on their own. Are they ISO codes? Supposedly, but they don’t match the numerics.\nThat suggests Australia is 036, but HydroATLAS_v10_Legends.xlsx has it as 14. The country_code does match though.\nThis takes a LONG time, and only returns 5375 shapes, which is way fewer than HydroRIVERS has for Australia.\n\naus_rivers &lt;- st_read(file.path('data',\n                    'RiverATLAS_Data_v10', 'RiverATLAS_v10.gdb'),\n                   query = sprintf(\"SELECT * FROM \\\"%s\\\" WHERE gad_id_cmj = 14\", 'RiverATLAS_v10'))\n\nReading query `SELECT * FROM \"RiverATLAS_v10\" WHERE gad_id_cmj = 14'\nfrom data source `C:\\Users\\galen\\Documents\\code\\web_testing\\galen_website\\data\\RiverATLAS_Data_v10\\RiverATLAS_v10.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 359556 features and 296 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 112.9271 ymin: -54.73958 xmax: 158.9187 ymax: -9.23125\nGeodetic CRS:  WGS 84\n\n\nMapping that\n\nggplot() +\n  geom_sf(data = aus_rivers |&gt; filter(ORD_STRA &gt;= 3), mapping = aes(color = log10(DIST_UP_KM)))\n\n\n\n\n\n\n\n\nWe can also use a spatial filter see wkt_filter argument to pass e.g. a bounding box. I’m guessing that’s super slow.\nSome other interesting variables\nirrigated area upstream\n\nggplot() +\n  geom_sf(data = aus_rivers |&gt; filter(ORD_STRA &gt;= 3), mapping = aes(color = log10(ire_pc_use)))\n\n\n\n\n\n\n\n\nPET\n\nggplot() +\n  geom_sf(data = aus_rivers |&gt; filter(ORD_STRA &gt;= 3), mapping = aes(color = (pet_mm_uyr))) +\n  scale_color_viridis_c(option = 'inferno')\n\n\n\n\n\n\n\n\nprecip\n\nggplot() +\n  geom_sf(data = aus_rivers |&gt; filter(ORD_STRA &gt;= 3), mapping = aes(color = pre_mm_uyr)) +\n  scale_color_viridis_c(option = 'mako', trans = 'log10')\n\n\n\n\n\n\n\n\ntemp\n\nggplot() +\n  geom_sf(data = aus_rivers |&gt; filter(ORD_STRA &gt;= 3), mapping = aes(color = (tmp_dc_uyr))) +\n  scale_color_viridis_c(option = 'magma')\n\n\n\n\n\n\n\n\nDischarge\n\nggplot() +\n  geom_sf(data = aus_rivers |&gt; filter(ORD_STRA &gt;= 3), mapping = aes(color = dis_m3_pyr)) +\n  scale_color_viridis_c(option = 'mako', trans = 'log10')\n\nWarning in scale_color_viridis_c(option = \"mako\", trans = \"log10\"): log-10\ntransformation introduced infinite values.\n\n\n\n\n\n\n\n\n\nRegulation\n\nggplot() +\n  geom_sf(data = aus_rivers |&gt; filter(ORD_STRA &gt;= 3), mapping = aes(color = dor_pc_pva)) +\n  scale_color_viridis_c(option = 'rocket', trans = 'log10')\n\nWarning in scale_color_viridis_c(option = \"rocket\", trans = \"log10\"): log-10\ntransformation introduced infinite values.",
    "crumbs": [
      "Code Demos",
      "Data",
      "HydroSheds river atlas"
    ]
  },
  {
    "objectID": "drones/overlaps.html",
    "href": "drones/overlaps.html",
    "title": "Overlaps",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Figuring out flight plan calcs"
    ]
  },
  {
    "objectID": "drones/overlaps.html#calculating-photo-separation-for-desired-overlaps",
    "href": "drones/overlaps.html#calculating-photo-separation-for-desired-overlaps",
    "title": "Overlaps",
    "section": "Calculating photo separation for desired overlaps",
    "text": "Calculating photo separation for desired overlaps\nThere are a lot of variables that go into this. My goal here is to start by making a plot of how this varies with height. And then sort out how to calculate photo frequency given speed (or vice versa). Though Litchi allows photos by distance interval, which is nice.",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Figuring out flight plan calcs"
    ]
  },
  {
    "objectID": "drones/overlaps.html#gsd",
    "href": "drones/overlaps.html#gsd",
    "title": "Overlaps",
    "section": "GSD",
    "text": "GSD\nGround sampling distance. Lots of people determine heights to fly for a desired ground sampling distance (GSD) in cm/pixel. That’s useful to know, but we’re almost certainly always going to be flying low enough for really good GSD. Our main need here is that the overlap calculations rely on GSD. So, let’s find it. The equation is \\[GSD_w=\\frac{HS_w}{FI_w}\\]\nand\n\\[GSD_h=\\frac{HS_h}{FI_h}\\]\nwhere the subscripts h and w are the dimensions height (along path) and width (across path), H is flight altitude in meters, S is sensor width or height (ie physical size of the sensor in mm), F is focal length (true, in mm. not 35mm equivalent), and I is image width or height in pixels (be careful here- often the default photo settings are 16:9 but the sensor dims are given for 4:3, and the 16:9 crops to get there. Easiest to change to shoot in 4:3, and get better coverage to boot. This gives GSD in m/px. We typically want it in cm/px, so we adjust H in the calculations below.\n\nWhere to get those values?\n\nSensor size I\nDJI sensor sizes are given here https://www.djzphoto.com/blog/2018/12/5/dji-drone-quick-specs-amp-comparison-page. For the drones we have, the Mini 2 is 6.3w x 4.7h (called 1/2.3” sensor), and the Phantom 4 Pro V2 is 13.2w x 8.8h (called 1” sensor).\nParameterise for Mini 2\n\nsw &lt;- 6.3\nsh &lt;- 4.7\n\n\n\nImage size\nImage sizes depend on photo settings, and are given for the Mini 2 and Phantom 4 Pro V2 by DJI. You can also get them from the photo’s EXIF data (either in Properties, or more completely from https://jimpl.com/. It’s a good idea to check, because the set aspect ratio can change. Mini can be 16:9 or 4:3 (best), and Phantom can be 16:9, 4:3, or 3:2 (best).\nMini 2\n\n4:3: 4000×3000\n16:9: 4000×2250\n\nPhantom 4 Pro V2\n\n3:2 Aspect Ratio: 5472×3648\n4:3 Aspect Ratio: 4864×3648\n16:9 Aspect Ratio: 5472×3078\n\nParameterise for Mini 2 at 4:3\n\nIw &lt;- 4000\nIh &lt;- 3000\n\n\n\nFocal length\nI found the true focal length from the EXIF data, though it can be back-calculated from the 35mm format equivalent on the specs pages above.\nThe EXIF had Mini 2 at 4.5, Phantom at 8.8.\nTo back-calculate, the expression is \\(Focal*Scale=35mmequiv\\) where the scale factor is found in the EXIF, or is known and can be looked up for the sensor size. Both drones have a 24mm 35mm equivalent, but scale factors differ.\nParameterise for Mini 2\n\nFocal &lt;- 4.5 \n\n\n\n\nCalculate GSD\nWe now have what we need to get the two GSDs. Let’s get one as a function of height, and another to solve for height given GSD\n\nget_gsd &lt;- function(H_m, focal_mm, sensor_mm, image_px, h_units = 'm') {\n  # make cm\n  if (h_units == 'm') {H_m = H_m * 100} else if (h_units != 'cm') {stop(\"units not supported\")}\n  gsd &lt;- (H_m * sensor_mm)/(focal_mm * image_px)\n}\n\nAnd typically, the recommendation is to use the maximum of the height and width GSDs, as that’s the worst resolution.\n\nworst_gsd &lt;- function(H_m, focal_mm, sensor_w, sensor_h, image_w, image_h, h_units = 'm') {\n  gsd_w &lt;- get_gsd(H_m, focal_mm, sensor_w, image_w)\n  gsd_h &lt;- get_gsd(H_m, focal_mm, sensor_h, image_h)\n  \n  gsd &lt;- pmax(gsd_w, gsd_h)\n}\n\nWhat is that for the mini 2 for a range of heights? I’ll look at all of them, even if that’s not typically what we’d do.\n\nheights &lt;- seq(0, 100, by = 0.1)\nmini_gsd_w &lt;- get_gsd(heights, Focal, sw, Iw)\nmini_gsd_h &lt;- get_gsd(heights, Focal, sh, Ih)\n\nmini_gsd_worst &lt;- worst_gsd(heights, Focal, sw, sh, Iw, Ih)\n\nPlot\n\nmini_gsd &lt;- tibble(altitude_m = heights, width_gsd = mini_gsd_w, height_gsd = mini_gsd_w, worst_gsd = mini_gsd_worst)\n\n\nmini_gsd |&gt; \n  tidyr::pivot_longer(cols = ends_with('gsd'), names_to = 'gsd_type', values_to = 'GSD_cmperpx') |&gt; \nggplot(aes(x = altitude_m, y = GSD_cmperpx, color = gsd_type)) + geom_line()\n\n\n\n\n\n\n\n\nNot much difference there between h and w.\nDoes it match the Pix4d calculator?\nThat gives a GSD of 0.07 for a height of 2m, I get\n\nformat(mini_gsd_w[which(heights == 2)], scientific = FALSE)\n\n[1] \"0.07\"",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Figuring out flight plan calcs"
    ]
  },
  {
    "objectID": "drones/overlaps.html#distance-and-overlap",
    "href": "drones/overlaps.html#distance-and-overlap",
    "title": "Overlaps",
    "section": "Distance and overlap",
    "text": "Distance and overlap\nWhat we really want is to get the flight distance we need to get a desired overlap. That will depend on height and GSD (from which we get the ground area covered per photo). And if we want photo timings, we need speed as well. Using the equations from pix4d, but rearranged so the order of operations makes sense to me.\nThe image size on the ground in meters (again using h and w for height (along path) and width (across path), given image height in px and GSD in cm/px is \\[D_h=(I_hGSD)/100\\]\nThen the flight distance needed for an overlap % O_p expressed in 0-1 is \\[d = D_h-O_pD_h = D_h(1-O_p)\\]\nThe pix4d then goes on to back that back out to the definition of D_h, \\[d=((I_hGSD)/100)(1-O_p)\\]\nIf we can’t set a travel distance d for the drone, we will need to adjust it’s velocity v in m/s and the photo interval t in seconds. In practice, well want to adjust them in tandem (and for a given height). To get the photo interval for a given velocity, it’s simply the desired distance divided by velocity, \\[t = d/v\\]\nand so the velocity for a given interval is\n\\[\nv=d/t\n\\]\nWe can obviously break this down into the equation for d, e.g. \\[t = D_h(1-O_p)/v\\].\n\nFunctions\n\nGiven GSD\nFirst, the ground distance in m\n\nground_dist &lt;- function(image_px, gsd_cmpx) {\n  D &lt;- image_px*gsd_cmpx/100\n}\n\nNext, the distance the drone should travel, given overlap\n\ndrone_dist &lt;- function(ground_dist_m, overlap_prop) {\n  d &lt;- ground_dist_m * (1-overlap_prop)\n}\n\nThe time interval, given velocity\n\nphoto_interval &lt;- function(drone_dist_m, v_ms) {\n  t &lt;- drone_dist_m/v_ms\n}\n\nThe velocity, given interval (not typical, but we might want it since we can only set intervals down to 2 seconds when hand-flying.\n\nvelocity &lt;- function(drone_dist_m, p_s) {\n  v &lt;- drone_dist_m/p_s\n}\n\n\n\nWrap those up\nDrone dist is just one level\n\ndrone_dist_from_gsd &lt;- function(image_px, gsd_cmpx, overlap_prop) {\n  D &lt;- ground_dist(image_px, gsd_cmpx)\n  dd &lt;- drone_dist(D, overlap_prop)\n}\n\nPhoto interval and velocity need to depend on that\n\nphoto_interval_from_gsd &lt;- function(image_px, gsd_cmpx, overlap_prop, v_ms) {\n  dd &lt;- drone_dist_from_gsd(image_px, gsd_cmpx, overlap_prop)\n  t &lt;- photo_interval(dd, v_ms)\n}\n\n\nvelocity_from_gsd &lt;- function(image_px, gsd_cmpx, overlap_prop, p_s) {\n  dd &lt;- drone_dist_from_gsd(image_px, gsd_cmpx, overlap_prop)\n  v &lt;- velocity(dd, p_s)\n}",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Figuring out flight plan calcs"
    ]
  },
  {
    "objectID": "drones/overlaps.html#values-from-h-and-overlap",
    "href": "drones/overlaps.html#values-from-h-and-overlap",
    "title": "Overlaps",
    "section": "Values from H and overlap",
    "text": "Values from H and overlap\nDo this separately for h and w, I guess?\n\nDrone distance\nThis might be all we need for litchi, and regardless, it will tell us about how close flightpaths need to be.\n\ndrone_dist_H_o &lt;- function(H_m, overlap_prop, \n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  gsd &lt;- get_gsd(H_m, focal_mm, \n                   sensor_mm, \n                   image_px, \n                   h_units = 'm')\n  \n  gD &lt;- ground_dist(image_px, gsd)\n  \n  dd &lt;- drone_dist(gD, overlap_prop)\n  \n  return(dd)\n}\n\n\n\nVelocity and photo intervals\n\nv_H_o &lt;- function(H_m, overlap_prop, p_s,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  dd &lt;- drone_dist_H_o(H_m, overlap_prop,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm')\n  v &lt;- dd/p_s # could use velocity(dd/ps), but not worth it here.\n  return(v)\n}\n\n\nt_H_o &lt;- function(H_m, overlap_prop, v_ms,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  dd &lt;- drone_dist_H_o(H_m, overlap_prop,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm')\n  t &lt;- dd/v_ms # could use velocity(dd/ps), but not worth it here.\n  \n  return(t)\n}",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Figuring out flight plan calcs"
    ]
  },
  {
    "objectID": "drones/overlaps.html#plots",
    "href": "drones/overlaps.html#plots",
    "title": "Overlaps",
    "section": "Plots",
    "text": "Plots\n\nDrone distance\nHow does the drone distance depend on height and overlap?\nFor a fixed overlap of 80%, and the same heights sequences as above, for the h dimension (along flight path) and w (across),\n\ndd_h &lt;- drone_dist_H_o(heights, 0.8,\n                       Focal, sh, Ih)\n\ndd_w &lt;- drone_dist_H_o(heights, 0.8,\n                       Focal, sw, Iw)\n\nThose are fairly different, actually\n\ndd_hw &lt;- tibble(altitude_m = heights, width_dd = dd_w, height_dd = dd_h) |&gt; \n  tidyr::pivot_longer(cols = ends_with('dd'), names_to = 'dd_direction', values_to = 'drone_photo_distance')\n\n\nggplot(dd_hw, aes(x = altitude_m, y = drone_photo_distance, color = dd_direction)) + geom_line()\n\n\n\n\n\n\n\n\nCould use plotly here to have mouseover. Or use observable or shiny.\nWhat if we zoom in on the lower altitudes (&lt; 10m)?\n\ndd_hw |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \nggplot(aes(x = altitude_m, y = drone_photo_distance, color = dd_direction)) + geom_line()\n\n\n\n\n\n\n\n\nWe could make a heatmap with overlaps, but I’m not sure we really care that much? We’d really only be interested in maybe 75, 80, 85 or something, and this is for rule of thumb. Do that later.\nWhat are those at 2 and 4 m?\n\ndd_hw |&gt; \n  dplyr::filter(altitude_m %in% c(2,4))\n\n# A tibble: 4 × 3\n  altitude_m dd_direction drone_photo_distance\n       &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;\n1          2 width_dd                    0.56 \n2          2 height_dd                   0.418\n3          4 width_dd                    1.12 \n4          4 height_dd                   0.836\n\n\n\n\nVelocity from interval\nHere, let’s say we have a fixed overlap, and want to know the velocity we need to fly to get that at a given height and photo interval. This sounds contrived, but is pretty much exactly our situation when hand flying- the shortest interval we have is 2seconds, so how fast/slow do we need to fly to get 80% overlap at a range of heights?\nHere, we’ll focus on the h dimension. While we could fly sideways, we usually will fly with forward velocity.\nLet’s say 2 seconds, and then look at a heatmap.\n\nv_h &lt;- v_H_o(heights, 0.8, 2,\n                       Focal, sh, Ih)\n\nv_tib &lt;- tibble(altitude_m = heights, velocity_ms = v_h)\n\n\nggplot(v_tib, aes(x = altitude_m, y = velocity_ms)) + geom_line()\n\n\n\n\n\n\n\n\nAnd again zoom in\n\nv_tib |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \nggplot(aes(x = altitude_m, y = velocity_ms)) + geom_line()\n\n\n\n\n\n\n\n\nSo, to get 80% overlap if we’re limited to intervals of 2 seconds, we’d need to fly at about 0.2m/s at 2m or 0.4 at 4m. Should do a tooltip kinda thing. But for now\n\nv_tib |&gt; \n  dplyr::filter(altitude_m %in% c(2, 4)) \n\n# A tibble: 2 × 2\n  altitude_m velocity_ms\n       &lt;dbl&gt;       &lt;dbl&gt;\n1          2       0.209\n2          4       0.418\n\n\nHow about a heatmap of heights and intervals?\n\nintervals &lt;- seq(0.1, 5, by = 0.1)\n\nvel_map &lt;- tidyr::expand_grid(altitude_m = heights, photo_intervals = intervals) |&gt; \n  mutate(velocity_ms = v_H_o(altitude_m, 0.8, photo_intervals,\n                       Focal, sh, Ih))\n\n\nvel_map |&gt; \nggplot(aes(x = altitude_m, y = photo_intervals, fill = velocity_ms)) + geom_raster()\n\n\n\n\n\n\n\n\nThat’s a dumb scale. Obviously we can fly super fast at 100m and super fast photo intervals\n\nvel_map |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \nggplot(aes(x = altitude_m, y = photo_intervals, fill = velocity_ms)) + geom_raster()\n\n\n\n\n\n\n\n\nStill not particularly useful. Moving on.\n\n\nInterval from velocity\nHow often do we need to take photos given a velocity and height? Let’s start by saying velocity of 1m/s (3.6km/h).\n\nt_h &lt;- t_H_o(heights, 0.8, 1,\n                       Focal, sh, Ih)\n\nt_tib &lt;- tibble(altitude_m = heights, photo_interval = t_h)\n\n\nt_tib |&gt; \n  ggplot(aes(x = altitude_m, y = photo_interval)) + geom_line()\n\n\n\n\n\n\n\n\n\nt_tib |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \n  ggplot(aes(x = altitude_m, y = photo_interval)) + geom_line()\n\n\n\n\n\n\n\n\nSo, at that velocity, the photo interval can’t be as long as 2 seconds without flying at 10m. Clearly that would change as we fly slower.\nMake a heatmap again, I guess, even if it wasn’t very useful before and we know this is just flipping axes and colors.\n\nspeeds &lt;- seq(0.1, 5, by = 0.1)\n\ninterval_map &lt;- tidyr::expand_grid(altitude_m = heights, velocity_m = speeds) |&gt; \n  mutate(photo_interval = t_H_o(altitude_m, 0.8, velocity_m,\n                       Focal, sh, Ih))\n\n\ninterval_map |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \nggplot(aes(x = altitude_m, y = velocity_m, fill = photo_interval)) + geom_raster()",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Figuring out flight plan calcs"
    ]
  },
  {
    "objectID": "drones/overlaps.html#next-steps",
    "href": "drones/overlaps.html#next-steps",
    "title": "Overlaps",
    "section": "Next steps",
    "text": "Next steps\nThe obvious thing to do here is to make an observable quarto where we can select drone type, desired overlap, things we want to set, and it makes the plot and returns values we want. For now though, these plots get us most of the way there.\nWhat do those distances look like for the phantom?",
    "crumbs": [
      "Code Demos",
      "Drones",
      "Figuring out flight plan calcs"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_api_howtocall.html",
    "href": "hydrogauge/hydrogauge_api_howtocall.html",
    "title": "hydrogauge api crude testing",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls for {hydrogauge}"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_api_howtocall.html#access-victoria-water-data-through-api",
    "href": "hydrogauge/hydrogauge_api_howtocall.html#access-victoria-water-data-through-api",
    "title": "hydrogauge api crude testing",
    "section": "Access Victoria water data through API",
    "text": "Access Victoria water data through API\nWe want to access victorian water data for a set of sites. That requires using the api at https://data.water.vic.gov.au/cgi/webservice.exe?[JSON_request] , but it’s poorly documented, and I’ve maybe done one API call ever. Time to figure this out. Will start by piggybacking on the mdba-gauge-getter python that gets water levels as a starting point and then try to get other data.\nFirst, how do we make an API request? Most tutorials use twitter or github, which are well-documented. But let’s try something similar.\npurrr conflicts with jsonlite::flatten, so don’t load tidyverse.\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(httr)\nlibrary(jsonlite)\n\n# Actually end up using\nlibrary(httr2)\n\nLooks like the first thing is the base url. The web says it’s this, and the mdba-gauge-getter uses the same, and then appends json_data\n\nvicurl &lt;- \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\n\nI guess I need to specify something to get. But there is no documentation I can find for what the parameters are. The gauge-getter has a few, so I guess start picking things apart.\n\nparams &lt;- list(\"site_list\" = '232202')\n\n\nresponse &lt;- GET(vicurl, query = params)\nresponse\n\nResponse [https://data.water.vic.gov.au/WMIS/cgi/webservice.exe?site_list=232202]\n  Date: 2024-09-10 23:47\n  Status: 200\n  Content-Type: application/json\n  Size: 103 B\n\n\nFollowing the R api vignette,\n\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nInteresting. It looked like it returned 200 (good) when I printed response and when I look at it in the View, but actually had errors. Where ARE these results?\nso, can we add those missing ‘top-level’ items? I see now that the gauge-getter has a two-level dict\n\nparams &lt;- list(\"version\" = '2')\nresponse &lt;- GET(vicurl, params = params)\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\nparsed\n\nTry the example. can’t get it to even be a character vector\n\n# demourl &lt;- https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}\n\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\nresponse &lt;- GET(vicurl, query = params)\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 122\n\n$error_msg\n[1] \"Parameter error(s) for function get_db_info:Missing: table_name\"\n\n\nWell, that’s a start. at least I’m not getting the top-level errors. Can i just smash that whole demo into a single params list? without the sublists of ‘params’ and ‘filter_values’?\n\n# params &lt;- list(\"function\" = 'get_db_info',\n#                \"version\" = \"3\",\n#                \"table_name\" = \"site\",\n#                \"station\" = \"221001\")\n# response &lt;- GET(vicurl, query = params)\n# \n# # The parsed barfs\n# # parsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n# # parsed\n# \n# response\n\nWhat am I actually asking for here? GET is using modify_url\n\nmodify_url(vicurl, query = params)\n\n[1] \"https://data.water.vic.gov.au/cgi/webservice.exe?function=get_db_info&version=3\"\n\n\nSo that’s using the ’conventional parameter pairs’ option here, not the json . How do I generate some json so I can see if I’m matching the format? auto_unbox = TRUE is needed to not wrap the second values in brackets.\n\ntoJSON(params, auto_unbox = TRUE)\n\n{\"function\":\"get_db_info\",\"version\":\"3\"} \n\n\nOK, so that looks vaguely right, but not leveled. Can we do lists of lists?\n\nnestparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"hash\",\n                               \"filter_values\" = list(\"station\" = \"221001\")))\ntoJSON(nestparams, auto_unbox = TRUE)\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}} \n\n\nWhoa! that looks right. Now, let’s try it. It immediately fails to just use GET. try looking at modify_url to see why.\n\njsonbit &lt;- toJSON(nestparams, auto_unbox = TRUE)\n\n\nmodify_url(vicurl, query = jsonbit)\n\n[1] \"https://data.water.vic.gov.au/cgi/webservice.exe?{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\n\nmodify_url(vicurl, path = jsonbit)\n\n[1] \"https://data.water.vic.gov.au/{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\nGetting a lot of slashes. does it matter? Maybe?\n\nmodify_url(vicurl, scheme = nestparams)\n\n[1] \"get_db_info://data.water.vic.gov.au/cgi/webservice.exe\"                                                                                    \n[2] \"3://data.water.vic.gov.au/cgi/webservice.exe\"                                                                                              \n[3] \"list(table_name = \\\"site\\\", return_type = \\\"hash\\\", filter_values = list(station = \\\"221001\\\"))://data.water.vic.gov.au/cgi/webservice.exe\"\n\n\nIs httpbin a way to test?\n\nbinurl &lt;- \"http://httpbin.org/get\"\n\nbinr &lt;- GET(binurl, query = jsonbit)\nbinr\n\nResponse [http://httpbin.org/get?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2024-09-10 23:47\n  Status: 200\n  Content-Type: application/json\n  Size: 688 B\n{\n  \"args\": {\n    \"{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name...\n  }, \n  \"headers\": {\n    \"Accept\": \"application/json, text/xml, application/xml, */*\", \n    \"Accept-Encoding\": \"deflate, gzip\", \n    \"Host\": \"httpbin.org\", \n    \"User-Agent\": \"libcurl/8.3.0 r-curl/5.2.2 httr/1.4.7\", \n    \"X-Amzn-Trace-Id\": \"Root=1-66e0daae-027da1bb4bf98ad047390fc2\"\n...\n\n\n\nparsedB &lt;- fromJSON(content(binr, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsedB\n\n$args\n$args$`{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}`\n[1] \"\"\n\n\n$headers\n$headers$Accept\n[1] \"application/json, text/xml, application/xml, */*\"\n\n$headers$`Accept-Encoding`\n[1] \"deflate, gzip\"\n\n$headers$Host\n[1] \"httpbin.org\"\n\n$headers$`User-Agent`\n[1] \"libcurl/8.3.0 r-curl/5.2.2 httr/1.4.7\"\n\n$headers$`X-Amzn-Trace-Id`\n[1] \"Root=1-66e0daae-027da1bb4bf98ad047390fc2\"\n\n\n$origin\n[1] \"128.184.189.49\"\n\n$url\n[1] \"http://httpbin.org/get?{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\nThat call looks right.\n\nresponse &lt;- GET(vicurl, query = jsonbit, encode = 'json')\nresponse\n\nResponse [https://data.water.vic.gov.au/WMIS/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2024-09-10 23:47\n  Status: 200\n  Content-Type: application/json\n  Size: 99 B\n\n\n\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Request is not well-formed JSON\\r\\nInput request was not valid JSON\"\n\n\nI pasted it in to notebook++ and it’s exactly the same as the example. So, why isn’t it working?\n\nmodurl &lt;- modify_url(vicurl, query = jsonbit)\nresponse &lt;- GET(modurl)\nresponse\n\nResponse [https://data.water.vic.gov.au/WMIS/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2024-09-10 23:47\n  Status: 200\n  Content-Type: application/json\n  Size: 99 B\n\n\n\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Request is not well-formed JSON\\r\\nInput request was not valid JSON\"",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls for {hydrogauge}"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_api_howtocall.html#httr2",
    "href": "hydrogauge/hydrogauge_api_howtocall.html#httr2",
    "title": "hydrogauge api crude testing",
    "section": "httr2",
    "text": "httr2\nHmmm. I see Hadley has released a v2. And it has a req_body_json. See if that works\n\nlibrary(httr2)\n\nThe req_dry_run lets us see what it’s passing. THat looks right? I think?\n\nreq &lt;- request(vicurl)\nreq %&gt;% \n  req_body_json(nestparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 129\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}\n\n\n\nresp &lt;- req %&gt;% \n  req_body_json(nestparams) %&gt;% \n  req_perform()\n\n\nresp %&gt;% resp_raw()\n\nHTTP/1.1 200 OK\nDate: Tue, 10 Sep 2024 23:48:00 GMT\nContent-Type: application/json\nContent-Length: 103\nConnection: keep-alive\nServer: Microsoft-IIS/10.0\nContent: \n\n{\"error_num\":120,\"error_msg\":\"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"}\n\n\nI thought it was json? but resp_body_json fails with defaults\n\nrj &lt;- resp %&gt;% resp_body_json(check_type = FALSE)\nrj\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nIf I hack it together to check, first note that the resp_body is raw, as we see in the str of resp_raw and in resp_body_raw\n\nresp %&gt;% resp_raw() %&gt;% str()\n\nHTTP/1.1 200 OK\nDate: Tue, 10 Sep 2024 23:48:00 GMT\nContent-Type: application/json\nContent-Length: 103\nConnection: keep-alive\nServer: Microsoft-IIS/10.0\nContent: \n\n{\"error_num\":120,\"error_msg\":\"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"}\nList of 7\n $ method     : chr \"POST\"\n $ url        : chr \"https://data.water.vic.gov.au/WMIS/cgi/webservice.exe\"\n $ status_code: int 200\n $ headers    :List of 6\n  ..$ Date          : chr \"Tue, 10 Sep 2024 23:48:00 GMT\"\n  ..$ Content-Type  : chr \"application/json\"\n  ..$ Content-Length: chr \"103\"\n  ..$ Connection    : chr \"keep-alive\"\n  ..$ Server        : chr \"Microsoft-IIS/10.0\"\n  ..$ Content       : chr \"\"\n  ..- attr(*, \"class\")= chr \"httr2_headers\"\n $ body       : raw [1:103] 7b 22 65 72 ...\n $ request    :List of 7\n  ..$ url     : chr \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\n  ..$ method  : NULL\n  ..$ headers : list()\n  ..$ body    :List of 4\n  .. ..$ data        :List of 3\n  .. .. ..$ function: chr \"get_db_info\"\n  .. .. ..$ version : chr \"3\"\n  .. .. ..$ params  :List of 3\n  .. .. .. ..$ table_name   : chr \"site\"\n  .. .. .. ..$ return_type  : chr \"hash\"\n  .. .. .. ..$ filter_values:List of 1\n  .. .. .. .. ..$ station: chr \"221001\"\n  .. ..$ type        : chr \"json\"\n  .. ..$ content_type: chr \"application/json\"\n  .. ..$ params      :List of 3\n  .. .. ..$ auto_unbox: logi TRUE\n  .. .. ..$ digits    : num 22\n  .. .. ..$ null      : chr \"null\"\n  ..$ fields  : list()\n  ..$ options : list()\n  ..$ policies: list()\n  ..- attr(*, \"class\")= chr \"httr2_request\"\n $ cache      :&lt;environment: 0x0000020c13f8f580&gt; \n - attr(*, \"class\")= chr \"httr2_response\"\n\n\n\nresp %&gt;% resp_body_raw()\n\n  [1] 7b 22 65 72 72 6f 72 5f 6e 75 6d 22 3a 31 32 30 2c 22 65 72 72 6f 72 5f 6d\n [26] 73 67 22 3a 22 4d 69 73 73 69 6e 67 20 74 6f 70 2d 6c 65 76 65 6c 20 5c 22\n [51] 76 65 72 73 69 6f 6e 5c 22 20 69 74 65 6d 5c 72 5c 6e 4d 69 73 73 69 6e 67\n [76] 20 74 6f 70 2d 6c 65 76 65 6c 20 5c 22 70 61 72 61 6d 73 5c 22 20 69 74 65\n[101] 6d 22 7d\n\n\nSo, if we get the raw, convert to char, then pass to JSON, it looks the same as what I’m getting out of resp_body_json.\n\nresp %&gt;% resp_body_raw() %&gt;% rawToChar() %&gt;% fromJSON()\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nNow can I clean that up? THere’s a lot of nesting but some of it is pointless?\n\nnames(rj)\n\n[1] \"error_num\" \"error_msg\"\n\n\nand we know error_num is a single int from the str above. And $return is length 1 with only rows and rows only has the gauge number. After that there’s actually some data.\n\nnames(rj$return)\n\nNULL\n\nnames(rj$return$rows)\n\nNULL\n\nnames(rj$return$rows$`221001`)\n\nNULL\n\n\nNow, can we put that in a dataframe? Is each one length 1?\n\nactualdata &lt;- rj$return$rows$`221001`\n\nall(purrr::map_int(actualdata, length) == 1)\n\n[1] TRUE\n\n\n\ntibout &lt;- as_tibble(actualdata)\ntibout\n\n# A tibble: 0 × 0\n\n\nand toss cols with no data\n\ntibout %&gt;% select(where(~!all(. == '')))\n\n# A tibble: 0 × 0\n\n\nCool, I have data and can do stuff with it. NOW, how do I get the data I actually want, vs the data that happens to be in the one demo on the website?",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls for {hydrogauge}"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_api_howtocall.html#querying-options",
    "href": "hydrogauge/hydrogauge_api_howtocall.html#querying-options",
    "title": "hydrogauge api crude testing",
    "section": "Querying options",
    "text": "Querying options\nBefore we go get data, we need to figure out what we can ask for. First, sort out those functions.\n\nDatasources\nLet’s try get_datasources_by_site. Takes site_list params. Dunno what versions it works for? Tried 2, gave error says has to be 1. I assume the “A” datasource means archive here, since that’s what it means in QLD.\n\nds_s_params &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217\"))\n\n# req &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 84\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217\"}}\n\nresp_ds_s &lt;- req %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_perform()\n\nrbody_ds_s &lt;- resp_ds_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\n\n\nSitelist\nOk, I could make that into a tibble easily enough. It tells me what that site has for datasources, how about another? Can I figure out how to use a sitelist? That’d be really nice, and applies in a lot of places. Cool. I had tried to do \"sitelist\" = c('site', 'site') , and that failed. But it works to have \"site, site\"\n\nds_s_params &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\"))\n\n# req &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 92\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328\"}}\n\nresp_ds_s &lt;- req %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_perform()\n\nrbody_ds_s &lt;- resp_ds_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\n\n\nVariables\nNow, for a given site, we want to know what variables are available. (and I also eventually want to know what all possible variables are, and what happens if we ask for variables that aren’t there). Let’s start with the same two sites. I’m\n\nv_s_params &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\",\n                               \"datasource\" = \"A\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 103\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328\",\"datasource\":\"A\"}}\n\nresp_v_s &lt;- req %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_perform()\n\nrbody_v_s &lt;- resp_v_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nSo, that gives me the number and name of each variable at each site. But it does not give derived variables (discharge being the main one).",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls for {hydrogauge}"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_api_howtocall.html#location-etc",
    "href": "hydrogauge/hydrogauge_api_howtocall.html#location-etc",
    "title": "hydrogauge api crude testing",
    "section": "Location etc",
    "text": "Location etc\nSo, get_db_info seems like it should be useful, but kind of isn’t (see above). Maybe I’ll come back to that. It does let us do geofilters, but they seem both crude and complex. I think I’d probably rather do geofiltering myself and then turn that into a site_list. But might come back to this. The complex_filter might be useful if we can use it to choose sites based on something. But again, I’d probably do that myself? Again, come back to this maybe?\n\nCan we get a list of all sites and all variables?\nMaybe? Do we want to?",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls for {hydrogauge}"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_api_howtocall.html#getting-timeseries",
    "href": "hydrogauge/hydrogauge_api_howtocall.html#getting-timeseries",
    "title": "hydrogauge api crude testing",
    "section": "Getting timeseries",
    "text": "Getting timeseries\nNow, let’s go back to get timeseries, now we know what the variables are. Just for the Barwon at first, and way fewer days. There’s a var_list option, or varfrom and varto. It’s unclear whether the from and to version is numerivally inclusive- ie if we have 100 and 10000, does it get everything? I’ll try with varto = 820, since that’s the highest number avail at the barwon. Gives cryptic error. Try 210? Also, why isn’t 141 available in teh lsit above? Again, cryptic “Assumed fail to reload varcon for 233217: 100.00-&gt; 210.00, as we failed loading it last time”. does it work for 141? Yes.\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"100\",\n                               \"interval\" = \"day\",\n                               \"varto\" = \"141\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"100\",\"interval\":\"day\",\"varto\":\"141\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nDo the other vars work if we ask for them separately? Try pH (which failed above)\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"210\",\n                               \"interval\" = \"day\",\n                               \"varto\" = \"210\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"210\",\"interval\":\"day\",\"varto\":\"210\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nInteresting. How about a var_list?\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nThat works, seems to set the varfrom and varto to each value in the list. I wonder if things like 141 are done in the from/to way because they are derived from 100. But how do we find them when they don’t appear in the get_variable_list? Can I include them in var_list? Hmm. No. what’s going on? see table 3 in qld doc- they are derived, and it gives numbers. Is there a get_available_varcons or similar?\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,141,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\",\n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 227\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,141,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nHow about asking for variables that don’t exist- ie can we just ask for all of them, and it just gives us whatever’s available? The other site (Steavenson, 405328) only has variable 100, so ask for some others. Just gives 100.\n\nsparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"405328\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(sparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"405328\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nresps &lt;- req %&gt;% \n  req_body_json(sparams) %&gt;% \n  req_perform()\n\nrbodys &lt;- resps %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodys)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\n\nCan we get derived (141, etc) with varlist\nIt looks like it’s really just 140 (cumecs) and 141 (ML/d) we’d want, if Vic matches QLD. There are a couple other varcons, but they’re about groundwater.\nLet’s see what get_varcon gives us. I can’t get this not to error, and the examples online have square brackets mixed in the json. I think some combo of c and list might do it but not worth it.\n\nvc_params &lt;- list(\"function\" = 'get_varcon',\n               \"version\" = \"2\",\n               \"params\" = list(\"varcons\" = list(\"varfrom\" = \"100\",\n                               \"varto\" = \"141\",\n                               \"site_list\" = \"233217, 405328\",\n                               \"datasource\" = \"A\",\n                               \"requests\" = list(\"qf1\" = \"1\", \n                                                 \"t1\" = \"20200101000000\",\n                                                 \"t2\" = \"20200131000000\"))))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(vc_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 205\n\n{\"function\":\"get_varcon\",\"version\":\"2\",\"params\":{\"varcons\":{\"varfrom\":\"100\",\"varto\":\"141\",\"site_list\":\"233217, 405328\",\"datasource\":\"A\",\"requests\":{\"qf1\":\"1\",\"t1\":\"20200101000000\",\"t2\":\"20200131000000\"}}}}\n\nresp_vc &lt;- req %&gt;% \n  req_body_json(vc_params) %&gt;% \n  req_perform()\n\nrbody_vc &lt;- resp_vc %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_vc)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nThis isn’t worth it. If we ask for 141 or 140, just do another round with varfrom and varto. Or always get 100, 140, 141, then only sometimes get the others if asked?",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls for {hydrogauge}"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_api_howtocall.html#geolocation",
    "href": "hydrogauge/hydrogauge_api_howtocall.html#geolocation",
    "title": "hydrogauge api crude testing",
    "section": "Geolocation",
    "text": "Geolocation\nSo, get_db_info seems to have a way to get sites by radius or bounding box.. And the flipside is we might want to get sites within a polygon, and so need their locations, which should be available as geoJSON. Try to figure both those out.\nI think get_site_geojson is going to be simpler. start there. Not sure why the site_list can’t have a c(), but the fields has to use it. Works though, gives a feature list. I think those are readable by sf, so that’s good. Not sure what the fields even are though. The help says “Any field that is part of the site table”. So I guess we need to sort that out. On to get_db_info.\n\ng_j_params &lt;- list(\"function\" = 'get_site_geojson',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\",\n                               \"get_elev\" = \"1\",\n                               \"fields\" = c(\"zone\",\"region\")))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(g_j_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 127\n\n{\"function\":\"get_site_geojson\",\"version\":\"2\",\"params\":{\"site_list\":\"233217, 405328\",\"get_elev\":\"1\",\"fields\":[\"zone\",\"region\"]}}\n\nresp_g_j &lt;- req %&gt;% \n  req_body_json(g_j_params) %&gt;% \n  req_perform()\n\nrbody_g_j &lt;- resp_g_j %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_g_j)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls for {hydrogauge}"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_api_howtocall.html#db-info",
    "href": "hydrogauge/hydrogauge_api_howtocall.html#db-info",
    "title": "hydrogauge api crude testing",
    "section": "DB info",
    "text": "DB info\nI was using get_db_info to test above, so let’s just go back to that as a start and think a bit more about what we want. look at the barown. Cannot feed it a list of sites for fitler_values. It does give lat/long/northing, etc, so could use this instead of geoJSON, but geoJSON probably better if we want geo. Using return_type = hash is not noticably different than return_type = array. All examples use hash, so I guess keep using that moving forward. I think we can filter on lots of things in this list, both here and in the geojson.\n\ndbparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"station\" = \"233217\")))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 130\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"station\":\"233217\"}}}\n\nrespdb &lt;- req %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_perform()\n\nrbodydb &lt;- respdb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodydb)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\n\nGeofiltering the db to select sites\nOK, so there are lots of ways to filter (sitename, date, name, etc). Some of those like Name or region or active might be useful, but for now let’s try the geo filters (boudning box and radius).\nTry circle (lat, long, radius in degrees)- use the Barwon lat/long.\nKeeps crashing with timeouts. is it just too much to ask for? Or is the json not right?\n\n# dbparams &lt;- list(\"function\" = 'get_db_info',\n#                \"version\" = \"3\",\n#                \"params\" = list(\"table_name\" = \"site\",\n#                                \"return_type\" = \"hash\",\n#                                \"geo_filter\" = list(\"circle\" = c(\"-38.16\", \"144.35\", \"0.25\"))))\n# \n# req &lt;- request(vicurl)\n# \n# req %&gt;% \n#   req_body_json(dbparams) %&gt;% \n#   req_dry_run()\n# \n# respdb &lt;- req %&gt;% \n#   req_body_json(dbparams) %&gt;% \n#   req_perform()\n# \n# rbodydb &lt;- respdb %&gt;% resp_body_json(check_type = FALSE)\n# \n# str(rbodydb)\n\nLet’s try one of the other geofilters. Otherwise this will work better to write my own if I can et the geojson of all the sites.\nUgh. the rectangle (and region) need nested square brackets. I can make one with c(),\n\ndbparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"hash\",\n                               \"geo_filter\" = list(\"rectangle\" = \n                                                     c(c(\"-38.126\", \"144.282\"),\n                                                       c(\"-38.223\", \"144.406\")))))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 161\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"geo_filter\":{\"rectangle\":[\"-38.126\",\"144.282\",\"-38.223\",\"144.406\"]}}}\n\n# \n# respdb &lt;- req %&gt;% \n#   req_body_json(dbparams) %&gt;% \n#   req_perform()\n# \n# rbodydb &lt;- respdb %&gt;% resp_body_json(check_type = FALSE)\n# \n# str(rbodydb)\n\nthe locations of all guages it’d be\nOK, generating the json for these geo selections is horrible. If I can pull faster to do it myself.\nCan I get a complete gaugelist, nad then pull geojson?",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls for {hydrogauge}"
    ]
  },
  {
    "objectID": "hydrogauge/hydrogauge_api_howtocall.html#get-all-gauges",
    "href": "hydrogauge/hydrogauge_api_howtocall.html#get-all-gauges",
    "title": "hydrogauge api crude testing",
    "section": "Get all gauges",
    "text": "Get all gauges\nWhat’s the best way? with get_db_info? With get_sites_by_datasource? The latter would assume we know all datasources. We probably do just want those in ‘A’ but not positive.\nSo, how about db_info, but maybe not all columns? Tempted to get lat/long or easting/northing.\nTakes a while, but it does run. 189,464 sites??? Yikes. WHY? Clearly i need to filter on something.\n\ndbparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"field_list\" = c(\"station\", \"stname\", \"shortname\")))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 139\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"field_list\":[\"station\",\"stname\",\"shortname\"]}}\n\nrespdb &lt;- req %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_perform()\n\nrbodydb &lt;- respdb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodydb)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nwhat are the variables at some of those sites? Can we figure out what’s up that way? I have a feeling some are groundewater, but there’s no obvious field for that.\n\nv_s_params &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"100089, 100079\",\n                               \"datasource\" = \"A\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/1.0.3 r-curl/5.2.2 libcurl/8.3.0\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 103\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"100089, 100079\",\"datasource\":\"A\"}}\n\nresp_v_s &lt;- req %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_perform()\n\nrbody_v_s &lt;- resp_v_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 120\n $ error_msg: chr \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nUhhh, those have no variables? WHat’s going on here?",
    "crumbs": [
      "Code Demos",
      "Data",
      "Experimenting with API calls for {hydrogauge}"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I am a quantitative community ecologist with a focus on aquatic systems. I address questions of biodiversity maintenance and climate change responses using theoretical and empirical approaches, and also apply these skills to developing models for ecological and water management. I do quite a lot of coding in R and Matlab (and some Python), including data cleaning, statistical analysis, and developing packages and software.\nI am a Research Fellow in the QAEL Lab at Deakin University, where I pursue these research directions and supervise Honours, Masters, and PhD students.\n\n\n\n\nBear creek, California"
  },
  {
    "objectID": "package/httptest2.html",
    "href": "package/httptest2.html",
    "title": "httptest2",
    "section": "",
    "text": "I’m running into all sorts of issues with testing my API package {hydrogauge} and building vignettes. In theory, {httptest2} is the answer, but the examples are all a bit simple, so I want to test them quickly.\nBasically, the examples there just return the raw json anyway, whereas most of my tests are of the outer functions that call the API and then process the outputs. I need that wrapper setup, and want to be able to test cleanly, so want to see if httptest2 actually mocks the api return when it’s not the returned object.\nlibrary(testthat)\nlibrary(httr2)\n\n\nAttaching package: 'httr2'\n\n\nThe following objects are masked from 'package:testthat':\n\n    local_mock, with_mock\n\nlibrary(httptest2)\n\nWarning: package 'httptest2' was built under R version 4.4.2\nStart by using their example\nfaker_person &lt;- function(gender = NULL, birthday_start = NULL, birthday_end = NULL, quantity = 1, locale = \"en_US\", seed = NULL) {\n  faker(\n    \"persons\",\n    gender = gender,\n    birthday_start = birthday_start,\n    birthday_end = birthday_end,\n    quantity = quantity,\n    locale = locale,\n    seed = seed\n  )\n}\n\nfaker &lt;- function(resource, ..., quantity = 1, locale = \"en_US\", seed = NULL) {\n  params &lt;- list(\n    ...,\n    quantity = quantity,\n    locale = locale,\n    seed = seed\n  )\n  names(params) &lt;- paste0(\"_\", names(params))\n\n  request(\"https://fakerapi.it/api/v1\") %&gt;%\n    req_url_path_append(resource) %&gt;%\n    req_url_query(!!!params) %&gt;%\n    req_user_agent(\"my_package_name (http://my.package.web.site)\") %&gt;%\n    req_perform() %&gt;%\n    resp_body_json()\n}\nHa! The example in their example has changed. Guess a good illustration of why this sort of thing is necessary. Used to be length 2. It’s not that the quantity isn’t working right, we’re getting headers. Though that would seem to imply the quantity isn’t working right.\ntest_that(\"We can get people\", {\n  expect_length(faker_person(\"female\", quantity = 2), 6)\n})\n\nTest passed\nNow we get the verbose\noptions(httptest2.verbose = TRUE)\n\nwith_mock_dir(\"person\", {\n  system.time(peeps &lt;- faker_person(\"female\", quantity = 2))\n})\n\nUsing mocks found in tests/testthat/person\n\n\nReading C:\\Users\\galen\\Documents\\code\\web_testing\\galen_website\\tests\\testthat\\person\\fakerapi.it\\api\\v1\\persons-fdcc43.json\n\n\n   user  system elapsed \n   0.03    0.00    0.04\nAnd now it should work from that mocked data\nwith_mock_dir(\"person\", {\n  system.time(peeps2 &lt;- faker_person(\"female\", quantity = 2))\n})\n\nUsing mocks found in tests/testthat/person\n\n\nReading C:\\Users\\galen\\Documents\\code\\web_testing\\galen_website\\tests\\testthat\\person\\fakerapi.it\\api\\v1\\persons-fdcc43.json\n\n\n   user  system elapsed \n   0.02    0.00    0.02\nWhat does that mocked data look like?",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing API packages with httptest2"
    ]
  },
  {
    "objectID": "package/httptest2.html#wrapped",
    "href": "package/httptest2.html#wrapped",
    "title": "httptest2",
    "section": "Wrapped",
    "text": "Wrapped\nNow, the situation I have is that I have a lot of wrapper functions that don’t return raw responses. Does httrtest2 return save the API response, or the function output?\n\nclean_people &lt;- function(fakepeeps) {\n  cleanpeeps &lt;-purrr::map(fakepeeps[[6]], tibble::as_tibble) |&gt; \n    purrr::list_rbind() |&gt; \n    dplyr::select(-'address') |&gt; \n    dplyr::distinct()\n  \n  return(cleanpeeps)\n}\n\nWe then wrap together\n\npullnclean &lt;- function(gender, quantity) {\n  fp &lt;- faker_person(gender = gender, quantity = quantity)\n  cl &lt;- clean_people(fp)\n  return(cl)\n}\n\n\ntest_that(\"Cleaning cleans\", {\n  np &lt;- pullnclean('female', 2)\n  expect_s3_class(np, 'tbl_df')\n})\n\nTest passed \n\n\nNow, what happens if we mock that\n\nwith_mock_dir(\"cleaned\", {\n  system.time(np &lt;- pullnclean('female', 2))\n})\n\nUsing mocks found in tests/testthat/cleaned\n\n\nReading C:\\Users\\galen\\Documents\\code\\web_testing\\galen_website\\tests\\testthat\\cleaned\\fakerapi.it\\api\\v1\\persons-fdcc43.json\n\n\n   user  system elapsed \n   0.04    0.00    0.06 \n\n\n\nwith_mock_dir(\"cleaned\", {\n  system.time(np &lt;- pullnclean('female', 2))\n})\n\nUsing mocks found in tests/testthat/cleaned\n\n\nReading C:\\Users\\galen\\Documents\\code\\web_testing\\galen_website\\tests\\testthat\\cleaned\\fakerapi.it\\api\\v1\\persons-fdcc43.json\n\n\n   user  system elapsed \n   0.03    0.00    0.01 \n\n\nAnd what’s in that file?\nThe json. So maybe this will work. Is there a limit to depth? maybe. Now I just have to go wrap a million tests with with_mock_dir. Better than everything failing though. Ideally I’d reuse calls, but that’ll be harder than just wrapping each test.\n{\n    \"status\": \"OK\",\n    \"code\": 200,\n    \"locale\": \"en_US\",\n    \"seed\": null,\n    \"total\": 2,\n    \"data\": [\n        {\n            \"id\": 1,\n            \"firstname\": \"Della\",\n            \"lastname\": \"Kuhn\",\n            \"email\": \"carter.shields@yahoo.com\",\n            \"phone\": \"+14585126853\",\n            \"birthday\": \"1987-08-09\",\n            \"gender\": \"female\",\n            \"address\": {\n                \"id\": 1,\n                \"street\": \"2158 Cristina Park\",\n                \"streetName\": \"Agnes Tunnel\",\n                \"buildingNumber\": \"94274\",\n                \"city\": \"East Norma\",\n                \"zipcode\": \"73534\",\n                \"country\": \"Iran\",\n                \"country_code\": \"IR\",\n                \"latitude\": 58.805162,\n                \"longitude\": -21.448584\n            },\n            \"website\": \"http://fisher.com\",\n            \"image\": \"http://placeimg.com/640/480/people\"\n        },\n        {\n            \"id\": 2,\n            \"firstname\": \"Sheila\",\n            \"lastname\": \"Goldner\",\n            \"email\": \"goyette.oliver@bruen.com\",\n            \"phone\": \"+17347563160\",\n            \"birthday\": \"2003-07-19\",\n            \"gender\": \"female\",\n            \"address\": {\n                \"id\": 1,\n                \"street\": \"64811 Delfina Ridge Apt. 475\",\n                \"streetName\": \"Dale Orchard\",\n                \"buildingNumber\": \"980\",\n                \"city\": \"North Henri\",\n                \"zipcode\": \"13607\",\n                \"country\": \"Falkland Islands\",\n                \"country_code\": \"FK\",\n                \"latitude\": 48.213107,\n                \"longitude\": -9.087171\n            },\n            \"website\": \"http://padberg.org\",\n            \"image\": \"http://placeimg.com/640/480/people\"\n        }\n    ]\n}",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Testing API packages with httptest2"
    ]
  },
  {
    "objectID": "package/package_creation.html",
    "href": "package/package_creation.html",
    "title": "Creating a package",
    "section": "",
    "text": "I’ve always meant to build packages, but never quite have the time, and often end up with very convoluted projects that are not ideal for shoehorning into a typical package structure, particularly as a first try.\nIn part, I think, that is because my code is often a combination of package-type-things (functions, tests, other software flow) and analyses. It’s unclear what the best approach to this sort of flow is, where we absolutely want functions, but they are very specific to the analyses, of which there are many. Do the analyses go in the package? In two projects, but that’s a hassle? Anyway, that’s a topic for a longer post.\nHere, I have a self-contained, broadly usable bit of code I’m working on to extract information from the Victoria (Australia) waterdata network API. It’s more interesting than a Hello World type package, but also constrained in scope and the analyses can clearly go elsewhere.\nThis doc will be developed as I go, and so like most docs on this site isn’t a tutorial per se, but a sequence of steps, including pitfalls and recoveries (hopefully).\n\n\nFirst, created a repo in git.\nFor the main package development, I’m largely going to follow https://r-pkgs.org/, though I’m hoping I don’t have to read the whole thing (I know I should, but time is time).\nOpened a new Rstudio session (I use renv, but want to adjust some things globally- particularly {devtools}).\ninstall_packages(\"devtools\"), then devtools::dev_sitrep() and install any requested updates (in my case, {roxygen2} was out of date.\n\ndevtools::dev_sitrep()\n\n── R ───────────────────────────────────────────────────────────────────────────\n• version: 4.4.1\n• path: 'C:/Program Files/R/R-4.4.1/'\n── Rtools ──────────────────────────────────────────────────────────────────────\n\n\n• RTools is not installed:\n  Download and install it from: https://cloud.r-project.org/bin/windows/Rtools/\n\n\n── devtools ────────────────────────────────────────────────────────────────────\n• version: 2.4.5\n\n\n• devtools or its dependencies out of date:\n  'cpp11'\n  Update them with `devtools::update_packages(\"devtools\")`\n\n\n── dev package ─────────────────────────────────────────────────────────────────\n• package: 'galenR'\n• path: 'C:/Users/galen/Documents/code/web_testing/galen_website/'\n\n\nR is also out of date (at the time of writing). Fix it with rig, then re-run and update the packages.\n\ndevtools::update_packages('devtools')\n\nCheck the name I used works.\n\n# browse = FALSE to avoid loading browsers rendering this website\navailable::available('hydrogauge', browse = FALSE)\n\n── hydrogauge ──────────────────────────────────────────────────────────────────\nName valid: ✔\nAvailable on CRAN: ✔ \nAvailable on Bioconductor: ✔\nAvailable on GitHub:  ✔ \nAbbreviations: http://www.abbreviations.com/hydrogauge\nWikipedia: https://en.wikipedia.org/wiki/hydrogauge\nWiktionary: https://en.wiktionary.org/wiki/hydrogauge\nSentiment:???\n\n\nLooks good.\nQuestion- I typically use Rprojects and renv to manage dependencies and sandbox projects. I also know that I can just devtools::create() (which I think just wraps usethis::create_package(). Can I start with the Rproj and then turn it into a package? Should I want to?\nAnswer- I just needed to read a bit further. Rstudio has devtools and Rprojects working together. So calling usethis::create_package() builds the project and puts all the scaffolding where it needs to be. I’ll need to cross the existing complex Rproj –&gt; package bridge with another project later, but this is fairly straightforward here.\nSo, let’s create the package.\n\nusethis::create_package('~Galen/Documents/hydrogauge')\n\nAnd that worked with an existing directory. Was kind of worried about that. And it auto-opens a new Rstudio session.\nNow I’m mostly moving over there, but I ran usethis::use_mit_license() to set the license. Looks like description and namespace need work, but do that later.\nLet’s start building.\n\n\n\nI’ve been testing and poking at the API in some qmds here. I expect a lot of that ends up as vignettes in the package, and some is ready to become functions. I’ll likely maintain that flow- test in the qmd, make into functions there, repeat.\nI’m going to go write a function, and then figure out how to use it.\nSwitching to the native pipe |&gt; to see how it goes and reduce dependencies.\n\n\nFor dependencies, I used usethis::use_package(), which installs and auto-populates the DESCRIPTION file. But I think I’m going to try using renv in here too, so I don’t always overwrite system-wide libraries. Hope it doesn’t screw anything up. Usual renv::init().\npackages that are nice to have (e.g. to allow parallelisation) are usethis::use_package('packagename', type = \"suggests\"). And if we want to import a function and not use package::function, use_import_from()- see below for the %dopar%.\n\n\n\nSo, I think usually the thing to do is run devtools::load_all() within the package project. I’m sure I’ll end up doing that. But it is also be possible to run it here, just passing the path, e.g. devtools::load_all(\"path/to/package/dir\"). That lets me work on test and development qmds and scripts here. For a bit. But why? For one, seems like vignettes have to use rmd at least at present. And it keeps all the trial and error out of that repo.\nI got hung up here for a while trying to pre-figure out how I’d install it once it was on github. Turns out it’s super straightforward (see below). It ends up just working as long as the thing on github has basic package structure.\n\n\n\nI’m using roxygen comments, as in the package dev book and roxygen docs for things like inheriting parameters and sections. Running devtools::document() builds the .rd files and means ?function works. There’s a lot of fancy stuff we could do there, but keeping it simple at first.\n\n\n\nI like having actual demonstrations of the code, rather than just function docs, so using usethis::use_vignette to start building some. They have to be in rmd, not qmd. But the visual editor still works, which is nice. Just going to have to re-remember rmd chunk headers.\nI can’t get df_print: paged to work. I think it might be a difference between html and html_vignette, but it is listed as an option in the help. For now using kable even though it’s huge for tables.\nI ended up using the main vignette as an example in the primary github readme. To do that, I did usethis::use_readme_rmd(). Would be good to sort out {pkgdown}, or maybe there’s a streamlined quarto version that builds a website?\n\n\n\nUsing usethis::use_testthat(3) and writing tests was fairly straighforward, but I think there will be a learning curve about what and how to test. I tend to look very granularly at ad-hoc tests, i.e. scanning for weird NA, types, etc. But testthat and the expect_* functions lend themselves to simpler checks.\nIt gets sort of cumbersome if a function takes a while and generates something complex. In that case, I built tests that run the function (and so are fragile to the function just erroring out), and then run multiple different expect_* tests against it to make sure the output is right. As an example,\n\ntest_that(\"derived variables work for ts\", {\n  s3 &lt;- get_response(\"https://data.water.vic.gov.au/cgi/webservice.exe?\",\n                     paramlist = list(\"function\" = 'get_ts_traces',\n                                      \"version\" = \"2\",\n                                      \"params\" = list(\"site_list\" = '233217',\n                                                      \"start_time\" = 20200101,\n                                                      \"varfrom\" = \"100\",\n                                                      \"varto\" = \"140\",\n                                                      \"interval\" = \"day\",\n                                                      \"datasource\" = \"A\",\n                                                      \"end_time\" = 20200105,\n                                                      \"data_type\" = \"mean\",\n                                                      \"multiplier\" = 1)))\n  expect_equal(class(s3), 'list')\n  expect_equal(s3[[1]], 0)\n\n})\n\nAnd then, if I want to hit the function with edge cases, etc, I have to do that over and over. There’s likely a better way, but I’ll need to experiment.\n\n\n\n\nTrying to use %dopar%, but can’t get foreach::%dopar% to work, or with backticks. Putting it in a roxygen comment as @importFrom foreach %dopar% failed too. Seems to have worked to do usethis::use_import_from('foreach', '%dopar%'), which built some new files.\nHaving a hard time testing with doFuture, since it can’t find this package. pause that for a while\n\n\n\nOnce it’s pushed to github, it’s fairly straightforward to install- just\n\ndevtools::install_github(\"galenholt/hydrogauge\")\n\n\n\n\nIt ended up being pretty straightforward to use devtools::check() and using continuous integration with github to run the checks and put the little badges on, as described in the book.\nIt is easy to end up with funny missing pieces and issues if you forget to run devtools::check() and just push to github followed by devtools::install_github or even more likely if you just devtools::install_local from the directory with the code in it. In general, I think the github actions should take care of the check, but I never seem to get the emails that say it’s happened.\nI often forget these steps. But to actually make the package usable other than with load_all(), we seem to need to devtools::check() and if there’s an rmd readme, knit that.\nThe readme ends up being hard to build when it gets updated without reinstalling the package. I ended up in a weird loop once where I couldn’t build the package with a broken readme, but couldn’t update the readme without package updates. The solution is devtools::build_readme() to install a temp package and build the readme from that, and then devtools::check().\n\n\n\n\n\nI use {renv} for package management and reproducibility, which usually (in a non-package Rproject) puts symlinks to the package in a projdir/renv/library/R-4.x/CPUtype/ directory. But interestingly, in a package project, it puts the symlinked library/R4.x/... in a central location (in my case, ~/AppData/Local/R/cache/R/renv/library/PACKAGENAME-HASH/R4.x/….\n\n\n\nI need to make some standard figure functions as part of the package. To test them, I’ve found the {vdiffr} package. It saves a figure if one doesn’t exist and if one does exist, it checks against the saved version. It seems to work well, the only trick is to remember to usethis::use_package('vdiffr', 'Suggests'), or it won’t be available to use by devtools::check().\n\n\n\nWhen I devtools::check() on a package using dplyr, I get a million errors about ‘no visible binding for global variable ’variable_name’’. The issue is that R CMD CHECK is interpreting the bare variable names in mutate, summarise, etc as variables and can’t find them. The code runs fine, but it’s annoying.\nThe answer, unfortunately, is to use the .data[['variable_name']] or .data$variable_name convention everywhere and usethis::use_import_from('rlang', '.data'). That works to get rid of the errors, but now we’ve lost one of the really nice things about writing dplyr code- the simplicity of bare data variable names.\nA similar issue crops up with foreach, and the solution is to pre-initialise them.\nSee no visible binding on check for more details.\n\n\n\nData in /data needs to be included with the package (unsurprisingly- it obviously can’t be accessed if it’s not there). This is easy to have bite though, since my typical default is to gitignore data. Then everything works locally (where the package is originally built and data created), but fails when, for example, we install from github. The obvious answer is to not gitignore data (and so don’t change it very often). We do not seem to need to export the data like we do functions to have access to it in an installed package, (though documenting data is clearly ideal).\n\n\n\nSetting up badges and github actions sounds relatively straightforward, but there are tricks that either aren’t well-documented or I couldn’t figure out at all.\nFor example, if we set up automated coverage and checking,\n\nusethis::use_github_action('test-coverage')\nusethis::use_github_action('check-standard')\n\nThe first thing we need to do is run\n\ndevtools::build_readme()\n\nTo put the badges in.\nGetting the coverage to work was really unclear to me. After cobbling it together, what seems to work is go to the codecov website and sign up for an account. Link it to your github, and then choose the repo you want and click ‘setup’. That tells you to put in a “repository secret”. Do that. It also says to ‘add codecov to your github actions’, which I didn’t do and it seems to work, I think because the use_github_action has already set that bit up (though it doesn’t have the particular text they say to add). That action seems to only run on master or main, so if nothing happens on a branch, try pushing there. You can add branches in the github action in test-coverage.yaml.\nI’ve lately had a hard time getting it to actually connect to repos. Even not being able to use check_results &lt;- covr::package_coverage() followed by covr::codecov(coverage = check_results) to upload manually. I did manage to get it to upload manually by switching from the global token to repo-specific, found on the codecov page, click on the repo, then Configuration then General, then add the repo token to codecov.yml.\n\n\n\n\n\n\nTipManual uploading\n\n\n\nIt does work (the badges in the readme update) to use the manual approach above- we don’t need to re-render the readme locally or run it through CI/CD.\n\n\nBut the larger issue was that the repo had been deactivated. Most of them have (likely start that way), but in the setup steps activation is never mentioned. So go to the repo in codecov, Configuration, General, down in the Danger Zone, click Activate if it isn’t already.\n\n\n\nI have a lot of projects that I want to make into packages, largely to be able to have cleaner testing structure and be able to use devtools::load_all() to access functions instead of source in the head of everything. Pretty simple, just call\n{r}} usethis::create_package(getwd(), check_name = FALSE)\nafter you double check the working directory is right (should be, if you’re already in a project), and the check_name = FALSE because we likely don’t care if it’s a name that makes CRAN happy (if we do, obviously set that to TRUE).\nThis did cause a series of cascading issues that should have been expected. First, since {renv} puts the library in a different place for packages vs projects, it wanted to reinstall all packages. That took a while because I hadn’t been keeping up. Then it took a few restarts of Rstudio to get it to register the structure and make things like shortcuts to devtools::load_all() available.\nIt sure is nice to be able to load functions in though.\n\n\n\nDon’t rely on the github action, it fails largely silently and 404s. Instead, run pkgdown::build_site_github_pages() before pushing to make sure things actually work.\npkgdown::preview_site() to see it locally.\nI had a really hard time getting a github icon/link in the header. It turns out you need two URLs in the DESCRIPTION. like\nURL: https://github.com/galenholt/hydrogauge/, https://galenholt.github.io/hydrogauge/\nThe trailing slashes might be required too.\nI’ve had major problems with github actions failing on vignettes. If the site builds locally, a workaround is to\npkgdown::build_site_github_pages()\npkgdown::deploy_to_branch()\nwhich pushes the locally-produced html.\nLinks between articles is a pain. It would be really nice if it worked like quarto, but doesn’t seem to. For (true) Vignettes (not articles), apparently vignette(\"some-topic\") works, according to the R packages book.\nBut articles don’t link that way, and ‘Non-vignette articles must be linked like any other URL.’ What does that mean in practice? We have to use the website they will be published to, e.g. :\n[their vignette](https://galenholt.github.io/hydrogauge/articles/kiwis-wrapper.html)\nIf you change things in the Readme.Rmd, which is the site homepage, you have to run devtools::build_readme(), or it won’t update.\nI’ve tried using Quarto articles and vignettes, and no matter what I do I cannot get them to build. I just repeatedly get\nRunning `quarto render`\nError: \n! in callr subprocess.\nCaused by error in `.f(.x[[i]], .y[[i]], ...)`:\n! No built file found for vignettes/articles/Data-portals.qmd\nℹ See `$stdout` and `$stderr` for standard output and error.\nAnd there’s nothing useful in .Last.error or $stderr or $stdout. Switching to Rmd seems to have fixed it, but I really wish Quarto would work; all my figure refs etc are broken in Rmd.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "First pass at building a package"
    ]
  },
  {
    "objectID": "package/package_creation.html#getting-started",
    "href": "package/package_creation.html#getting-started",
    "title": "Creating a package",
    "section": "",
    "text": "First, created a repo in git.\nFor the main package development, I’m largely going to follow https://r-pkgs.org/, though I’m hoping I don’t have to read the whole thing (I know I should, but time is time).\nOpened a new Rstudio session (I use renv, but want to adjust some things globally- particularly {devtools}).\ninstall_packages(\"devtools\"), then devtools::dev_sitrep() and install any requested updates (in my case, {roxygen2} was out of date.\n\ndevtools::dev_sitrep()\n\n── R ───────────────────────────────────────────────────────────────────────────\n• version: 4.4.1\n• path: 'C:/Program Files/R/R-4.4.1/'\n── Rtools ──────────────────────────────────────────────────────────────────────\n\n\n• RTools is not installed:\n  Download and install it from: https://cloud.r-project.org/bin/windows/Rtools/\n\n\n── devtools ────────────────────────────────────────────────────────────────────\n• version: 2.4.5\n\n\n• devtools or its dependencies out of date:\n  'cpp11'\n  Update them with `devtools::update_packages(\"devtools\")`\n\n\n── dev package ─────────────────────────────────────────────────────────────────\n• package: 'galenR'\n• path: 'C:/Users/galen/Documents/code/web_testing/galen_website/'\n\n\nR is also out of date (at the time of writing). Fix it with rig, then re-run and update the packages.\n\ndevtools::update_packages('devtools')\n\nCheck the name I used works.\n\n# browse = FALSE to avoid loading browsers rendering this website\navailable::available('hydrogauge', browse = FALSE)\n\n── hydrogauge ──────────────────────────────────────────────────────────────────\nName valid: ✔\nAvailable on CRAN: ✔ \nAvailable on Bioconductor: ✔\nAvailable on GitHub:  ✔ \nAbbreviations: http://www.abbreviations.com/hydrogauge\nWikipedia: https://en.wikipedia.org/wiki/hydrogauge\nWiktionary: https://en.wiktionary.org/wiki/hydrogauge\nSentiment:???\n\n\nLooks good.\nQuestion- I typically use Rprojects and renv to manage dependencies and sandbox projects. I also know that I can just devtools::create() (which I think just wraps usethis::create_package(). Can I start with the Rproj and then turn it into a package? Should I want to?\nAnswer- I just needed to read a bit further. Rstudio has devtools and Rprojects working together. So calling usethis::create_package() builds the project and puts all the scaffolding where it needs to be. I’ll need to cross the existing complex Rproj –&gt; package bridge with another project later, but this is fairly straightforward here.\nSo, let’s create the package.\n\nusethis::create_package('~Galen/Documents/hydrogauge')\n\nAnd that worked with an existing directory. Was kind of worried about that. And it auto-opens a new Rstudio session.\nNow I’m mostly moving over there, but I ran usethis::use_mit_license() to set the license. Looks like description and namespace need work, but do that later.\nLet’s start building.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "First pass at building a package"
    ]
  },
  {
    "objectID": "package/package_creation.html#building",
    "href": "package/package_creation.html#building",
    "title": "Creating a package",
    "section": "",
    "text": "I’ve been testing and poking at the API in some qmds here. I expect a lot of that ends up as vignettes in the package, and some is ready to become functions. I’ll likely maintain that flow- test in the qmd, make into functions there, repeat.\nI’m going to go write a function, and then figure out how to use it.\nSwitching to the native pipe |&gt; to see how it goes and reduce dependencies.\n\n\nFor dependencies, I used usethis::use_package(), which installs and auto-populates the DESCRIPTION file. But I think I’m going to try using renv in here too, so I don’t always overwrite system-wide libraries. Hope it doesn’t screw anything up. Usual renv::init().\npackages that are nice to have (e.g. to allow parallelisation) are usethis::use_package('packagename', type = \"suggests\"). And if we want to import a function and not use package::function, use_import_from()- see below for the %dopar%.\n\n\n\nSo, I think usually the thing to do is run devtools::load_all() within the package project. I’m sure I’ll end up doing that. But it is also be possible to run it here, just passing the path, e.g. devtools::load_all(\"path/to/package/dir\"). That lets me work on test and development qmds and scripts here. For a bit. But why? For one, seems like vignettes have to use rmd at least at present. And it keeps all the trial and error out of that repo.\nI got hung up here for a while trying to pre-figure out how I’d install it once it was on github. Turns out it’s super straightforward (see below). It ends up just working as long as the thing on github has basic package structure.\n\n\n\nI’m using roxygen comments, as in the package dev book and roxygen docs for things like inheriting parameters and sections. Running devtools::document() builds the .rd files and means ?function works. There’s a lot of fancy stuff we could do there, but keeping it simple at first.\n\n\n\nI like having actual demonstrations of the code, rather than just function docs, so using usethis::use_vignette to start building some. They have to be in rmd, not qmd. But the visual editor still works, which is nice. Just going to have to re-remember rmd chunk headers.\nI can’t get df_print: paged to work. I think it might be a difference between html and html_vignette, but it is listed as an option in the help. For now using kable even though it’s huge for tables.\nI ended up using the main vignette as an example in the primary github readme. To do that, I did usethis::use_readme_rmd(). Would be good to sort out {pkgdown}, or maybe there’s a streamlined quarto version that builds a website?\n\n\n\nUsing usethis::use_testthat(3) and writing tests was fairly straighforward, but I think there will be a learning curve about what and how to test. I tend to look very granularly at ad-hoc tests, i.e. scanning for weird NA, types, etc. But testthat and the expect_* functions lend themselves to simpler checks.\nIt gets sort of cumbersome if a function takes a while and generates something complex. In that case, I built tests that run the function (and so are fragile to the function just erroring out), and then run multiple different expect_* tests against it to make sure the output is right. As an example,\n\ntest_that(\"derived variables work for ts\", {\n  s3 &lt;- get_response(\"https://data.water.vic.gov.au/cgi/webservice.exe?\",\n                     paramlist = list(\"function\" = 'get_ts_traces',\n                                      \"version\" = \"2\",\n                                      \"params\" = list(\"site_list\" = '233217',\n                                                      \"start_time\" = 20200101,\n                                                      \"varfrom\" = \"100\",\n                                                      \"varto\" = \"140\",\n                                                      \"interval\" = \"day\",\n                                                      \"datasource\" = \"A\",\n                                                      \"end_time\" = 20200105,\n                                                      \"data_type\" = \"mean\",\n                                                      \"multiplier\" = 1)))\n  expect_equal(class(s3), 'list')\n  expect_equal(s3[[1]], 0)\n\n})\n\nAnd then, if I want to hit the function with edge cases, etc, I have to do that over and over. There’s likely a better way, but I’ll need to experiment.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "First pass at building a package"
    ]
  },
  {
    "objectID": "package/package_creation.html#issues",
    "href": "package/package_creation.html#issues",
    "title": "Creating a package",
    "section": "",
    "text": "Trying to use %dopar%, but can’t get foreach::%dopar% to work, or with backticks. Putting it in a roxygen comment as @importFrom foreach %dopar% failed too. Seems to have worked to do usethis::use_import_from('foreach', '%dopar%'), which built some new files.\nHaving a hard time testing with doFuture, since it can’t find this package. pause that for a while",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "First pass at building a package"
    ]
  },
  {
    "objectID": "package/package_creation.html#installing",
    "href": "package/package_creation.html#installing",
    "title": "Creating a package",
    "section": "",
    "text": "Once it’s pushed to github, it’s fairly straightforward to install- just\n\ndevtools::install_github(\"galenholt/hydrogauge\")",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "First pass at building a package"
    ]
  },
  {
    "objectID": "package/package_creation.html#checking-building-etc",
    "href": "package/package_creation.html#checking-building-etc",
    "title": "Creating a package",
    "section": "",
    "text": "It ended up being pretty straightforward to use devtools::check() and using continuous integration with github to run the checks and put the little badges on, as described in the book.\nIt is easy to end up with funny missing pieces and issues if you forget to run devtools::check() and just push to github followed by devtools::install_github or even more likely if you just devtools::install_local from the directory with the code in it. In general, I think the github actions should take care of the check, but I never seem to get the emails that say it’s happened.\nI often forget these steps. But to actually make the package usable other than with load_all(), we seem to need to devtools::check() and if there’s an rmd readme, knit that.\nThe readme ends up being hard to build when it gets updated without reinstalling the package. I ended up in a weird loop once where I couldn’t build the package with a broken readme, but couldn’t update the readme without package updates. The solution is devtools::build_readme() to install a temp package and build the readme from that, and then devtools::check().",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "First pass at building a package"
    ]
  },
  {
    "objectID": "package/package_creation.html#ephemera",
    "href": "package/package_creation.html#ephemera",
    "title": "Creating a package",
    "section": "",
    "text": "I use {renv} for package management and reproducibility, which usually (in a non-package Rproject) puts symlinks to the package in a projdir/renv/library/R-4.x/CPUtype/ directory. But interestingly, in a package project, it puts the symlinked library/R4.x/... in a central location (in my case, ~/AppData/Local/R/cache/R/renv/library/PACKAGENAME-HASH/R4.x/….\n\n\n\nI need to make some standard figure functions as part of the package. To test them, I’ve found the {vdiffr} package. It saves a figure if one doesn’t exist and if one does exist, it checks against the saved version. It seems to work well, the only trick is to remember to usethis::use_package('vdiffr', 'Suggests'), or it won’t be available to use by devtools::check().\n\n\n\nWhen I devtools::check() on a package using dplyr, I get a million errors about ‘no visible binding for global variable ’variable_name’’. The issue is that R CMD CHECK is interpreting the bare variable names in mutate, summarise, etc as variables and can’t find them. The code runs fine, but it’s annoying.\nThe answer, unfortunately, is to use the .data[['variable_name']] or .data$variable_name convention everywhere and usethis::use_import_from('rlang', '.data'). That works to get rid of the errors, but now we’ve lost one of the really nice things about writing dplyr code- the simplicity of bare data variable names.\nA similar issue crops up with foreach, and the solution is to pre-initialise them.\nSee no visible binding on check for more details.\n\n\n\nData in /data needs to be included with the package (unsurprisingly- it obviously can’t be accessed if it’s not there). This is easy to have bite though, since my typical default is to gitignore data. Then everything works locally (where the package is originally built and data created), but fails when, for example, we install from github. The obvious answer is to not gitignore data (and so don’t change it very often). We do not seem to need to export the data like we do functions to have access to it in an installed package, (though documenting data is clearly ideal).\n\n\n\nSetting up badges and github actions sounds relatively straightforward, but there are tricks that either aren’t well-documented or I couldn’t figure out at all.\nFor example, if we set up automated coverage and checking,\n\nusethis::use_github_action('test-coverage')\nusethis::use_github_action('check-standard')\n\nThe first thing we need to do is run\n\ndevtools::build_readme()\n\nTo put the badges in.\nGetting the coverage to work was really unclear to me. After cobbling it together, what seems to work is go to the codecov website and sign up for an account. Link it to your github, and then choose the repo you want and click ‘setup’. That tells you to put in a “repository secret”. Do that. It also says to ‘add codecov to your github actions’, which I didn’t do and it seems to work, I think because the use_github_action has already set that bit up (though it doesn’t have the particular text they say to add). That action seems to only run on master or main, so if nothing happens on a branch, try pushing there. You can add branches in the github action in test-coverage.yaml.\nI’ve lately had a hard time getting it to actually connect to repos. Even not being able to use check_results &lt;- covr::package_coverage() followed by covr::codecov(coverage = check_results) to upload manually. I did manage to get it to upload manually by switching from the global token to repo-specific, found on the codecov page, click on the repo, then Configuration then General, then add the repo token to codecov.yml.\n\n\n\n\n\n\nTipManual uploading\n\n\n\nIt does work (the badges in the readme update) to use the manual approach above- we don’t need to re-render the readme locally or run it through CI/CD.\n\n\nBut the larger issue was that the repo had been deactivated. Most of them have (likely start that way), but in the setup steps activation is never mentioned. So go to the repo in codecov, Configuration, General, down in the Danger Zone, click Activate if it isn’t already.\n\n\n\nI have a lot of projects that I want to make into packages, largely to be able to have cleaner testing structure and be able to use devtools::load_all() to access functions instead of source in the head of everything. Pretty simple, just call\n{r}} usethis::create_package(getwd(), check_name = FALSE)\nafter you double check the working directory is right (should be, if you’re already in a project), and the check_name = FALSE because we likely don’t care if it’s a name that makes CRAN happy (if we do, obviously set that to TRUE).\nThis did cause a series of cascading issues that should have been expected. First, since {renv} puts the library in a different place for packages vs projects, it wanted to reinstall all packages. That took a while because I hadn’t been keeping up. Then it took a few restarts of Rstudio to get it to register the structure and make things like shortcuts to devtools::load_all() available.\nIt sure is nice to be able to load functions in though.\n\n\n\nDon’t rely on the github action, it fails largely silently and 404s. Instead, run pkgdown::build_site_github_pages() before pushing to make sure things actually work.\npkgdown::preview_site() to see it locally.\nI had a really hard time getting a github icon/link in the header. It turns out you need two URLs in the DESCRIPTION. like\nURL: https://github.com/galenholt/hydrogauge/, https://galenholt.github.io/hydrogauge/\nThe trailing slashes might be required too.\nI’ve had major problems with github actions failing on vignettes. If the site builds locally, a workaround is to\npkgdown::build_site_github_pages()\npkgdown::deploy_to_branch()\nwhich pushes the locally-produced html.\nLinks between articles is a pain. It would be really nice if it worked like quarto, but doesn’t seem to. For (true) Vignettes (not articles), apparently vignette(\"some-topic\") works, according to the R packages book.\nBut articles don’t link that way, and ‘Non-vignette articles must be linked like any other URL.’ What does that mean in practice? We have to use the website they will be published to, e.g. :\n[their vignette](https://galenholt.github.io/hydrogauge/articles/kiwis-wrapper.html)\nIf you change things in the Readme.Rmd, which is the site homepage, you have to run devtools::build_readme(), or it won’t update.\nI’ve tried using Quarto articles and vignettes, and no matter what I do I cannot get them to build. I just repeatedly get\nRunning `quarto render`\nError: \n! in callr subprocess.\nCaused by error in `.f(.x[[i]], .y[[i]], ...)`:\n! No built file found for vignettes/articles/Data-portals.qmd\nℹ See `$stdout` and `$stderr` for standard output and error.\nAnd there’s nothing useful in .Last.error or $stderr or $stdout. Switching to Rmd seems to have fixed it, but I really wish Quarto would work; all my figure refs etc are broken in Rmd.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "First pass at building a package"
    ]
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html",
    "href": "parallelism/changing_batchtools_template.html",
    "title": "Changing batchtools template",
    "section": "",
    "text": "I got future.batchtools working, and now I have a bunch of follow-up tests.\nCan we call a different template, even if it’s not named batchtools.slurm.tmpl?\nIn my slurm testing repo, I have the default templates from future.batchtools and batchtools saved in /batchtools_templates. The one from future.batchtools is also saved as batchtools.slurm.tmpl in the outer directory (where it gets found by default and we know it works).",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Switching templates, matching to resources"
    ]
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html#modify-path",
    "href": "parallelism/changing_batchtools_template.html#modify-path",
    "title": "Changing batchtools template",
    "section": "Modify path",
    "text": "Modify path\nFirst, I’ll just send a path, as in plan(batchtools_slurm, template = \"/path/to/batchtools.slurm.tmpl\") to the original template from future.batchtools that I copied to make /batchtools.slurm.tmpl.\nAnd then I’ll use the batchtools template and see how that works.\nTo do both of these, I’ll modify slurm_r_tests/testing_future_batchtools.R to take the path as an argument so I can pass it from the command line.\nI could loop over that like I did when testing single-node plans but I think I don’t for the moment.\nCheck that the default works (no path argument at command line, sbatch batchtools_R.sh testing_future_batchtools.R. That works.\nThe same template but with a different name and in a subdirectory works if we’re careful with the path-\nsbatch batchtools_R.sh testing_future_batchtools.R ./batchtools_templates/slur\nm.tmpl\nThe template that comes from batchtools doesn’t work.\nsbatch batchtools_R.sh testing_future_batchtools.R ./batchtools_templates/slur\nm-simple.tmpl\nWhy not? I think because it doesn’t actually specify any resources. It expects that to come in from elsewhere.\nSo, let’s figure out resource specification, and then try that one again.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Switching templates, matching to resources"
    ]
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html#specifying-resources",
    "href": "parallelism/changing_batchtools_template.html#specifying-resources",
    "title": "Changing batchtools template",
    "section": "Specifying resources",
    "text": "Specifying resources\nRemember, this is per job.\nWhat I want to do is use tweak- e.g. tweak(batchtools_slurm, resources = list(ncpus = 10, nodes = 2)). Again, remembering this is what gets requested for every job-\n\nso does it ever make sense to request nodes &gt; 1?\nShould I only request ncpus &gt; 1 if I use list-plans to then go to multisession or multicore?\n\nBut first, does that let me use the slurm-simple template that didn’t request anything itself?\nI’ve done it by hardcoding some resource requests in tweak_resources.R in the test repo, but they could be build in a script if we wanted.\nDidn’t work. But I think the answer is in th %&lt; resources$whatever &gt;% - THOSE ARE THE NAMES. AND I CAN CHANGE WHAT THE LIST RETURNS- character instead of integer, for ex.\nSo the slurm-simple.tmpl from batchtools has\n#SBATCH --job-name=&lt;%= job.name %&gt;\n#SBATCH --output=&lt;%= log.file %&gt;\n#SBATCH --error=&lt;%= log.file %&gt;\n#SBATCH --time=&lt;%= ceiling(resources$walltime / 60) %&gt;\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=&lt;%= resources$ncpus %&gt;\n#SBATCH --mem-per-cpu=&lt;%= resources$memory %&gt;\nAnd so only lets us set those values (job-name, output, error, time, ntasks, cpus-per-task, and mem-per-cpu), and we have to do that in the right slots of resources and with the right type- e.g. resources$walltime sets --time, and has to be numeric. BUT, we could change that with a different template file that just uses the character vector “hh:mm:ss” (as I do with the non-template any_R.sh. Or passing things like 4GB instead of memory in mb.\nNow, if I run that with resources$ncpus = 12, I get the same output as before. But I think I’m using 100 cpus, but each one is also sitting on 11 others. I’m just not saving what I need to check. The ntasks instead of nodes is a bit confusing too- I thought tasks were threads on the cpu. Maybe that is the case- the hardcode nthreads = 1 here says don’t thread below the cpu level. And no node request I assume just defaults to 1.\nOR if we aren’t defining nodes, does slurm just auto-assign work to cpus across nodes? ie node-agnostic? And then we don’t have to worry about necessarily matching work to CPUs on nodes?\nI think the future.batchtools template batchtools_templates/slurm.tmpl is more flexible. Instead of individually filling parts of the slurm script as above, it just fills whatever options we want. It has the minimal set to get things to run hardcoded, but the section\n## Resources needed:\n&lt;% if (length(resources) &gt; 0) {\n  opts &lt;- unlist(resources, use.names = TRUE)\n  opts &lt;- sprintf(\"--%s=%s\", names(opts), opts)\n  opts &lt;- paste(opts, collapse = \" \") %&gt;\n#SBATCH &lt;%= opts %&gt;\n&lt;% } %&gt;\nJust writes in anything. So, can I get that to work in tweak_resources.R? It should be easier, but wasn’t working for me.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Switching templates, matching to resources"
    ]
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html#tweak-has-to-match-the-template",
    "href": "parallelism/changing_batchtools_template.html#tweak-has-to-match-the-template",
    "title": "Changing batchtools template",
    "section": "Tweak has to match the template!",
    "text": "Tweak has to match the template!\nSo, that means the way tweak(batchtools_slurm, resources = …) works is template-specific. Some might not have parsing for what gets passed, sometimes it might be the wrong type, etc).\n\nExamples\nplan(tweak(batchtools_slurm,\n           template = \"./batchtools_templates/slurm-simple.tmpl\",\n           resources = list(ncpus = 12,\n                            memory = 1000,\n                            walltime=60*5)))\nFor the slurm-simple.tmpl, the SLURM --time is referenced to resources$walltime and gets divided by 60 so has to be a numeric in seconds.\nThe ncpus = 12 here gets 12 CPUs that all get assigned (kinda weirdly though, with --cpus-per-task), even though we only use one- see the top of the output\n## Nodes and pids\n# A tibble: 100 × 6\n# Groups:   all_job_nodes, node, pid, taskid [100]\n    all_job_nodes node             pid taskid cpus_avail n_reps\n    &lt;chr&gt;         &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;int&gt;\n  1 gandalf-vm02  gandalf-vm02 3329413 0      12              6\n  2 gandalf-vm02  gandalf-vm02 3329479 0      12              7\n  3 gandalf-vm02  gandalf-vm02 3329545 0      12              7\n  4 gandalf-vm02  gandalf-vm02 3329611 0      12              6\n  5 gandalf-vm02  gandalf-vm02 3329679 0      12              7\n  6 gandalf-vm02  gandalf-vm02 3329747 0      12              6\n  7 gandalf-vm02  gandalf-vm02 3329811 0      12              6\nWhereas cpus_avail is 1 without that line-\n## Nodes and pids\n# A tibble: 100 × 6\n# Groups:   all_job_nodes, node, pid, taskid [100]\n    all_job_nodes node             pid taskid cpus_avail n_reps\n    &lt;chr&gt;         &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;int&gt;\n  1 gandalf-vm01  gandalf-vm01   12812 0      1               6\n  2 gandalf-vm01  gandalf-vm01   12881 0      1               6\n  3 gandalf-vm01  gandalf-vm01   12966 0      1               6\n  4 gandalf-vm01  gandalf-vm01   13043 0      1               6\n  5 gandalf-vm01  gandalf-vm01   13117 0      1               6\n  6 gandalf-vm01  gandalf-vm01   13179 0      1               6\n  7 gandalf-vm01  gandalf-vm01   13261 0      1               6\nIt looks like a major catch here is I can’t use names in the resources list with dashes, e.g. ntasks-per-node. And so to set those i’ll have to translate, as in slurm-simple, I think. Unless batchtools auto-translates under the hood, but I think not.\nGoing to have to come back to this. It’d be nice if it were possible. How did that github issue do it? It used ncpus in slurm-simple.tmpl. So maybe I’ll just do that for now. Then I can get on with checking the use of chunks and arrays and nodes and nesting. And whether i can use the ncpus thing to get (and use) more cpus than exist on single nodes.\nDoes that mean I can auto-generate jobnames???? That would be great",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Switching templates, matching to resources"
    ]
  },
  {
    "objectID": "parallelism/globals_speed.html",
    "href": "parallelism/globals_speed.html",
    "title": "Foreach globals and speed",
    "section": "",
    "text": "I previously tested the impact of unused globals on speed, but only briefly. Here, I’ll be more systematic, because it gets tricky fast if we need to be super careful about what objects exist in the global environment.\nThere are a couple things to check here\nI’ll tackle these by\nlibrary(doFuture)\nlibrary(future.apply)\nlibrary(furrr)\nlibrary(doRNG)\nlibrary(microbenchmark)\nregisterDoFuture()\nplan(multisession)",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Global objects and parallel speed"
    ]
  },
  {
    "objectID": "parallelism/globals_speed.html#nothing-exists",
    "href": "parallelism/globals_speed.html#nothing-exists",
    "title": "Foreach globals and speed",
    "section": "Nothing exists",
    "text": "Nothing exists\nWell, almost nothing. I’m going to set a couple scalars and define a function for furrr and future.apply . I’m not using any of the globals or export arguments in the functions.\n\nBare\n\nn_reps = 100\nsize &lt;- 1000\n\nfn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n\n\nBenchmark\n\nmicrobenchmark(\n  dofut0 = {foreach(i = 1:n_reps, \n                       .combine = cbind) %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  furr0 = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE))},\n  \n  fuapply0 = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE)},\n  times = 10\n)\n\nUnit: seconds\n     expr      min       lq     mean   median       uq      max neval\n   dofut0 3.082435 3.167459 3.289073 3.304749 3.409156 3.455932    10\n    furr0 2.996915 3.189255 3.787848 3.425178 3.551950 7.862844    10\n fuapply0 3.127753 3.205452 3.283468 3.256335 3.342933 3.515369    10\n\n\nSo, doFuture and furrr are slower than future.apply, but not by a ton. The key thing here is this sets the baseline, so we can see if things slow down once we have big objects in memory.\n\n\n\nInside a function\nThese functions are from testing parallel speed, though they have different names here. I’ve added the ability to change the way they handle globals so I don’t have to write new functions for comparing that later, with the default set at the function default.\n\nforeach\n\nforeach_fun &lt;- function(n_reps = 100, size = 1000, .export = NULL, .noexport = NULL) {\n  c_foreach &lt;- foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .export = .export,\n                       .noexport = .noexport) %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  return(c_foreach)\n}\n\n\n\nfurrrr\n\nfurrr_fun &lt;- function(n_reps = 100, size = 1000, globals = TRUE) {\n  fn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  \n  c_map &lt;- future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, \n                                               globals = globals))\n  matrix(unlist(c_map), ncol = n_reps)\n}\n\n\n\nfuture.apply\n\nfuapply_fun &lt;- function(n_reps = 100, size = 1000, future.globals = TRUE) {\n    fn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n    }\n    \n  c_apply &lt;- future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE,\n                           future.globals = future.globals)\n  \n    matrix(unlist(c_apply), ncol = n_reps)\n}\n\n\n\nBenchmark\n\nmicrobenchmark(\n  dofut_fun = foreach_fun(n_reps = 100, size = 1000),\n  fur_fun = furrr_fun(n_reps = 100, size = 1000),\n  app_fun = fuapply_fun(n_reps = 100, size = 1000),\n  times = 10\n)\n\nUnit: seconds\n      expr      min       lq     mean   median       uq      max neval\n dofut_fun 2.894709 2.943020 3.032240 3.025625 3.048341 3.243421    10\n   fur_fun 2.907043 2.959023 3.009576 2.999129 3.035445 3.159187    10\n   app_fun 2.874484 3.003329 3.057768 3.066695 3.118491 3.288474    10\n\n\nThis sets the other baseline before we have big objects in memory, so we can see if things respond differently when used inside a function’s environment vs directly in the global. Now all three functions are basically equivalent.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Global objects and parallel speed"
    ]
  },
  {
    "objectID": "parallelism/globals_speed.html#with-big-global",
    "href": "parallelism/globals_speed.html#with-big-global",
    "title": "Foreach globals and speed",
    "section": "With big global",
    "text": "With big global\nDefault future.globals.maxsize is 500MB. Should i increase that, or just try to hit it? I think just try to get just under it.\n\n# This is 1.6GB\n# big_obj &lt;- matrix(rnorm(20000*10000), nrow = 10000)\n# 496 MB\nbig_obj &lt;- matrix(rnorm(10000*6200), nrow = 10000)\n\nNow, same tests as before, and some that reference it but don’t use it.\nThe comparisons to make here are:\n\nMatched to above- does just having the object exist slow things down, even if not called?\nReferenced and not- does it only get passed in if asked for and slow things down?\n\nNot exactly sure how I’ll check that. Maybe instead of referencing it in the function (which is hard to do without using it, especially with furrr and future.apply), I’ll explicitly send it in with their globals arguments.\n\n\n\nBare\n\nBenchmark\nI’m going to run this for default (no global argument), explicitly sending them in, and explicitly excluding them.\n\nmicrobenchmark(\n  # default- same as above, but now big_obj exists, but is not used in the actual processing\n  dofut0 = {foreach(i = 1:n_reps, \n                       .combine = cbind) %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  furr0 = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE))},\n  \n  fuapply0 = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE)},\n  \n  # Explicitly telling it not to send big global (I can't sort out getting .export to work)\n  dofut_no_g = {foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .noexport = \"big_obj\") %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  \n  furr_no_g = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, \n                                               globals = FALSE))},\n  \n  fuapply_no_g = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE,\n                           future.globals = FALSE)},\n  \n  # Explicitly telling it to send the unused global\n  dofut_g = {foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .export = 'big_obj') %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  \n  furr_g = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, \n                                               globals = 'big_obj'))},\n  \n  fuapply_g = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE,\n                           future.globals = 'big_obj')},\n  \n  times = 10\n)\n\nUnit: seconds\n         expr       min        lq      mean    median        uq       max neval\n       dofut0  2.922838  3.000344  3.249182  3.182809  3.262618  4.264726    10\n        furr0  2.949936  2.981717  3.102662  3.052143  3.217054  3.398791    10\n     fuapply0  2.992581  3.023170  3.220997  3.148239  3.436145  3.624800    10\n   dofut_no_g  2.945903  3.066295  3.140082  3.118970  3.276743  3.335264    10\n    furr_no_g  2.874103  3.076374  3.204159  3.253832  3.306481  3.568914    10\n fuapply_no_g  2.912136  3.057193  3.108612  3.094443  3.177574  3.341436    10\n      dofut_g 13.140054 13.625128 14.188250 14.119873 14.878983 15.087787    10\n       furr_g 13.786607 14.154656 14.397246 14.394677 14.609236 15.120794    10\n    fuapply_g 13.455395 13.675143 14.085484 13.918776 14.251080 15.601045    10\n\n\nNow there’s a big object sitting in global memory, but it does not slow down the default run relative to the enforced-non-pass version or the version from before it existed (above). It does show major slowdown when it is explicitly passed.\nUnused globals therefore are NOT passed by default, even when code is running straight in the global environment.\n\n\n\nInside functions\nThe functions have an option to change the way globals are handled.\n\nBenchmark\n\nmicrobenchmark(\n  # default\n  dofut_default = foreach_fun(n_reps = 100, size = 1000),\n  fur_default = furrr_fun(n_reps = 100, size = 1000),\n  app_default = fuapply_fun(n_reps = 100, size = 1000),\n  \n  # No globals\n  dofut_no_g = foreach_fun(n_reps = 100, size = 1000, .noexport = 'big_obj'),\n  fur_no_g = furrr_fun(n_reps = 100, size = 1000,\n                          globals = FALSE),\n  app_no_g = fuapply_fun(n_reps = 100, size = 1000,\n                            future.globals = FALSE),\n  \n  # Explicit globals\n  dofut_g = foreach_fun(n_reps = 100, size = 1000,\n                              .export = 'big_obj'),\n  fur_g = furrr_fun(n_reps = 100, size = 1000, \n                          globals = 'big_obj'),\n  app_g = fuapply_fun(n_reps = 100, size = 1000,\n                            future.globals = 'big_obj'),\n  \n  \n  times = 10\n)\n\nUnit: seconds\n          expr       min        lq      mean    median        uq       max\n dofut_default  2.820070  3.133994  3.418282  3.494786  3.643098  4.122530\n   fur_default  2.955963  3.170553  3.401131  3.247953  3.674548  4.264421\n   app_default  2.777021  2.957015  3.264931  3.286259  3.545510  3.748255\n    dofut_no_g  2.741006  3.076965  3.271767  3.285788  3.454271  3.721240\n      fur_no_g  2.961480  3.051561  3.413412  3.445728  3.553680  4.109865\n      app_no_g  2.835050  3.020232  3.264410  3.235451  3.499824  3.772277\n       dofut_g 13.666079 14.104627 15.448594 15.349449 15.978492 18.617738\n         fur_g 13.761059 15.330178 15.521168 15.707339 16.317537 16.369009\n         app_g 12.988882 14.554517 15.117242 15.415819 15.672232 16.115645\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nUsing functions yields the same result as before- the big objects sitting in the global environment do not get passed in and slow things down if they aren’t actually used in the functions (or explicitly sent in).\nUnused globals therefore are NOT passed by default into parallelised functions.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Global objects and parallel speed"
    ]
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html",
    "href": "parallelism/initial_future_batchtools.html",
    "title": "Using future.batchtools",
    "section": "",
    "text": "I need to sort out how to use futures for parallel processing on the HPC.There’s a few things I’ve tried previously that didn’t work, and a way I’ve cobbled together that’s not ideal.\nThere are some issues with my current approach",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Figuring out future.batchtools"
    ]
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#what-do-i-want",
    "href": "parallelism/initial_future_batchtools.html#what-do-i-want",
    "title": "Using future.batchtools",
    "section": "What do I want?",
    "text": "What do I want?\nThere are some improvements I want to make that will make my code work better (and faster)\n\nMinor (if any) changes when running locally or on an HPC\nIn-code splitting of work into nodes to balance the work across them\nMake sure we’re using both nodes and cores within them\n\nThe main solution seems to be using future.batchtools, but I wasn’t able to get it working quickly.\n\nSome questions\nThere are a few big-picture things I have questions about that aren’t clear from reading the docs (in addition to just ‘how do we get a run to work’)\n\nHow do jobs actually start? I think, but am not 100% sure, that the R script essentailly builds slurm bash scripts and then calls sbatch. Is that what happens?\nDo we still use sbatch or other command-line bash at all? Or is everything managed in R? If so, how do we actually start the runs? Rscript? srun on an R control script?\n\nif Rscript, do we end up with that main R process running in the login node the whole time? What about if srun or…\nThis issue implies we need to leave R running. But can it run with Rscript? srun?sinteractive? sbatch any_R.sh analysis_script.R? With any_R.sh having low resources but maybe long walltime? It looks like srun barfs to the terminal and blocks, while sbatch outputs to file and is non-blocking. So that’s likely the way to go.\n\nDoes it make sense to manage nodes and cores separately, or do we just ask for a ton of cores and it auto-manages nodes to get them?\n\nI think {slurmR} with plan(cluster) does the latter, but not positive\nI’m not actually sure what future.batchtools does by default (will check), but I think a list-plan likely makes sense.\n\ndo we use SLURM job arrays? Or does it generate a bunch of batch scripts that get called as separate jobs instead of array jobs? Does it matter?\n\nwhat are chunks.\n\nIf we feed it a big set of iterations, does it send each one to its own node? Its own core? Is there any chunking?",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Figuring out future.batchtools"
    ]
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#templates",
    "href": "parallelism/initial_future_batchtools.html#templates",
    "title": "Using future.batchtools",
    "section": "Templates",
    "text": "Templates\nI’m still a bit confused by the overall workflow, but it’s clear I need a template. There’s one in future.batchtools github, and a few at the batchtools github.\nThen I think in the plan call, we tweak that? Let’s just get it working. Trying first with the one that comes from future.batchtools. Though the one from batchtools looks like it has more capability for doing things like managing cores on nodes. Maybe try them both as we go?\nI have both of those. I kind of want to test both. The docs say the template should be either at ./batchtools.slurm.tmpl (associated with a particular working directory) or ~/.batchtools.slurm.tmpl (for all processes to find it). But I want to be able to test multiple templates. I should be able to use\nplan(batchtools_slurm, template = \"/path/to/batchtools.slurm.tmpl\")\nbut its a bit unclear whether the templates still need to be named batchtools.slurm.tmpl, or can have whatever filename we want, as long as I give the path. Guess I’ll test that. Try first with it as batchtools.slurm.tmpl in the repo directory first though.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Figuring out future.batchtools"
    ]
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#try-a-simple-job",
    "href": "parallelism/initial_future_batchtools.html#try-a-simple-job",
    "title": "Using future.batchtools",
    "section": "Try a simple job",
    "text": "Try a simple job\nUse the future.batchtools template, and a foreach loop using %:%.\nStart with Rscript filename.R\nIt starts printing directly to terminal, which is annoying.\nOpening another terminal and typing squeue shows that I have 4 nodes- though actually that was just at that moment.\nIt doesn’t seem to produce stdout or stderr, which is going to make it tricky to see what happened. See below- this is true unless we run the master R session through sbatch.\nI can copy in from the terminal output:\n\nLoading required package: foreach Loading required package: future Warning message: package ‘future’ was built under R version 4.0.5 Loading required package: parallelly Warning messages: 1: package ‘future.batchtools’ was built under R version 4.0.5 2: package ‘parallelly’ was built under R version 4.0.5\nPlan is: List of future strategies: 1. batchtools_slurm: - args: function (expr, envir = parent.frame(), substitute = TRUE, globals = TRUE, label = NULL, template = NULL, resources = list(), workers = NULL, registry = list(), …) - tweaked: FALSE - call: plan(batchtools_slurm)\n\navailable workers:\nlocalhost localhost localhost localhost localhost localhost localhost localhost\n\n\ntotal workers:\n8\n\n\nunique workers:\nlocalhost\n\n\navailable Cores:\n\nnon-slurm\n8\n\n\nslurm method\n1\n\n\n\nMain PID:\n3195570 There were 50 or more warnings (use warnings() to see the first 50)\n\n\nUnique processes\n100\nIDs of all cores used\n238059 1524768 1518289 1500375 1513942 3264114 1269189 1345061 2623254 238128 3264182 1269259 1345124 1524852 1518372 3264251 238197 1500450 1514026 1524930 1518447 238268 3264327 1500525 1514101 238335 1525010 1518527 1500596 1514176 3264408 238401 1525084 1518604 1500666 1514254 3264478 1518681 238466 1525164 1500739 1518756 1514328 238530 1500811 1525241 1514402 1518828 1500879 238594 1525316 1514479 3264562 1518901 1500952 238662 1525391 1518977 1514555 1501019 3264640 238727 1525467 1519049 1514630 238791 1501090 1525542 3264715 1519121 1501158 238857 1525621 1519194 1514708 1501225 3264796 238922 1525693 1519271 1514786 1501292 3264863 1525766 238989 1519345 1514861 1525839 1501361 239060 3264939 1519420 1514936 1501428 1525912 239129 3265003 1519494 1501496 1525987\n\n\nNodes and pids\n           238059 238128 238197 238268 238335 238401 238466 238530 238594\nbilbo 6 6 6 6 6 6 7 6 7 frodo-vs01 0 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0 0\n           238662 238727 238791 238857 238922 238989 239060 239129 1269189\nbilbo 6 7 7 6 7 6 7 6 0 frodo-vs01 0 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 7 gandalf-vm04 0 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0 0\n           1269259 1345061 1345124 1500375 1500450 1500525 1500596 1500666\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 6 6 6 6 7 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 6 0 0 0 0 0 0 0 gandalf-vm04 0 6 6 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1500739 1500811 1500879 1500952 1501019 1501090 1501158 1501225\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 6 6 6 6 6 6 6 6 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1501292 1501361 1501428 1501496 1513942 1514026 1514101 1514176\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 7 6 7 6 0 0 0 0 frodo-vs04 0 0 0 0 6 7 6 6 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1514254 1514328 1514402 1514479 1514555 1514630 1514708 1514786\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 6 7 7 6 6 6 6 6 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1514861 1514936 1518289 1518372 1518447 1518527 1518604 1518681\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 7 7 6 6 6 6 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 6 6 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1518756 1518828 1518901 1518977 1519049 1519121 1519194 1519271\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 6 6 7 7 6 7 7 6 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1519345 1519420 1519494 1524768 1524852 1524930 1525010 1525084\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 6 6 6 7 6 frodo-vs02 7 6 7 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1525164 1525241 1525316 1525391 1525467 1525542 1525621 1525693\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 6 6 6 6 6 6 6 6 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1525766 1525839 1525912 1525987 2623254 3264114 3264182 3264251\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 6 6 6 6 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 6 7 6 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 6 0 0 0\n           3264327 3264408 3264478 3264562 3264640 3264715 3264796 3264863\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 7 7 6 6 6 6 6 6 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           3264939 3265003\nbilbo 0 0 frodo-vs01 0 0 frodo-vs02 0 0 frodo-vs03 0 0 frodo-vs04 0 0 gandalf-vm02 6 6 gandalf-vm03 0 0 gandalf-vm04 0 0 gandalf-vm05 0 0\n\n\nThe formatting of the table is all boogered up, but that looks like I got 9 nodes, and used something like 9-14 PIDs each, each about 7-6 times. It’s a bit confusing, because sinfo --Node --long shows that the number of CPUs is different across those nodes. And that seems to be what happened here. I need to change the output so I can better see what I used. But, roughly, that looks about right- it send jobs to CPUs as needed.\n\n# number of iterations\n25*25\n\n[1] 625\n\n9*12*6\n\n[1] 648",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Figuring out future.batchtools"
    ]
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#nodes-and-pids",
    "href": "parallelism/initial_future_batchtools.html#nodes-and-pids",
    "title": "Using future.batchtools",
    "section": "Nodes and pids",
    "text": "Nodes and pids\n           238059 238128 238197 238268 238335 238401 238466 238530 238594\nbilbo 6 6 6 6 6 6 7 6 7 frodo-vs01 0 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0 0\n           238662 238727 238791 238857 238922 238989 239060 239129 1269189\nbilbo 6 7 7 6 7 6 7 6 0 frodo-vs01 0 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 7 gandalf-vm04 0 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0 0\n           1269259 1345061 1345124 1500375 1500450 1500525 1500596 1500666\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 6 6 6 6 7 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 6 0 0 0 0 0 0 0 gandalf-vm04 0 6 6 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1500739 1500811 1500879 1500952 1501019 1501090 1501158 1501225\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 6 6 6 6 6 6 6 6 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1501292 1501361 1501428 1501496 1513942 1514026 1514101 1514176\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 7 6 7 6 0 0 0 0 frodo-vs04 0 0 0 0 6 7 6 6 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1514254 1514328 1514402 1514479 1514555 1514630 1514708 1514786\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 6 7 7 6 6 6 6 6 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1514861 1514936 1518289 1518372 1518447 1518527 1518604 1518681\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 7 7 6 6 6 6 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 6 6 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1518756 1518828 1518901 1518977 1519049 1519121 1519194 1519271\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 6 6 7 7 6 7 7 6 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1519345 1519420 1519494 1524768 1524852 1524930 1525010 1525084\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 6 6 6 7 6 frodo-vs02 7 6 7 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1525164 1525241 1525316 1525391 1525467 1525542 1525621 1525693\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 6 6 6 6 6 6 6 6 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1525766 1525839 1525912 1525987 2623254 3264114 3264182 3264251\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 6 6 6 6 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 6 7 6 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 6 0 0 0\n           3264327 3264408 3264478 3264562 3264640 3264715 3264796 3264863\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 7 7 6 6 6 6 6 6 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           3264939 3265003\nbilbo 0 0 frodo-vs01 0 0 frodo-vs02 0 0 frodo-vs03 0 0 frodo-vs04 0 0 gandalf-vm02 6 6 gandalf-vm03 0 0 gandalf-vm04 0 0 gandalf-vm05 0 0",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Figuring out future.batchtools"
    ]
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#calling-and-output",
    "href": "parallelism/initial_future_batchtools.html#calling-and-output",
    "title": "Using future.batchtools",
    "section": "Calling and output",
    "text": "Calling and output\nWe seem to need to leave an R session running to operate the futures. We can probably do that interactively module load R, R, then run the code, or Rscript. But that’s all interactive. As is srun. And it all prints all printable output to the terminal, and doesn’t save it. I think an sbatch might be the way to go. Try that.\nThat does run, and spawn the others\n\nIt’s silly that we need another wrapper call, but maybe that’s ok. Gives us the option to do some auto-setup in an intermediate script.\nrepeatedly calling squeue shows something interesting- it looks like it’s creating separate jobs for each task, not doing anything node-aware (e.g. chunking jobs to a node, and then multisessioning). I guess that makes sense, based on the help and other testing, but we can almost certainly speed things up by chunking and list-planning.\nIf the passing-object overhead really becomes an issue, we might want to just use batchtools directly, which I think also could remove the need to leave a master R session running. But I think that gets rid of the major advantage of being able to use essentially the same code locally and on the HPC, just by changing the plan.\nThe output using sbatch batchtools_R.sh testing_future_batchtools.R (where batchtools_R.sh is a lightweight master job) is\nℹ Using R 4.0.3 (lockfile was generated with R 4.2.2)\n\n Plan is:\nList of future strategies:\n1. batchtools_slurm:\n   - args: function (expr, envir = parent.frame(), substitute = TRUE, globals = TRUE, label = NULL, template = NULL, resources = list(), workers = NULL, registry = list(), ...)\n   - tweaked: FALSE\n   - call: plan(batchtools_slurm)\n\n### available workers:\ngandalf-vm01\n\n\n### total workers:\n1\n\n### unique workers:\ngandalf-vm01\n\n### available Cores:\n\n#### non-slurm\n1\n\n#### slurm method\n1\n\n### Main PID:\n4135554\n\n### Unique processes\n100\n\nIDs of all cores used\n\n241089\n1528094\n1522003\n1503388\n1516888\n3269690\n1522077\n241160\n1528170\n1503459\n1516963\n3269759\n1522156\n1528243\n241230\n3269825\n1271878\n1528317\n1522235\n241297\n1503531\n1528392\n1517044\n1522308\n241367\n1503606\n1528471\n1522380\n1517120\n241436\n3269894\n1528548\n1522456\n1503681\n1517195\n241505\n1522531\n1528626\n3269975\n1271968\n1503749\n241573\n1522605\n3270042\n1528703\n1503817\n241641\n1522682\n1528778\n1517275\n3270121\n1272046\n241709\n1528853\n1522758\n1503892\n1517348\n241778\n1528932\n1522829\n1503961\n1517420\n3270199\n241843\n1529005\n1522901\n1504030\n241910\n1529082\n1517500\n3270277\n1522975\n1504098\n241978\n1529157\n1523051\n1504167\n1517581\n3270355\n1529231\n242042\n1523125\n1504238\n1529306\n1517659\n3270422\n242108\n1523197\n1529384\n1504307\n1517731\n3270488\n242173\n1523272\n1529459\n1504376\n1517812\n3270555\n242239\n1529532\n\n## Nodes and pids\n# A tibble: 100 × 3\n# Groups:   node [7]\n    node             pid n_reps\n    &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;\n  1 bilbo         241089      6\n  2 bilbo         241160      6\n  3 bilbo         241230      7\n  4 bilbo         241297      6\n  5 bilbo         241367      6\n  6 bilbo         241436      6\n  7 bilbo         241505      6\n  8 bilbo         241573      6\n  9 bilbo         241641      7\n 10 bilbo         241709      6\n 11 bilbo         241778      7\n 12 bilbo         241843      6\n 13 bilbo         241910      6\n 14 bilbo         241978      7\n 15 bilbo         242042      6\n 16 bilbo         242108      6\n 17 bilbo         242173      6\n 18 bilbo         242239      6\n 19 frodo-vs01   1528094      6\n 20 frodo-vs01   1528170      6\n 21 frodo-vs01   1528243      6\n 22 frodo-vs01   1528317      6\n 23 frodo-vs01   1528392      6\n 24 frodo-vs01   1528471      7\n 25 frodo-vs01   1528548      6\n 26 frodo-vs01   1528626      6\n 27 frodo-vs01   1528703      6\n 28 frodo-vs01   1528778      6\n 29 frodo-vs01   1528853      7\n 30 frodo-vs01   1528932      6\n 31 frodo-vs01   1529005      6\n 32 frodo-vs01   1529082      6\n 33 frodo-vs01   1529157      6\n 34 frodo-vs01   1529231      6\n 35 frodo-vs01   1529306      6\n 36 frodo-vs01   1529384      6\n 37 frodo-vs01   1529459      6\n 38 frodo-vs01   1529532      6\n 39 frodo-vs02   1522003      7\n 40 frodo-vs02   1522077      7\n 41 frodo-vs02   1522156      6\n 42 frodo-vs02   1522235      7\n 43 frodo-vs02   1522308      6\n 44 frodo-vs02   1522380      6\n 45 frodo-vs02   1522456      6\n 46 frodo-vs02   1522531      6\n 47 frodo-vs02   1522605      7\n 48 frodo-vs02   1522682      6\n 49 frodo-vs02   1522758      6\n 50 frodo-vs02   1522829      6\n 51 frodo-vs02   1522901      7\n 52 frodo-vs02   1522975      6\n 53 frodo-vs02   1523051      6\n 54 frodo-vs02   1523125      7\n 55 frodo-vs02   1523197      6\n 56 frodo-vs02   1523272      7\n 57 frodo-vs03   1503388      6\n 58 frodo-vs03   1503459      6\n 59 frodo-vs03   1503531      6\n 60 frodo-vs03   1503606      6\n 61 frodo-vs03   1503681      6\n 62 frodo-vs03   1503749      6\n 63 frodo-vs03   1503817      6\n 64 frodo-vs03   1503892      6\n 65 frodo-vs03   1503961      6\n 66 frodo-vs03   1504030      6\n 67 frodo-vs03   1504098      6\n 68 frodo-vs03   1504167      6\n 69 frodo-vs03   1504238      6\n 70 frodo-vs03   1504307      7\n 71 frodo-vs03   1504376      6\n 72 frodo-vs04   1516888      6\n 73 frodo-vs04   1516963      7\n 74 frodo-vs04   1517044      7\n 75 frodo-vs04   1517120      6\n 76 frodo-vs04   1517195      7\n 77 frodo-vs04   1517275      7\n 78 frodo-vs04   1517348      6\n 79 frodo-vs04   1517420      7\n 80 frodo-vs04   1517500      7\n 81 frodo-vs04   1517581      7\n 82 frodo-vs04   1517659      6\n 83 frodo-vs04   1517731      6\n 84 frodo-vs04   1517812      6\n 85 gandalf-vm02 3269690      6\n 86 gandalf-vm02 3269759      6\n 87 gandalf-vm02 3269825      6\n 88 gandalf-vm02 3269894      7\n 89 gandalf-vm02 3269975      7\n 90 gandalf-vm02 3270042      6\n 91 gandalf-vm02 3270121      6\n 92 gandalf-vm02 3270199      6\n 93 gandalf-vm02 3270277      6\n 94 gandalf-vm02 3270355      6\n 95 gandalf-vm02 3270422      7\n 96 gandalf-vm02 3270488      6\n 97 gandalf-vm02 3270555      7\n 98 gandalf-vm03 1271878      6\n 99 gandalf-vm03 1271968      6\n100 gandalf-vm03 1272046      6\n\nTime taken for code: 185\nSo, we can see the lightweight wrapper uses 1 cpu on one node, but spawns 100 workers, each of which gets used 6-7 times.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Figuring out future.batchtools"
    ]
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#todo",
    "href": "parallelism/initial_future_batchtools.html#todo",
    "title": "Using future.batchtools",
    "section": "TODO",
    "text": "TODO\nother templates\nmanaging resources\nlist-plans and nesting\nchunks.as.array\nauto-managing plan locally vs HPC\nconsequences of master R session dying (if Rscript, if sbatched)\netc",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Figuring out future.batchtools"
    ]
  },
  {
    "objectID": "parallelism/master_persistence.html",
    "href": "parallelism/master_persistence.html",
    "title": "Control process dying or timing out",
    "section": "",
    "text": "The way future.batchtools works there’s a controlling R process that starts the futures and waits for them to return. In typical use, the value() of futures are intended to be used, whereas in some cases, particularly when I use them on the cluster, I’m using them to fire off a bunch of HPC jobs that save output, and don’t care if they return. This is a very similar issue to the discussion here.\nThere are a couple issues this use-case brings up\n\nWhat happens if that master R script that creates the futures dies?\n\nDo we need to tie up a core just to run that? (YES)\nDoes it have to stay in use for the entire period all futures take to finish? (NO- see next)\nWhat if it dies once the jobs have started running vs before they’ve started\n\ne.g. does it create a queue that just runs no matter what? (NO)\nonce a job starts, does it finish no matter what? (YES, unless it times out)\n\n\nDoes it matter if the master runs through sbatch, sinteractive, or just Rscript on the login node? (Probably not, but sbatch is likely safest/most robust).\nShould I actually be using batchtools::submitJobs() instead, as suggested in that github thread? I really like the automatic control of jobs and globals etc in future.batchtools, and the ability to be portable to local computers. (Quite possibly, but that’s for another day, I think. And we’d potentially lose a lot of the advantages of future).",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Consequences of master timing out"
    ]
  },
  {
    "objectID": "parallelism/master_persistence.html#conclusions",
    "href": "parallelism/master_persistence.html#conclusions",
    "title": "Control process dying or timing out",
    "section": "Conclusions",
    "text": "Conclusions\nThe master needs to run until all jobs have started, but not necessarily until all jobs have finished (unless it actually does something with the output).\nThat will be hard to manage, since the time it takes to start jobs depends on SLURM queues.\nThere’s no reason to use srun or sinteractive- they still need to ask for resources, and die when disconnect, so even more finicky.\nUsing Rscript on the login node is I guess a potential workaround, but I think it also dies when we disconnect and potentially times out anyway. It’s still tying up a node, just in a different place. I think that’d make people even grumpier. It’s also hard to test with short-ish runs.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Consequences of master timing out"
    ]
  },
  {
    "objectID": "parallelism/nested_parallel.html",
    "href": "parallelism/nested_parallel.html",
    "title": "Nested parallelism",
    "section": "",
    "text": "I tend to run a lot of simulation code that consists of some core functions that are then run for a large number of different parameter values. This bit is entirely independent and clearly parallelisable. There are also typically large calculations inside the simulation functions that can be parallelised. What’s not clear to me is whether I should write them all with parallel-friendly code (foreach %dopar%, furrr, etc), or just one or the other. While my particular situation gives me the option to choose, it’s likely not uncommon to call parallelisable functions from a package in code that is itself parallelised, and so is useful to know how this works.\nI already did some tests of what sort of work makes most sense to be parallelised so I’ll try to follow those ideas as I do these tests- assuming that the internal parallel code actually makes sense to be parallel, and wouldn’t just be faster sequential anyway. To test this, I’ll attempt to build an example that is non-trivial, but still try to stay minimally complex to avoid getting into writing a complex population dynamics model.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Testing nested parallelism"
    ]
  },
  {
    "objectID": "parallelism/nested_parallel.html#packages-and-setup",
    "href": "parallelism/nested_parallel.html#packages-and-setup",
    "title": "Nested parallelism",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}, because I tend to like writing for loops (there’s a reason I’ll try to write up sometime later). I test other packages in the {future} family (furrr, future_apply) where I try to better understand when they do and don’t give speed advantages.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\nlibrary(foreach)\nlibrary(doRNG)\nlibrary(ggplot2)\n\nregisterDoFuture()\nplan(multisession)",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Testing nested parallelism"
    ]
  },
  {
    "objectID": "parallelism/nested_parallel.html#built-in-nesting",
    "href": "parallelism/nested_parallel.html#built-in-nesting",
    "title": "Nested parallelism",
    "section": "Built-in nesting",
    "text": "Built-in nesting\nThe foreach package provides built-in nesting, with constructions using %:%. This is designed for loops that can be written tightly together (no processing between them). For example, we might write a nested loop over two sets of parameters mean and sd and calculate the realised coefficient of variation and return it as a matrix. Situations with more complexity around dependency in these loops is here.\n\nrealised_cv &lt;- foreach(i = 1:10, .combine = cbind) %:%\n  foreach(j = seq(from = 0, to = 1, by = 0.1), .combine = rbind) %dopar% {\n    a &lt;- rnorm(1000, mean = 1, sd = j)\n    sd(a)/mean(a)\n  }\nrealised_cv\n\n               [,1]       [,2]      [,3]       [,4]      [,5]       [,6]\nresult.1  0.0000000 0.00000000 0.0000000 0.00000000 0.0000000 0.00000000\nresult.2  0.1008283 0.09970288 0.1012399 0.09970637 0.1004789 0.09746728\nresult.3  0.1948659 0.20180507 0.2061238 0.19891376 0.2020873 0.20023736\nresult.4  0.2878997 0.30436740 0.2961359 0.29949007 0.3065985 0.32072136\nresult.5  0.4121646 0.40179953 0.3978699 0.40560251 0.3959687 0.39083267\nresult.6  0.4915330 0.53295027 0.4871696 0.51901392 0.4918437 0.49519587\nresult.7  0.6088233 0.63926486 0.6041769 0.58187246 0.5878398 0.60517688\nresult.8  0.7292209 0.69859582 0.7247529 0.70454796 0.6613891 0.68523825\nresult.9  0.8263427 0.79534527 0.8379815 0.80307838 0.7935680 0.79027229\nresult.10 0.9423638 0.96120868 0.7991695 0.89693690 0.8607557 0.89615792\nresult.11 0.9565427 1.03372959 0.9769392 1.01646019 0.9654055 1.01248284\n               [,7]      [,8]      [,9]     [,10]\nresult.1  0.0000000 0.0000000 0.0000000 0.0000000\nresult.2  0.1014378 0.1030771 0.1013798 0.1027107\nresult.3  0.2031344 0.1998193 0.1986128 0.2087609\nresult.4  0.2859058 0.2966124 0.3011449 0.3021254\nresult.5  0.4322656 0.3828475 0.4033456 0.4090713\nresult.6  0.4974277 0.5106743 0.4971039 0.5181352\nresult.7  0.5819782 0.5871467 0.6213671 0.6182626\nresult.8  0.7049132 0.6751651 0.6919145 0.6955699\nresult.9  0.8317784 0.7846865 0.8260782 0.7633198\nresult.10 0.8796149 0.9013733 0.9094938 0.8614506\nresult.11 1.0341074 1.0785966 1.0039062 0.9962265\n\n\nThat can be super useful, but isn’t the goal here- I’m interested in the situation where we have\nforloop () {\n  lots of processing\n  \n  forloop2(outcomes of the processing) {\n    more processing\n  }\n  \n  more processing\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Testing nested parallelism"
    ]
  },
  {
    "objectID": "parallelism/nested_parallel.html#inner-loop",
    "href": "parallelism/nested_parallel.html#inner-loop",
    "title": "Nested parallelism",
    "section": "Inner loop",
    "text": "Inner loop\n\nParallel version\n\ninner_par &lt;- function(in_vec, size) {\n  inner_out &lt;- foreach(j = in_vec,\n                       .combine = c) %dorng% {\n    d &lt;- rnorm(size, mean = j)\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    mean(g)\n    \n  }\n}\n\n\n\nSequential version\n\ninner_seq &lt;- function(in_vec, size) {\n  inner_out &lt;- foreach(j = in_vec,\n                       .combine = c) %do% {\n    d &lt;- rnorm(size, mean = j)\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    mean(g)\n    \n  }\n}\n\n\n\nUsing preallocated for\nThis is likely to be faster than the sequential\n\ninner_for &lt;- function(in_vec, size) {\n  inner_out &lt;- vector(mode = 'numeric', length = size)\n  \n  for(j in 1:length(in_vec)) {\n    d &lt;- rnorm(size, mean = in_vec[j])\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    inner_out[j] &lt;- mean(g)\n    \n  }\n  return(inner_out)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Testing nested parallelism"
    ]
  },
  {
    "objectID": "parallelism/nested_parallel.html#outer-loop",
    "href": "parallelism/nested_parallel.html#outer-loop",
    "title": "Nested parallelism",
    "section": "Outer loop",
    "text": "Outer loop\n\nparallel\n\nouter_par &lt;- function(size, innerfun) {\n  outer_out &lt;- foreach(i = 1:size,\n                       .combine = c) %dorng% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a &lt;- rnorm(size, mean = i)\n                         \n                         b &lt;- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec &lt;- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(in_vec = cvec, size = size)\n                         \n                         h &lt;- sd(cvec)/inner_out\n                         \n                         \n                       }\n  \n  return(outer_out)\n}\n\n\n\nsequential\n\nouter_seq &lt;- function(size, innerfun) {\n  outer_out &lt;- foreach(i = 1:size,\n                       .combine = c) %do% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a &lt;- rnorm(size, mean = i)\n                         \n                         b &lt;- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec &lt;- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(in_vec = cvec, size = size)\n                         \n                         h &lt;- sd(cvec)/inner_out\n                         \n                         \n                       }\n  \n  return(outer_out)\n}\n\n\n\nPreallocated for\n\nouter_for &lt;- function(size, innerfun) {\n  outer_out &lt;- matrix(nrow = size, ncol = size)\n  for(i in 1:size) {\n    \n    # Do a matrix mult on a vector specified with i\n    a &lt;- rnorm(size, mean = i)\n    \n    b &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    cvec &lt;- a %*% b\n    \n    # Now iterate over the values in c to do somethign else\n    inner_out &lt;- innerfun(in_vec = cvec, size = size)\n    \n    outer_out[, i] &lt;- sd(cvec)/inner_out\n    \n  }\n  outer_out &lt;- c(outer_out) \n  \n  return(outer_out)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Testing nested parallelism"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html",
    "href": "parallelism/parallel_speed.html",
    "title": "Investigating parallel speedups",
    "section": "",
    "text": "I tend to run a lot of code that can be parallelised, but it’s not always clear when it’s worth it and how best to structure the paralellisation. Should it be at the outermost layer, where I’m typically looping over parameters, some intermediate layer where I might be looping over indices or iterators, or to handle large datasets?\nFor reference, I often have population dynamics models with many species and locations. At each timestep I need to make a lot of calculations on the species, including some large matrix multiplications to get dispersal. These could be parallelised over species. And after simulations are complete, I calculate a lot of covariances over species, space, and time that can be parallelised over pairwise combinations. Both of these cases operate on large arrays, and so would feed large amounts of data to parallelised functions, which would then do some limited processing on it (e.g. calculate covariances and clean them up for return). At the other extreme, each of these situations is governed by an initial set of parameters, giving, for example, environmental conditions, species growth rates, etc. These are often just vectors, and so parallelising over them would feed the parallel function small amounts of data and kick off large amounts of work.\nTo test parallel performance under these different situations, I’ll attempt to build an example that is non-trivial, but still try to stay minimally complex to avoid getting into writing a complex population dynamics model.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#packages-and-setup",
    "href": "parallelism/parallel_speed.html#packages-and-setup",
    "title": "Investigating parallel speedups",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}, because I tend to like writing for loops (there’s a reason- I’ll try to write up sometime later). I’ll also test {furrr} and {future.apply} to see if they differ in any appreciable way.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\nlibrary(foreach)\nlibrary(furrr)\nlibrary(future.apply)\nlibrary(doRNG)\nlibrary(listenv)\n\nJust set up a typical doFuture situation with plan(multisession). Sorting out plans is a topic for another day.\n\nregisterDoFuture()\nplan(multisession)",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#foreach",
    "href": "parallelism/parallel_speed.html#foreach",
    "title": "Investigating parallel speedups",
    "section": "foreach",
    "text": "foreach\n\nmult_foreach &lt;- function(a, b) {\n  c_foreach &lt;- foreach(i = 1:ncol(a), .combine = rbind) %dopar% {\n    a[,i] %*% b\n  }\n  return(t(c_foreach))\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#furrr",
    "href": "parallelism/parallel_speed.html#furrr",
    "title": "Investigating parallel speedups",
    "section": "furrr",
    "text": "furrr\npurrr (and so furrr) don’t seem to work on matrices. So, I guess have a silly pre-step to make it a list. I’m going to do that outside the function, simply because if we went this way, we’d set the data up to work.\n\nmult_furrr &lt;- function(a_list, b) {\n  c_map &lt;- future_map(a_list, \\(x) x %*% b)\n  matrix(unlist(c_map), ncol = 2)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#future.apply",
    "href": "parallelism/parallel_speed.html#future.apply",
    "title": "Investigating parallel speedups",
    "section": "future.apply",
    "text": "future.apply\n\nmult_apply &lt;- function(a, b) {\n  future_apply(a, MARGIN = 2, FUN = function(x) x %*% b)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#simple-for",
    "href": "parallelism/parallel_speed.html#simple-for",
    "title": "Investigating parallel speedups",
    "section": "simple for",
    "text": "simple for\nPreallocate, because I’m not a heathen\n\nmult_for &lt;- function(a, b) {\n  \n  c_for &lt;- a\n  for(i in 1:ncol(a)) {\n    c_for[,i] &lt;- a[,i] %*% b\n  }\n  return(c_for)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#future-for",
    "href": "parallelism/parallel_speed.html#future-for",
    "title": "Investigating parallel speedups",
    "section": "future for",
    "text": "future for\nWe can write a usual for loop if we use futures directly. the futures themselves have to go in a list, because they are futures, not values, and so can’t go straight into a matrix. That list can be preallocated.\nThere are two ways to do this- explicit and implicit- see the future docs.\n\nExplicit futures\n\nmult_for_future_e &lt;- function(a, b) {\n  \n  c_for &lt;- vector(mode = 'list', length = ncol(a))\n  \n  for(i in 1:ncol(a)) {\n    c_for[[i]] &lt;- future({a[,i] %*% b})\n  }\n  # get values and make a matrix\n  v_for &lt;- lapply(c_for, FUN = value)\n  \n  return(matrix(unlist(v_for), ncol = ncol(a)))\n}\n\n\n\nImplicit futures\nusing listenv\n\nmult_for_future_i &lt;- function(a, b) {\n  \n  c_for &lt;- listenv()\n  \n  for(i in 1:ncol(a)) {\n    c_for[[i]] %&lt;-% {a[,i] %*% b}\n  }\n  # get values and make a matrix\n  v_for &lt;- as.list(c_for)\n  \n  return(matrix(unlist(v_for), ncol = ncol(a)))\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#linear-algebra",
    "href": "parallelism/parallel_speed.html#linear-algebra",
    "title": "Investigating parallel speedups",
    "section": "linear algebra",
    "text": "linear algebra\n\nmult_linear &lt;- function(a,b) {\n  t(a %*% b)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#preallocate-the-foreach",
    "href": "parallelism/parallel_speed.html#preallocate-the-foreach",
    "title": "Investigating parallel speedups",
    "section": "preallocate the foreach",
    "text": "preallocate the foreach\nI typically don’t do this, since my understanding of foreach is that it builds them with the .combine, so preallocating doesn’t do anything. But maybe?\n\nmult_foreach_pre &lt;- function(a, b) {\n  c_foreach &lt;- t(a)\n  c_foreach &lt;- foreach(i = 1:ncol(a), .combine = rbind) %dopar% {\n    a[,i] %*% b\n  }\n  return(t(c_foreach))\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#return-the-foreach-as-a-list",
    "href": "parallelism/parallel_speed.html#return-the-foreach-as-a-list",
    "title": "Investigating parallel speedups",
    "section": "Return the foreach as a list",
    "text": "Return the foreach as a list\nIt is possible that using .combine is forcing slower behaviour for the foreach, and it’s optimized for a list?\n\nmult_foreach_list &lt;- function(a, b) {\n  c_foreach &lt;- foreach(i = 1:ncol(a)) %dopar% {\n    a[,i] %*% b\n  }\n  \n  # Do the binding in one step\n  return(matrix(unlist(c_foreach), ncol = 2))\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#dont-preallocate-the-for",
    "href": "parallelism/parallel_speed.html#dont-preallocate-the-for",
    "title": "Investigating parallel speedups",
    "section": "Don’t preallocate the for",
    "text": "Don’t preallocate the for\nHow bad is this- I almost always DO preallocate (it’s faster, and the loop starts at 1, and it’s just cleaner), but it’s possible not to.\n\nmult_for_build &lt;- function(a, b) {\n  \n  c_for &lt;- a[,1] %*% b\n  \n  for(i in 2:ncol(a)) {\n    ctemp &lt;- a[,i] %*% b\n    c_for &lt;- rbind(c_for, ctemp)\n  }\n  return(t(c_for))\n}\n\nI’ll include the furrr and future.apply here too, I guess as reference.\n\nmicrobenchmark(\n  futurefurrr = mult_furrr(a100_l, b),\n  futureapply = mult_apply(a100, b),\n  futureforeach = mult_foreach(a100, b),\n  preallocate_foreach = mult_foreach_pre(a100, b),\n  foreach_list = mult_foreach_list(a100, b),\n  bare_for = mult_for(a100, b),\n  unallocate_for = mult_for_build(a100, b),\n  bare_linear = mult_linear(t(a100), b),\n  times = 10\n)\n\nUnit: milliseconds\n                expr       min        lq       mean    median        uq\n         futurefurrr 2571.1246 2767.6439 2857.16904 2866.4501 2897.1658\n         futureapply 2507.3732 2652.2697 2799.40048 2846.0770 2917.3628\n       futureforeach 2293.2059 2438.9546 2593.98055 2637.0460 2725.0197\n preallocate_foreach 2343.7728 2587.9531 2620.46876 2612.0335 2671.4092\n        foreach_list 2291.8114 2404.4547 2544.75845 2517.0032 2714.5863\n            bare_for   93.8633   99.2174  100.78608   99.9848  101.3844\n      unallocate_for  117.2363  120.5628  124.45097  124.5449  127.4235\n         bare_linear   40.2038   40.7167   42.76278   42.6698   43.5522\n       max neval\n 3297.9195    10\n 3056.1415    10\n 2785.5028    10\n 2820.4449    10\n 2794.5918    10\n  109.7385    10\n  132.2418    10\n   47.7955    10\n\n\nThese results are interesting, and the actual slowdown (not just lack of speedup) is worrying for how I do some things. I think the overhead of shifting the data around is absolutely killing these parallel processes. And I definitely have code that does this sort of thing.\nI have two more questions now (plus one for later):\n\nDoes the foreach loop work as fast as a for if I use %do% instead of %dopar%? Or is the overhead still there?\n\nAnd same with all the futures if I set plan(sequential)?\n\nIf I don’t send prebuilt data, but just some parameters and build the data internally, how do they compare?\n\nSometimes this flow makes sense, and sometimes it doesn’t– e.g. if I’m simulating populations, the outer set of parallelisation just sends parameters. But the internal set often needs to work on things like population matrices. And so maybe that internal loop just shouldn’t be parallel.\n\nHow do other plan options affect these answers? I think this deserves its own page, and could get very complicated once I get into future.callr, future.batchtools, cluster, etc. And multicore might avoid the passing and use pointers, but i can’t test on windows?",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#foreach-do",
    "href": "parallelism/parallel_speed.html#foreach-do",
    "title": "Investigating parallel speedups",
    "section": "foreach %do%",
    "text": "foreach %do%\nLet’s try shifting to %do% for the foreach\n\nmult_foreach_do &lt;- function(a, b) {\n  c_foreach &lt;- foreach(i = 1:ncol(a), .combine = rbind) %do% {\n    a[,i] %*% b\n  }\n  return(t(c_foreach))\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#list-foreach-do",
    "href": "parallelism/parallel_speed.html#list-foreach-do",
    "title": "Investigating parallel speedups",
    "section": "list foreach %do%",
    "text": "list foreach %do%\n\nmult_foreach_list_do &lt;- function(a, b) {\n  c_foreach &lt;- foreach(i = 1:ncol(a)) %do% {\n    a[,i] %*% b\n  }\n  \n  # Do the binding in one step\n  return(matrix(unlist(c_foreach), ncol = 2))\n}\n\nTest against parallel foreach, bare for, and the unallocated for (since that’s kind of what the foreach is doing- building up an object). Since that’s sometimes nice behaviour and leads to cleaner code than a for loop, I’d like to see how they compare.\n\nmicrobenchmark(\n  futureforeach = mult_foreach(a100, b),\n  foreachdo = mult_foreach_do(a100, b),\n  foreachlistdo = mult_foreach_list_do(a100, b),\n  bare_for = mult_for(a100, b),\n  unallocate_for = mult_for_build(a100, b),\n  bare_linear = mult_linear(t(a100), b),\n  times = 10\n)\n\nUnit: milliseconds\n           expr       min        lq       mean    median        uq       max\n  futureforeach 2294.3315 2413.2883 2441.58145 2426.2623 2491.8492 2593.4213\n      foreachdo  108.1478  110.7516  117.48842  118.1481  122.2505  128.2518\n  foreachlistdo  110.9159  111.7610  118.85446  117.0267  119.7476  144.9399\n       bare_for   91.0620   96.5476  106.97241  108.5482  112.5208  131.5772\n unallocate_for  116.1919  120.7025  124.04948  124.7919  127.6500  130.3755\n    bare_linear   39.1770   40.9914   42.25231   42.5430   43.0318   46.4730\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nInteresting. Nearly identical to the unallocated for and quite a bit faster than the parallel version, which seems to hint that it’s the data transfer that’s killing things. And backs up my assumption of what’s going on under the hood in terms of constructing the object as in an unallocated for loop. There’s no appreciable difference in using the foreach with a list and then combining vs combining as we go with .combine.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#plansequential",
    "href": "parallelism/parallel_speed.html#plansequential",
    "title": "Investigating parallel speedups",
    "section": "plan(sequential)",
    "text": "plan(sequential)\nHow do the parallel versions work with plan(sequential)? Do they all get a speedup from avoiding data transfer? This is the same benchmark test as above, but now run sequentially.\n\nplan(sequential)\n\nmicrobenchmark(\n  futurefurrr = mult_furrr(a100_l, b),\n  futureapply = mult_apply(a100, b),\n  futureforeach = mult_foreach(a100, b),\n  bare_for = mult_for(a100, b),\n  unallocate_for = mult_for_build(a100, b),\n  bare_linear = mult_linear(t(a100), b),\n  times = 10\n)\n\nUnit: milliseconds\n           expr      min       lq      mean   median       uq      max neval\n    futurefurrr 116.6781 119.5224 124.35162 125.1431 127.5115 134.7231    10\n    futureapply 105.6057 110.9022 114.05056 113.2071 119.2455 120.0307    10\n  futureforeach 115.8364 123.0657 127.43182 125.7630 128.5088 141.5289    10\n       bare_for  92.1208 103.9310 108.69223 110.5858 113.8986 118.0519    10\n unallocate_for 118.7530 123.6296 127.38882 125.9754 130.5657 140.5084    10\n    bare_linear  40.6936  42.4847  43.83156  44.3669  44.9938  45.7687    10\n\n\nInteresting. foreach and furrr both sped up about as expected (furrr just seems a bit slower in general), but future.apply had much less of a speedup. It must not fall back to a simpler function, and still tries to use the parallel data shuffling? It is still much faster than with plan(multisession) (was 590 microseconds), so something is happening, but it’s not getting down to the speeds of the other futures. And the simple for is still fastest (other than just using linear algebra, obviously).\n\nThe message so far\nTest the parallel implementation at different points in the code- if there’s no way to avoid data passing, a simple for (or other sequential function like apply could be fastest.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#future-for-1",
    "href": "parallelism/parallel_speed.html#future-for-1",
    "title": "Investigating parallel speedups",
    "section": "future for",
    "text": "future for\nWe can write a usual for loop if we use futures directly. the futures themselves have to go in a list, because they are futures, not values, and so can’t go straight into a matrix. That list can be preallocated.\nThere are two ways to do this- explicit and implicit- see the future docs.\n\nExplicit futures\n\nmult_for_future_internal_e &lt;- function(n_reps = 100, size = 1000) {\n  \n  c_for &lt;- vector(mode = 'list', length = n_reps)\n  \n  for(i in 1:n_reps) {\n    c_for[[i]] &lt;- future({a &lt;- rnorm(size, mean = i)\n        b &lt;- matrix(rnorm(size * size), nrow = size)\n        a %*% b}, seed = TRUE)\n  }\n  # get values and make a matrix\n  v_for &lt;- lapply(c_for, FUN = value)\n  \n  return(matrix(unlist(v_for), ncol = n_reps))\n}\n\n\n\nImplicit futures\nusing listenv\nThat’s a funny way to set the seed. Good to know.\n\nmult_for_future_internal_i &lt;- function(n_reps = 100, size = 1000) {\n  \n  c_for &lt;- listenv()\n  \n  for(i in 1:n_reps) {\n    c_for[[i]] %&lt;-% {a &lt;- rnorm(size, mean = i)\n        b &lt;- matrix(rnorm(size * size), nrow = size)\n        a %*% b} %seed% TRUE\n  }\n  # get values and make a matrix\n  v_for &lt;- as.list(c_for)\n  \n  return(matrix(unlist(v_for), ncol = n_reps))\n}\n\nThere’s no reason to have a linear algebra version here, since we’re by definition not operating on existing matrices.\nTry that without adjusting what globals are passed to the futures\n\nmicrobenchmark(\n  futurefurrr = mult_furrr_internal(n_reps = 100, size = 1000),\n  futureapply = mult_apply_internal(n_reps = 100, size = 1000),\n  futureforeach = mult_foreach_internal(n_reps = 100, size = 1000),\n  futurefor_e = mult_for_future_internal_e(n_reps = 100, size = 1000),\n  futurefor_i = mult_for_future_internal_i(n_reps = 100, size = 1000),\n  bare_for = mult_for_internal(n_reps = 100, size = 1000),\n  times = 10\n)\n\nUnit: seconds\n          expr       min        lq      mean    median        uq       max\n   futurefurrr  2.911519  2.937818  3.446520  3.176596  3.296120  6.415807\n   futureapply  2.824095  2.847494  3.067614  3.021533  3.228708  3.546101\n futureforeach  2.829144  2.940019  3.237918  3.160864  3.245488  4.222348\n   futurefor_e 12.650138 13.192132 13.856649 13.579496 14.916949 15.034860\n   futurefor_i 13.213725 13.252212 13.510405 13.378892 13.803717 14.127473\n      bare_for  3.626247  3.706880  3.980060  4.089718  4.211040  4.323426\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nNow the parallelisation is helping quite a bit, even the for loops with futures, though they’re still much slower than the other future methods. However, this is much slower than creating the matrices as we actually should do for this particular calculation, and is even slower than passing those matrices in. So, if the operations need to happen this way anyway (parallel lots of stuff over parameters), this makes lots of sense and speeds up. If we CAN pre-generate matrices, linear algebra is fastest (unsurprisingly), and the loops are next, even if we have to eat pass-in cost. Though in that case sequential is probably better.\nI think the slower bare futures are likely because there’s no chunking being done, and so data is copied to each iteration, rather than to chunks of iterations (which is built into doFuture and I think furrr and future.apply. I could try to manually chunk to test that, but I think I’m just going to skip it.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#globals",
    "href": "parallelism/parallel_speed.html#globals",
    "title": "Investigating parallel speedups",
    "section": "Globals",
    "text": "Globals\nOne of the nice things about future compared to some other parallel backends is that it does pass the global environment, so i don’t have to manage what it gets- it works as it does interactively. BUT, if data passing is what’s killing the speed, maybe I do need to manage what gets passed. In theory, the test above should get even faster if we don’t pass it the global environment, but only the arguments.\nSo, a new version of the above, explicitly limiting passing.\n\nforeach\n\nmult_foreach_internal_g &lt;- function(n_reps = 100, size = 1000) {\n  c_foreach &lt;- foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .export = NULL) %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  return(c_foreach)\n}\n\n\n\nfurrrr\n\nmult_furrr_internal_g &lt;- function(n_reps = 100, size = 1000) {\n  fn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  \n  c_map &lt;- future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, globals = NULL))\n  matrix(unlist(c_map), ncol = n_reps)\n}\n\n\n\nfuture.apply\n\nmult_apply_internal_g &lt;- function(n_reps = 100, size = 1000) {\n    fn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n    }\n    \n  c_apply &lt;- future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE, future.globals = FALSE)\n  \n    matrix(unlist(c_apply), ncol = n_reps)\n}",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/parallel_speed.html#benchmark-1",
    "href": "parallelism/parallel_speed.html#benchmark-1",
    "title": "Investigating parallel speedups",
    "section": "Benchmark",
    "text": "Benchmark\n\nmicrobenchmark(\n  futurefurrr = mult_furrr_internal(n_reps = 100, size = 1000),\n  futureapply = mult_apply_internal(n_reps = 100, size = 1000),\n  futureforeach = mult_foreach_internal(n_reps = 100, size = 1000),\n  futurefurrr_g = mult_furrr_internal_g(n_reps = 100, size = 1000),\n  futureapply_g = mult_apply_internal_g(n_reps = 100, size = 1000),\n  futureforeach_g = mult_foreach_internal_g(n_reps = 100, size = 1000),\n  bare_for = mult_for_internal(n_reps = 100, size = 1000),\n  times = 10\n)\n\nUnit: seconds\n            expr      min       lq     mean   median       uq      max neval\n     futurefurrr 2.934941 2.991153 3.073430 3.036014 3.115680 3.383588    10\n     futureapply 2.961602 2.986763 3.314419 3.268155 3.529925 3.972054    10\n   futureforeach 2.981263 2.996048 3.114741 3.084971 3.208947 3.359805    10\n   futurefurrr_g 2.852686 2.938126 3.049807 3.008130 3.150936 3.425673    10\n   futureapply_g 2.933841 3.054000 3.218608 3.119524 3.173425 3.860498    10\n futureforeach_g 2.941432 3.039213 3.142060 3.138042 3.216203 3.484246    10\n        bare_for 3.615668 3.733735 3.783454 3.758691 3.801958 4.025737    10\n\n\nSo, that’s not hauling around a ton of extra stuff. I think, based on the doFuture vignette, that future will send the globals, but only those that are needed. So it’s not that they’re in functions, it’s that future can tell it doesn’t need to pass extra variables. This means we don’t really get performance gains because future already had our back.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "Speed-testing parallel"
    ]
  },
  {
    "objectID": "parallelism/template_modification.html",
    "href": "parallelism/template_modification.html",
    "title": "Template modification",
    "section": "",
    "text": "I don’t want to spend a ton of time on this, but if I want to add the ability to adjust any of the SLURM variables, including those that have dashes ‘-’ in their names, and I don’t want to use the format of the resources list enforced by the {batchtools} template slurm-simple.tmpl, I’ll need to modify the template.\nTo do that, it’s a bit helpful to know how {brew} works, though it’s mostly self-explanatory from reading the templates. I’m primarily confused why I can’t pass names with backticks, when somethign like\nresources &lt;- list('name-with-ticks' = 17, namenoticks = 'a')\nresources\n\n$`name-with-ticks`\n[1] 17\n\n$namenoticks\n[1] \"a\"\n\nnames(resources)\n\n[1] \"name-with-ticks\" \"namenoticks\"\nWorks, since the future.batchtools template just uses unlist and names. e.g. the relevant bit is\nopts &lt;- unlist(resources, use.names = TRUE)\nopts &lt;- sprintf(\"--%s=%s\", names(opts), opts)\nopts &lt;- paste(opts, collapse = \" \")\n\nopts\n\n[1] \"--name-with-ticks=17 --namenoticks=a\"\nIt must be enforced by the batchtools wrapper for brew, because this works\nresources &lt;- list('name-with-ticks' = 17, namenoticks = 'a')\njob.name &lt;- 'testjob'\nlog.file = 'testlog'\nuri &lt;- 'testuri'\nbrew::brew(file = file.path('batchtools_templates', 'slurm.tmpl'), output = 'temp.txt')\nyeilding\nDo I want to dig into why batchtools enforces syntactic names? Not really.\nDo I want to bypass batchtools slurm generation? Not really.\nSo, can I modify the slurm script to do what I want? I think probably.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Modifying templates"
    ]
  },
  {
    "objectID": "parallelism/template_modification.html#what-do-i-want",
    "href": "parallelism/template_modification.html#what-do-i-want",
    "title": "Template modification",
    "section": "What do I want?",
    "text": "What do I want?\n\nTo pass arbitrary slurm arguments, similar to how future.batchtools ’s slurm.tmpl works, but including dash names. I really don’t like that batchtools takes over some of those arguments and calls them different things.\nTo be able to pass the options in the formats slurm allows (e.g. --time in seconds or as 00:05:00, --mem as an integer of megabytes, or as \"1GB\")\n\nI think what might work is to use dots in the names of resources and then translate to dashes?\ne.g. insert one small gsub line in the future.batchtools script:\n\nresources_dots &lt;- list('name.with.ticks' = 17, namenoticks = 'a')\nopts &lt;- unlist(resources_dots, use.names = TRUE)\nopts &lt;- sprintf(\"--%s=%s\", names(opts), opts)\nopts &lt;- gsub('\\\\.', '-', opts)\nopts &lt;- paste(opts, collapse = \" \")\nopts\n\n[1] \"--name-with-ticks=17 --namenoticks=a\"\n\n\nI generally like to structure SLURM scripts with\n#SBATCH --option1=value1\n#SBATCH --option2=value2\nAnd this template structures them\n#SBATCH --option1=value1 --option2=value2\nI could almost certainly write a thing that put carriage returns and #SBATCH in front of each, but we never see these templates, so I think I’ll skip that.\nI’ve changed it in batchtools.slurm.tmpl, and now the question is whether it runs afoul of some other batchtools error-catcher.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Modifying templates"
    ]
  },
  {
    "objectID": "parallelism/template_modification.html#outcome",
    "href": "parallelism/template_modification.html#outcome",
    "title": "Template modification",
    "section": "Outcome",
    "text": "Outcome\nIt seems to work\nI can also change --job-name now with resources$job.name, but the catch is I don’t know if there’s a way to change it on a per-job basis, since I have to specify it in plan with the resources list, and the iteration won’t be known until later. I would have thought that’s where the default job.name was coming from in slurm.tmpl and slurm-simple.tmpl, but they seem to just call themselves ‘doFuture’.",
    "crumbs": [
      "Code Demos",
      "R parallel processing on HPC",
      "Modifying templates"
    ]
  },
  {
    "objectID": "plotting/faded_colors.html",
    "href": "plotting/faded_colors.html",
    "title": "Faded colors",
    "section": "",
    "text": "library(tidyverse) # Overkill, but easier than picking and choosing\nlibrary(colorspace)\nlibrary(sf)\nlibrary(patchwork)\nThere are a number of reasons we might want bivariate color axes in plots. The particular use I’m looking for now is to use a faded color to indicate less certainty in a result. Other uses will be developed later or elsewhere, but should build on this fairly straightforwardly.\nI’m doing this with colorspace because it’s hue-chroma-luminance approach makes it at least appear logical to shift along those dimensions. We might want hue (or luminance) to show one thing, and intensity to show another. Though we will play around with how that looks in practice. The specific use motivatiung this is to show the predicted amount of something with hue, and certainty with chroma or luminance (in particular, we have a model that makes predictions more accurately in some places than others). But there are many other potential uses.\nIn the HCL exploration file, I figure out HOW to generate faded colors and find some palettes that might work. Here, I’m going to sort out how to go from there to using them in plots, including creating legends.",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Faded 2d colour ramps"
    ]
  },
  {
    "objectID": "plotting/faded_colors.html#plot-the-bivariate-colors",
    "href": "plotting/faded_colors.html#plot-the-bivariate-colors",
    "title": "Faded colors",
    "section": "Plot the bivariate colors",
    "text": "Plot the bivariate colors\nBefore trying to plot with the colors, first I want to actually plot them themselves. One reason is to test how they are being created and specified, and the other is potentially to use the plot as a legend.\nWhy? The legend() part of ggplot may not handle the bivariate nature of the colors well, so need to basically homebrew one. This is the most flexible option- make the plot, then shrink and pretend it’s a legend. But, could also make a legend in vector form, then stack. Just not sure how well that’ll work. The shrunk plot would work better for continuous variables, the legend probably works better to use other parts of ggplot and not always have to screw around with grobs or ggarrange or patchwork or cowplot. I’ll try them all, I guess.\nFirst, make a matrix of colors. Take the base palette, fade it and save the color values for the whole thing. The for loop is lame, should be a function, but I’m just looking right now.\n\nbaseramp &lt;- sequential_hcl(8, 'ag_Sunset')\n\nfadesteps &lt;- seq(0,1, by = 0.25)\n\ncolormat &lt;- matrix(rep(baseramp, length(fadesteps)), nrow = 5, byrow = TRUE)\n\nfor(i in 1:length(fadesteps)) {\n  colormat[i, ] &lt;- lighten(colormat[i, ], amount = fadesteps[i]) %&gt;%\n    desaturate(amount = fadesteps[i])\n}\n\nOption 1 is to make that into a plot that we can then smash on top\n\n# Make a tibble from the matrix to feed to ggplot\ncoltib &lt;- as_tibble(colormat, rownames = 'row') %&gt;%\n  pivot_longer(cols = starts_with('V'), names_to = 'column')\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n# coltib\n\nggplot(coltib, aes(y = row, x = column, fill = value)) + \n  geom_tile() + scale_fill_identity()\n\n\n\n\n\n\n\n\nThat’s upside-down with how I tend to think about it. How about flipping the construction?\n\nfadesteps &lt;- rev(seq(0,1, by = 0.25))\ncolormat &lt;- matrix(rep(baseramp, length(fadesteps)), nrow = 5, byrow = TRUE)\n\nfor(i in 1:length(fadesteps)) {\n  colormat[i, ] &lt;- lighten(colormat[i, ], amount = fadesteps[i]) %&gt;%\n    desaturate(amount = fadesteps[i])\n}\n\ncoltib &lt;- as_tibble(colormat, rownames = 'row') %&gt;%\n  pivot_longer(cols = starts_with('V'), names_to = 'column')\n\n\nggplot(coltib, aes(y = row, x = column, fill = value)) +\n  geom_tile() + scale_fill_identity()",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Faded 2d colour ramps"
    ]
  },
  {
    "objectID": "plotting/faded_colors.html#programmatic-color-setting",
    "href": "plotting/faded_colors.html#programmatic-color-setting",
    "title": "Faded colors",
    "section": "Programmatic color setting",
    "text": "Programmatic color setting\nCreate a function basically following the above. But allow it to take palettes by name or raw hue values if they are obtained elsewhere (like from a manually specified hue ramp). hex color vals and pal names are both characters, but hex always starts with ‘#’, so should be able to auto-detect. It can take a number of fades, or a vector of specific fade levels, and returns the matrix of colors.\n\ncol2dmat &lt;- function(pal, n1, n2 = 2, dropwhite = TRUE, fadevals = NULL) {\n  # pal can be either a palette name or a vector of hex colors (or single hex color)\n  # dropwhite is there to by default knock off the bottom row that's all white\n  # fadevals is a way to bypass the n2 and specify specific fade levels (ie if nonlinear)\n\n  if (all(str_detect(pal, '#'))) {\n    baseramp &lt;- pal\n  } else {\n    baseramp &lt;- sequential_hcl(n1, pal)\n  }\n\n  if (is.null(fadevals)) {\n    if (dropwhite) {n2 = n2+1}\n\n    fadesteps &lt;- rev(seq(0,1, length.out = n2))\n\n    if (dropwhite) {fadesteps &lt;- fadesteps[2:length(fadesteps)]}\n\n  }\n\n  if (!is.null(fadevals)) {\n    fadesteps &lt;- sort(fadevals, decreasing = TRUE)\n  }\n\n  colormat &lt;- matrix(rep(baseramp, length(fadesteps)), nrow = length(fadesteps), byrow = TRUE)\n\n\n  for(i in 1:length(fadesteps)) {\n    colormat[i, ] &lt;- lighten(colormat[i, ], amount = fadesteps[i]) %&gt;%\n      desaturate(amount = fadesteps[i])\n  }\n\n  return(colormat)\n}\n\nCreate another function that plots a matrix of colors. Typically that matrix comes out of col2dmat. Why not make one big function? because we will often want to access the color values themselves, and not always just plot them.\n\nplot2dcols &lt;- function(colmat) {\n  coltib &lt;- as_tibble(colmat, rownames = 'row') %&gt;%\n    pivot_longer(cols = starts_with('V'), names_to = 'column') %&gt;%\n    mutate(row = as.numeric(row), column = as.numeric(str_remove(column, 'V')))\n\n  colplot &lt;- ggplot(coltib, aes(y = row, x = column, fill = value)) +\n    geom_tile() + scale_fill_identity()\n\n  return(colplot)\n}\n\nTest that works with a given number of fades\n\nnewcolors &lt;- col2dmat('ag_Sunset', n1 = 8, n2 = 4)\nplot2dcols(newcolors)\n\n\n\n\n\n\n\n\nTest with set fade levels. REMEMBER FADE is FADE, not intensity. ie 0 is darkest.\n\nnewcolsuneven &lt;- col2dmat('ag_Sunset', n1 = 8, fadevals = c(0, 0.33, 0.8))\nplot2dcols(newcolsuneven)\n\n\n\n\n\n\n\n\nTest with non-built in palettes- ie setting hue manually. This could be particularly useful if we want quantitative hues. This tests the ability to auto-detect a vector of colors.\nUse the manual-set colors from hcl exploration for testing.\n\nhclmat &lt;- cbind(50, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 50, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg &lt;- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\n\n\n\n\nWorks!\n\npgmat &lt;- col2dmat(hex(pg), n2 = 4)\nplot2dcols(pgmat)",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Faded 2d colour ramps"
    ]
  },
  {
    "objectID": "plotting/faded_colors.html#plotting-the-data",
    "href": "plotting/faded_colors.html#plotting-the-data",
    "title": "Faded colors",
    "section": "Plotting the data",
    "text": "Plotting the data\nAbove, we were trying to plot the colors. Now, we want to assign those colors to data so we can plot the data with the appropriate color.\n\nSingle datapoint\nThe above is fine for looking at a color matrix, but in general, we’ll have a dataframe with a value for each dimension, and need to assign it a single color. Step one is figuring out how to do that assignment.\nCan I take a ‘datapoint’ with arbitrary values on both axes and choose its color?\nCan we do that for both color bins or continuous color?\nWe’ll need to relativise the data, since neither hue or fade are defined on the real line, but by their endpoints.\nLet’s fake some data. Don’t use round numbers (e.g. 0, 100) to avoid making stupid mistakes relating to relativising the scale. We need to know the endpoints of the data to match the endpoints of the hue and fade, and then a datapoint somewhere in the middle to create.\n\n# what is the range of the data?\n  # don't use round numbers (e.g. 0, 100)\nmax1 &lt;- 750\nmin1 &lt;- 150\n\nmax2 &lt;- 67\nmin2 &lt;- -55\n\n\n# get color for a single value pair\nval1 &lt;- 455\nval2 &lt;- 8\n\njust use a simple linear transform to get position on the min-max axes. Could use logit or something for either, but keeping it simple. The value above the min divided by the range gives where the data point is on a 0-1 scale from min to max. In reality, we will have two vectors (well, cols in a dataframe), and this is actually easier to do in that case because we can just get the min and max directly.\n\nvalpos1 &lt;- (val1-min1)/(max1-min1)\nvalpos2 &lt;- (val2-min2)/(max2-min2)\n\nThat’s easy to vectorize, which is basically how we’ll do it with a dataframe.\nFor now, can we just get individual colors to assign to a value pair?\nNeed to specify the min and max hue- these are the hue endpoints, not data endpoints.\n\nminhue &lt;- 130\nmaxhue &lt;- 275\n\nfind the hue value at the same relative position as the datapoint\n\nmatchH1 &lt;- (maxhue-minhue)*valpos1 + minhue\n\nUsing the manual colors\n\nsinglehclmat1 &lt;- cbind(50, max_chroma(h = matchH1, l = 50, floor = TRUE),\n                matchH1)\n\npgsingle1 &lt;- polarLUV(singlehclmat1)\nswatchplot(hex(pgsingle1))\n\n\n\n\n\n\n\n\nalso need the other axis. That’s also just on 0-1 (well, 1-0, since it’s fade, not intensity) and so would be done the same way.\n\nsinglecol &lt;- col2dmat(hex(pgsingle1), fadevals = (1-valpos2))\nswatchplot(singlecol)\n\n\n\n\n\n\n\n\nIt’s clear we can write all this as functions, and that we’ll need to. So…\n\n\nProgramatically finding colors\nEarlier, we made col2dmat, which found colors and faded them. We want to do something similar here, but the goal isn’t quite the same- we don’t really care about the full matrix, but about a single point. We could modify col2dmat, but probably easier (and fewer horrible logicals) to just write purpose-built functions.\nNeed new functions to 1) find the hue, 2) adjust the fade\n\nFind the hue\nTakes either a number of bins or Inf for continuous.\n\nhuefinder &lt;- function(hueval, minhue, maxhue, n = Inf, palname = NULL) {\n\n  # If continuous, use the value\n  # If binned, find the value of the bin the value is in\n  if (is.infinite(n)) {\n    matchH &lt;- (maxhue-minhue)*hueval + minhue\n  } else if (!is.infinite(n)) {\n\n    nvec &lt;- seq(from = 0, to = 1, length.out = n)\n\n    # The nvecs need to choose the COLOR, but the last one gets dropped in\n    # findInterval, so need an n+1\n    whichbin &lt;- findInterval(hueval,\n                             seq(from = 0, to = 1, length.out = n+1),\n                             rightmost.closed = TRUE)\n\n    # Don't build if using named palette because won't have min and max\n    if (is.null(palname)) {\n      binhue &lt;- nvec[whichbin]\n      matchH &lt;- (maxhue-minhue)*binhue + minhue\n    }\n\n  }\n\n  if (is.null(palname)) {\n    h &lt;- cbind(50, max_chroma(h = matchH, l = 50, floor = TRUE),\n               matchH)\n    h &lt;- hex(polarLUV(h))\n  } else {\n    h &lt;- sequential_hcl(n, palname)[whichbin]\n  }\n\n  return(h)\n}\n\n\n\nFind the fade\nThis takes the just found hue as basehue, and fades it. Again, n specifies either a number of fade bins or if infinite it is continuous and so just fades by whatever the value is.\n\nfadefinder &lt;- function(fadeval, basehue, n = Inf) {\n\n  # If n is infinite, just use fadeval. Otherwise, bin, dropping the all-white level\n  if (is.infinite(n)) {\n    fadeval &lt;- fadeval\n  } else {\n    # The +1 drops the white level\n    fadevec &lt;- seq(from = 0, to = 1, length.out = n + 1)\n\n    # Rightmost closed fixes an issue right at 1\n    fadeval &lt;- fadevec[findInterval(fadeval, fadevec, rightmost.closed = TRUE) + 1]\n  }\n\n  fadedcol &lt;- lighten(basehue, amount = 1-fadeval) %&gt;%\n    desaturate(amount = 1-fadeval)\n}\n\n\n\nHue and fade\nThis is meant to use in a mutate to take two columns of data and find the appropriate color. Should use … to pass, but whatever\n\ncolfinder &lt;- function(hueval, fadeval, minhue, maxhue, nhue = Inf, nfade = Inf, palname = NULL) {\n  thishue &lt;- huefinder(hueval, minhue, maxhue, nhue, palname)\n  thiscolor &lt;- fadefinder(fadeval, thishue, nfade)\n}\n\nQuick tests\n\nfunhue &lt;- huefinder(valpos1, minhue = minhue, maxhue = maxhue)\nfunfaded &lt;- fadefinder(valpos2, funhue)\nswatchplot(funfaded)\n\n\n\n\n\n\n\n\nshould be the same as\n\nfunboth &lt;- colfinder(valpos1, valpos2, minhue, maxhue)\nswatchplot(funboth)\n\n\n\n\n\n\n\n\n\n\n\nCalculating for dataframes\nVectorizing the relativization calculations is straightforward.\n\nvec1 &lt;- c(150, 588, 750, 455, 234)\n\n# get it for each value in vectorized way\n(vec1 - min(vec1))/(max(vec1)-min(vec1))\n\n[1] 0.0000000 0.7300000 1.0000000 0.5083333 0.1400000\n\n\nMaking a function to get the relative position. We can use this in the mutate once we move on to dataframes.\n\nrelpos &lt;- function(vec) {\n  (vec - min(vec))/(max(vec)-min(vec))\n}\n\nNow, let’s make a dataframe of fake data, with one column that should map to hue and the other mapping to fade. This just puts points all across the space of both variables so we can make sure everything is getting assigned correctly. Then, we’ll use the functions we just created to do a few different things:\n\ncustom hue ramps and built-in palettes\nbinned hue and fade\ncontinuous hue and binned fade\nboth continuous\n\nThe ‘continuous’ examples using inbuilt palettes are only pseudo-continuous by using large numbers of bins because that’s easier for the moment given the way sequential_hcl() works. There’s probably a way around it, but for the moment I’ll ignore it.\n\ncolortibble &lt;- tibble(rvec1 = runif(10000, min = -20, max = 50),\n       rvec2 = runif(10000, min = 53, max = 99)) %&gt;%\n  mutate(rel1 = relpos(rvec1),\n         rel2 = relpos(rvec2)) %&gt;%\n  mutate(colorval = colfinder(rel1, rel2, minhue, maxhue),\n         binval = colfinder(rel1, rel2, minhue, maxhue, nhue = 8, nfade = 4),\n         # need to bypass some args\n         binsun = colfinder(rel1, rel2, nhue = 8, nfade = 4, palname = 'ag_Sunset',\n                            minhue = NULL, maxhue = NULL),\n         pseudoconsun = colfinder(rel1, rel2, nhue = 1000, nfade = 4, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL),\n         pseudoconsun2 = colfinder(rel1, rel2, nhue = 1000, nfade = Inf, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL))\n\nContinuous in both dimensions, using custom hue ramp\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = colorval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\n\n\nBinned both dims, custom ramp\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = binval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\n\n\nInbuilt palette, binned both dims.\nThere is a spot in this ag_Sunset palette that matches the ggplot default grey background and so hard to see, but I’ll ignore that for the moment since it doesn’t affect the main thing we’re doing. THese aren’t production plots.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = binsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\n\n\nPseudo-continuous, binned fades.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = pseudoconsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\n\n\nPseudo-continuous both dimensions.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = pseudoconsun2)) +\n  geom_point() +\n  scale_color_identity()",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Faded 2d colour ramps"
    ]
  },
  {
    "objectID": "plotting/faded_colors.html#plotting-data",
    "href": "plotting/faded_colors.html#plotting-data",
    "title": "Faded colors",
    "section": "Plotting data",
    "text": "Plotting data\nNow, let’s see how that might look for some real data. I’ll use some with point data (iris) and then move on to maps, since that’s originally what this was developed for. It should easily extend to anything we can aes() on, e.g. barplot fills, etc.\n\nScatterplot\nTo keep it simple, let’s use iris\nIt won’t span the full space because of the relationship, but that’s OK, I think. We did that above. Here’s iris- now let’s color this plot.\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width)) + geom_point()\n\n\n\n\n\n\n\n\n\nFade defined by an axis\nThis is how we did it above when plotting the colors to make sure they were working.\nRelativize the x and y to define colors.\n\ncoloriris &lt;- iris %&gt;%\n  mutate(rel1 = relpos(Sepal.Length),\n         rel2 = relpos(Petal.Width)) %&gt;%\n  mutate(colorval = colfinder(rel1, rel2, minhue, maxhue),\n         binval = colfinder(rel1, rel2, minhue, maxhue, nhue = 8, nfade = 4),\n         # need to bypass some args\n         binsun = colfinder(rel1, rel2, nhue = 8, nfade = 4, palname = 'ag_Sunset',\n                            minhue = NULL, maxhue = NULL),\n         pseudoconsun = colfinder(rel1, rel2, nhue = 1000, nfade = 4, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL),\n         pseudoconsun2 = colfinder(rel1, rel2, nhue = 1000, nfade = Inf, palname = 'ag_Sunset',\n                                   minhue = NULL, maxhue = NULL))\n\nMake some plots to see the colors and fades correspond to the axis values in binned and unbinned ways.\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = colorval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = pseudoconsun2)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = binsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\n\n\n\n\nFade as a new aesthetic\nTo actually match what I want to use this for, it’s more like we’d say versicolor is less certain. IE Species defines the fade. This is like fade is an aesthetic in ggplot, but we’re sort of manually doing it.\nLet’s set hue by sepal length, and fade by species\n\nuncertainVers &lt;- iris %&gt;%\n  mutate(rel1 = relpos(Sepal.Length),\n         faded = ifelse(Species == 'versicolor', 0.50, 1)) %&gt;%\n  mutate(binhue = huefinder(rel1, n = 8, palname = 'ag_Sunset'),\n         conhue = huefinder(rel1, n = 1000, palname = 'ag_Sunset'),\n         binfade = fadefinder(faded, binhue),\n         confade = fadefinder(faded, conhue))\n\nNow, versicolor should be faded relative to the others\n\nggplot(uncertainVers, aes(x = Sepal.Length, y = Petal.Width, color = binfade)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\n\nggplot(uncertainVers, aes(x = Sepal.Length, y = Petal.Width, color = confade)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\n\n\nThat seems to be working, both binned and continous on the hue scale.\n\n\n\nMaps\nWhat I really want this for is a map, with each polygon having a value of the variable of interest mapped to hue, and a ‘certainty’ determining the fade. Though that axis could really be any other value. Can I mock that up?\nRead a map in of catchments in Australia.\n\nallbasins &lt;- read_sf(file.path('data', '42343_shp', 'rbasin_polygon.shp'))\n\nIgnoring fade for the minute, what should we color by? Probably should be random, really, for the demo.\nColoring by centroid will just put a cross-country fade on:\n\nggplot(allbasins, aes(fill = CENTROID_X)) + geom_sf() + scale_fill_continuous_sequential('ag_Sunset')\n\n\n\n\n\n\n\n\nLet’s make a column representing the value we want to plot for each basin, just chosen at random\n\nallbasins &lt;- allbasins %&gt;%\n  mutate(fakevals = runif(nrow(allbasins))) %&gt;%\n  mutate(rel1 = relpos(fakevals)) %&gt;%\n  mutate(binhue = huefinder(rel1, n = 8, palname = 'ag_Sunset'),\n         conhue = huefinder(rel1, n = 1000, palname = 'ag_Sunset'))\n\nI can use the values directly here with scale_fill_XX if I don’t care about fade\n\nggplot(allbasins, aes(fill = fakevals)) + geom_sf() + scale_fill_continuous_sequential('ag_Sunset')\n\n\n\n\n\n\n\n\nbut the hues for the faded should match the set hues. Now, I need to use scale_fill_identity(). Works for binned and pseudo-continuous. I’ll save the binned to compare later with the faded version.\n\nhuesonly &lt;- ggplot(allbasins, aes(fill = binhue)) +\n  geom_sf() +\n  scale_fill_identity()\nhuesonly\n\n\n\n\n\n\n\nggplot(allbasins, aes(fill = conhue)) +\n  geom_sf() +\n  scale_fill_identity()\n\n\n\n\n\n\n\n\nNow, fade some out (with relatively low probability)\n\nallbasins &lt;- allbasins %&gt;%\n  mutate(faded = sample(x = c(1, 0.5),\n                           size = nrow(allbasins),\n                           replace = TRUE,\n                           prob = c(0.8, 0.2))) %&gt;%\n  mutate(binfade = fadefinder(faded, binhue),\n         confade = fadefinder(faded, conhue))\n\nBinned and continuous. Again, save the binned for comparison\n\nhuefade &lt;- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity()\nhuefade\n\n\n\n\n\n\n\nggplot(allbasins, aes(fill = confade)) +\n  geom_sf() +\n  scale_fill_identity()\n\n\n\n\n\n\n\n\nplot the raw and faded next to each other using patchwork. We can now see that some of the catchments are faded versions of the original hue.\n\nhuesonly + huefade\n\n\n\n\n\n\n\n\n\nLegends\nWe need legends. Could be done by playing with the actual ggplot legend or making mini plot and gluing on.\nQuick attempt at guide fails, because the colors are mixed up because of the RGB sorting.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend') +\n  guides(fill = guide_legend(ncol = 2))\n\n\n\n\n\n\n\n\nCan I change the order by basing it on the hues and then the fades? Does ‘breaks’ work? Yeah, sort of. And need to sort them in the right way.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = unique(allbasins$binhue))\n\n\n\n\n\n\n\n\nI think that will basically work, but I’ll need to edit a bit There’s probably a way to write the functions better to just do this all in the mutates, but for now, I can create a tibble of breaks and labels using summarise.\n\nbreaksnlabels &lt;- allbasins %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(binhue) %&gt;%\n  summarize(minbin = min(fakevals),\n            maxbin = max(fakevals),\n            fromto = paste0(as.character(round(minbin, 2)),\n                            ' to ',\n                            as.character(round(maxbin, 2)))) %&gt;%\n  ungroup() %&gt;%\n  arrange(minbin)\n\nWorks for the unfaded\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = breaksnlabels$binhue,\n                      labels = breaksnlabels$fromto)\n\n\n\n\n\n\n\n\nI could now ALSO fade those, but I might be able to do it as one summarise using the faded column\n\nfadebreaks &lt;- allbasins %&gt;%\n  st_drop_geometry() %&gt;%\n  # needs to capture the color boundaries, whether or not faded\n  group_by(binhue) %&gt;%\n  mutate(minbin = min(fakevals),\n            maxbin = max(fakevals),\n            fromto = paste0(as.character(round(minbin, 2)),\n                            ' to ',\n                            as.character(round(maxbin, 2)))) %&gt;%\n  ungroup() %&gt;%\n  group_by(binfade, faded) %&gt;%\n  summarize(minbin = first(minbin),\n            maxbin = first(maxbin),\n            fromto = first(fromto)) %&gt;%\n  ungroup() %&gt;%\n  arrange(minbin, desc(faded))\n\n`summarise()` has grouped output by 'binfade'. You can override using the\n`.groups` argument.\n\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fadebreaks$binfade,\n                      labels = fadebreaks$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nPlot tweaking\nThat’s close. Can I make the labels better? Ideally, drop from the faded, and make them at 45 or something. and fix up the size.\nFirst, drop the labels on the faded, since they are the same as the base hue.\n\nfb2 &lt;- fadebreaks %&gt;%\n  mutate(fromto = ifelse(faded == 0.5, '', fromto))\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\nFixing up the sizes and angles. The size doesn’t do what I want (square), because the text is too big.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n      legend.background = element_blank(),\n      legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n      legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\n\n\n\n\nCan I fake it on the row labels by inserting line breaks? The number of lines is really unstable across device sizes or saving the figure, so the number of line breaks will have to be adjusted every time this gets saved etc. But it might kind of work.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value\\n\\n\\n\\nCertain\\n\\n\\nUncertain', title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\n\n\n\n\nCan I bold that title?\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = expression(atop(bold('Value'),atop('Certain','Uncertain'))),\n                             title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\n\n\n\n\nThat doesn’t work very well. Does ggtext do it? Allows markdown syntax and HTML (hence the  instead of ). It works, but still, the number of breaks will depend on the size of the figure device or file\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = '**Value**&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;Certain&lt;br&gt;&lt;br&gt;Uncertain',\n                             title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'), # This should make them square, but isn't because the angled value labels don't allow it.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\n\n\n\n\nIf we want square legend boxes and readable text for the value labels, might have to go vertical and that means re-doing the breaks and labels dataframe\n\nfbv &lt;- fadebreaks %&gt;%\n  mutate(fromto = ifelse(faded == 1, '', fromto)) %&gt;%\n  arrange(desc(faded), minbin)\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  guides(fill = guide_legend(title = '**Value**&lt;br&gt;&lt;br&gt;Certain Uncertain',\n                             title.position = 'top',\n                             ncol = 2, label.position = 'right')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'right',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'))\n\n\n\n\n\n\n\n\nThat works pretty well. If we wanted multiple levels of uncertainty (fades), a similar thing would work with just having more columns. That basically works. If I want to label the fades more robustly, I think I’ll likely need to resort to grobs, in which case I probably might as well do the figure as legend method.\n\n\nMini-figure legends\nSometimes we want to create a legend and then add it back into a figure (maybe if it’s shared, or we want a standard legend across a group of figures). Here, we might want to create a different legend for the certian and uncertain, glue them together, and then glue them back on the main figure.\nto show how this might make sense, let’s make three plots- one with just the certain, one with uncertain, and one with no legend, and then glue together Making this as vertical, but easy enough to swap\nMake the map alone\n\njustmap &lt;- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  theme(legend.position = 'none')\n\n# used later- continuous specification of color and fade\njustmapcon &lt;- ggplot(allbasins, aes(fill = confade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  theme(legend.position = 'none')\n\nGet the indices for the two fades\n\ncerts &lt;- which(fbv$faded == 1)\nuncerts &lt;- which(fbv$faded == 0.5)\n\nMake the legend for the unfaded\n\ncertleg &lt;- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade[certs],\n                      labels = fbv$fromto[certs]) +\n  guides(fill = guide_legend(title = 'Certain',\n                             title.position = 'top',\n                             ncol = 1, label.position = 'right')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'right',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'))\n\n# I don't actually want the plot, just the legend, so\n certleg &lt;- ggpubr::get_legend(certleg)\n\nAnd the faded\n\n uncertleg &lt;- ggplot(allbasins, aes(fill = binfade)) +\n   geom_sf() +\n   scale_fill_identity(guide = 'legend',\n                       breaks = fbv$binfade[uncerts],\n                       labels = fbv$fromto[uncerts]) +\n   guides(fill = guide_legend(title = 'Uncertain',\n                              title.position = 'top',\n                              ncol = 1, label.position = 'right')) +\n   theme(legend.title = ggtext::element_markdown(),\n         legend.position = 'right',\n         legend.background = element_blank(),\n         legend.key.size = unit(0.5, 'cm'))\n\n # I don't actually want the plot, just the legend, so\n uncertleg &lt;- ggpubr::get_legend(uncertleg)\n\nGlue those legends\n\nbothleg &lt;- ggpubr::ggarrange(certleg, uncertleg)\n\nand glue on the plot\n\n plotpluslegs &lt;- ggpubr::ggarrange(justmap, bothleg, widths = c(8,2))\n plotpluslegs\n\n\n\n\n\n\n\n\nThat’s not really any better than what I had before. It is useful to have this level of control sometimes though. In particular, we might want to use a PLOT as a legend, either binned or not.\nTo use a plot as a legend\nHere, binned is obviously the way to go, especially for the two fade levels, but let’s demo both.\nabove, we defined a function col2dmat that makes a plot of the color matrix. Let’s use that to demo a few options. First create the figures that will be the legends.\nBinned both dims, two fades, but just low-high labels\n\nbinnedplotmat &lt;- col2dmat('ag_Sunset', n1 = 8, fadevals = c(0, 0.5))\n bin2legqual &lt;- plot2dcols(binnedplotmat) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = c(1, 2), labels = c('Uncertain', 'Certain')) +\n   # Vague levels\n   scale_x_continuous(breaks = c(1, 8), labels = c('Low', 'High')) +\n   theme(axis.text = element_text())\n bin2legqual\n\n\n\n\n\n\n\n\nBinned both dims, but now the hue values are quantitatively labeled\n\nnamedlabs &lt;- filter(fb2, fromto != '') %&gt;% select(fromto) %&gt;% pull()\n bin2legquant &lt;- plot2dcols(binnedplotmat) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = c(1, 2), labels = c('Uncertain', 'Certain')) +\n   # Vague levels\n   scale_x_continuous(breaks = 1:8, labels = namedlabs) +\n   theme(axis.text.y = element_text(),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n   ggtitle('Value')\n bin2legquant\n\n\n\n\n\n\n\n\nA few levels of fade. Very similar to above\n\nmat4fade &lt;- col2dmat('ag_Sunset', n1 = 8, n2 = 4)\n\n fadevals &lt;- rev(seq(0,1, length.out = 4+1))[1:4]\n bin4leg &lt;- plot2dcols(mat4fade) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = 1:4, labels = rev(fadevals), name = 'Certainty') +\n   scale_x_continuous(breaks = 1:8, labels = namedlabs, name = 'Value') +\n   theme(axis.text.y = element_text(),\n         axis.title.y = element_text(angle = 90),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n         axis.title.x = element_text())\n bin4leg\n\n\n\n\n\n\n\n\npseudo-continuous. put the x-axis on top, because that’s what we’d expect for a legend, really. Labels can take a lambda function of the breaks, allowing us to use auto-chosen breaks. But probably better to reference the values they correspond to. It’s just that for this silly demo they are 0-1. Let’s pretend for the minute that they’re logged just for fun and to demo how to do it.\n\nmatcfade &lt;- col2dmat('ag_Sunset', n1 = 100, n2 = 100)\n conleg &lt;- plot2dcols(matcfade) +\n   theme_void() +\n   scale_y_continuous(name = 'Certainty %') +\n   #\n   scale_x_continuous(labels = ~round(log(.), 2), name = 'Value', position = 'top') +\n   theme(axis.text.y = element_text(),\n         axis.title.y = element_text(angle = 90),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n         axis.title.x = element_text())\n conleg\n\n\n\n\n\n\n\n\nNow, attach those to the map as legends.\nI’ll use patchwork for most of them, but ggpubr::ggarrange would work too, just with different tweaking. The way patchwork does insets and sizes is working better for me right now, so that’s what I’ll use.\nTaking the grey background off because it’s distracting with inset legends.\nTwo-level binned legend with high-low\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element(bin2legqual, left = 0.1, bottom = 0.1, right = 0.5, top = 0.2)\n\n\n\n\n\n\n\n\nSame, but quantitative legend labels. Text is a bit absurd.\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((bin2legquant + theme(axis.text = element_text(size = 8),\n                                       title = element_text(size = 8))),\n                 left = 0.1, bottom = 0, right = 0.5, top = 0.25)\n\n\n\n\n\n\n\n\nA 4-fade example with quantitative fades as well. That’s not our immediate need, but good to be able to do. maybe fade according to standard error or something.\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((bin4leg + theme(axis.text = element_text(size = 8),\n                                       title = element_text(size = 8))),\n                 left = 0.1, bottom = 0, right = 0.5, top = 0.25)\n\n\n\n\n\n\n\n\nContinuous values in both dimensions. Here, we use a map where colors and fades are both defined continuously.\n\n(justmapcon + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((conleg + coord_fixed() +\n                    theme(axis.text = element_text(size = 8),\n                          title = element_text(size = 8))),\n                 left = 0.1, bottom = 0.05, right = 0.5, top = 0.25)\n\n\n\n\n\n\n\n\nCan I put the legend off to the side just by specifying bigger coords? sort of- it goes but gets lost\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((conleg + coord_fixed() +\n                    theme(axis.text = element_text(size = 8),\n                          title = element_text(size = 8))),\n                 left = 1, bottom = 0.4, right = 1.5, top = 0.75)\n\n\n\n\n\n\n\n\nWorks with making a small plot with spacers and then glueing that onto the big plot\n\nguidespot &lt;- plot_spacer() /\n   (conleg + coord_fixed() +\n   theme(axis.text = element_text(size = 8),\n         title = element_text(size = 8))) /\n   plot_spacer()\n\n (justmap + theme_bw() + theme(legend.position = 'none')) +\n   guidespot +\n   plot_layout(widths = c(9, 1))\n\n\n\n\n\n\n\n\nDoes that work with the simpler ones? Yeah, although the 2-fades makes more sense horizontal, so do that\n\n# I can't fiugre out why this creates a dataframe. results = 'hide' doesn't hide it, wrapping with invisible(), etc. I give up. Giving it its own code block\nguidespot2 &lt;- plot_spacer() |\n   (bin2legquant + theme(axis.text = element_text(size = 8),\n                         title = element_text(size = 8))) |\n   plot_spacer()\n\n\n (justmap + theme_bw() + theme(legend.position = 'none')) /\n   guidespot2 +\n   plot_layout(heights = c(9, 1))\n\n\n\n\n\n\n\n\nA very similar approach would work for ggpubr::ggarrange\nThere’s quite a lot more that could be done here, but this gets me what I need for now.",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Faded 2d colour ramps"
    ]
  },
  {
    "objectID": "plotting/faded_colors.html#notes",
    "href": "plotting/faded_colors.html#notes",
    "title": "Faded colors",
    "section": "Notes",
    "text": "Notes\nif this were truly bivariate (ie two variables of interest), could rotate 45 degrees to equally weight (and likely use different color ramps). But it’s not- it’s certainty along one axis, so leaving horiz and having a lightness axis fits what we’re doing here better.",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Faded 2d colour ramps"
    ]
  },
  {
    "objectID": "plotting/ggplot_themes.html",
    "href": "plotting/ggplot_themes.html",
    "title": "Custom ggplot themes",
    "section": "",
    "text": "I often want to consistently theme my ggplots across projects. I’ve developed some custom themes, but they’re usually ad-hoc, and don’t work particularly well in packages, because the simple way to do it isn’t a function.\nlibrary(ggplot2)\nFor example, we might have a theme that’s good for publication, as in Saving and theming plots, where we specify size, backgrounds, and text. We load new fonts first.\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galenR\n\nshowtext::showtext_auto()\npubfont &lt;- 'Cambria'\nloadfonts(fontvec = pubfont)\npubtheme &lt;- theme_bw(base_size = 10) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=pubfont))\nHere, the theme is essentially hardcoded, and we can add it to a figure.\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) + \n  geom_point() + \n  facet_wrap(~Species) + \n  pubtheme",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "ggplot themes"
    ]
  },
  {
    "objectID": "plotting/ggplot_themes.html#can-we-pass-other-arguments",
    "href": "plotting/ggplot_themes.html#can-we-pass-other-arguments",
    "title": "Custom ggplot themes",
    "section": "Can we pass other arguments?",
    "text": "Can we pass other arguments?\nIf the goal is to enforce a style, we might not want to allow passing other arguments to theme, but can we with …?\n\ntheme_pub_dots &lt;- function(base_size = 10, font, ...) {\n  if (!(font %in% sysfonts::font_families())) {\n      loadfonts(font)\n  }\n\n  ggplot2::theme_bw(base_size = base_size) +\n    theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=font), \n        ...)\n}\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) + \n  geom_point() + \n  facet_wrap(~Species) +\n  theme_pub_dots(base_size = 8, font = 'Arial', legend.position = 'none')",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "ggplot themes"
    ]
  },
  {
    "objectID": "plotting/legend_in_facet.html",
    "href": "plotting/legend_in_facet.html",
    "title": "Legend in missing facet",
    "section": "",
    "text": "I often want to use the existence of a ‘missing’ facet to put in a legend, and always have to re-figure out how, so I’m writing it down here.\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\nMake a plot with a gap.\n\nirisplot &lt;- iris |&gt; \n  ggplot(aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  facet_wrap('Species', nrow = 2)\n\nirisplot\n\n\n\n\n\n\n\n\nWe want the legend in that lower right hole. The easiest is to just use theme(legend.position = c(x, y)) where x and y are in plot units.\n\nirisplot + theme(legend.position = c(0.6, 0.2))\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nOther options that can give more flexibility, particularly across plots, are to get the legend grob and treat it as another plot. That can’t be plotted directly, but can with other plots. Because we can treat this as a plot, we can do some complex things with ggpubr and patchwork, including overlaying it into that spot or using it across figures or subsets of figures. Though here we just do something simple to demo.\nggpubr can use the grob directly\n\nirislegend &lt;- ggpubr::get_legend(irisplot)\n\n\nggpubr::ggarrange(irisplot + theme(legend.position = 'none'), irislegend)\n\n\n\n\n\n\n\n\npatchwork needs to make it a ggplot\n\n(irisplot + theme(legend.position = 'none')) + ggpubr::as_ggplot(irislegend)\n\n\n\n\n\n\n\n\nWith patchwork we can use inset_element if we want to overlay and not treat as external plot\n\n(irisplot + theme(legend.position = 'none')) + \n  inset_element(ggpubr::as_ggplot(irislegend), left = 0.6, bottom = 0.2, right = 0.8, top = 0.4)",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Legends in empty facets"
    ]
  },
  {
    "objectID": "plotting/rayshader.html",
    "href": "plotting/rayshader.html",
    "title": "Rayshading",
    "section": "",
    "text": "I want to figure out how to use rayshader. It has the potential to be really good for heatmaps, maps, etc.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rayshader)\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(stars)\n\nLoading required package: abind\nMostly I just want to play around with both making heatmaps and maps.",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "3d plotting with rayshader"
    ]
  },
  {
    "objectID": "plotting/rayshader.html#plots",
    "href": "plotting/rayshader.html#plots",
    "title": "Rayshading",
    "section": "Plots",
    "text": "Plots\nLet’s first see about using it to make 3d plots. The 2-d autocorr should be a good example. From that notebook, let’s generate some data.\n\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galenR\n\n\n\nacmatrix_7_9 &lt;- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is -0.012\"\n[1] \"Var of all points is 1.006\"\n[1] \"Mean y AC is 0.892\"\n[1] \"Mean x AC is 0.698\"\n\nactib_7_9 &lt;- tibble::as_tibble(acmatrix_7_9)  |&gt; \n  mutate(y = row_number())  |&gt; \n  pivot_longer(cols = starts_with('V'))  |&gt; \n  mutate(x = as.numeric(str_remove(name, 'V')))  |&gt; \n  select(-name)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\nand the flat ggplot\n\ngg2d &lt;- ggplot(filter(actib_7_9, x &gt; 100 & x &lt;= 200 & y &gt; 300 & y &lt; 400), \n               aes(x = x, y = y, fill = value)) + \n  geom_tile() +\n  viridis::scale_fill_viridis(option = 'viridis')\n\ngg2d\n\n\n\n\n\n\n\n\n\nRayshade version\nThere are a lot of arguments we can play with, but a simple default set works. Just have to remember to render_snapshot to see it.\n\nplot_gg(gg2d)\nrender_snapshot(clear = TRUE)\n\n\n\n\n\n\n\n\nA few tweaks, but basically, that’s what we want- the tweaks needed for any particular plot will be plot-specific.\nThe flat_plot_render option is interesing, as it builds the typical heatmap as comparison.\n\nplot_gg(gg2d, width = 10, height = 10, flat_plot_render = TRUE,\n        # These are arguments passed to plot_3d, just playing around\n        solid = FALSE,\n        theta = 60,\n        phi = 30)\nrender_snapshot()\n\n\n\n\n\n\n\n\nThere’s also a render_highquality too, Though that seems to need some tweaks to look right, both in the notebook or the Rstudio viewer.\n\nplot_gg(gg2d)\nrender_highquality(clear = TRUE)",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "3d plotting with rayshader"
    ]
  },
  {
    "objectID": "plotting/rayshader.html#maps",
    "href": "plotting/rayshader.html#maps",
    "title": "Rayshading",
    "section": "Maps",
    "text": "Maps\nI found some huge DEMs, but figure I should start smaller, so I chose a small area of the 10m Vic DEM.\nI’d typically use stars, but the examples all use raster. Not sure it’ll work with stars.\n\nvicdem &lt;- read_stars(file.path('data', 'DATA_362071', 'VIC Government', 'DEM', '10 Metre', 'vmelev_dem10m.tif'))\n\nPlot that with plot.stars\n\nplot(vicdem)\n\ndownsample set to 3\n\n\n\n\n\n\n\n\n\nNow, can I make a map? The examples use raster, but I think we need a matrix. Each sheet in the stars is a matrix, so just get it directly.\n\nvicdem[[1]] %&gt;%\n  sphere_shade(texture = \"imhof2\") %&gt;%\n  plot_map()\n\n\n\n\n\n\n\n\nMake it 3d- this is interesting. It takes the DEM as the second argument too, because the arguments are hillshade, which is generated by sphere_shade and heightmap, which is just the DEM.\n\nvicdem[[1]] %&gt;%\n  sphere_shade(texture = \"imhof3\") %&gt;%\n  plot_3d(vicdem[[1]], zscale = 10)\n\nrender_snapshot()\n\n\n\n\n\n\n\n\nCan we use geom_stars to generate a {rayshader} fig from plot_gg?\n\ndemgg &lt;- ggplot() +\n  geom_stars(data = vicdem)\ndemgg\n\n\n\n\n\n\n\n\nTry the plot_gg. It does have some hillshade- easier to confirm if we move it around in the window that pops up.\n\nplot_gg(demgg, width = 10, height = 10)\nrender_snapshot()\n\n\n\n\n\n\n\n\nNow I want to overlay stream lines and municipalities on there. Will need to get those shapefiles.\nThe BOM geofabric is in the ANAE, and I have that already. So\n\nstreams &lt;- sf::read_sf(file.path('data', 'ANAE_Rivers_v3_23mar2021', 'ANAE_Rivers_v3_23mar2021', 'Waterways_ANAE_Geofabric3.shp'))\n\nThat makes a nice plot just on its own\n\nstreams |&gt; \n  dplyr::mutate(slope = ifelse(slope &lt; 0, 0, slope)) |&gt; \n  ggplot() +\n  geom_sf(mapping = aes(color = log(slope + 0.01))) +\n  scale_color_viridis_c()\n\n\n\n\n\n\n\n\nGet towns and roads- Marysville is about the only town in the chosen area of the victorian DEM?\n\ntowns &lt;- sf::read_sf(dsn = file.path('data', 'MDB_ANAE_Aug2017/MDB_ANAE.gdb'), layer = 'MajorTowns')\nroads &lt;- sf::read_sf(dsn = file.path('data', 'MDB_ANAE_Aug2017/MDB_ANAE.gdb'), layer = 'MajorRoads')\n\nClip to the vicdem. This is a bit funny, because the streams_mville dataset from the ANAE is clipped to the Murray-Darling Basin, but the DEM is only partially in the basin, and so we lose some of it.\n\nvicbb &lt;- st_bbox(vicdem) |&gt; st_as_sfc()\n\nstreams_mville &lt;- streams |&gt; \n  st_transform(st_crs(vicdem)) |&gt; \n  st_intersection(vicbb)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\ntowns &lt;- towns |&gt; \n  st_transform(st_crs(vicdem)) |&gt; \n  st_intersection(vicbb)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nroads &lt;- roads |&gt; \n  st_transform(st_crs(vicdem)) |&gt; \n  st_intersection(vicbb)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nQuick gg\n\ndemgg_streams_mville &lt;- ggplot() +\n  geom_stars(data = vicdem) +\n  geom_sf(data = streams_mville, color = 'dodgerblue', linewidth = 2) +\n  geom_sf(data = roads, color = 'black', linewidth = 2) +\n  #geom_sf(data = towns, color = 'firebrick') +\n  colorspace::scale_fill_continuous_sequential(palette = 'Terrain 2')\n#demgg_streams_mville\n\nUse plot_gg\n\nplot_gg(demgg_streams_mville, width = 10, height = 10)\nrender_snapshot()\n\n\n\n\n\n\n\n\nThe streams don’t really show up well. Do they with a flat map? is it just that they’re too narrow to render well in 3d?\n\ndemgg_streams_mville\n\n\n\n\n\n\n\n\nCan I make that with plot3d? I can’t get the extent argument to work without passing heightmap as well. Not sure why- they seem to both define the cropping extent.\nCan I get it to work at all? Using almost exactly the example code, just modified for this dataset. Those are really wonky, and it’s not because of an issue with height- the roads were just given a length and width. I have to make the streams have huge linewidth to see them.\n\nvicdem[[1]] |&gt; \n  height_shade() |&gt; \n  add_overlay(generate_line_overlay(streams_mville, color = 'dodgerblue',\n                                    extent = st_bbox(vicdem),\n                                    heightmap = vicdem[[1]],\n                                    linewidth = 10)) |&gt; \n  add_overlay(generate_line_overlay(roads, color = 'black',\n                                     extent = st_bbox(vicdem),\n                                    width = 1080, height = 1080)) |&gt; \n  plot_map()\n\n\n\n\n\n\n\n\nSo does this not work because the streams_mville just get lost? e.g. are they there, they just don’t show up?\n\nvicdem[[1]] %&gt;%\n  sphere_shade(texture = \"imhof3\") |&gt; \n  # add_overlay(generate_line_overlay(roads, color = 'black',\n  #                                   extent = attr(vicdem[[1]], 'extent'))) |&gt; \n  # add_overlay(generate_point_overlay(towns, color = 'firebrick',\n  #                                   extent = attr(vicdem[[1]], 'extent'))) |&gt; \n    add_overlay(generate_line_overlay(streams_mville, color = 'dodgerblue',\n                                    extent = st_bbox(vicdem),\n                                    heightmap = vicdem[[1]],\n                                    linewidth = 10)) |&gt; \n  plot_3d(vicdem[[1]], zscale = 10)\n\n\nrender_snapshot()\n\n\n\n\n\n\n\n\nThe streams layer itself isn’t so bad when looked at alone. So somehow it’s sort of disintegrating when used as an overlay, whether with the gg method or not.\n\nggplot(streams_mville) + geom_sf()",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "3d plotting with rayshader"
    ]
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html",
    "href": "plotting/setting_colors_for_groups.html",
    "title": "Setting colors for groups",
    "section": "",
    "text": "I often want the same colors to map to the same levels across many plots within a project, even if those levels aren’t included in a plot. E.g. if we plot the iris dataset with and without ‘setosa’, the colors change.\n\nlibrary(ggplot2)\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_brewer(palette = 'Dark2')\n\n\n\n\n\n\n\n\nvs\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_brewer(palette = 'Dark2')\n\n\n\n\n\n\n\n\nNow, virginica and versicolor have changed colors, and that’s confusing.",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Consistent colors"
    ]
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#assign-colors",
    "href": "plotting/setting_colors_for_groups.html#assign-colors",
    "title": "Setting colors for groups",
    "section": "Assign colors",
    "text": "Assign colors\nNow use setNames to match\n\nsppal &lt;- setNames(iriscols, unique(iris$Species))",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Consistent colors"
    ]
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#use-manual-scale-and-check",
    "href": "plotting/setting_colors_for_groups.html#use-manual-scale-and-check",
    "title": "Setting colors for groups",
    "section": "Use manual scale and check",
    "text": "Use manual scale and check\nand re-do the above two plots to demonstrate that dropping levels keeps colors the same\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_manual(values = sppal)\n\n\n\n\n\n\n\n\nand drop setosa\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_manual(values = sppal)\n\n\n\n\n\n\n\n\nNow they match.",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Consistent colors"
    ]
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#creating-a-scale_-function",
    "href": "plotting/setting_colors_for_groups.html#creating-a-scale_-function",
    "title": "Setting colors for groups",
    "section": "Creating a scale_ function",
    "text": "Creating a scale_ function\nWe should also probably provide a function to create the standard color-level matching. And maybe that’s the solution to the above. If a palette isn’t passed in, create one in-function.\nFirst, can I make a silly wrapper that just takes the values? It’s no different than scale_color_manual at this point\n\nscale_color_custom &lt;- function(pal) {\n  scale_color_manual(values = pal)\n}\n\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = sppal)\n\n\n\n\n\n\n\n\nNow, can we catch times when that’s not a named vector and if not pass something else? Do we want to? It starts getting infinite what we want to handle (e.g. palette names? from which packages?)\nI suppose as long as it’s a vector, use it but warn. I think otherwise just fail- it’s too hard to catch everything. Something like\n\nscale_color_custom &lt;- function(pal) {\n  if (!is.character(pal)) {\n    stop(\"pal needs to be a character vector, ideally named\")\n  }\n  if (is.null(names(pal))) {\n    warning(\"unnamed vector, colors may not be consistent between plots.\")\n  }\n  \n  scale_color_manual(values = pal)\n}",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Consistent colors"
    ]
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#palette-generation",
    "href": "plotting/setting_colors_for_groups.html#palette-generation",
    "title": "Setting colors for groups",
    "section": "Palette generation",
    "text": "Palette generation\nHow about a palette generating function?\nThis will be easiest if I enforce a package, though I’m sure it can be made more flexible. For the sake of this quick demo, let’s just use {paletteer}, since it has access to lots of options. And we at least are limited to discrete scales, since we’re level-matching.\nOne thing I often want to do is set specific colors to specific levels (e.g. a reference). So make that possible.\n\nmake_pal &lt;- function(levels, palette, refvals = NULL, refcols = NULL) {\n  if (is.factor(levels)) {levels &lt;- as.character(levels)}\n  nonrefs &lt;- levels[!(levels %in% refvals)]\n  cols &lt;- paletteer::paletteer_d(palette, length(nonrefs))\n  \n  namedcols &lt;- setNames(c(refcols, cols), c(refvals, nonrefs))\n}",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Consistent colors"
    ]
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#workflow",
    "href": "plotting/setting_colors_for_groups.html#workflow",
    "title": "Setting colors for groups",
    "section": "Workflow",
    "text": "Workflow\nFirst, we set colors with make_pal\n\nircols &lt;- make_pal(unique(iris$Species), palette = 'calecopal::kelp2')\n\nThen we plot with scale_color_custom\nFirst with all three species\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircols)\n\n\n\n\n\n\n\n\nAnd removing setosa\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircols)\n\n\n\n\n\n\n\n\n\nReference level\nLet’s make setosa a reference level that stands out like purple.\n\nircolsS &lt;- make_pal(unique(iris$Species), palette = 'calecopal::kelp2', refvals = 'setosa', refcols = 'purple')\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS)\n\n\n\n\n\n\n\n\nNote that now the colors of the others have shifted, because the reference-levelling happened at the start. If a reference is always a reference, that might be fine. But it might be the case that we only sometimes want to call attention to a particular level, and so we want to assign colors including it, so we can switch between an accentuated and unaccentuated palette. Let’s put another argument in make_pal (which requires some reframing of how we assign things). We also might want to return both the unreferenced and the referenced palettes at the same time, rather than calling the function twice.\n\nmake_pal &lt;- function(levels, palette, refvals = NULL, refcols = NULL, includeRef = FALSE, returnUnref = FALSE) {\n  \n  if (returnUnref) {\n    if (!includeRef) {\n      stop(\"does not make sense to return a reffed and unreffed palette that don't match\")\n    }\n    }\n  \n  if (is.factor(levels)) {levels &lt;- as.character(levels)}\n  \n  if (!includeRef) {levels &lt;- levels[!(levels %in% refvals)]}\n  \n  # nonrefs &lt;- levels[!(levels %in% refvals)]\n  cols &lt;- paletteer::paletteer_d(palette, length(levels))\n  \n  if (returnUnref & includeRef) {unref &lt;- setNames(cols, levels)}\n  \n  # delete the reference levels out of the vectors\n  whichlevs &lt;- which(!(levels %in% refvals))\n  nonrefs &lt;- levels[whichlevs]\n  nonrefcols &lt;- cols[whichlevs]\n  \n  namedcols &lt;- setNames(c(refcols, nonrefcols), c(refvals, nonrefs))\n  \n  if (returnUnref & includeRef) {\n    return(list(refcols = namedcols, unrefcols = unref))\n    } else {\n      return(namedcols)\n  }\n\n}\n\nNow test that- setting includeref = TRUE should keep these the same as earlier plots except for setosa (i.e. the light blue that would go to setosa just drops out\n\nircolsS_include &lt;- make_pal(unique(iris$Species), palette = 'calecopal::kelp2', refvals = 'setosa', refcols = 'purple', includeRef = TRUE)\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS_include)\n\n\n\n\n\n\n\n\nAnd if I want to get both palettes (reffed and unreffed) so I can accentuate sometimes,\n\nircolsS_both &lt;- make_pal(unique(iris$Species), palette = 'calecopal::kelp2', refvals = 'setosa', refcols = 'purple', includeRef = TRUE, returnUnref = TRUE)\n\nWith setosa as a reference- should match above\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS_both$refcols)\n\n\n\n\n\n\n\n\nWithout setosa as a reference, others retain same colors.\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS_both$unrefcols)",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Consistent colors"
    ]
  },
  {
    "objectID": "plotting/venn_diagrams.html",
    "href": "plotting/venn_diagrams.html",
    "title": "Venn diagrams",
    "section": "",
    "text": "I need to create something like a Venn diagram, but scaled. None of the venn packages seem to do it. Can I get it to work with geom_circle and some math?\nlibrary(ggforce)\nlibrary(tidyverse)\ndevtools::load_all()\n\nℹ Loading galenR\nFirst, let’s say we have two sets of different size, with some overlap\nset1 &lt;- 1:100\nset2 &lt;- 75:125\nIf we want to just represent each set as a circle with area proportional to the number of items and not worrying about overlap, we can\ncirtib &lt;- tibble(set_num = c(1,2), set_size = c(length(set1), length(set2))) |&gt; \n  mutate(set_r = sqrt(set_size/pi))\nggplot(cirtib) + \n  geom_circle(mapping = aes(x0 = 0, y0 = 0, \n                            r = set_r, \n                            color = factor(set_num))) +\n  coord_fixed()\nNow, though, I want an area of overlap of 25. So really, I need to find an offset for (let’s say) x0 for set2 that shifts that circle half in and half out.\nWolfram has a complicated formula (14) to get A (the area of the ‘lens’- the overlap) given the distance d between centers and the radii. Here, we have the area and radii, and want to solve for d. So can we do that?\nAccording to Sage\nThat’s not particularly helpful; d is not isolated.\nI should be able to just do this numerically with optim or optimize, I think.\n# calc_a &lt;- function(d, radius1, radius2) {\n#   A &lt;- radius1^2*acos((d^2 + radius1^2 - radius2^2) / (2*d*radius1)) + \n#     radius2^2*acos((d^2 - radius1^2 + radius2^2) / (2*d*radius1)) -\n#     0.5*sqrt((d+radius1-radius2) * (d-radius1+radius2) * (-d+radius1+radius1) * (d+radius1+radius2))\n#   \n#   return(A)\n# }\n# \n# opt_d &lt;- function(d, area, radius1, radius2) {\n#     \n#   dcheck &lt;- calc_a(d, radius1, radius2)\n#   \n#   return(dcheck - area)\n# }\n# get the optimal shift\n  #opt_d and calc_a are defined in venn_distances.R\n# find_d &lt;- function(area, radii, radius1, radius2) {\n#   if (!missing(radii) & (!missing(radius1) | !missing(radius2))) {\n#     rlang::abort('either use radii or radius1, radius2')\n#   }\n#   \n#   if (missing(radii)) {\n#     radii &lt;- c(radius1, radius2)\n#   }\n#   bestd &lt;- optimize(opt_d, lower = abs(diff(radii)), upper =  sum(radii),\n#                     area = area, radii = radii)\n# }\n# need to get this into the mutate somehow\nd &lt;- find_d(area = length(intersect(set1, set2)), radii = cirtib$set_r)\ncirtib$set_d = c(0, d)\nggplot(cirtib) + \n  geom_circle(mapping = aes(x0 = set_d, y0 = 0, \n                            r = set_r, \n                            color = factor(set_num))) +\n  coord_fixed()\nTo get that to work with a mutate, we need to feed it both radii. we could do that long or wide, but given that we usually have long data:\nMake a column identifying the set, another with the set intersection, and use that to make the d in a grouped mutate.\ncirtib &lt;- cirtib |&gt; \n  mutate(setpair = 1,\n         overlap = length(intersect(set1, set2))) |&gt; \n  mutate(setd_tidy = find_d(unique(overlap), set_r), .by = setpair) |&gt; \n  # we don't want to shift BOTH circles. If we want them centered, we can shift +- half\n  mutate(centers = setd_tidy*c(-0.5, 0.5))\nggplot(cirtib) + \n  geom_circle(mapping = aes(x0 = centers, y0 = 0, \n                            r = set_r, \n                            fill = factor(set_num)),\n              alpha = 0.4) +\n  coord_fixed()\nThat really is fairly contrived to have the separate circles long. But when it comes time to plot, it’s WAY nicer.\nLet’s say we have a dataset where we have already calculated the sizes and overlaps of a bunch of sets. Just make it long, since that’s how the data we want to use will be, and it makes the plotting easier.\nmanysets &lt;- tibble(set_pair = rep(1:11, each = 2),\n                   set_num = rep(1:2, 11),\n                   area= runif(22)*100) |&gt; \n  mutate(overlapfrac = rep(seq(0, 1, 0.1), each = 2)) |&gt; \n  mutate(overlap = overlapfrac*min(area), .by = set_pair)\nNow, to make the circles, we need to calculate radii and distance between centers. This is now equally as annoyign to be wide\nmanysets &lt;- manysets |&gt; \n  mutate(radius = sqrt(area/pi)) |&gt; \n  mutate(d = find_d(unique(overlap), radii = radius), .by = set_pair)\n\n# makes testing easier\nmanysets &lt;- manysets |&gt; \n  # and a check\n  mutate(area_calc = calc_a(unique(d), radii = radius), .by = set_pair) |&gt; \n  mutate(area_error = abs(overlap - area_calc)) |&gt; \n  mutate(centers = d*c(-0.5, 0.5))\nNow to plot\nggplot(manysets) + \n  geom_circle(mapping = aes(x0 = centers, y0 = (set_pair-1)*10, \n                            r = radius, \n                            fill = factor(set_num)),\n              alpha = 0.4) +\n  coord_fixed()\nAnd the area_error is always very low\nmanysets\n\n# A tibble: 22 × 10\n   set_pair set_num  area overlapfrac overlap radius     d  area_calc area_error\n      &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1        1       1 73.7          0      0      4.84  7.42 0.00000176 0.00000176\n 2        1       2 20.8          0      0      2.58  7.42 0.00000176 0.00000176\n 3        2       1 15.8          0.1    1.58   2.24  6.56 1.58       0.0000554 \n 4        2       2 81.7          0.1    1.58   5.10  6.56 1.58       0.0000554 \n 5        3       1  8.83         0.2    1.77   1.68  6.36 1.77       0.0000764 \n 6        3       2 98.5          0.2    1.77   5.60  6.36 1.77       0.0000764 \n 7        4       1 17.3          0.3    5.20   2.35  5.21 5.20       0.0000348 \n 8        4       2 67.5          0.3    5.20   4.63  5.21 5.20       0.0000348 \n 9        5       1 24.9          0.4    9.96   2.81  5.55 9.96       0.0000984 \n10        5       2 89.7          0.4    9.96   5.34  5.55 9.96       0.0000984 \n# ℹ 12 more rows\n# ℹ 1 more variable: centers &lt;dbl&gt;\nThe way this work is probably easier to see not with random sizes, but consistent\nevensets &lt;- tibble(set_pair = rep(1:11, each = 2),\n                   set_num = rep(1:2, 11),\n                   area = rep(c(100, 50), 11)) |&gt; \n  mutate(overlapfrac = rep(seq(0, 1, 0.1), each = 2)) |&gt; \n  mutate(overlap = overlapfrac*min(area), .by = set_pair)\nNow get r, d, and centers\nevensets &lt;- evensets |&gt; \n  mutate(radius = sqrt(area/pi)) |&gt; \n  mutate(d = find_d(unique(overlap), radii = radius), .by = set_pair) |&gt; \n  mutate(centers = d*c(-0.5, 0.5))\n\n# makes testing easier\nevensets &lt;- evensets |&gt; \n  # and a check\n  mutate(area_calc = calc_a(unique(d), radii = radius), .by = set_pair) |&gt; \n  mutate(area_error = abs(overlap - area_calc))\nNow to plot\nggplot(evensets) + \n  geom_circle(mapping = aes(x0 = centers, y0 = (set_pair-1)*12, \n                            r = radius, \n                            fill = factor(set_num)),\n              alpha = 0.4) +\n  coord_fixed()\nOne thing I’m going to want to do when I use this is scale them (e.g. I’ll have massive numbers, but need to scale them down). I think the easiest way to do that is to just operate on scaled areas and overlaps.\nSo, recapitulating the above but with larger numers and then scaled\nscaledsets &lt;- tibble(set_pair = rep(1:11, each = 2),\n                   set_num = rep(1:2, 11),\n                   area = rep(c(10000, 5000), 11)) |&gt; \n  mutate(overlapfrac = rep(seq(0, 1, 0.1), each = 2)) |&gt; \n  mutate(overlap = overlapfrac*min(area), .by = set_pair) |&gt; \n  mutate(area_scaled = area/100, overlap_scaled = overlap/100)\nNow get r, d, and centers\nscaledsets &lt;- scaledsets |&gt; \n  mutate(radius = sqrt(area/pi),\n         radius_scaled = sqrt(area_scaled/pi)) |&gt; \n  mutate(d = find_d(unique(overlap), radii = radius), \n         d_scaled = find_d(unique(overlap_scaled), radii = radius_scaled),\n         .by = set_pair) |&gt; \n  mutate(centers = d*c(-0.5, 0.5),\n         centers_scaled = d_scaled*c(-0.5, 0.5))\nNow to plot First the raw\nggplot(scaledsets) + \n  geom_circle(mapping = aes(x0 = centers, y0 = (set_pair-1)*12, \n                            r = radius, \n                            fill = factor(set_num)),\n              alpha = 0.4) +\n  coord_fixed()\nThen the scaled, looks like before.\nggplot(scaledsets) + \n  geom_circle(mapping = aes(x0 = centers_scaled, y0 = (set_pair-1)*12, \n                            r = radius_scaled, \n                            fill = factor(set_num)),\n              alpha = 0.4) +\n  coord_fixed()",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Venn diagrams in ggplot2"
    ]
  },
  {
    "objectID": "plotting/venn_diagrams.html#extensions",
    "href": "plotting/venn_diagrams.html#extensions",
    "title": "Venn diagrams",
    "section": "Extensions",
    "text": "Extensions\nNow, if the goal is just to plot, can I (should I) write a little function that just returns radii and centers? It would require a very specific data format, so not sure it’s worth it. Obviously it could be generalised to long, wide, and as here, partially-long (areas in col, but overlap in another). I think I might see how useful it would be when I actually try to do this for my data and then decide\nWe could also use d to go in any direction, with any offset. E.g. from a base 0,0, we could shift only one circle right or left or up or down or given an angle and pythagoras off at an arbitrary angle.",
    "crumbs": [
      "Code Demos",
      "Plotting",
      "Venn diagrams in ggplot2"
    ]
  },
  {
    "objectID": "publishing/github_citation_files.html",
    "href": "publishing/github_citation_files.html",
    "title": "Github citations",
    "section": "",
    "text": "Use cffr to create repo citations.\nThough it needs a package (or at least a DESCRIPTION). If you don’t have that, what we really need is something like this in CITATION.cff\nmessage: 'To cite this repo use:'\ntype: software\nlicense: MIT\ntitle: 'Repo title'\nversion: 0.1.0\nabstract: What the repo's for\nauthors:\n- family-names: Holt\n  given-names: Galen\n  email: g.holt@deakin.edu.au\n  orcid: https://orcid.org/0000-0002-7455-9275\n- family-names: Last\n  given-names: First\n  email: first.last@email.com\n  orcid: https://orcid.org/0000-0002-1234-5678\ncontact:\n- family-names: Holt\n  given-names: Galen\n  email: g.holt@deakin.edu.au\n  orcid: https://orcid.org/0000-0002-7455-9275",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Adding citation files to github"
    ]
  },
  {
    "objectID": "publishing/private_github_figshare.html",
    "href": "publishing/private_github_figshare.html",
    "title": "Private github and figshare",
    "section": "",
    "text": "While open-access from the get-go is nice, we often are in a situation where we are working in private git repos until publication or other forms of release. The catch is that when the code is needed for journal submission, we need to provide it to the reviewers without a full public release until acceptance. How can we do this?\nMy university uses Figshare as the backend to our mandated data repo, so this will address using figshare, though Dryad etc are hopefully similar.\nFigshare has the ability to generate a private embargoed dataset with pre-allocated DOI and private link, which can be made public on acceptance, as outlined in their docs. So one option would just be to do that with a raw dump of the source code. That’s less than ideal- once publication happens, we want to point people to github, not a single snapshot of copied code.\nThe solution is to link figshare with github. The docs suggest it will only link with public repos, but it can see all of them. One potential catch is that my institutional figshare simply does not have the button to connect to github (or gitlab, or bitbucket …). I have no idea why, but the only solution I’ve found is to just create a personal figshare account, where the login works fine.\nThen, select the repo, and create a new figshare item. The docs linked above for making something private until publication would work, but at the initial submission stage, we may not want to actually reserve DOIs for this particular version- it’s likely there will be changes in revision and those are what we will want to actually have linked to the paper with a DOI. It doesn’t look like we can update the data, so I’m assuming that what will end up happening is we will create a new Figshare item with whatever the code is at acceptance (and make a Github ‘release’ for that).",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Creating Figshare from private github"
    ]
  },
  {
    "objectID": "publishing/tables_in_word.html",
    "href": "publishing/tables_in_word.html",
    "title": "Less ugly tables",
    "section": "",
    "text": "Tables tend to look fine in html, but when rendering to word they completely fall down. Below, I test a bunch of the standard packages. *The one that seems to work best is [flextable::flecxtable()], which is interesting, since [huxtable::huxtable()] claims to build on it but doesn’t work well at all.\nNOTE: this doc probably looks fine on the website. The issue is word rendering.\nI’ll test two tables- iris, which has narrow columns, and a table with lots of text, which tends to cause the most problems.\ntexttab &lt;- readr::read_csv(file.path('data/component_table.csv'), \n                           show_col_types = FALSE) |&gt; \n  dplyr::select(-tidyselect::last_col())",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Testing word table formatting"
    ]
  },
  {
    "objectID": "publishing/tables_in_word.html#the-solution",
    "href": "publishing/tables_in_word.html#the-solution",
    "title": "Less ugly tables",
    "section": "The solution",
    "text": "The solution\nMaybe. This exact code works great in another project, and not at all here. I have no idea what’s going on.\n\nlibrary(flextable)\n\nWarning: package 'flextable' was built under R version 4.4.2\n\nset_flextable_defaults(font.family = 'Calibri',\n                       font.size = 10,\n                       table.layout = 'autofit')\n\nUse flextable with set_table_properties(layout = 'autofit') .\n\nhead(iris) |&gt; \n  flextable()\n\n\n\nTable 1: This is a table with flextable\n\n\n\nSepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies5.13.51.40.2setosa4.93.01.40.2setosa4.73.21.30.2setosa4.63.11.50.2setosa5.03.61.40.2setosa5.43.91.70.4setosa\n\n\n\n\n\n\ntexttab |&gt; \n  flextable() #|&gt; \n  # set_table_properties(layout = \"autofit\")\n\n\n\nTable 2: This is a table with lots of text in flextable\n\n\n\nGeneral HydroBOT componentsTypeGeneral component definitionsSpecific components used in our exampleInput dataNot part of HydroBOTHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. Modified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)ControllerWorkflowInterface between input data, response model, and other toolkit components. Sets up run(s).Sets up links to data and parameters for EWR tool and aggregations.Response modelsExternal, integratedA model of the response of values, e.g.  social, cultural, environmental, or economic values in response to hydrologic drivers.EWR toolAggregatorWorkflowAggregates response model results to scales across the dimensions of time, space, and theme. Response model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).ComparerWorkflowCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.Comparison of environmental values at various theme scales for the example climate and adaptation scenariosCausal networksExternal, integratedDescribe causal relationships between values.Long Term Water Plan (LTWP)Spatial dataExternal, integratedDescribe spatial relationshipsGauge locations, Sustainable Diversion Limits (SDL) units, Murray-Darling Basin\n\n\n\n\n\nNote that there is an issue still if the table prints into a div. I think because divs render in word as tables, and so flextable struggles with what it controls. I’ll show that below.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Testing word table formatting"
    ]
  },
  {
    "objectID": "publishing/tables_in_word.html#the-others",
    "href": "publishing/tables_in_word.html#the-others",
    "title": "Less ugly tables",
    "section": "The others",
    "text": "The others\nThese all look fine in html, most things do. But the word doc is where things seem to fall down. Especially if they have lots of text.\n\nhead(iris) |&gt; \n  knitr::kable()\n\n\n\nTable 3: This is a table with kable\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\n\n\n\n\nTable 4: This is a table with kableExtra. It does not render to word. It used to with prefer-html: true, but that workarounds seems not to work anymore.\n\n\n# head(iris) |&gt; \n#   knitr::kable() |&gt; \n#   kableExtra::kable_styling()\n\n\n\n\nhead(iris) |&gt; \n  huxtable::huxtable() |&gt; \n  huxtable::theme_article()\n\n\n\nTable 5: This is a table with huxtable\n\n\n\n\n\nSepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies\n\n5.13.51.40.2setosa\n\n4.93  1.40.2setosa\n\n4.73.21.30.2setosa\n\n4.63.11.50.2setosa\n\n5  3.61.40.2setosa\n\n5.43.91.70.4setosa\n\n\n\n\n\n\n\nhead(iris) |&gt; \n gt::gt()\n\n\n\nTable 6: This is a table with gt\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Testing word table formatting"
    ]
  },
  {
    "objectID": "publishing/tables_in_word.html#long-text",
    "href": "publishing/tables_in_word.html#long-text",
    "title": "Less ugly tables",
    "section": "Long text",
    "text": "Long text\nThis table is giving me fits elsewhere, let’s try it here. It has a common issue of wrapping words.\n\n\n\nTable 7: This is just printed\n\n\ntexttab\n\n# A tibble: 7 × 4\n  General HydroBOT compone…¹ Type  General component de…² Specific components …³\n  &lt;chr&gt;                      &lt;chr&gt; &lt;chr&gt;                  &lt;chr&gt;                 \n1 Input data                 Not … Hydrologic data (time… Modified historical h…\n2 Controller                 Work… Interface between inp… Sets up links to data…\n3 Response models            Exte… A model of the respon… EWR tool              \n4 Aggregator                 Work… Aggregates response m… Response model sets t…\n5 Comparer                   Work… Compares scenarios (t… Comparison of environ…\n6 Causal networks            Exte… Describe causal relat… Long Term Water Plan …\n7 Spatial data               Exte… Describe spatial rela… Gauge locations, Sust…\n# ℹ abbreviated names: ¹​`General HydroBOT components`,\n#   ²​`General component definitions`,\n#   ³​`Specific components used in our example`\n\n\n\n\n\ntexttab |&gt; \n  knitr::kable()\n\n\n\nTable 8: This is a table with kable\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral HydroBOT components\nGeneral component definitions\nSpecific components used in our example\nDetails of Specific components\n\n\n\n\nInput data\nHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. \nModified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)\nDaily flow rates for 45 for 15 scenarios\n\n\nController\nInterface between input data, response model, and other toolkit components. Sets up run(s).\nSets up links to data and parameters for EWR tool and aggregations.\nNA\n\n\nResponse models\nA model of the response of values, e.g. social, cultural, environmental, or economic values in response to hydrologic drivers.\nEWR tool\nThe EWR tool holds databases of the EWRs required to meet the environmental objectives of the basin, which protect or enhance environmental assets that are valued based on ecological significance.\n\n\nAggregator\nAggregates response model results to scales across the dimensions of time, space, and theme. \nResponse model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).\nThe spatial dimension consists of gauges nested within planning units within the basin. The time dimension assessess EWR performance averaged over the yearly data returned by the EWR tool. The value dimension consists of multiple EWRs (hydrologic indicators) that apply to environmental values (many-to-many), at multiple levels in the causal network (e.g. life-cycle components, species, groups of species, or long-term planning targets).\n\n\nComparer\nCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.\nComparison of environmental values at various theme scales for the example climate and adaptation scenarios\nExplored in detail in figures and tables\n\n\nCausal networks\nDescribe causal relationships between values.\nLong Term Water Plan (LTWP)\nProvided by HydroBOT. LTWP required of Basin States by the Murray-Darling Basin Plan and give effect to the Basin-wide Environmental Watering Strategy. \n\n\n\n\n\n\n\n\n\ntexttab |&gt; \n  huxtable::huxtable() |&gt; \n  huxtable::theme_article()\n\n\n\nTable 9: This is a table with huxtable\n\n\n\n\n\nGeneral HydroBOT componentsGeneral component definitionsSpecific components used in our exampleDetails of Specific components\n\nInput dataHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. Modified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)Daily flow rates for 45 for 15 scenarios\n\nControllerInterface between input data, response model, and other toolkit components. Sets up run(s).Sets up links to data and parameters for EWR tool and aggregations.\n\nResponse modelsA model of the response of values, e.g.  social, cultural, environmental, or economic values in response to hydrologic drivers.EWR toolThe EWR tool holds databases of the EWRs required to meet the environmental objectives of the basin, which protect or enhance environmental assets that are valued based on ecological significance.\n\nAggregatorAggregates response model results to scales across the dimensions of time, space, and theme. Response model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).The spatial dimension consists of gauges nested within planning units within the basin. The time dimension assessess EWR performance averaged over the yearly data returned by the EWR tool. The value dimension consists of multiple EWRs (hydrologic indicators) that apply to environmental values (many-to-many), at multiple levels in the causal network (e.g. life-cycle components, species, groups of species, or long-term planning targets).\n\nComparerCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.Comparison of environmental values at various theme scales for the example climate and adaptation scenariosExplored in detail in figures and tables\n\nCausal networksDescribe causal relationships between values.Long Term Water Plan (LTWP)Provided by HydroBOT. LTWP required of Basin States by the Murray-Darling Basin Plan and give effect to the Basin-wide Environmental Watering Strategy. \n\n\n\n\n\n\n\ntexttab |&gt; \n gt::gt()\n\n\n\nTable 10: This is a table with gt\n\n\n\n\n\n\n\n\n\nGeneral HydroBOT components\nGeneral component definitions\nSpecific components used in our example\nDetails of Specific components\n\n\n\n\nInput data\nHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. \nModified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)\nDaily flow rates for 45 for 15 scenarios\n\n\nController\nInterface between input data, response model, and other toolkit components. Sets up run(s).\nSets up links to data and parameters for EWR tool and aggregations.\nNA\n\n\nResponse models\nA model of the response of values, e.g. social, cultural, environmental, or economic values in response to hydrologic drivers.\nEWR tool\nThe EWR tool holds databases of the EWRs required to meet the environmental objectives of the basin, which protect or enhance environmental assets that are valued based on ecological significance.\n\n\nAggregator\nAggregates response model results to scales across the dimensions of time, space, and theme. \nResponse model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).\nThe spatial dimension consists of gauges nested within planning units within the basin. The time dimension assessess EWR performance averaged over the yearly data returned by the EWR tool. The value dimension consists of multiple EWRs (hydrologic indicators) that apply to environmental values (many-to-many), at multiple levels in the causal network (e.g. life-cycle components, species, groups of species, or long-term planning targets).\n\n\nComparer\nCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.\nComparison of environmental values at various theme scales for the example climate and adaptation scenarios\nExplored in detail in figures and tables\n\n\nCausal networks\nDescribe causal relationships between values.\nLong Term Water Plan (LTWP)\nProvided by HydroBOT. LTWP required of Basin States by the Murray-Darling Basin Plan and give effect to the Basin-wide Environmental Watering Strategy. \n\n\n\n\n\n\n\n\n\n\ngt seems to really have a lot of issues here. What if I use divs?\n\n\n\nTable 11: This is a table with gt in a div.\n\n\n\ntexttab |&gt; \n gt::gt()\n\n\n\n\n\n\n\nGeneral HydroBOT components\nType\nGeneral component definitions\nSpecific components used in our example\n\n\n\n\nInput data\nNot part of HydroBOT\nHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. \nModified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)\n\n\nController\nWorkflow\nInterface between input data, response model, and other toolkit components. Sets up run(s).\nSets up links to data and parameters for EWR tool and aggregations.\n\n\nResponse models\nExternal, integrated\nA model of the response of values, e.g. social, cultural, environmental, or economic values in response to hydrologic drivers.\nEWR tool\n\n\nAggregator\nWorkflow\nAggregates response model results to scales across the dimensions of time, space, and theme. \nResponse model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).\n\n\nComparer\nWorkflow\nCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.\nComparison of environmental values at various theme scales for the example climate and adaptation scenarios\n\n\nCausal networks\nExternal, integrated\nDescribe causal relationships between values.\nLong Term Water Plan (LTWP)\n\n\nSpatial data\nExternal, integrated\nDescribe spatial relationships\nGauge locations, Sustainable Diversion Limits (SDL) units, Murray-Darling Basin",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Testing word table formatting"
    ]
  },
  {
    "objectID": "publishing/tables_in_word.html#column-widths.",
    "href": "publishing/tables_in_word.html#column-widths.",
    "title": "Less ugly tables",
    "section": "Column widths.",
    "text": "Column widths.\nNone of those handle column widths with long text. kable smashes the first col, huxtable and gt smash them all.\nBased on some github issues, does it work if I just don’t use a label at all?\nGt no label\n\ntexttab |&gt; \n gt::gt()\n\n\n\n\n\n\n\nGeneral HydroBOT components\nType\nGeneral component definitions\nSpecific components used in our example\n\n\n\n\nInput data\nNot part of HydroBOT\nHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. \nModified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)\n\n\nController\nWorkflow\nInterface between input data, response model, and other toolkit components. Sets up run(s).\nSets up links to data and parameters for EWR tool and aggregations.\n\n\nResponse models\nExternal, integrated\nA model of the response of values, e.g. social, cultural, environmental, or economic values in response to hydrologic drivers.\nEWR tool\n\n\nAggregator\nWorkflow\nAggregates response model results to scales across the dimensions of time, space, and theme. \nResponse model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).\n\n\nComparer\nWorkflow\nCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.\nComparison of environmental values at various theme scales for the example climate and adaptation scenarios\n\n\nCausal networks\nExternal, integrated\nDescribe causal relationships between values.\nLong Term Water Plan (LTWP)\n\n\nSpatial data\nExternal, integrated\nDescribe spatial relationships\nGauge locations, Sustainable Diversion Limits (SDL) units, Murray-Darling Basin\n\n\n\n\n\n\n\nHuxtable no label\n\ntexttab |&gt; \n  huxtable::huxtable() |&gt; \n  huxtable::theme_article()\n\n\n\n\nGeneral HydroBOT componentsTypeGeneral component definitionsSpecific components used in our example\n\n\n\nInput dataNot part of HydroBOTHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. Modified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)\n\nControllerWorkflowInterface between input data, response model, and other toolkit components. Sets up run(s).Sets up links to data and parameters for EWR tool and aggregations.\n\nResponse modelsExternal, integratedA model of the response of values, e.g.  social, cultural, environmental, or economic values in response to hydrologic drivers.EWR tool\n\nAggregatorWorkflowAggregates response model results to scales across the dimensions of time, space, and theme. Response model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).\n\nComparerWorkflowCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.Comparison of environmental values at various theme scales for the example climate and adaptation scenarios\n\nCausal networksExternal, integratedDescribe causal relationships between values.Long Term Water Plan (LTWP)\n\nSpatial dataExternal, integratedDescribe spatial relationshipsGauge locations, Sustainable Diversion Limits (SDL) units, Murray-Darling Basin",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Testing word table formatting"
    ]
  },
  {
    "objectID": "publishing/tables_in_word.html#divs",
    "href": "publishing/tables_in_word.html#divs",
    "title": "Less ugly tables",
    "section": "Divs",
    "text": "Divs\nFlextable struggles in divs. The best we can do is to set width = 1 to at least make all the cols the same width and use the whole space. Though usually what i end up doing is rendering those tables outside a div and pasting in.\n\n\ntexttab |&gt; \n  flextable::flextable() |&gt; \n  flextable::set_table_properties(layout = \"autofit\", width = 1)\n\n\n\nTable 12: This is a flextable in a div with width = 1\n\n\n\nGeneral HydroBOT componentsTypeGeneral component definitionsSpecific components used in our exampleInput dataNot part of HydroBOTHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. Modified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)ControllerWorkflowInterface between input data, response model, and other toolkit components. Sets up run(s).Sets up links to data and parameters for EWR tool and aggregations.Response modelsExternal, integratedA model of the response of values, e.g.  social, cultural, environmental, or economic values in response to hydrologic drivers.EWR toolAggregatorWorkflowAggregates response model results to scales across the dimensions of time, space, and theme. Response model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).ComparerWorkflowCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.Comparison of environmental values at various theme scales for the example climate and adaptation scenariosCausal networksExternal, integratedDescribe causal relationships between values.Long Term Water Plan (LTWP)Spatial dataExternal, integratedDescribe spatial relationshipsGauge locations, Sustainable Diversion Limits (SDL) units, Murray-Darling Basin\n\n\n\n\n\nIf we don’t set widths, it’s smooshed, even with autofit.\n\ntexttab |&gt; \n  flextable::flextable() |&gt; \n  flextable::set_table_properties(layout = \"autofit\")\n\n\n\nTable 13: This is a flextable in a div with width = 1\n\n\n\nGeneral HydroBOT componentsTypeGeneral component definitionsSpecific components used in our exampleInput dataNot part of HydroBOTHydrologic data (timeseries). Typically representing multiple scenarios, e.g. climate and climate adaptations. May include other inputs as needed by response models. Modified historical hydrographs to represent hypothetical climate change and adaptations (45 gauges, 15 scenarios)ControllerWorkflowInterface between input data, response model, and other toolkit components. Sets up run(s).Sets up links to data and parameters for EWR tool and aggregations.Response modelsExternal, integratedA model of the response of values, e.g.  social, cultural, environmental, or economic values in response to hydrologic drivers.EWR toolAggregatorWorkflowAggregates response model results to scales across the dimensions of time, space, and theme. Response model sets the base scale for aggregation. EWR tool assesses hydrologic indicators (value) at gauges (space) and year (time).ComparerWorkflowCompares scenarios (typically) or other groupings. Provides standard outputs including comparison methods, plots, and tables.Comparison of environmental values at various theme scales for the example climate and adaptation scenariosCausal networksExternal, integratedDescribe causal relationships between values.Long Term Water Plan (LTWP)Spatial dataExternal, integratedDescribe spatial relationshipsGauge locations, Sustainable Diversion Limits (SDL) units, Murray-Darling Basin",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Testing word table formatting"
    ]
  },
  {
    "objectID": "research/management_modeling.html",
    "href": "research/management_modeling.html",
    "title": "Ecological modelling projects",
    "section": "",
    "text": "The majority of my work on ecological modelling addresses water management in the Murray-Darling Basin, with a focus on ecological responses to water delivery. I develop tools and models that capture those responses at ecologically-relevant scales and scale up to the basin.",
    "crumbs": [
      "Research",
      "Topics",
      "Management modeling"
    ]
  },
  {
    "objectID": "research/management_modeling.html#md-werp",
    "href": "research/management_modeling.html#md-werp",
    "title": "Ecological modelling projects",
    "section": "MD-WERP",
    "text": "MD-WERP\n\nThe Murray-Darling Water & Environment Research Program is an initiative of the Murray-Darling Basin Authority to improve knowledge and management of the Murray-Darling Basin across a range of outcomes.\nI am a member of the Climate Adaptation Theme, where I lead development of a toolkit to incorporate disparate models of response to environmental condition and synthesize their outcomes into usable information for decisionmaking. Most scenario modelling for water management stops at hydrology; this toolkit targets the subsequent responses to that hydrology across a range of values. The primary target of this toolkit is the assesment of climate scenarios and scenarios representing adaptation to those scenarios, but its capabilities extend to assessment of any hydrologic scenario.\nA beta version of a website documenting the toolkit and its capabilities is now live, with updated documentation and publicly available R package to come shortly.",
    "crumbs": [
      "Research",
      "Topics",
      "Management modeling"
    ]
  },
  {
    "objectID": "research/management_modeling.html#flow-mer",
    "href": "research/management_modeling.html#flow-mer",
    "title": "Ecological modelling projects",
    "section": "Flow-MER",
    "text": "Flow-MER\n\nThe Flow-MER project is a large, multi-institution, collaborative project initiated by the Australian Commonwealth Environmental Water Holder designed to better understand how environmental water delivery in the Murray-Darling Basin affects ecological outcomes across the basin.\nI was a member of the Modelling Cross-Cutting Theme, where we developed a framework for ecological modelling from local scales to the scale of the Murray-Darling Basin, designed to assess the impacts of environmental water. Key to this framework was the ability to use data available at the basin scale (often remotely-sensed), while modelling close to the scale at which ecological processes occur. The outcomes of these processes can then be scaled up in time and space. Our framework provides the capacity for this modelling in a consistent way across a range of ecological outcomes, and is designed to flexibly incorporate new models.",
    "crumbs": [
      "Research",
      "Topics",
      "Management modeling"
    ]
  },
  {
    "objectID": "research/management_modeling.html#ewkr",
    "href": "research/management_modeling.html#ewkr",
    "title": "Ecological modelling projects",
    "section": "EWKR",
    "text": "EWKR\n\nThe EWKR project was a precursor to Flow-MER, designed to improve the science underpinning water management in the Murray-Darling Basin.\nI was a member of the foodwebs theme, where I developed models of foodweb response to different watering scenarios. These models included producer production as a consequence of environmental watering actions, models of consumer diet sources derived from Bayesian mixing models, and assessment of uncertainty.",
    "crumbs": [
      "Research",
      "Topics",
      "Management modeling"
    ]
  },
  {
    "objectID": "setup/R_in_databricks.html",
    "href": "setup/R_in_databricks.html",
    "title": "R in databricks setup",
    "section": "",
    "text": "I’m working on setting up to run some R in databricks. It’s not exactly traditional ‘big data’ analyses, so I’m curious if we can achieve the payoffs we want with Spark. Will be some interesting testing.\nThe project code is all in an R project, living in a private github repo, and that’s the first issue. We can’t just set ssh keys that I can figure out, as I usually would. Try to create a git folder, it tries to login, then add access to the owner of the repo, not necessarily your own account, if the repo is owned by e.g. an organisation. In theory I need a GitHub PAT classic, but it’s unclear where that gets used. Looks like I just add databricks to my account (or the org account).\nThen, we have to get it to run. Each Notebook seems to be essentially its own instance, and so needs to full install all dependencies in the environment. That means we need to do a lot of installation management, which is further complicated because one of the dependencies is an R package that also lives in a private github repo.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "R in databricks setup"
    ]
  },
  {
    "objectID": "setup/R_in_databricks.html#installing-from-github",
    "href": "setup/R_in_databricks.html#installing-from-github",
    "title": "R in databricks setup",
    "section": "Installing from github",
    "text": "Installing from github\nI can’t get any of the usual ways of installing from private github repos to work.\nrenv::install() (and by extension, remotes::install_github() can’t authenticate, even when passing in the PAT directly to auth_token, and accessing the PAT with the gitcreds load-in trick doesn’t work because gitcreds::gitcreds_set() only runs in an interactive session, and for some reason the Databricks notebooks aren’t (i.e. interactive() returns TRUE).\nTrying pak::pkg_install('org/repo') also fails, because it should be auto-hitting that gitcred and it doesn’t exist.\nSo, until I sort that out, I’ve been cloning the package repo into my Workspace, and installing from there with renv::install('path/to/repo'). For some reason, pak::local_install() hangs.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "R in databricks setup"
    ]
  },
  {
    "objectID": "setup/R_in_databricks.html#installing-r-packages",
    "href": "setup/R_in_databricks.html#installing-r-packages",
    "title": "R in databricks setup",
    "section": "Installing R packages",
    "text": "Installing R packages\n\nEnvironment management\nI typically use renv for environment management, and there are instructions from databricks providing some instructions. The catch is, they’re primarily focused on persisting the lockfile, not the cache. And since we have to reinstall packages for every notebook (and packages take an extrodinarily long time to install on databricks), cache persistence is super important. I’ve figured out a few things, but no perfect solution.\nFor a single notebook, the cache seems to persist for a day (or maybe the duration of the cluster?). So re-runs re-install quickly from cache.\nI’ve tried to set the RENV_PATHS_CACHE environment variable to somewhere on dbfs to persist it with Sys.setenv('dbfs/path/to/renv/cache'), and that looks right when I say renv::paths$cache(). But once I do that, when I do renv::install(), it just works for hours and installs nothing.\nIt seems to be working to set RENV_PATHS_CACHE to inside my working directory. Since I always try to work out of git repos, this might work. It’s not really a global cache though, but maybe I’m OK persisting a project-level cache. Maybe I could put it in my User dir in the Workspace? I know persistent data should be in dbfs, but it the Workspace location works, it might just be the way to go.\nI’ve also explored just giving up on renv for this. I can’t get pak to work at all (it just builds a metadata database and hangs for hours). r2u might be the way to go, but has issues for this particular workflow due to dependence on a package not on CRAN.\n\n\nC system dependencies\nWe run into the same C system dependency issues here as anywhere else on Ubuntu/Linux. Unfortunately, the easy fix (using pak::sysreqs_*) to find and fix them doesn’t work here. It always says there aren’t any issues (even if I feed it the OS explicitly). There’s clearly some weird layer here that’s blocking its ability to query. So we have to manually build a shell script that installs all the system libraries.\nThe other option is to use r2u. That actually seems to work following the instructions at the r2u github (except I couldn’t get bspm to install). In theory, r2u might be the best option, since fast, system-infomed installs might obviate the need for cache persistence above. But for this particular workflow, it will be a hassle. The main package is on github, so won’t be on r2u. I could probably use r2u for all its dependencies, but that would involve enumerating them in a shell script e.g. sudo apt install r-cran-dep1 r-cran-dep2 etc. Which is annoying and fragile.\nWhat if I build binaries for the github package and provide it on dbfs? Worth a shot?",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "R in databricks setup"
    ]
  },
  {
    "objectID": "setup/R_in_databricks.html#other-issues",
    "href": "setup/R_in_databricks.html#other-issues",
    "title": "R in databricks setup",
    "section": "Other issues",
    "text": "Other issues\nThe interface is just really buggy. Constant loss of runtime and having to start back over from the beginning. Which makes it really slow to prototype or debug or run long jobs.\nDplyr seems to work differently. I assume it’s an issue of being a wrapper for spark, and something isn’t working right. Writing specifically for spark I guess might work in a limited workflow, but when it’s happening deep in an R package that needs to be cross-compatible, that doesn’t work. And debugging hits the earlier issue- it’s very hard to trace, especially when the notebook loses state and has to be restarted constantly.",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "R in databricks setup"
    ]
  },
  {
    "objectID": "setup/R_in_databricks.html#parallelism",
    "href": "setup/R_in_databricks.html#parallelism",
    "title": "R in databricks setup",
    "section": "Parallelism",
    "text": "Parallelism\nI’ll likely get into this in another notebook once I’m able to really dig into it, but I’m curious how spark actually works for the sorts of workflows I tend to have. They’re usually not about having a gajillion lines of data, but instead having to do the same set of computations on a lot of different data. In the most extreme, there might not be any input data, if the project is simulation population modeling for massively factorial parameter values. In other cases, we might have something like smallish input datasets for 1,000 scenarios, and want to do a set of fairly complex operations on each of them. In both of those cases, it’s relatively easy to see how it works on something like an HPC, where we can assign each of those scenarios or parameter combinations to a process and massively parallelise. But my understanding is that spark is all about datasets too big for memory and chunking them up. Hopefully there’s a way to ‘trick’ it to take those chunks in some other way?",
    "crumbs": [
      "Code Demos",
      "R parallel processing (general)",
      "R in databricks setup"
    ]
  },
  {
    "objectID": "simmodelling/negbin_autocorr.html",
    "href": "simmodelling/negbin_autocorr.html",
    "title": "Negative binomial autocorr",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\nlibrary(tidyverse)\nlibrary(fitdistrplus)\ndevtools::load_all()\nI have data that is almost certainly autocorrelated and negative binomial distributed. I’m analysing it elsewhere, but bringing over two simplified datasets here. Here, x is a spatial location, and n is a count at that location.\nThe questions are:\nThat last bit is harder than the typical Gaussian AR sequence defined by \\[X_{t+1} = \\rho X_{t} + \\epsilon_t\\]\nwhere if we have rho and the \\(\\mu\\) and \\(\\sigma\\) for the \\(X\\) distribution, we can calculate the appropriate \\(\\mu_\\epsilon\\) and \\(\\sigma_\\epsilon\\) using standard, well-known equations."
  },
  {
    "objectID": "simmodelling/negbin_autocorr.html#clean-simulation-given-mu-or-p-n-delta-and-rho",
    "href": "simmodelling/negbin_autocorr.html#clean-simulation-given-mu-or-p-n-delta-and-rho",
    "title": "Negative binomial autocorr",
    "section": "Clean simulation given \\(\\mu\\) or \\(p\\), \\(n\\) (\\(\\delta\\)), and \\(\\rho\\)",
    "text": "Clean simulation given \\(\\mu\\) or \\(p\\), \\(n\\) (\\(\\delta\\)), and \\(\\rho\\)\nSo, we want to be able to fit an autocorrelated negative binomial distribution given the complete distribution and the AR(1) \\(\\rho\\).\nI mostly follow Gouriéroux and Lu (2019), with modifications that make it possible to set both \\(\\rho\\) and \\(\\mu\\) separately. In short, They have \\(\\rho = \\beta c\\), but set \\(c=1\\) because they say there’s no way to independently estimate both. The catch is, that links \\(\\rho\\) and the \\(\\mu\\) of the negative binomial, and we can’t set both. But, I’ve altered the estimation (and more importantly, setting) to get the beta from mu from the overall negbin and then adjust \\(c\\) to give the correct \\(\\rho\\), i.e. \\(c = \\frac{\\rho}{\\beta}\\).\nI’ll try to use R’s nbinom parameter names, which use size and either prob or mu. I will refer to size as \\(\\delta\\) because we are using it here as the dispersion parameter (shape parameter of the gamma mixing distribution). Like R, I’ll allow either prob or mu, though in practice I expect mu will be more common, since it is what’s returned by fitdistrplus::fitdist(), and so will be easier to use when we have empirical distributions and for checking."
  },
  {
    "objectID": "simmodelling/negbin_autocorr.html#set-up-the-functions",
    "href": "simmodelling/negbin_autocorr.html#set-up-the-functions",
    "title": "Negative binomial autocorr",
    "section": "Set up the functions",
    "text": "Set up the functions\nThese will all be done in /R and exported as well.\nFirst, we define some helpers to move between \\(\\beta\\), \\(p\\), and \\(\\mu\\).\n\np_from_beta &lt;- function(beta) {\n  beta/(beta + 1)\n}\n\np_from_mu &lt;- function(mu, n) {\n  n / (n + mu)\n}\n\nbeta_from_p &lt;- function(p) {\n  -p/(p-1)\n}\n\nmu_from_p &lt;- function(p, n) {\n  (n*(1-p))/p\n}\n\nbeta_from_mu &lt;- function(mu, n) {\n  p &lt;- p_from_mu(mu, n)\n  beta_from_p(p)\n}\n\nmu_from_beta &lt;- function(beta, n) {\n  p &lt;- p_from_beta(beta)\n  mu_from_p(p, n)\n}\n\nThen, we can write the main function to generate the AR(1) sequence\n\nrnbinomAR &lt;- function(n, size, prob, mu, rho, return_Y = FALSE) {\n  \n  if (!missing(prob) & !missing(mu)) {\n    rlang::abort('Use either `prob` or `mu`')\n  }\n  \n  if (!missing(prob)) {\n    beta &lt;- beta_from_p(prob)\n  }\n  if (missing(prob)) {\n    beta &lt;- beta_from_mu(mu, size)\n  }\n  \n  # This differs from Gourieroux & Lu, as theirs (c = 1) yielded incorrect means.\n  c_param = rho/beta\n  \n  # just initialise the whole vector\n  if (!missing(prob)) {\n    X &lt;- rnbinom(n, size = size, prob = prob)\n  }\n  if (!missing(mu)) {\n    X &lt;- rnbinom(n, size = size, mu = mu)\n  }\n  \n  # Initialise the intensity process\n  Y &lt;- X*NA\n  \n  # Build the sequence one step at a time according to Gourieroux & Lu Definition 1.\n  for (i in 2:length(X)) {\n    Y[i] &lt;- rgamma(1, shape = size+X[i-1], scale = c_param)\n    X[i] &lt;- rpois(1, lambda = beta*Y[i])\n  }\n  \n  if (return_Y) {\n    return(list(X = X, Y = Y))\n  } else {\n    return(X)\n  }\n}\n\nA standard set of checks are then to fit that and return the mu, size, and rho. Could do the acf plot too, but that’s just using acf. These can then be compared to the set values.\n\nfit_nbinomAR &lt;- function(X) {\n  # check the size and mu\n  musize &lt;- fitdistrplus::fitdist(X, 'nbinom')\n  # check the AR\n  ac_x1 &lt;- acf(X)$acf[2]\n  \n  # return tibble\n  nbinar_est &lt;- tibble::tibble(term = c(names(musize$estimate), 'rho'),\n                               estimate = c(musize$estimate, ac_x1),\n                               std_error = c(musize$sd, NA))\n  \n  return(nbinar_est)\n  \n}\n\nWe can also check statistical correspondence of the complete distribution using a chi-square test and visualisations. These just check the distribution, however, not the AR. We’ll have to take the rho estimation from acf’s word on that.\n\nnbin_emp_pmf &lt;- function(X, size, prob, mu) {\n  \n  freq_x &lt;- tibble::tibble(count = X) |&gt; \n  dplyr::summarise(empirical = dplyr::n()/length(X),\n                   .by = count) |&gt; \n  dplyr::arrange(count)\n\n    if (!missing(prob) & !missing(mu)) {\n    rlang::abort('Use either `prob` or `mu`')\n  }\n  \n  if (!missing(prob)) {\n    x_dist &lt;- tibble::tibble(count = 0:max(freq_x$count),\n                         pmf = dnbinom(count,\n                                       size = size,\n                                       prob = prob))\n  }\n  if (missing(prob)) {\n    x_dist &lt;- tibble::tibble(count = 0:max(freq_x$count),\n                         pmf = dnbinom(count,\n                                       size = size,\n                                       mu = mu))\n  }\n  \n  # Do I want to be able to do this for the *given* params and the *fitted* params? Or just do it twice? I think probably do it twice, otherwise this gets VERY specific.\n\n\n# Join into one dataframe\nx_dist &lt;- x_dist |&gt; \n  dplyr::left_join(freq_x)\n\nreturn(x_dist)\n}\n\nThe plot and chi\n\nnbin_gg &lt;- function(distdf) {\n  \n  nbin_gg_check &lt;- distdf |&gt; \n  pivot_longer(-count) |&gt; \n  ggplot(aes(x = count, y = value, color = name)) +\n  geom_line() +\n    labs(y = 'P(X=x)', color = '')\n  \n  return(nbin_gg_check)\n}\n\n\nnbin_chi &lt;- function(distdf, grouper) {\n  distdf |&gt; \n  summarise(chi_p = chisq.test(empirical, pmf)$p.value,\n            .by = {{grouper}})\n}"
  },
  {
    "objectID": "simmodelling/negbin_autocorr.html#simulation-and-checking",
    "href": "simmodelling/negbin_autocorr.html#simulation-and-checking",
    "title": "Negative binomial autocorr",
    "section": "Simulation and checking",
    "text": "Simulation and checking\nI’ll demonstrate with a set of test parameters:\n\nrho &lt;- 0.75 \ndelta &lt;- 1.5 \nmu &lt;- 5\n\nGenerate a length-1000 sequence\n\ntest_seq &lt;- rnbinomAR(1000, size = delta, mu = mu, rho = rho)\n\nMake a quick plot\n\nplot(test_seq, type = 'l')\n\n\n\n\n\n\n\n\nAnd check it worked\n\nfit_nbinomAR(test_seq)\n\n\n\n\n\n\n\n\n# A tibble: 3 × 3\n  term  estimate std_error\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 size     1.29     0.0776\n2 mu       4.44     0.141 \n3 rho      0.776   NA     \n\n\nThose are reasonably close.\nTo do the chi square and plot checks of the overall distribution, we need to get the frequencies with nbin_emp_pmf\n\ntest_freq &lt;- nbin_emp_pmf(test_seq, size = delta, mu = mu)\n\nJoining with `by = join_by(count)`\n\n\nThe chi is &gt; 0.05 so empirical and theoretical are not distinguishable.\n\nnbc &lt;- nbin_chi(test_freq)\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `chi_p = chisq.test(empirical, pmf)$p.value`.\nCaused by warning in `chisq.test()`:\n! Chi-squared approximation may be incorrect\n\nnbc\n\n# A tibble: 1 × 1\n  chi_p\n  &lt;dbl&gt;\n1 0.272\n\n\nAnd the plot of the empirical and theoretical pmfs\n\nnbin_gg(test_freq)"
  },
  {
    "objectID": "simmodelling/negbin_autocorr.html#estimation-and-simulation",
    "href": "simmodelling/negbin_autocorr.html#estimation-and-simulation",
    "title": "Negative binomial autocorr",
    "section": "Estimation and simulation",
    "text": "Estimation and simulation\nAbove, we just chose some parameter values. In many cases, we’ll want to estimate them from empirical data, and then simulate new data with the same distribution. I’ve saved two sets of empirical data here that I’ll demonstrate with. This is where what looked good above falls apart.\n\nnb_a &lt;- readRDS('data/negbin_testing/nb_testA.rds')\nnb_s &lt;- readRDS('data/negbin_testing/nb_testS.rds')\n\nWe use fit_nbinomAR to get the estimates of \\(\\delta\\), \\(\\mu\\) and \\(\\rho\\) (which calls fitdistrplus::fitdist and acf).\n\n  afit &lt;- fit_nbinomAR(nb_a$count)\n\n\n\n\n\n\n\n  sfit &lt;- fit_nbinomAR(nb_s$count)\n\n\n\n\n\n\n\n\nThose seem to fit the nbinom pretty well\n\ntest_freqa &lt;- nbin_emp_pmf(nb_a$count, size = afit$estimate[afit$term == 'size'], mu = afit$estimate[afit$term == 'mu'])\n\nJoining with `by = join_by(count)`\n\nnbin_gg(test_freqa)\n\n\n\n\n\n\n\n\n\ntest_freqs &lt;- nbin_emp_pmf(nb_s$count, size = sfit$estimate[sfit$term == 'size'], mu = sfit$estimate[sfit$term == 'mu'])\n\nJoining with `by = join_by(count)`\n\nnbin_gg(test_freqs)\n\n\n\n\n\n\n\n\nNow we use those fits to generate new data. Though given how far off those PMFs are, not much hope that this will work.\n\nsim_a &lt;- rnbinomAR(1000,\n                   size = afit$estimate[afit$term == 'size'],\n                   mu = afit$estimate[afit$term == 'mu'],\n                   rho = afit$estimate[afit$term == 'rho'])\n\nsim_s &lt;- rnbinomAR(1000,\n                   size = sfit$estimate[sfit$term == 'size'],\n                   mu = sfit$estimate[sfit$term == 'mu'],\n                   rho = sfit$estimate[sfit$term == 'rho'])\n\nNow let’s check those.\n\nsimfita &lt;- fit_nbinomAR(sim_a)\n\n\n\n\n\n\n\nsimfitb &lt;- fit_nbinomAR(sim_s)\n\n\n\n\n\n\n\n\nDespite working well above, this has now completely fallen apart- the mu in particular are MUCH smaller.\nTo directly compare the original fits and the simulated data, we can join them. The size parameter isn’t terrible, rho is pretty good, but mu is horrible.\n\nafitsim &lt;- afit |&gt; \n  rename(data_estimate = estimate, data_se = std_error) |&gt; \n  bind_cols(simfita |&gt; \n              dplyr::select(-term) |&gt; \n              rename(sim_estimate = estimate, sim_se = std_error))\nafitsim\n\n# A tibble: 3 × 5\n  term  data_estimate data_se sim_estimate  sim_se\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n1 size          0.507  0.0334        0.379  0.0424\n2 mu           12.4    0.776         0.581  0.0383\n3 rho           0.551 NA             0.545 NA     \n\n\n\nsfitsim &lt;- sfit |&gt; \n  rename(data_estimate = estimate, data_se = std_error) |&gt; \n  bind_cols(simfitb |&gt; \n              dplyr::select(-term) |&gt; \n              rename(sim_estimate = estimate, sim_se = std_error))\nsfitsim\n\n# A tibble: 3 × 5\n  term  data_estimate data_se sim_estimate  sim_se\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n1 size          0.750  0.0688        0.317  0.0350\n2 mu           22.1    1.66          0.490  0.0353\n3 rho           0.355 NA             0.372 NA     \n\n\nHow do those look plotted?\n\nplot(nb_a$count, col = 'black', type = 'l')\nlines(sim_a, col = 'firebrick')\n\n\n\n\n\n\n\n\n\nplot(nb_s$count, col = 'black', type = 'l')\nlines(sim_s, col = 'firebrick')\n\n\n\n\n\n\n\n\nSo, this is clearly not really working. Some things to investigate further\n\nIf I ignore AR, does fitdistrplus::fitdist(X, 'nbinom') return the right \\(\\mu\\)?\nSimilarly, if I simulate data with rnbinom (instead of rnbinomAR), do I get an nbinom with the right \\(\\mu\\)?\n\nThis is probably the thing to check first.\n\n\nIt does look like rnbinom works fine, that blue line has similar mean to the black.\n\nsimanr &lt;- rnbinom(1000,\n                  size = afit$estimate[afit$term == 'size'],\n                   mu = afit$estimate[afit$term == 'mu'])\n\nplot(nb_a$count, col = 'black', type = 'l')\nlines(sim_a, col = 'firebrick')\nlines(simanr, col = 'dodgerblue')\n\n\n\n\n\n\n\n\nSo, the issue is in rnbinomAR, likely in a decay of the c_param in the gamma in Y. Will need to look more deeply into what’s happening there, and if it’s fixable. Likely start with setting returnY = TRUE and seeing if that gamma holds its mean or decays. Maybe there’s a nonlinearity in there that lets it decay (well, makes the decay noticeable) at high mu."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html",
    "href": "simmodelling/twoDautocorr.html",
    "title": "2d autocorrelation",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\nlibrary(tidyverse)",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Generate 2d autocorrelation"
    ]
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#motivation",
    "href": "simmodelling/twoDautocorr.html#motivation",
    "title": "2d autocorrelation",
    "section": "Motivation",
    "text": "Motivation\nI often need to simulate processes that are autocorrelated in two dimensions. Sometimes that’s time and 1d space, sometimes 2d space. Clearly 3d is likely needed as well, and I’ll update this with that once I get to it.\nThis is code that builds on work I’ve done in a couple projects, both across matlab and R. I’m doing it here in R because that’s the most up to date and open-source, but the matlab translation is straightforward.\nWe want to be able to generate a set of values with given statistical properties- means, standard deviations, and correlations in both dimensions. For the moment, I’m developing this with a gaussian random variable, but extensions to other random variables that are transforms from gaussian are relatively straightforward by backcalculating the needed \\(\\mu\\) and \\(\\sigma\\). Care must be taken if the correlations need to also be defined on the final scale.\n\nFuture/elsewhere\nI’ve done the back-calculations for the lognormal to allow setting desired correlations, means, and variances on the lognormal scale, and will add it in here later as an example. Likewise, we might want to set the correlation length \\(\\tau\\) rather than the correlation \\(\\rho\\), and in that case we need to back-calculate \\(\\rho\\) from the desired \\(\\tau\\). I’ve done that as well and will add it in. Finally, I have written up the math to obtain the equations used in this function, and will add that later as well.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Generate 2d autocorrelation"
    ]
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#process",
    "href": "simmodelling/twoDautocorr.html#process",
    "title": "2d autocorrelation",
    "section": "Process",
    "text": "Process\nThe goal is a U matrix that is 2d AC, on the normal scale\n\nSet up autocorrelation in the y dimension in U with a usual \\(y+1 = y*\\rho + a\\) formulation, where \\(a\\) is uncorrelated errors\nSet up autocorrelation in the x dimension\n\n\n\nthe errors here (\\(\\varepsilon\\) matrix) need to be correlated in the y dimension\nthese errors are thus generated by an AC process and so need their own set of errors (which are uncorrelated) for that AC\n\nVariances are set for all error matrices (\\(a\\), \\(\\varepsilon\\), and sub-errors (\\(z\\) matrix)) according to the relationships between normVar (the desired \\(\\sigma^2\\) of the final distribution) and the \\(\\rho_y\\) and \\(\\rho_x\\) (the desired correlations in both dimensions).",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Generate 2d autocorrelation"
    ]
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#function",
    "href": "simmodelling/twoDautocorr.html#function",
    "title": "2d autocorrelation",
    "section": "Function",
    "text": "Function\nI usually do a bunch of demos, but here I’ve developed this and just want it available more easily. So I’ll lead with the function and then demonstrate it and a few extensions.\n\nac2d &lt;- function(n_x, n_y, \n                 rho_y = 0, rho_x = 0, \n                 normVar = 1,\n                 printStats = FALSE,\n                 returnStats = FALSE) {\n  # n_x = number of sites along the x-dimension\n  # n_y = number of sites along the y-dimension\n  # rho_y = desired autocorr in the x direction\n  # rho_x = desired autocorr in the y direction\n  # normVar = desired variance of the underlying normal distribution\n  \n  # The goal is a U matrix that is 2d AC, on the normal scale\n  \n  # make the U matrix as rnorms to initialise\n  U &lt;- matrix(rnorm(n_x*n_y)*sqrt(normVar), nrow = n_y)\n  \n  # Set up the errors for the y process alone\n  # generate the errors - set the SD of these (hence the sqrt around the\n  # variance)\n  a &lt;- rnorm(n_y) * sqrt((normVar*(1-rho_y^2)))\n  \n  # Make the y ac for the U matrix\n  for (i in 1:(n_y-1)) {\n    U[i+1, ] &lt;- (rho_y * U[i, ]) + a[i]\n  }\n  \n  # Set up for the x-autocorr, which needs to have errors autocorred in the y-dimension\n  \n  # first, generate a z error matrix- these are the errors for epsilon, which\n  # are in turn the errors for U(t,x).\n  # What should var(z) be theoretically?\n  varZ &lt;- normVar*(1-rho_y^2)*(1-rho_x^2)\n  \n  # Make z, adjusting its standard deviation\n  # should have 'y' rows\n  z &lt;- matrix(rnorm(n_x*n_y), nrow = n_y) * \n    (sqrt(normVar * (1-rho_y^2) * (1-rho_x^2)))\n  \n  # now let's generate an epsilon matrix\n  # These are the errors for x part of the 2d ac process. These errors are\n  # themselves autocorrelated in the y dimension.\n  vareps &lt;- normVar * (1-rho_x^2)\n  eps &lt;- matrix(rnorm(n_x*n_y), nrow = n_y) * sqrt(vareps)\n  \n  # Now, generate the eps matrix y-autocorrelated (that is, going down rows within each column)\n  # eps is already created, so just write into the rows\n  for (i in 1:(n_y-1)) {\n    eps[i+1, ] &lt;- (rho_y * eps[i, ]) + z[i, ]\n  }\n  \n  # Now, make the U matrix x-autocorrelated\n  for (t in 1:(n_x-1)) {\n    U[ ,t+1] &lt;- (rho_x * U[ ,t]) + eps[ ,t]\n    \n  }\n  \n  # Check the stats if asked\n  if (printStats | returnStats) {\n    # calc stats in both dimensions\n    acstats &lt;- ac2dstats(U)\n    \n    if (printStats) {\n      print(paste0('Mean of all points is ', round(mean(c(U)), 3)))\n      print(paste0('Var of all points is ', round(var(c(U)), 3)))\n      print(paste0('Mean y AC is ', round(mean(acstats$ac_y), 3)))\n      print(paste0('Mean x AC is ', round(mean(acstats$ac_x), 3)))\n    }\n  }\n  \n  # usually don't want a list with the stats, and can always get later if needed, I suppose\n  if (returnStats) {\n    return(lst(U, acstats))\n  } else {\n    return(U)\n  }\n  \n}\n\nThat potentially calls another function to get the stats, which is here.\n\n# 2d ac stats function, useful for calling elsewhere\nac2dstats &lt;- function(acmatrix) {\n  # Calculate the autocorrs in both dimensions\n  \n  # Conditionals on 0 variance are because ar throws an error if there's no variance. Could have set up a try, but this is clearer\n  # Using 1 as the ac in that case because with no variance each value is the same as previous and so perfectly correlated. NA would be another option.\n  \n  # Get the ac in x-dimension: do this for each y (row)\n  ac_x &lt;- vector(mode = 'numeric', length = nrow(acmatrix)-1)\n  for (i in 1:(nrow(acmatrix)-1)) {\n    if (sd(acmatrix[i, ]) == 0) {\n      ac_x &lt;- 1\n    } else {\n      ac_x[i] &lt;- acf(acmatrix[i, ], lag.max = 1, type = 'correlation', plot = FALSE, demean = TRUE)$acf[2]\n    }\n    \n  } \n  \n  # Get the ac acorss the stream: do this for each x (column)\n  ac_y &lt;- vector(mode = 'numeric', length = ncol(acmatrix)-1)\n  for (i in 1:(ncol(acmatrix)-1)) {\n    \n    if (sd(acmatrix[,i]) == 0) {\n      ac_y[i] &lt;- 1\n    } else {\n      ac_y[i] &lt;- acf(acmatrix[ ,i], lag.max = 1, type = 'correlation', plot = FALSE, demean = TRUE)$acf[2]\n    }\n    \n  } \n  \n  return(lst(ac_y, ac_x))\n}",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Generate 2d autocorrelation"
    ]
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#testing",
    "href": "simmodelling/twoDautocorr.html#testing",
    "title": "2d autocorrelation",
    "section": "Testing",
    "text": "Testing\nA couple edge cases to make sure it doesn’t break. 0 and 1 correlations.\n\nacmatrix_0_1 &lt;- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0, rho_y = 1,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is -0.046\"\n[1] \"Var of all points is 0.999\"\n[1] \"Mean y AC is 1\"\n[1] \"Mean x AC is -0.02\"\n\n\n0 variance, but try to set autocorrelations- forces all points equal, which is right.\n\nacmatrix_0_1 &lt;- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 0, printStats = TRUE)\n\n[1] \"Mean of all points is 0\"\n[1] \"Var of all points is 0\"\n[1] \"Mean y AC is 1\"\n[1] \"Mean x AC is 1\"",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Generate 2d autocorrelation"
    ]
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#demonstration",
    "href": "simmodelling/twoDautocorr.html#demonstration",
    "title": "2d autocorrelation",
    "section": "Demonstration",
    "text": "Demonstration\nHow do we use that? Let’s say we want to create an environment that is 1000 x 500 sites, with \\(\\rho_y = 0.9\\) and \\(\\rho_x = 0.7\\), with the whole environment having a variance of 1 (for simplicity).\nSetting printstats = TRUE prints out the statistics and confirms the final matrix has been created with the desired correlations.\n\nacmatrix_7_9 &lt;- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is 0.006\"\n[1] \"Var of all points is 1.018\"\n[1] \"Mean y AC is 0.892\"\n[1] \"Mean x AC is 0.697\"\n\n\nWe can plot that up, easiest is to use ggplot because that’s what I’m used to. First, make it a tibble\n\nactib_7_9 &lt;- tibble::as_tibble(acmatrix_7_9) %&gt;%\n  mutate(y = row_number()) %&gt;%\n  pivot_longer(cols = starts_with('V')) %&gt;%\n  mutate(x = as.numeric(str_remove(name, 'V'))) %&gt;%\n  select(-name)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\nPlot it two different ways. It’s a monster though, so cut it to just a 100x100 block.\nFirst, a contour\n\nggplot(filter(actib_7_9, x &gt; 100 & x &lt;= 200 & y &gt; 300 & y &lt; 400), aes(x = x, y = y, z = value)) +\n  geom_contour_filled()\n\n\n\n\n\n\n\n\nAnd a tiled version, which is more precisely the data.\n\nggplot(filter(actib_7_9, x &gt; 100 & x &lt;= 200 & y &gt; 300 & y &lt; 400), aes(x = x, y = y, fill = value)) + \n  geom_tile() +\n  viridis::scale_fill_viridis(option = 'viridis')",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Generate 2d autocorrelation"
    ]
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#extensions",
    "href": "simmodelling/twoDautocorr.html#extensions",
    "title": "2d autocorrelation",
    "section": "Extensions",
    "text": "Extensions\n\n2 species\nA crude step toward 3d autocorr is to say we want 2d autocorr for two species (or really, just a second set of 2d autocorrelated values) with known correlation to the first set. I’ve done that, but it’s very task-specific and so not including here until I generalise a bit better.\n\n\nCross-correlation\nBy definition, the 2d autocorrelated matrices here have embedded nonzero cross-correlations at different lags (see analytical work for what they are once I put it in here). As a quick example, we can use ccf to get the cross correlation between two adjacent vectors along the x-dimension (columns), or the same along the y-dimension (rows).\nColumns\n\nccf(x = acmatrix_7_9[,100], y = acmatrix_7_9[,101], lag.max = 10, type = 'correlation')\n\n\n\n\n\n\n\n\nRows\n\nccf(x = acmatrix_7_9[100,], y = acmatrix_7_9[101,], lag.max = 10, type = 'correlation')",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Generate 2d autocorrelation"
    ]
  },
  {
    "objectID": "small_helpers/dplyr_filter_matrix_errors.html",
    "href": "small_helpers/dplyr_filter_matrix_errors.html",
    "title": "Dplyr filter matrices",
    "section": "",
    "text": "As of dplyr 1.1.0, we get warnings about using 1-d matrices instead of logical vectors.\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\nℹ Please use one dimensional logical vectors instead.\nThis is hard to find and debug, because the warning is lifecycle-limited, but setting the lifecycle warning options makes it possible.\n\nrlang::local_options(lifecycle_verbosity = \"warning\")\n\nIn most cases, it seems to happen with the column name to filter on as a character. It is a strange pattern, but comes up a fair amount programming.\n\nfiltercol &lt;- 'Species'\n\niris |&gt; \n  dplyr::filter(dplyr::across(tidyselect::any_of(filtercol)) == 'virginica')\n\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\nℹ Please use one dimensional logical vectors instead.\n\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1           6.3         3.3          6.0         2.5 virginica\n2           5.8         2.7          5.1         1.9 virginica\n3           7.1         3.0          5.9         2.1 virginica\n4           6.3         2.9          5.6         1.8 virginica\n5           6.5         3.0          5.8         2.2 virginica\n6           7.6         3.0          6.6         2.1 virginica\n7           4.9         2.5          4.5         1.7 virginica\n8           7.3         2.9          6.3         1.8 virginica\n9           6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n11          6.5         3.2          5.1         2.0 virginica\n12          6.4         2.7          5.3         1.9 virginica\n13          6.8         3.0          5.5         2.1 virginica\n14          5.7         2.5          5.0         2.0 virginica\n15          5.8         2.8          5.1         2.4 virginica\n16          6.4         3.2          5.3         2.3 virginica\n17          6.5         3.0          5.5         1.8 virginica\n18          7.7         3.8          6.7         2.2 virginica\n19          7.7         2.6          6.9         2.3 virginica\n20          6.0         2.2          5.0         1.5 virginica\n21          6.9         3.2          5.7         2.3 virginica\n22          5.6         2.8          4.9         2.0 virginica\n23          7.7         2.8          6.7         2.0 virginica\n24          6.3         2.7          4.9         1.8 virginica\n25          6.7         3.3          5.7         2.1 virginica\n26          7.2         3.2          6.0         1.8 virginica\n27          6.2         2.8          4.8         1.8 virginica\n28          6.1         3.0          4.9         1.8 virginica\n29          6.4         2.8          5.6         2.1 virginica\n30          7.2         3.0          5.8         1.6 virginica\n31          7.4         2.8          6.1         1.9 virginica\n32          7.9         3.8          6.4         2.0 virginica\n33          6.4         2.8          5.6         2.2 virginica\n34          6.3         2.8          5.1         1.5 virginica\n35          6.1         2.6          5.6         1.4 virginica\n36          7.7         3.0          6.1         2.3 virginica\n37          6.3         3.4          5.6         2.4 virginica\n38          6.4         3.1          5.5         1.8 virginica\n39          6.0         3.0          4.8         1.8 virginica\n40          6.9         3.1          5.4         2.1 virginica\n41          6.7         3.1          5.6         2.4 virginica\n42          6.9         3.1          5.1         2.3 virginica\n43          5.8         2.7          5.1         1.9 virginica\n44          6.8         3.2          5.9         2.3 virginica\n45          6.7         3.3          5.7         2.5 virginica\n46          6.7         3.0          5.2         2.3 virginica\n47          6.3         2.5          5.0         1.9 virginica\n48          6.5         3.0          5.2         2.0 virginica\n49          6.2         3.4          5.4         2.3 virginica\n50          5.9         3.0          5.1         1.8 virginica\n\n\nThe solution is to use .data\n\niris |&gt; \n  dplyr::filter(.data[[filtercol]] == 'virginica')\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1           6.3         3.3          6.0         2.5 virginica\n2           5.8         2.7          5.1         1.9 virginica\n3           7.1         3.0          5.9         2.1 virginica\n4           6.3         2.9          5.6         1.8 virginica\n5           6.5         3.0          5.8         2.2 virginica\n6           7.6         3.0          6.6         2.1 virginica\n7           4.9         2.5          4.5         1.7 virginica\n8           7.3         2.9          6.3         1.8 virginica\n9           6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n11          6.5         3.2          5.1         2.0 virginica\n12          6.4         2.7          5.3         1.9 virginica\n13          6.8         3.0          5.5         2.1 virginica\n14          5.7         2.5          5.0         2.0 virginica\n15          5.8         2.8          5.1         2.4 virginica\n16          6.4         3.2          5.3         2.3 virginica\n17          6.5         3.0          5.5         1.8 virginica\n18          7.7         3.8          6.7         2.2 virginica\n19          7.7         2.6          6.9         2.3 virginica\n20          6.0         2.2          5.0         1.5 virginica\n21          6.9         3.2          5.7         2.3 virginica\n22          5.6         2.8          4.9         2.0 virginica\n23          7.7         2.8          6.7         2.0 virginica\n24          6.3         2.7          4.9         1.8 virginica\n25          6.7         3.3          5.7         2.1 virginica\n26          7.2         3.2          6.0         1.8 virginica\n27          6.2         2.8          4.8         1.8 virginica\n28          6.1         3.0          4.9         1.8 virginica\n29          6.4         2.8          5.6         2.1 virginica\n30          7.2         3.0          5.8         1.6 virginica\n31          7.4         2.8          6.1         1.9 virginica\n32          7.9         3.8          6.4         2.0 virginica\n33          6.4         2.8          5.6         2.2 virginica\n34          6.3         2.8          5.1         1.5 virginica\n35          6.1         2.6          5.6         1.4 virginica\n36          7.7         3.0          6.1         2.3 virginica\n37          6.3         3.4          5.6         2.4 virginica\n38          6.4         3.1          5.5         1.8 virginica\n39          6.0         3.0          4.8         1.8 virginica\n40          6.9         3.1          5.4         2.1 virginica\n41          6.7         3.1          5.6         2.4 virginica\n42          6.9         3.1          5.1         2.3 virginica\n43          5.8         2.7          5.1         1.9 virginica\n44          6.8         3.2          5.9         2.3 virginica\n45          6.7         3.3          5.7         2.5 virginica\n46          6.7         3.0          5.2         2.3 virginica\n47          6.3         2.5          5.0         1.9 virginica\n48          6.5         3.0          5.2         2.0 virginica\n49          6.2         3.4          5.4         2.3 virginica\n50          5.9         3.0          5.1         1.8 virginica",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Dplyr filter matrices"
    ]
  },
  {
    "objectID": "small_helpers/error_safely.html",
    "href": "small_helpers/error_safely.html",
    "title": "Errors with map",
    "section": "",
    "text": "Sometimes when we use purrr::map or similar functions, one of the iterations hits an error. When this happens, we lose the whole set of runs, even if the others would run or have already run. That can be a waste of time, make it hard to find the issue, and prevent re-running just the failed bits if the error is intermittent (e.g. many HTTP errors).\nUsing a test function from error handling that errors on even numbers, warns if the number is 5, and otherwise returns the input.\nerr_even_warn5 &lt;- function(x) {\n  if ((x %% 2) == 0) {\n    stop('Even numbers are error')\n  } else if (x == 5) {\n    warning('5 throws a warning')\n  } else {x}\n}\nThe issue here is that if we try to run that, not only does it error for 5, we don’t get any of the results\npurrr::map(1:10, err_even_warn5)\n\nError in `purrr::map()`:\nℹ In index: 2.\nCaused by error in `.f()`:\n! Even numbers are error\nBut what if we want to get all the other results, and possibly identify the failures and correct them or retry?\nOne option is to use purrr::safely, as it returns a list with a result and error item. This means that purrring over things where some may fail doesn’t kill everything, but we need to unpack it a bit.\nThe syntax is typical map, but with the function to apply wrapped in the ‘adverb’ safely.\nerrpurr &lt;- purrr::map(1:10, \n                      purrr::safely(err_even_warn5))\n\nWarning in .f(...): 5 throws a warning\n\nerrpurr\n\n[[1]]\n[[1]]$result\n[1] 1\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\nNULL\n\n[[2]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[3]]\n[[3]]$result\n[1] 3\n\n[[3]]$error\nNULL\n\n\n[[4]]\n[[4]]$result\nNULL\n\n[[4]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[5]]\n[[5]]$result\n[1] \"5 throws a warning\"\n\n[[5]]$error\nNULL\n\n\n[[6]]\n[[6]]$result\nNULL\n\n[[6]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[7]]\n[[7]]$result\n[1] 7\n\n[[7]]$error\nNULL\n\n\n[[8]]\n[[8]]$result\nNULL\n\n[[8]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[9]]\n[[9]]$result\n[1] 9\n\n[[9]]$error\nNULL\n\n\n[[10]]\n[[10]]$result\nNULL\n\n[[10]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\nNote that safely only deals with errors, the ‘warning’ at index 5 just passes through and is included in the result. We could use quietly instead if we want to capture all possibilities except errors, which still cause quietly to fail. We can do things like look for the values with or without errors\nwhicherrors &lt;- purrr::map(errpurr, \n                          \\(x) !is.null(x$error)) |&gt; \n  unlist() |&gt; \n  which()\n\nwhicherrors \n\n[1]  2  4  6  8 10\nThose without errors (or with a non-null result )can be used to extract the clean outputs. Note that this includes the warning.\nnoterrors &lt;- purrr::map(errpurr,\n                        \\(x) purrr::pluck(x, 'result'))\n\nnoterrors\n\n[[1]]\n[1] 1\n\n[[2]]\nNULL\n\n[[3]]\n[1] 3\n\n[[4]]\nNULL\n\n[[5]]\n[1] \"5 throws a warning\"\n\n[[6]]\nNULL\n\n[[7]]\n[1] 7\n\n[[8]]\nNULL\n\n[[9]]\n[1] 9\n\n[[10]]\nNULL\nAnother option is to use list_transpose and then get the result and error lists. Two plucks is likely better, especially if we usually only need one.\nterr &lt;- purrr::list_transpose(errpurr)\n\nterr$result\n\n[[1]]\n[1] 1\n\n[[2]]\nNULL\n\n[[3]]\n[1] 3\n\n[[4]]\nNULL\n\n[[5]]\n[1] \"5 throws a warning\"\n\n[[6]]\nNULL\n\n[[7]]\n[1] 7\n\n[[8]]\nNULL\n\n[[9]]\n[1] 9\n\n[[10]]\nNULL\n\nterr$error\n\n[[1]]\nNULL\n\n[[2]]\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n[[3]]\nNULL\n\n[[4]]\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n[[5]]\nNULL\n\n[[6]]\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n[[7]]\nNULL\n\n[[8]]\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n[[9]]\nNULL\n\n[[10]]\n&lt;simpleError in .f(...): Even numbers are error&gt;\nThe use of safely above is really handy if we want to read the errors. If not, and we just want to save the non-errors, possibly with a default is likely better (cleaner).\nerrpurrP &lt;- purrr::map(1:10, \n                       purrr::possibly(err_even_warn5,\n                                       NA))\n\nWarning in .f(...): 5 throws a warning\n\nerrpurrP\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] NA\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] NA\n\n[[5]]\n[1] \"5 throws a warning\"\n\n[[6]]\n[1] NA\n\n[[7]]\n[1] 7\n\n[[8]]\n[1] NA\n\n[[9]]\n[1] 9\n\n[[10]]\n[1] NA\nNote that in use we’d likely still want to have a cleanup step/function to chuck out the warnings before concatenating the rest.\nThere’s also a question of what happens if an iteration has a warning and a result. For example\nerr_even_warn510 &lt;- function(x) {\n  if ((x %% 2) == 0) {\n    stop('Even numbers are error')\n  } else if (x == 5) {\n    warning('5 doubles')\n    x &lt;- 10\n  } else {x}\n  return(x)\n}\nFor both safely and possibly, a real result plus warning ends up with the real result in the output and the warning bubbling up. So that’s good- warnings don’t change the structure of the data if there is data.\nep5 &lt;- purrr::map(1:10, \n                      purrr::safely(err_even_warn510))\n\nWarning in .f(...): 5 doubles\n\nep5\n\n[[1]]\n[[1]]$result\n[1] 1\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\nNULL\n\n[[2]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[3]]\n[[3]]$result\n[1] 3\n\n[[3]]$error\nNULL\n\n\n[[4]]\n[[4]]$result\nNULL\n\n[[4]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[5]]\n[[5]]$result\n[1] 10\n\n[[5]]$error\nNULL\n\n\n[[6]]\n[[6]]$result\nNULL\n\n[[6]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[7]]\n[[7]]$result\n[1] 7\n\n[[7]]$error\nNULL\n\n\n[[8]]\n[[8]]$result\nNULL\n\n[[8]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[9]]\n[[9]]$result\n[1] 9\n\n[[9]]$error\nNULL\n\n\n[[10]]\n[[10]]$result\nNULL\n\n[[10]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\nepP &lt;- purrr::map(1:10, \n                       purrr::possibly(err_even_warn510,\n                                       NA))\n\nWarning in .f(...): 5 doubles\n\nepP\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] NA\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] NA\n\n[[5]]\n[1] 10\n\n[[6]]\n[1] NA\n\n[[7]]\n[1] 7\n\n[[8]]\n[1] NA\n\n[[9]]\n[1] 9\n\n[[10]]\n[1] NA",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "`Safe` error passing in loops"
    ]
  },
  {
    "objectID": "small_helpers/error_safely.html#programmatic-use",
    "href": "small_helpers/error_safely.html#programmatic-use",
    "title": "Errors with map",
    "section": "Programmatic use",
    "text": "Programmatic use\nI have several cases where I want to run map in packages or large analyses, and assess what’s happening to the fails and possibly re-run. That needs a few wrappers or standard sequences of steps around what I have above. The standard steps might be better, then we don’t have to deal with function passing, which is a hassle.\nLet’s set up a function that will fail about half the time, but re-runs might work.\n\nfailhalf &lt;- function(x) {\n  if (runif(1) &lt;= 0.5) {\n    x &lt;- x+5\n  } else {\n    stop(\"random above 0.5\")\n  }\n  return(x)\n}\n\nI’ll work with safely- I think that’s more general than possibly, and gives a developer the ability to go in and look at the errors in debug, even if they’re not returned. Let’s assume we have a variable to feed it, as we usually would in a function.\n\nlarg &lt;- 1:10\n\n# first run\nx5 &lt;- purrr::map(larg, purrr::safely(failhalf))\n# get the results\nr5 &lt;- purrr::map(x5, purrr::pluck('result'))\n\n# Get the errors- we might have this somewhere a dev could get it, but not always use it.\ne5 &lt;- purrr::map(x5, purrr::pluck('error'))\n\nIf we don’t want to retry, that’s as far as we need to go. We could easily return a list just like what would be returned normally and another list of the errors. That’s as simple as\n\nsafepurr &lt;- function(input, fun) {\n  # first run\nx5 &lt;- purrr::map(input, purrr::safely(fun))\n# get the results\nr5 &lt;- purrr::map(x5, purrr::pluck('result'))\n\n# Get the errors- we might have this somewhere a dev could get it, but not always use it.\ne5 &lt;- purrr::map(x5, purrr::pluck('error'))\n\nreturn(list(r5, e5))\n}\n\nThough I’m not sure what the point is. By the time we unpack that we might as well have just done it inline with pluck().",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "`Safe` error passing in loops"
    ]
  },
  {
    "objectID": "small_helpers/error_safely.html#retries",
    "href": "small_helpers/error_safely.html#retries",
    "title": "Errors with map",
    "section": "Retries",
    "text": "Retries\nIf we do want to retry, we need to re-run the failures. This actually makes sense to do in a single while, rather than with the above first. We can put a retries argument in easily enough.\n\n# if we have e5, we could use it, but it's not any harder to get error indices directly\nlarg &lt;- 1:10\nwhicherrors &lt;- 1:length(larg)\nlout &lt;- vector(mode = 'list', length = 0)\nwhile(length(whicherrors) &gt; 0) {\n  larg &lt;- larg[whicherrors]\n  # first run\nx5 &lt;- purrr::map(larg, purrr::safely(failhalf))\n# get the results, dropping the NULLs\nr5 &lt;- purrr::map(x5, purrr::pluck('result'))\n\n# where are the errors\nwhicherrors &lt;- purrr::map(x5, \n                          \\(x) rlang::is_error(\n                            x$error)\n                          ) |&gt; \n  unlist() |&gt; \n  which()\n\n# append\nlout &lt;- c(lout, r5[-whicherrors])\n\n\n}\n\nThat works fine if we don’t care about order, but if we do, we’ll need to make sure we know which list items are erroring and replace them. That will almost always be what we want to do, and isn’t any more complicated.\n\nlarg &lt;- 1:10\nwhicherrors &lt;- 1:length(larg)\nlout &lt;- vector(mode = 'list', length = length(larg))\n# the indices, to track which are being filled/left\nindlist &lt;- 1:10\nwhile(length(whicherrors) &gt; 0) {\n  larg &lt;- larg[whicherrors]\n  # first run\nx5 &lt;- purrr::map(larg, purrr::safely(failhalf))\n# get the results, dropping the NULLs\nr5 &lt;- purrr::map(x5, purrr::pluck('result'))\n\n# replace the indices that were errors with new data. Some might still be errors, they will fill subsequently\nlout[indlist] &lt;- r5\n\n# where are the errors\nwhicherrors &lt;- purrr::map(x5, \n                          \\(x) rlang::is_error(\n                            x$error)\n                          ) |&gt; \n  unlist() |&gt; \n  which()\n# which ORIGINAL indices are we left with?\nindlist &lt;- indlist[whicherrors]\n\n\n}\n\nRather than a while, can we recurse? Yes, and it’s a bit cleaner. But, it’s not tail-recursive and there’s no obvious way to set a retries.\n\ngetsafe &lt;- function(larg) {\n  x5 &lt;- purrr::map(larg, purrr::safely(failhalf))\n  # get the results, dropping the NULLs\n  r5 &lt;- purrr::map(x5, purrr::pluck('result'))\n  \n  whicherrors &lt;- purrr::map(x5, \n                          \\(x) rlang::is_error(\n                            x$error)\n                          ) |&gt; \n  unlist() |&gt; \n  which()\n  \n  if (length(whicherrors &gt; 0)) {\n    eout &lt;- getsafe(larg[whicherrors])\n    r5[whicherrors] &lt;- eout\n  }\n  \n  return(r5)\n}\n\n\ngetsafe(1:10)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] 8\n\n[[4]]\n[1] 9\n\n[[5]]\n[1] 10\n\n[[6]]\n[1] 11\n\n[[7]]\n[1] 12\n\n[[8]]\n[1] 13\n\n[[9]]\n[1] 14\n\n[[10]]\n[1] 15\n\n\nHow bad is it to make a function that takes the input and the function and does the while loop?\n\nsafe_clean_retries &lt;- function(input, fun, retries) {\n  whicherrors &lt;- 1:length(input)\n  lout &lt;- vector(mode = 'list', length = length(input))\n  # the indices, to track which are being filled/left\n  indlist &lt;- 1:length(input)\n  counter = 0\n  \n  while (length(whicherrors) &gt; 0 & counter &lt;= retries) {\n    # run the purrr\n    x5 &lt;- purrr::map(input, purrr::safely(fun))\n    # get the results, dropping the NULLs\n    r5 &lt;- purrr::map(x5, purrr::pluck('result'))\n    \n    # if we want the errors, we could put in a debug here\n    e5 &lt;- purrr::map(x5, purrr::pluck('result'))\n    \n    # replace the indices that were errors with new data. Some might still be errors, they will fill subsequently\n    lout[indlist] &lt;- r5\n    \n    # where are the errors\n    whicherrors &lt;- purrr::map(x5,\n                              \\(x) rlang::is_error(x$error)) |&gt;\n      unlist() |&gt;\n      which()\n    \n    # Cut the data to the fails\n    input &lt;- input[whicherrors]\n\n    # which ORIGINAL indices are we left with?\n    indlist &lt;- indlist[whicherrors]\n    \n    counter &lt;- counter + 1\n    \n  }\n  \n  return(lout)\n}\n\nAnd that lets us use it\n\nsafe_clean_retries(1:10, failhalf, retries = 5) |&gt; \n  unlist()\n\n [1]  6  7  8  9 10 11 12 13 14 15\n\n\nIt should work to pass it anonymous functions or otherwise custom?\n\nsafe_clean_retries(1:10,\n                   \\(x) ifelse(sample(c(1,2), 1) == 1,\n                               stop(), x), \n                   retries = 10) |&gt; \n  unlist()\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nIt works for furrr too, though in this case it’s slower (not surprising, for this test case the overhead will be much bigger than the computation).\n\nsafe_clean_retries_f &lt;- function(input, fun, retries) {\n  whicherrors &lt;- 1:length(input)\n  lout &lt;- vector(mode = 'list', length = length(input))\n  # the indices, to track which are being filled/left\n  indlist &lt;- 1:length(input)\n  counter = 0\n  \n  while (length(whicherrors) &gt; 0 & counter &lt;= retries) {\n    # run the purrr\n    # Only parallel this one. The others are just indexing\n    x5 &lt;- furrr::future_map(input, purrr::safely(fun), .options = furrr_options(seed = TRUE))\n    # get the results, dropping the NULLs\n    r5 &lt;- purrr::map(x5, purrr::pluck('result'))\n    \n    # if we want the errors, we could put in a debug here\n    e5 &lt;- purrr::map(x5, purrr::pluck('result'))\n    \n    # replace the indices that were errors with new data. Some might still be errors, they will fill subsequently\n    lout[indlist] &lt;- r5\n    \n    # where are the errors\n    whicherrors &lt;- purrr::map(x5,\n                              \\(x) rlang::is_error(x$error)) |&gt;\n      unlist() |&gt;\n      which()\n    \n    # Cut the data to the fails\n    input &lt;- input[whicherrors]\n\n    # which ORIGINAL indices are we left with?\n    indlist &lt;- indlist[whicherrors]\n    \n    counter &lt;- counter + 1\n    \n  }\n  \n  return(lout)\n}\n\n\nlibrary(furrr)\n\nLoading required package: future\n\nplan(multisession)\n\nsafe_clean_retries_f(1:10,\n                   \\(x) ifelse(sample(c(1,2), 1) == 1,\n                               stop(), x), \n                   retries = 10) |&gt; \n  unlist()\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nAnd finally, we can clean that up to use the same arg names as purrr and do both parallel or not\n\nsafe_map &lt;- function(.x, .f, ..., retries = 10, parallel = FALSE) {\n  whicherrors &lt;- 1:length(.x)\n  result_list &lt;- vector(mode = 'list', length = length(.x))\n  # the indices, to track which are being filled/left\n  orig_indices &lt;- 1:length(.x)\n  counter = 0\n  \n  while (length(whicherrors) &gt; 0 & counter &lt;= retries) {\n    # run the purrr\n    # Only parallel this one. The others are just indexing\n    if (parallel) {\n          full_out &lt;- furrr::future_map(.x, purrr::safely(.f),\n                                        .options = furrr_options(seed = TRUE))\n    } else {\n      full_out &lt;- purrr::map(.x, purrr::safely(.))\n    }\n    # get the results, dropping the NULLs\n    intermed_result &lt;- purrr::map(full_out, purrr::pluck('result'))\n    \n    # if we want the errors, we could put in a debug here\n    err_list &lt;- purrr::map(full_out, purrr::pluck('result'))\n    \n    # replace the indices that were errors with new data. Some might still be errors, they will fill subsequently\n    result_list[orig_indices] &lt;- intermed_result\n    \n    # where are the errors\n    whicherrors &lt;- purrr::map(full_out,\n                              \\(x) rlang::is_error(x$error)) |&gt;\n      unlist() |&gt;\n      which()\n    \n    # Cut the data to the fails\n    .x &lt;- .x[whicherrors]\n\n    # which ORIGINAL indices are we left with?\n    orig_indices &lt;- orig_indices[whicherrors]\n    \n    counter &lt;- counter + 1\n    \n  }\n  \n  return(result_list)\n}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "`Safe` error passing in loops"
    ]
  },
  {
    "objectID": "small_helpers/error_safely.html#benchmarking",
    "href": "small_helpers/error_safely.html#benchmarking",
    "title": "Errors with map",
    "section": "Benchmarking",
    "text": "Benchmarking\nThe speed question is interesting- how much does it slow things down to run in this wrapper? Should I put everything in it, or is the speed hit only worth it where there’s a high likelihood of failure and each iteration is big?\nLet’s set something a bit bigger up and test. Just purrrr, assume furrr will scale similarly. I’m not going to have any errors- the point here is to ask how much this hurts when there aren’t errors. And if that tradeoff is worth the ability to fix others.\n\ninlist &lt;- list(iris, mtcars, iris, mtcars, iris, mtcars)\n\ntestfun &lt;- function(x) {\n x &lt;- x |&gt; \n   dplyr::mutate(across(where(is.numeric), mean)) |&gt; \n   dplyr::summarise(across(where(is.numeric), sum))\n \n return(x)\n}\n\nThe hit there isn’t too bad. Seems like it’s probably usually worth it, especially for big computations. For big jobs, the consequences of errors will be worse in terms of lost time/results, and the additional overhead will be a smaller proportion of the time compared to the main purrr call.\n\nmicrobenchmark::microbenchmark(\n  barepurrr = purrr::map(inlist, testfun),\n  safepurrr = safe_clean_retries(inlist, testfun, retries = 10),\n  times = 100\n)\n\nUnit: milliseconds\n      expr     min       lq     mean  median       uq      max neval\n barepurrr 19.8354 22.37945 25.35412 23.1743 25.61785 159.4209   100\n safepurrr 21.1953 22.90615 25.38399 24.4367 25.77285  88.9560   100",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "`Safe` error passing in loops"
    ]
  },
  {
    "objectID": "small_helpers/error_safely.html#function-construction",
    "href": "small_helpers/error_safely.html#function-construction",
    "title": "Errors with map",
    "section": "Function construction",
    "text": "Function construction\nThe functions above all just use a function with a single unspecified argument. But things get trickier with anonymous functions or multiple arguments. The locations for the arguments aren’t always intuitive- they go after the possibly(function()). The reason is because possibly and safely both create new functions.\nFor example, if we have a simple function, still just with one argument\n\nadd5 &lt;- function(x) {\n  x+5\n}\n\nThen the simple version works\n\npurrr::map(1:5, purrr::safely(add5))\n\n[[1]]\n[[1]]$result\n[1] 6\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\n[1] 7\n\n[[2]]$error\nNULL\n\n\n[[3]]\n[[3]]$result\n[1] 8\n\n[[3]]$error\nNULL\n\n\n[[4]]\n[[4]]$result\n[1] 9\n\n[[4]]$error\nNULL\n\n\n[[5]]\n[[5]]$result\n[1] 10\n\n[[5]]$error\nNULL\n\n\nIf we want to be more specific and make it anonymous, though, where does the x go? What is the safe equivalent of this?\n\npurrr::map(1:5, \\(x) add5(x))\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] 8\n\n[[4]]\n[1] 9\n\n[[5]]\n[1] 10\n\n\nThis works. The anonymous function is wholly inside safely, and so the whole anonymous function gets transformed into a safe version.\n\npurrr::map(1:5, purrr::safely(\\(x) add5(x)))\n\n[[1]]\n[[1]]$result\n[1] 6\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\n[1] 7\n\n[[2]]$error\nNULL\n\n\n[[3]]\n[[3]]$result\n[1] 8\n\n[[3]]$error\nNULL\n\n\n[[4]]\n[[4]]$result\n[1] 9\n\n[[4]]$error\nNULL\n\n\n[[5]]\n[[5]]$result\n[1] 10\n\n[[5]]$error\nNULL\n\n\nThis does not. The safely can’t be inside the anonymous function\n\npurrr::map(1:5, \\(x) purrr::safely(add5(x)))\n\n[[1]]\nfunction (...) \ncapture_error(.f(...), otherwise, quiet)\n&lt;bytecode: 0x000001c0b17027d8&gt;\n&lt;environment: 0x000001c0b56976d0&gt;\n\n[[2]]\nfunction (...) \ncapture_error(.f(...), otherwise, quiet)\n&lt;bytecode: 0x000001c0b17027d8&gt;\n&lt;environment: 0x000001c0b579a518&gt;\n\n[[3]]\nfunction (...) \ncapture_error(.f(...), otherwise, quiet)\n&lt;bytecode: 0x000001c0b17027d8&gt;\n&lt;environment: 0x000001c0b5797040&gt;\n\n[[4]]\nfunction (...) \ncapture_error(.f(...), otherwise, quiet)\n&lt;bytecode: 0x000001c0b17027d8&gt;\n&lt;environment: 0x000001c0b57a35b8&gt;\n\n[[5]]\nfunction (...) \ncapture_error(.f(...), otherwise, quiet)\n&lt;bytecode: 0x000001c0b17027d8&gt;\n&lt;environment: 0x000001c0b57a41d0&gt;\n\n\nBut this does- safely(fun) is a function, and so we can give it the argument.\n\npurrr::map(1:5, \\(x) purrr::safely(add5)(x))\n\n[[1]]\n[[1]]$result\n[1] 6\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\n[1] 7\n\n[[2]]$error\nNULL\n\n\n[[3]]\n[[3]]$result\n[1] 8\n\n[[3]]$error\nNULL\n\n\n[[4]]\n[[4]]$result\n[1] 9\n\n[[4]]$error\nNULL\n\n\n[[5]]\n[[5]]$result\n[1] 10\n\n[[5]]$error\nNULL\n\n\nThis can be useful with multiple arguments, e.g.\n\nadder &lt;- function(x,y) {\n  x + y\n}\n\nAgain, as anonymous, wholly inside works\n\npurrr::map(1:5, purrr::safely(\\(x) adder(x, 10)))\n\n[[1]]\n[[1]]$result\n[1] 11\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\n[1] 12\n\n[[2]]$error\nNULL\n\n\n[[3]]\n[[3]]$result\n[1] 13\n\n[[3]]$error\nNULL\n\n\n[[4]]\n[[4]]$result\n[1] 14\n\n[[4]]$error\nNULL\n\n\n[[5]]\n[[5]]$result\n[1] 15\n\n[[5]]$error\nNULL\n\n\nIt does not work if it’s not anonymous, ie just giving it the second argument. While this syntax works normally,\n\npurrr::map(1:5, adder, 10)\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 12\n\n[[3]]\n[1] 13\n\n[[4]]\n[1] 14\n\n[[5]]\n[1] 15\n\n\nSimilar does not work with safely.\n\npurrr::map(1:5, purrr::safely(adder, 10))\n\n[[1]]\n[[1]]$result\n[1] 10\n\n[[1]]$error\n&lt;simpleError in .f(...): argument \"y\" is missing, with no default&gt;\n\n\n[[2]]\n[[2]]$result\n[1] 10\n\n[[2]]$error\n&lt;simpleError in .f(...): argument \"y\" is missing, with no default&gt;\n\n\n[[3]]\n[[3]]$result\n[1] 10\n\n[[3]]$error\n&lt;simpleError in .f(...): argument \"y\" is missing, with no default&gt;\n\n\n[[4]]\n[[4]]$result\n[1] 10\n\n[[4]]$error\n&lt;simpleError in .f(...): argument \"y\" is missing, with no default&gt;\n\n\n[[5]]\n[[5]]$result\n[1] 10\n\n[[5]]$error\n&lt;simpleError in .f(...): argument \"y\" is missing, with no default&gt;\n\npurrr::map(1:5, purrr::safely(adder(10)))\n\nError in adder(10): argument \"y\" is missing, with no default\n\n\nTo get this to work with safely, we have to anonymize, but being careful to feed the arguments after the final safely parenthesis.\n\npurrr::map(1:5, \\(x) purrr::safely(adder)(x, y=10))\n\n[[1]]\n[[1]]$result\n[1] 11\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\n[1] 12\n\n[[2]]$error\nNULL\n\n\n[[3]]\n[[3]]$result\n[1] 13\n\n[[3]]$error\nNULL\n\n\n[[4]]\n[[4]]$result\n[1] 14\n\n[[4]]$error\nNULL\n\n\n[[5]]\n[[5]]$result\n[1] 15\n\n[[5]]$error\nNULL",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "`Safe` error passing in loops"
    ]
  },
  {
    "objectID": "small_helpers/git_remote_and_emails.html",
    "href": "small_helpers/git_remote_and_emails.html",
    "title": "Change git remote and edit emails",
    "section": "",
    "text": "I’ve recently had a bunch of git repos lose their remotes. It’s unclear why, but rather than look the solution up every time on stackexchange, do this to see what the remote is\ngit remote -v\nThen\ngit remote set-url origin path/to/remote.git\nthen\ngit remote -v\nto check it worked.\nI’ve also moved some repos from other remotes to github lately, and keep getting errors about publishing private emails. One option would be to make emails public, but the better option is to make sure the user email is the github one, e.g.\ngit config --global user.email\nThere was a way I did this before that was simpler, but what seemed to work (thanks stackoverflow) was to install git-filter-repo\npip3 install git-filter-repo  \nI had to use the one-line option in the comments (likely because of windows)\ngit filter-repo --force --email-callback \"    return email if email != b'wrong@email' else b'correct@email'\"\nIt said it hung, but ended up working. And retained dates, which lots of other options didn’t.\nPresumably this would work on unix\ngit filter-repo --email-callback '\n    return email if email != b\"incorrect@email\" else b\"correct@email\"\n'",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Changing git remote and private email"
    ]
  },
  {
    "objectID": "small_helpers/lifecycle_warnings.html",
    "href": "small_helpers/lifecycle_warnings.html",
    "title": "Intermittent warnings",
    "section": "",
    "text": "I’ve been dealing with debugging a package with a dplyr warning that only appears every 8 hours with the message\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nIt causes warnings every time I test the whole package with devtools::test(), but only every 8 hours otherwise, which makes tracing and debugging impossible.\n{rlang} provides a way to set {lifecycle} warning frequencies (docs) so I set in my test script\n\nrlang::local_options(lifecycle_verbosity = \"warning\")\n\nI think this should do it too\n\noptions(lifecycle_verbosity = \"warning\")\n\nSometimes those settings aren’t enough - they tend to work with devtools::test, but not for interactively running code and trying to use the debugger to find the issue. In those cases, it seems to work to set the verbosity to ‘error’\n\nrlang::local_options(lifecycle_verbosity = \"error\")\n# or\noptions(lifecycle_verbosity = \"error\")",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Intermittent lifecycle warnings"
    ]
  },
  {
    "objectID": "small_helpers/package_dll_error.html",
    "href": "small_helpers/package_dll_error.html",
    "title": "Package DLL not found",
    "section": "",
    "text": "Since upgrading Rstudio, I keep getting this error:\nError in library.dynam(lib, package, package.lib) :    DLL ‘testthat’ not found: maybe not installed for this architecture?\nIt’s intermittent- I could use testthat the day before. And it seems to hit packages randomly. Very frustrating. The only solution seems to be to use\n\nrenv::paths$cache()\n\n[1] \"C:\\\\Users\\\\galen\\\\AppData\\\\Local/R/cache/R/renv/cache/v5/windows/R-4.4/x86_64-w64-mingw32\"\n\n\nto find the cache, delete that package, and reinstall it.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Package DLL error"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html",
    "href": "small_helpers/quarto_notes.html",
    "title": "Quarto notes",
    "section": "",
    "text": "library(ggplot2)",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html#quarto-root-directory",
    "href": "small_helpers/quarto_notes.html#quarto-root-directory",
    "title": "Quarto notes",
    "section": "Quarto root directory",
    "text": "Quarto root directory\nIf there’s not a Quarto project (which is not the same as an Rproject), i.e. the .qmds are standalone, then the above rmarkdown method works to set the root to the Rproject. But if there is a Quarto project (I’ve moved to almost always doing this), then we can set the Quarto root directory in the _quarto.yml for the project. There are actually two useful settings we can make there- a project-wide output directory, and what to use to execute. There are (to my knowledge) two options for execute-dir- execute-dir: file and execute-dir: project, which set the root for rendering as the file location or the project. It almost always makes most sense to use project, because then everything uses the same reference for relative paths. For this website, that yaml is\n\nproject:\n  type: website\n  output-dir: docs\n  execute-dir: project\n\nLinking pages\nThis is fairly specific to websites (and I guess books?). Sorted out in the website-specific page. Takehome is links need to be relative to the file. And so those in different directories often have to use ../other_dir/other_file.qmd to get up and over.\n\n\nNested Rprojects\nThe only exception that I’ve run into so far is a weird situation where I have a Quarto project with an Rproject in a subdirectory because I want the Quarto to have access to several different code projects. The catch is that if I use paths relative to the R project and ask Quarto to render, all the paths are wrong. They are also wrong if I Run or run the code cells interactively. If I add the Rmarkdown setup chunk above knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()), then Run and interactive stuff works, but when I try to quarto render, it can’t find the Rproject because it’s looking at the same level and up, and the Rproj is in a directory down.\nSo, the workaround I’ve come up with is to set execute-dir: file, so the dir is set to the file dir, which is inside the Rproject. Then, use an Rmarkdown-style setup chunk with knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) to reset the directory to the Rproject root dir.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nThen the code both Runs and renders, and we don’t have to keep track of paths relative to two different nested projects manually. This approach works if the internal projects are self-contained (and we often want them to be). But if we need to render something across several, we’ll need to do something different.\nOther options that might work but I haven’t tried are parameterised notebooks (with conditional params?) Or using freeze and virtual environments (see docs)\nWhat I really want is a way to set the execute-dir on a file-by-file basis in the header yaml, but that doesn’t seem to work.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html#headers-in-vs-code-vs-rstudio",
    "href": "small_helpers/quarto_notes.html#headers-in-vs-code-vs-rstudio",
    "title": "Quarto notes",
    "section": "Headers in VS code vs Rstudio",
    "text": "Headers in VS code vs Rstudio\nI had a few notebooks that ran fine in Rstudio, but wouldn’t render in VS (or from command line). I got the very cryptic error YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key with a line and col number that didn’t seem to correspond to anything with YAML in the project- not the file header, not the _quarto.yml, not _quarto.yaml.local, and nothing particularly useful showed up on google.\nThe solution seems to be to add an explicit line for editor: visual in the file header, e.g.\ntitle: Test\nauthor: Galen Holt\nformat:\n  html:\n    df-print: paged\neditor: visual\nIt doesn’t seem to matter if the title and author are wrapped in double-quotes (which was one suggestion online).\nI think what’s happening is that Rstudio’s visual editor is adding some hidden formatting syntax that gets exposed and looks like YAML to VS and the quarto CLI.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html#python-paths",
    "href": "small_helpers/quarto_notes.html#python-paths",
    "title": "Quarto notes",
    "section": "Python paths",
    "text": "Python paths\nIf we use {reticulate}, R wants to know where the python lives, and gets grumpy if it can’t find it. Sometimes it silently points somewhere we don’t want, and other times we see errors like\nError creating conda environment 'C:/Users/galen/Documents/Website/galen_website/renv/python/r-reticulate' [exit code 1]\n\nRprofile\nThe main solution is to edit .Rprofile to set the RETICULATE_PYTHON environment variable,\nSys.setenv(RETICULATE_PYTHON = '../werptoolkitpy/.venv/Scripts/python.exe')\nNote that this can either be a full path, or relative to the R project directory- in the situation above, I have the R project nested in a Quarto project that also contains a py project, so we need to go up and over to get to the .venv.\nThis is needed to get any of the reticulate code to work (though you can set it on a file-by-file basis.\nAdditional issues can come up with Quarto though.\n\n\nQuarto environment\nWhen we run Quarto inside Rstudio in parallel with an R project, everything works fine. But, if Quarto is run through VScode, for some reason it doesn’t hit the .Rprofile and so doesn’t run the line we just added, and we’re back to using the wrong python and errors about conda. To make it more complex, Quarto has its own python environment variable QUARTO_PYTHON. This is all more complicated when the Quarto project directory doesn’t match the R project directory.\nThe solution is to set up a _environment file in the Quarto project directory to set those variables according to the docs (and maybe PY_PYTHON too, just to be safe- I can’t find the docs to know how these differ). For my case, my _environment file looks like\nRETICULATE_PYTHON='../werptoolkitpy/.venv/Scripts/python.exe'\nQUARTO_PYTHON='werptoolkitpy/.venv/Scripts/python.exe'\nPY_PYTHON='werptoolkitpy/.venv/Scripts/python.exe'\nHere, again, I have Rproject nested in Quarto proj, but they access this file differently so the paths differ even though they point to the same place. RETICULATE_PYTHON needs to get up and out of the R subdir and over to the py side of things, while the Quarto project is the outer dir and so QUARTO_PYTHON can go straight in to the py subdir.\nAs far as I can tell, this does NOT supersede the necessity of setting .Rprofile, which is needed for the R code to run. This stuff in _environment is in addition so Quarto (especially in VS) can access the right info.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html#table-printing",
    "href": "small_helpers/quarto_notes.html#table-printing",
    "title": "Quarto notes",
    "section": "Table printing",
    "text": "Table printing\n\nStyle\nUse the yaml header to declare style, one of paged, kable, tibble, and default as in documentation. In a twist, default seems to be the way to do some customisation using S3, see https://debruine.github.io/quarto_demo/table.html, though I haven’t played with that.\nSo, typically the yaml header would be something like\nformat:\n  html:\n    df-print: paged\nCan I change that for a single chunk? Work on that later. Putting it in as #| df-print: option isn’t recognized.\n\n\nRow number\nThe different df-print options have different defaults of how much they print (and with paged it doesn’t matter so much). As far as I can tell, tibble prints the whole thing, and kable prints 10 rows. Sometimes we want to control that though - maybe we have a df with 13 rows, and we want to just print the whole thing.\nThe default is 10, though that’s not working when I render to the web for some reason\n\n```{r}\niris\n```\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\n\nTo print more rows, we can use the rows.print option. NOTE- this used to work, and now does not. Will need to sort out a new solution.\n\n```{r}\n#| rows.print: 15\n\niris\n```\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\n\n\n\nFigure captions with variables\nSometimes we want figure captions to auto-update, e.g. maybe we want them to provide some parameter values. In that case, we can provide !expr, for example\n\nparam_val &lt;- 17\n\n\n```{r}\n#| fig-cap: !expr glue::glue(\"Iris petals multiplied by {param_val}\")\n\nggplot(iris, aes(x = Petal.Length * param_val, y = Petal.Width * param_val, color = Species)) + geom_point()\n```\n\n\n\n\nIris petals multiplied by 17\n\n\n\n\nThat’s a bit annoying that we have to build the whole cap as an R expression, but we should be able to use glue, paste, or whatever syntax (will need to sort something out for math and greek, maybe expression or bquote? latex2exp isn’t just working out of the box).\n\n```{r}\n#| fig-cap: !expr latex2exp::TeX(\"$\\\\alpha$\")\n\nggplot(iris, aes(x = Petal.Length * param_val, y = Petal.Width * param_val, color = Species)) + geom_point()\n```\n\n\n\n\nalpha",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html#website",
    "href": "small_helpers/quarto_notes.html#website",
    "title": "Quarto notes",
    "section": "Website",
    "text": "Website\nStarting and updating quarto websites has a few tricks, written up in more detail at those links.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html#rendering-1",
    "href": "small_helpers/quarto_notes.html#rendering-1",
    "title": "Quarto notes",
    "section": "Rendering 1",
    "text": "Rendering 1\nThe error\nError: The process cannot access the file because it is being used by another process. (os error 32),\nSeems to be due to dropbox or onedrive even when they are not connected to the folder with the code. Quit them, and it usually goes away. A restart is sometimes required.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html#rendering-2",
    "href": "small_helpers/quarto_notes.html#rendering-2",
    "title": "Quarto notes",
    "section": "Rendering 2",
    "text": "Rendering 2\nThe error\nError in `lazyLoadDBinsertVariable()`: ! long vectors not supported yet: connections.c:6201\nis caused by trying to read-in an object too big for a lazy cache. It often only occurs when rendering, not previewing or running the qmd. The solution is to put cache-lazy: false in the chunk header of the offending chunk. Make it go slower, but it goes.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html#quarto-for-github-readmes",
    "href": "small_helpers/quarto_notes.html#quarto-for-github-readmes",
    "title": "Quarto notes",
    "section": "Quarto for github readmes",
    "text": "Quarto for github readmes\nIf we want to use quarto for github readmes, see the quarto docs. The key is to use format: gfm and then render before pushing to generate the github flavored .md file.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/quarto_notes.html#styling-quarto",
    "href": "small_helpers/quarto_notes.html#styling-quarto",
    "title": "Quarto notes",
    "section": "Styling quarto",
    "text": "Styling quarto\nThe [styler](https://styler.r-lib.org/) package will style code in quarto, which is very useful. But it seems not to work well in visual mode. It repeatedly won’t change indents if I have the document in Visual, and then works great once I shift to Source. It does seem to work if I use styler::style_dir(), but that always makes me nervous.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Quarto notes"
    ]
  },
  {
    "objectID": "small_helpers/smallpieces.html",
    "href": "small_helpers/smallpieces.html",
    "title": "Small pieces",
    "section": "",
    "text": "This is mostly quick little code snippets to copy-paste and avoid re-writing. load tidyverse and get going.\n\nlibrary(tidyverse)\n\n\n\nWe often want to set the root directory not to the file but to the project. In Rmarkdown, we use the following in the setup chunk. Quarto typically uses a different method, but see the Quarto notes for some exceptions. Converting from Rmarkdown to quarto with knitr::convert_chunk_header kills this block, and it’s annoying to always have the header. In both Rmarkdown and Quarto, this has to be in a setup chunk.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nI thought it’d be easiest to set in the global options, but that doesn’t seem to persist to render or knit.\n\n\n\n\n\nnewdir &lt;- file.path('output', 'testdir')\nif (!dir.exists(newdir)) {dir.create(newdir, recursive = TRUE)}\n\n\n\n\nWindows paths come in with \\, which R treats as an escape character. We can use file.path to just avoid them, or replace them with / or \\\\. But sometimes we just want to paste a path in quickly and be done with it. As of R 4.0, we can do that with r. It requires the parentheses to be in a funny place- inside the quotes.\n\npastepath &lt;- r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\"\npastepath\n\n[1] \"C:\\\\Users\\\\username\\\\path\\\\to\\\\somewhere.csv\"\n\n\nAnd we can feed that straight into functions that need paths, eg.\n\nreadr::read_csv(r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\")\n\n\n\n\nFunctions like duplicated give the second (and greater) values that match. e.g.\n\nx &lt;- c(1,2,1,3,4,2)\nduplicated(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nBut we often want to grab all values that are repeated- ie if everything matches in one column what’s going on in the others. do do that we can use group_by and filter to get those with &gt; 1 row.\nIE, let’s compare cars with duplicated mpg values\n\nmtcars %&gt;%\n  dplyr::group_by(mpg) %&gt;%\n  dplyr::filter(n() &gt; 1) %&gt;%\n  dplyr::arrange(mpg) # makes the comparisons easier\n\n\n  \n\n\n\nWhy is that useful? We can see not only that these aren’t fully duplicated rows (which we also could have done with duplicated on the whole table), but also actually look at what differs easily.\n\n\nIf we do want to look at fully duplicated rows, we can use a similar approach, we just have to group_by everything. Duplicating three rows to demonstrate:\n\nmtcars |&gt; \n  # bind on some duplicates\n  dplyr::bind_rows(mtcars |&gt; dplyr::slice(c(1,9,12))) |&gt;\n  dplyr::group_by(dplyr::across(tidyselect::everything())) |&gt; \n  dplyr::filter(dplyr::n()&gt;1) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::arrange(dplyr::across(tidyselect::everything()))\n\n\n  \n\n\n\n\n\n\nWe might have a list with internal duplicates, e.g.\n\nduplist &lt;- list(a = c('a', 'b'), b = c('b', 'c'), d = c('a', 'c'), e = c('f', 'g'), f = c('f', 'h'), g = c('a', 'l'))\nduplist\n\n$a\n[1] \"a\" \"b\"\n\n$b\n[1] \"b\" \"c\"\n\n$d\n[1] \"a\" \"c\"\n\n$e\n[1] \"f\" \"g\"\n\n$f\n[1] \"f\" \"h\"\n\n$g\n[1] \"a\" \"l\"\n\n\nWe can see which values in the first position are duplicated, but again, not the first instances.\n\nthedups &lt;- duplist[duplicated(purrr::map_chr(duplist, \\(x) x[1]))] |&gt;\n      purrr::map_chr(\\(x) x[1])\nthedups\n\n  d   f   g \n\"a\" \"f\" \"a\" \n\n\nWe can get all of them by mapping whether the first value is in thedups and then dropping empties\n\nall_duplicated &lt;- purrr::map(duplist, \\(x) x[x[1] %in% thedups]) |&gt; \n  purrr::discard(\\(x) length(x) == 0)\nall_duplicated\n\n$a\n[1] \"a\" \"b\"\n\n$d\n[1] \"a\" \"c\"\n\n$e\n[1] \"f\" \"g\"\n\n$f\n[1] \"f\" \"h\"\n\n$g\n[1] \"a\" \"l\"\n\n\n\n\n\n\nSometimes with long csvs, readr’s guess of col type based on the first thousand rows is wrong. But only for some cols. If we want to not have to specify all of them, we can use .default and only specify the offending col.\nFirst, save dummy data\n\ndumtib &lt;- tibble(c1 = 1:3000, c2 = rep(letters, length.out = 3000), c3 = c(c1[1:2000], c2[2001:3000]))\n\nwrite_csv(dumtib, file = file.path(newdir, 'colspectest.csv'))\n\nIf we read in without the cols, it assumes c3 is numeric and we get errors. But it doesn’t. why not? It keeps getting me elsewhere, but now I can’t create the problem. FIgure this out later, I guess\n\nfilein &lt;- read_csv(file.path(newdir, 'colspectest.csv'), guess_max = 100)\n\nRows: 3000 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): c2, c3\ndbl (1): c1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTell it the third col is character.\n\nfilein &lt;- readr::read_csv(file.path(newdir, 'colspectest.csv'), col_types = cols(.default = \"?\", c3 = \"c\"))\n\n\n\n\nYes, we should be building as a library in this case, but it’s often easier at least initially to not deal with the overhead. If, for example, all functions are in the ‘functions’ directory,\n\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galenR\n\n\n\n\n\nRender in quarto defaults to making dfs text, and so often we can’t see all the columns (or rows), or access them. setting the df-print option to paged allows them to work. The header should look like this (commented out because this isn’t a header)\n\n# title: \"TITLE\"\n# author: \"AUTHOR\"\n# format:\n#   html:\n#     df-print: paged\n\n\n\n\nconvert_chunk_headers is the main thing, but I want to apply it to a full directory. Let’s get the dir for here.\n\nallrmd &lt;- list.files(rprojroot::find_rstudio_root_file(), pattern = '.Rmd', recursive = TRUE, full.names = TRUE)\n\nallrmd &lt;- allrmd[!stringr::str_detect(allrmd, 'renv')]\n\nallqmd &lt;- stringr::str_replace(allrmd, '.Rmd', '.qmd')\n\nCan I vectorize? No, but a loop works. Git commit first!\n\nfor (i in 1:length(allrmd)) {\n  knitr::convert_chunk_header(input = allrmd[i], output = allqmd[i])\n}\n\nNow, if you want to really go for it, delete the rmds. That makes git happier because then it can treat this as a rename and keep tracking the files.\nDangerous- make sure you’ve git-committed. I’m commenting out and eval: false ing this\n\n# file.remove(allrmd)",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Bits and pieces"
    ]
  },
  {
    "objectID": "small_helpers/smallpieces.html#what-is-this",
    "href": "small_helpers/smallpieces.html#what-is-this",
    "title": "Small pieces",
    "section": "",
    "text": "This is mostly quick little code snippets to copy-paste and avoid re-writing. load tidyverse and get going.\n\nlibrary(tidyverse)\n\n\n\nWe often want to set the root directory not to the file but to the project. In Rmarkdown, we use the following in the setup chunk. Quarto typically uses a different method, but see the Quarto notes for some exceptions. Converting from Rmarkdown to quarto with knitr::convert_chunk_header kills this block, and it’s annoying to always have the header. In both Rmarkdown and Quarto, this has to be in a setup chunk.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nI thought it’d be easiest to set in the global options, but that doesn’t seem to persist to render or knit.\n\n\n\n\n\nnewdir &lt;- file.path('output', 'testdir')\nif (!dir.exists(newdir)) {dir.create(newdir, recursive = TRUE)}\n\n\n\n\nWindows paths come in with \\, which R treats as an escape character. We can use file.path to just avoid them, or replace them with / or \\\\. But sometimes we just want to paste a path in quickly and be done with it. As of R 4.0, we can do that with r. It requires the parentheses to be in a funny place- inside the quotes.\n\npastepath &lt;- r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\"\npastepath\n\n[1] \"C:\\\\Users\\\\username\\\\path\\\\to\\\\somewhere.csv\"\n\n\nAnd we can feed that straight into functions that need paths, eg.\n\nreadr::read_csv(r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\")\n\n\n\n\nFunctions like duplicated give the second (and greater) values that match. e.g.\n\nx &lt;- c(1,2,1,3,4,2)\nduplicated(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nBut we often want to grab all values that are repeated- ie if everything matches in one column what’s going on in the others. do do that we can use group_by and filter to get those with &gt; 1 row.\nIE, let’s compare cars with duplicated mpg values\n\nmtcars %&gt;%\n  dplyr::group_by(mpg) %&gt;%\n  dplyr::filter(n() &gt; 1) %&gt;%\n  dplyr::arrange(mpg) # makes the comparisons easier\n\n\n  \n\n\n\nWhy is that useful? We can see not only that these aren’t fully duplicated rows (which we also could have done with duplicated on the whole table), but also actually look at what differs easily.\n\n\nIf we do want to look at fully duplicated rows, we can use a similar approach, we just have to group_by everything. Duplicating three rows to demonstrate:\n\nmtcars |&gt; \n  # bind on some duplicates\n  dplyr::bind_rows(mtcars |&gt; dplyr::slice(c(1,9,12))) |&gt;\n  dplyr::group_by(dplyr::across(tidyselect::everything())) |&gt; \n  dplyr::filter(dplyr::n()&gt;1) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::arrange(dplyr::across(tidyselect::everything()))\n\n\n  \n\n\n\n\n\n\nWe might have a list with internal duplicates, e.g.\n\nduplist &lt;- list(a = c('a', 'b'), b = c('b', 'c'), d = c('a', 'c'), e = c('f', 'g'), f = c('f', 'h'), g = c('a', 'l'))\nduplist\n\n$a\n[1] \"a\" \"b\"\n\n$b\n[1] \"b\" \"c\"\n\n$d\n[1] \"a\" \"c\"\n\n$e\n[1] \"f\" \"g\"\n\n$f\n[1] \"f\" \"h\"\n\n$g\n[1] \"a\" \"l\"\n\n\nWe can see which values in the first position are duplicated, but again, not the first instances.\n\nthedups &lt;- duplist[duplicated(purrr::map_chr(duplist, \\(x) x[1]))] |&gt;\n      purrr::map_chr(\\(x) x[1])\nthedups\n\n  d   f   g \n\"a\" \"f\" \"a\" \n\n\nWe can get all of them by mapping whether the first value is in thedups and then dropping empties\n\nall_duplicated &lt;- purrr::map(duplist, \\(x) x[x[1] %in% thedups]) |&gt; \n  purrr::discard(\\(x) length(x) == 0)\nall_duplicated\n\n$a\n[1] \"a\" \"b\"\n\n$d\n[1] \"a\" \"c\"\n\n$e\n[1] \"f\" \"g\"\n\n$f\n[1] \"f\" \"h\"\n\n$g\n[1] \"a\" \"l\"\n\n\n\n\n\n\nSometimes with long csvs, readr’s guess of col type based on the first thousand rows is wrong. But only for some cols. If we want to not have to specify all of them, we can use .default and only specify the offending col.\nFirst, save dummy data\n\ndumtib &lt;- tibble(c1 = 1:3000, c2 = rep(letters, length.out = 3000), c3 = c(c1[1:2000], c2[2001:3000]))\n\nwrite_csv(dumtib, file = file.path(newdir, 'colspectest.csv'))\n\nIf we read in without the cols, it assumes c3 is numeric and we get errors. But it doesn’t. why not? It keeps getting me elsewhere, but now I can’t create the problem. FIgure this out later, I guess\n\nfilein &lt;- read_csv(file.path(newdir, 'colspectest.csv'), guess_max = 100)\n\nRows: 3000 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): c2, c3\ndbl (1): c1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTell it the third col is character.\n\nfilein &lt;- readr::read_csv(file.path(newdir, 'colspectest.csv'), col_types = cols(.default = \"?\", c3 = \"c\"))\n\n\n\n\nYes, we should be building as a library in this case, but it’s often easier at least initially to not deal with the overhead. If, for example, all functions are in the ‘functions’ directory,\n\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galenR\n\n\n\n\n\nRender in quarto defaults to making dfs text, and so often we can’t see all the columns (or rows), or access them. setting the df-print option to paged allows them to work. The header should look like this (commented out because this isn’t a header)\n\n# title: \"TITLE\"\n# author: \"AUTHOR\"\n# format:\n#   html:\n#     df-print: paged\n\n\n\n\nconvert_chunk_headers is the main thing, but I want to apply it to a full directory. Let’s get the dir for here.\n\nallrmd &lt;- list.files(rprojroot::find_rstudio_root_file(), pattern = '.Rmd', recursive = TRUE, full.names = TRUE)\n\nallrmd &lt;- allrmd[!stringr::str_detect(allrmd, 'renv')]\n\nallqmd &lt;- stringr::str_replace(allrmd, '.Rmd', '.qmd')\n\nCan I vectorize? No, but a loop works. Git commit first!\n\nfor (i in 1:length(allrmd)) {\n  knitr::convert_chunk_header(input = allrmd[i], output = allqmd[i])\n}\n\nNow, if you want to really go for it, delete the rmds. That makes git happier because then it can treat this as a rename and keep tracking the files.\nDangerous- make sure you’ve git-committed. I’m commenting out and eval: false ing this\n\n# file.remove(allrmd)",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Bits and pieces"
    ]
  },
  {
    "objectID": "small_helpers/smallpieces.html#debugging-r-from-quarto",
    "href": "small_helpers/smallpieces.html#debugging-r-from-quarto",
    "title": "Small pieces",
    "section": "Debugging R from Quarto",
    "text": "Debugging R from Quarto\nSometimes (often) if you call a function with a breakpoint from a Quarto notebook, nothing prints to the console. I.e. I’ll type a variable name var_a to see what it is, and nothing will print. The issue is that values are going to a different stdout.\nType sink() and it should start working.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Bits and pieces"
    ]
  },
  {
    "objectID": "small_helpers/zip_downloading.html",
    "href": "small_helpers/zip_downloading.html",
    "title": "Download Zip helper",
    "section": "",
    "text": "When we download files from the internet, we often feed in a url, and it returns a zip, which we then want to unzip to access. There’s a fairly simple way to do that, but we can write a quicky function to do it and clean up the directory afterwards.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Download and unpack zips"
    ]
  },
  {
    "objectID": "small_helpers/zip_downloading.html#the-issue",
    "href": "small_helpers/zip_downloading.html#the-issue",
    "title": "Download Zip helper",
    "section": "",
    "text": "When we download files from the internet, we often feed in a url, and it returns a zip, which we then want to unzip to access. There’s a fairly simple way to do that, but we can write a quicky function to do it and clean up the directory afterwards.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Download and unpack zips"
    ]
  },
  {
    "objectID": "small_helpers/zip_downloading.html#the-function",
    "href": "small_helpers/zip_downloading.html#the-function",
    "title": "Download Zip helper",
    "section": "The function",
    "text": "The function\nwe want to give it the dirname for the file(s), the datadir that contains our data, and the URL. Then it checks if it exists, and downloads, unzips, and cleans up.\n\nzip_load &lt;- function(dirname, datadir, sourceurl,  \n                      existing_dirs = list.files(datadir)) {\n  print(existing_dirs)\n  if (!(dirname %in% existing_dirs)) {\n    \n    zippath &lt;- file.path(datadir, paste0(dirname, '.zip'))\n    download.file(sourceurl, destfile = zippath)\n    \n    unzip(zippath, exdir = file.path(datadir, dirname))\n    \n    file.remove(zippath)\n  }\n}",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Download and unpack zips"
    ]
  },
  {
    "objectID": "small_helpers/zip_downloading.html#an-example",
    "href": "small_helpers/zip_downloading.html#an-example",
    "title": "Download Zip helper",
    "section": "An example",
    "text": "An example\nGet the Murray-Darling basin boundary\n\nzip_load('mdb_boundary', 'data', \"https://data.gov.au/data/dataset/4ede9aed-5620-47db-a72b-0b3aa0a3ced0/resource/8a6d889d-723b-492d-8c12-b8b0d1ba4b5a/download/sworkingadhocjobsj4430dataoutputsmdb_boundarymdb_boundary.zip\")\n\n [1] \"42343_shp\"                              \n [2] \"ANAE_Rivers_v3_23mar2021\"               \n [3] \"ANAE_Wetlands_v3_24mar2021\"             \n [4] \"DATA_362071\"                            \n [5] \"geofabric\"                              \n [6] \"MDB_ANAE_Aug2017\"                       \n [7] \"mdb_boundary\"                           \n [8] \"negbin_testing\"                         \n [9] \"pix4dout\"                               \n[10] \"Surface Water Water Resource Plan Areas\"\n\n\nI’ve saved this in functions/ so I have easy access to it everywhere.",
    "crumbs": [
      "Code Demos",
      "Small how-tos",
      "Download and unpack zips"
    ]
  },
  {
    "objectID": "stats_probability/betadists.html",
    "href": "stats_probability/betadists.html",
    "title": "Beta distributions",
    "section": "",
    "text": "This is fairly sketchy, just trying to see how the \\(\\phi\\) from Harrison (2015) affect beta probability distributions.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\np &lt;- seq(from = 0, to = 1, by = 0.01)\nphi &lt;- seq(from = 0.1, to = 1, by = 0.1)\n\n\n# ai &lt;- p[1]/phi[1]\n# bi &lt;- (1-p[1])/phi[1]\n# \n# db &lt;- dbeta(p, ai, bi)\n\nprobdists &lt;- tibble(p = NA, db = NA, betap = NA, phi = NA, .rows = 0)\n\nfor (j in 1:length(phi)) {\n  for (i in 1:length(p)) {\n  ai &lt;- p[i]/phi[j]\n  bi &lt;- (1-p[i])/phi[j]\n  \n  db &lt;- dbeta(p, ai, bi)\n  \n  dbt &lt;- tibble(p, db, betap = p[i], phi = phi[j])\n  probdists &lt;- rbind(probdists, dbt)\n}\n}\n\n\nggplot(probdists |&gt; filter(betap %in% c(0.1, 0.25, 0.5, 0.75, 0.9)), \n       aes(x = p, y = db, color = factor(betap))) + geom_line() +\n  facet_wrap(\"phi\")\n\n\n\n\n\n\n\n\n\ntestx &lt;- seq(from = 0, to = 1, by = 0.01)\ntestbetaprob &lt;- dbeta(testx, ai, bi)\nprobdist &lt;- tibble(testx, testbetaprob)\n\nggplot(probdist, aes(x = testx, y = testbetaprob)) + geom_line()\n\n\n\n\n\n\n\n\nRandom draws\n\nintercept &lt;- -2\nbeta &lt;- 0.5\nall_x &lt;- seq(0, 10, 0.1)\nall_logit_p &lt;- intercept + beta*all_x\n\nall_p &lt;- 1/(1+exp(-all_logit_p))\n\n\nplot(all_x, all_p, type = 'l')\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHarrison, Xavier A. 2015. “A Comparison of Observation-Level Random Effect and Beta-Binomial Models for Modelling Overdispersion in Binomial Data in Ecology & Evolution.” PeerJ 3 (July): e1114. https://doi.org/10.7717/peerj.1114.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Quick look at beta pdfs"
    ]
  },
  {
    "objectID": "stats_probability/fitting_lognormals.html",
    "href": "stats_probability/fitting_lognormals.html",
    "title": "Fitting lognormals",
    "section": "",
    "text": "I need to fit some truncated lognormals, and want to work through how to do that with known distributions. Ignoring the truncated nature, I’m also getting weird outcomes where if I fit the data as lognormal, the distribution is way off, but when I manually log and then fit a normal, it’s fine.\nI’ll use fitdistr from MASS, but may move on.\nlibrary(MASS)\nlibrary(tidyverse)",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Intro to fitting lognormals"
    ]
  },
  {
    "objectID": "stats_probability/fitting_lognormals.html#the-data",
    "href": "stats_probability/fitting_lognormals.html#the-data",
    "title": "Fitting lognormals",
    "section": "The data",
    "text": "The data\nRather than use the real data, at least at the outset, I’ll generate data with known distribution. I’ll make it somewhere off the standard normal.\n\ntestdata &lt;- rlnorm(10000, meanlog = 5, sdlog = 1.5)\n\nDoes that look right? Yes, at least nothing obviously wrong.\n\nggplot(tibble(testdata), aes(x = testdata)) + geom_density()\n\n\n\n\n\n\n\nggplot(tibble(testdata), aes(x = log(testdata))) + geom_density()",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Intro to fitting lognormals"
    ]
  },
  {
    "objectID": "stats_probability/fitting_lognormals.html#fitting",
    "href": "stats_probability/fitting_lognormals.html#fitting",
    "title": "Fitting lognormals",
    "section": "Fitting",
    "text": "Fitting\nI’ll fit the raw data using lognormal, and the logged data using normal, then plot the pdfs and cdfs\n\nfit_log &lt;- fitdistr(testdata, densfun = 'lognormal')\nfit_n &lt;- fitdistr(log(testdata), densfun = 'normal')\n\nfit_log\n\n    meanlog       sdlog   \n  5.00313634   1.50099256 \n (0.01500993) (0.01061362)\n\nfit_n\n\n      mean          sd    \n  5.00313634   1.50099256 \n (0.01500993) (0.01061362)\n\n\nThat is clearly working the same, so that’s good.\nNow let’s make some plots of the CDF and PDF of the fit curves, along with the empirical fits.\nWe need to make some dataframes. Ideally we’d combine them and pivot_longer, but I’m not going to bother.\n\ndf_log &lt;- tibble(x = 0:10000, \n                 cdf = plnorm(x, fit_log$estimate[1], fit_log$estimate[2]),\n                 pdf = dlnorm(x, fit_log$estimate[1], fit_log$estimate[2]))\n\ndf_n &lt;- tibble(x = seq(0,10, by = 0.01), \n                 cdf = pnorm(x, fit_n$estimate[1], fit_n$estimate[2]),\n                 pdf = dnorm(x, fit_n$estimate[1], fit_n$estimate[2]))\n\nPlot the fits done with *lnorm on the raw data-\nFirst, pdf on the linear scale. Let’s zoom in, too. Use xlim instead of coord_cartesian or it doesn’t have enough points in the geom_density. That seems a bit shifted.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = testdata), color = 'black') +\n  geom_line(data = df_log, aes(x = x, y = pdf), color = 'firebrick') +\n  xlim(c(-1, 1000))\n\nWarning: Removed 1055 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 9000 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nPDF on the log scale (but this is still the fit done without pre-logging the data. This is dramatically shifted down. Same thing happened with my real data, so I’m trying to understand why. The mean should be at 4.97, and this clearly isn’t.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_log, aes(x = log(x), y = pdf*50), color = 'firebrick')\n\n\n\n\n\n\n\n\nPDF of the pre-logged and fit as normal. That fits great. So, why is there a shift in the lognormal? They’re using the same functions and parameters.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_n, aes(x = x, y = pdf), color = 'firebrick')\n\n\n\n\n\n\n\n\nIs the weirdness with the log possibly because of some strange discrepancy with the testdata vs evenly-spaced x? It shouldn’t be- it just gets P(x=X) at each x. But using testdata as x and getting the fit should remove that as an issue and focus just on the fit.\n\ndf_log2 &lt;- tibble(x = testdata, \n                 cdf = plnorm(x, fit_log$estimate[1], fit_log$estimate[2]),\n                 pdf = dlnorm(x, fit_log$estimate[1], fit_log$estimate[2]))\n\nSame issue.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_log2, aes(x = log(x), y = pdf*50), color = 'firebrick')\n\n\n\n\n\n\n\n\nDo we see this issue in the CDFs (plnorm)? or is it really just an issue with dlnorm? This doesn’t look obviously shifted down 2.\n\nggplot() + \n  stat_ecdf(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_log, aes(x = log(x), y = cdf), color = 'firebrick')\n\n\n\n\n\n\n\n\nWould we notice if it were shifted down? Build one on the normal scale where we know the parameters are working how we think. That is behaving how it should. SO. Why is dlnorm not producing the densities we expect? Especially when plnorm does produce the CDFs we expect?\n\ndf_n2 &lt;- tibble(x = seq(0,10, by = 0.01), \n                 cdf = pnorm(x, 2.5, fit_n$estimate[2]),\n                 pdf = dnorm(x, 2.5, fit_n$estimate[2]))\n\nggplot() + \n  stat_ecdf(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_log, aes(x = log(x), y = cdf), color = 'firebrick') +\n  geom_line(data = df_n2, aes(x = x, y = cdf), color = 'dodgerblue')\n\n\n\n\n\n\n\n\nWhat if we go back to the linear scale- would we see a shift there? e.g. is the issue in the translation from linear PDF to log, and I’m forgetting something about calculations around f(g(x))? Now, we use the df_log, but change the mean. This time I’ll shift UP to try to see if the resulting blue line gets closer than the red line. Nope. With that amount of shift, the fit is obviously different (and much worse). I had a bit of a play, and the closest I can get is with a mean of about 5.1, but this is only part of the distribution, so the values from the fit do seem to be getting the whole PDF as close as possible on this scale.\n\ndf_log2 &lt;- tibble(x = 0:10000, \n                 cdf = plnorm(x, 7.5, fit_log$estimate[2]),\n                 pdf = dlnorm(x, 7.5, fit_log$estimate[2]))\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = testdata), color = 'black') +\n  geom_line(data = df_log, aes(x = x, y = pdf), color = 'firebrick') +\n  geom_line(data = df_log2, aes(x = x, y = pdf), color = 'dodgerblue') + \n xlim(c(0, 1000))\n\nWarning: Removed 1055 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 9000 rows containing missing values or values outside the scale range\n(`geom_line()`).\nRemoved 9000 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe pre-logged and fit as normal works, as we expect.\n\nggplot() + \n  stat_ecdf(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_n, aes(x = x, y = cdf), color = 'firebrick')\n\n\n\n\n\n\n\n\nI think I’m being naive in my translation because I’m trying to get the PDF on a transformed variable, and so not accounting for nonlinear spacing. E.g. the f(g(x)) issue.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Intro to fitting lognormals"
    ]
  },
  {
    "objectID": "stats_probability/fitting_lognormals.html#random-numbers",
    "href": "stats_probability/fitting_lognormals.html#random-numbers",
    "title": "Fitting lognormals",
    "section": "Random numbers",
    "text": "Random numbers\nIf I find random numbers from rlnorm or rnorm, do they match the distribution? In both directions.\n\ndf_rand &lt;- tibble(r_lnorm = rlnorm(10000, \n                                   fit_log$estimate[1], fit_log$estimate[2]),\n                  r_norm = rnorm(10000,\n                                 fit_n$estimate[1], fit_n$estimate[2]),\n                  r_lnormlog = log(r_lnorm),\n                  r_normexp = exp(r_norm))\n\nOn the linear scale, r_lnorm and r_normexp should match the testdata, and they do.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = testdata), color = 'black') +\n  geom_density(data = df_rand, aes(x = r_lnorm), color = 'firebrick') +\n  geom_density(data =  df_rand, aes(x = r_normexp), color = 'dodgerblue') + \n xlim(c(0, 1000))\n\nWarning: Removed 1055 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 1065 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 1069 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nOn the log scale, r_norm and rlnormlog should match log(testdata)\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_density(data = df_rand, aes(x = r_norm), color = 'firebrick') +\n  geom_density(data =  df_rand, aes(x = r_lnormlog), color = 'dodgerblue')\n\n\n\n\n\n\n\n\nThat quite clearly works in both directions, further demonstrating that the naive logging of x for the PDF is just mathematically not appropriate. I should do the math to figure it out, but I need to move on now.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Intro to fitting lognormals"
    ]
  },
  {
    "objectID": "stats_probability/shifting_normals.html",
    "href": "stats_probability/shifting_normals.html",
    "title": "Shifting distributions",
    "section": "",
    "text": "Shifting normal distributions to have desired properties is straightforward, but I have to look it up every time.\nI typically need to do it for two reasons (which are really the same reason, when it comes down to it), plus a third that is also the same but requires transforms and backtransforms.",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Basic shifts of normal moments"
    ]
  },
  {
    "objectID": "stats_probability/shifting_normals.html#setup",
    "href": "stats_probability/shifting_normals.html#setup",
    "title": "Shifting distributions",
    "section": "Setup",
    "text": "Setup\nTo have some common references, let’s get a set of random numbers from a standard normal\n\nstdnorm &lt;- rnorm(1000)\n\nAnd set example mean and sd we want.\n\nmean_want &lt;- 5\nsd_want &lt;- 2",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Basic shifts of normal moments"
    ]
  },
  {
    "objectID": "stats_probability/shifting_normals.html#mean-shifts",
    "href": "stats_probability/shifting_normals.html#mean-shifts",
    "title": "Shifting distributions",
    "section": "Mean shifts",
    "text": "Mean shifts\n\nFrom 0\nMean shifts are just arithmetic. So, if we have a standard normal, we can just add the new mean we want.\n\nnorm5 &lt;- stdnorm + mean_want\n\nmean(norm5)\n\n[1] 4.968944\n\n\n\n\nFrom x to y\nIf we have a distribution with a nonzero mean (say, the one we just made) and want to shift it, we need to get the difference. This is often how we deal with correcting a set of generated numbers to have the right mean, but here I use a more extreme example.\nI’m also writing this as a function with variables to see the generality.\n\nshift_mean &lt;- function(sample_vals, mean_want) {\n  mean_sample &lt;- mean(sample_vals)\n\n  mean_shift &lt;- mean_want - mean_sample\n\n  shiftsample &lt;- sample_vals + mean_shift\n  \n  return(shiftsample)\n}\n\n\nnorm2 &lt;- shift_mean(norm5, 2)\nmean(norm2)\n\n[1] 2\n\n\nThis works for the simple case of the standard normal too, and unlike above, where we add 5 and get some error from the particular set of values, here the mean ends up exact because the shift isn’t 5, it’s the difference between 5 and the realised mean of the vector.\n\nmean(shift_mean(stdnorm, 5))\n\n[1] 5\n\n\nI can’t be bothered making this a tibble. Though maybe I should, I really am bad at plot .\n\nplot(density(stdnorm), type = 'lines')\n\nWarning in plot.xy(xy, type, ...): plot type 'lines' will be truncated to first\ncharacter\n\nlines(density(norm5), col = 'firebrick')\nlines(density(norm2), col = 'dodgerblue')",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Basic shifts of normal moments"
    ]
  },
  {
    "objectID": "stats_probability/shifting_normals.html#standard-deviation-shifts",
    "href": "stats_probability/shifting_normals.html#standard-deviation-shifts",
    "title": "Shifting distributions",
    "section": "Standard deviation shifts",
    "text": "Standard deviation shifts\nThe standard deviation requires a multiplicative shift. So if we want an sd of 2, we need to multiply the standard normal by 2.\n\nnormsd2 &lt;- stdnorm*sd_want\nsd(normsd2)\n\n[1] 1.983885\n\n\nAs with the mean, to shift from arbitrary sd to desired, we can write a function that finds the relative sds and shift.\n\nshift_sd &lt;- function(sample_vals, sd_want) {\n  sd_sample &lt;- sd(sample_vals)\n\n  sd_shift &lt;- sd_want/sd_sample\n\n  shiftsample &lt;- sample_vals * sd_shift\n  \n  return(shiftsample)\n}\n\nSo to move from the vector with sd 2 to one with sd 5\n\nnormsd5 &lt;- shift_sd(normsd2, 5)\nsd(normsd5)\n\n[1] 5\n\n\nAnd like the mean example above, we can use this to correct the realised mean of a set of random numbers, since it’s comparative to that realised mean.\n\nsd(shift_sd(stdnorm, sd_want))\n\n[1] 2\n\n\nQuick plots\n\nplot(density(stdnorm), type = 'lines')\n\nWarning in plot.xy(xy, type, ...): plot type 'lines' will be truncated to first\ncharacter\n\nlines(density(normsd5), col = 'firebrick')\nlines(density(normsd2), col = 'dodgerblue')",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Basic shifts of normal moments"
    ]
  },
  {
    "objectID": "stats_probability/shifting_normals.html#both-mean-and-sd",
    "href": "stats_probability/shifting_normals.html#both-mean-and-sd",
    "title": "Shifting distributions",
    "section": "Both mean and sd",
    "text": "Both mean and sd\nTo shift both mean and sd together, we first shift the sd, and then the mean. Again, we can make this a function.\n\nshift_mean_sd &lt;- function(sample_vals, \n                          sd_want, mean_want) {\n  \n  shiftedsd &lt;- shift_sd(sample_vals, sd_want)\n  \n  shiftedboth &lt;- shift_mean(shiftedsd, mean_want)\n}\n\nNow, we can use that to create a new distribution\n\nnorm52 &lt;- shift_mean_sd(stdnorm, sd_want, mean_want)\nmean(norm52)\n\n[1] 5\n\nsd(norm52)\n\n[1] 2\n\nplot(density(norm52))\n\n\n\n\n\n\n\n\nAnd we can shift that to something different again- we don’t need to start with a standard normal.\n\nnorm25 &lt;- shift_mean_sd(norm52, sd_want = 0.5, mean_want = 2)\nmean(norm25)\n\n[1] 2\n\nsd(norm25)\n\n[1] 0.5\n\nplot(density(norm25))\n\n\n\n\n\n\n\n\nSo, now we have a function that can shift any arbitrary set of numbers to have a desired mean and sd.\n\nThe wrong way- order of operations\nAs a quick aside, it does not work to shift the mean first.\n\nshift_mean_sd_BACKWARDS &lt;- function(sample_vals, \n                          sd_want, mean_want) {\n  \n  shiftedmean &lt;- shift_mean(sample_vals, mean_want)\n  \n  shiftedboth &lt;- shift_sd(shiftedmean, sd_want)\n}\n\nThis produces the right sd, but the mean is wrong, because the mulitplication happens after the mean shift, and so shifts the mean again.\n\nbacknorm25 &lt;- shift_mean_sd_BACKWARDS(norm52, sd_want = 0.5, mean_want = 5)\nmean(backnorm25)\n\n[1] 1.25\n\nsd(backnorm25)\n\n[1] 0.5\n\nplot(density(backnorm25))",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Basic shifts of normal moments"
    ]
  },
  {
    "objectID": "stats_probability/shifting_normals.html#other-distributions",
    "href": "stats_probability/shifting_normals.html#other-distributions",
    "title": "Shifting distributions",
    "section": "Other distributions",
    "text": "Other distributions\nFor other distributions based on the normal (e.g. lognormal), these equations shouldn’t be used directly to set means and sds on those scales. E.g. if we have data that is lognormal and we want it to have mean 5 and sd 10 and we naively apply the function above, it will have those moments, but the distribution will be nonlinearly altered and no longer really be lognormal.\nInstead, we need to find the desired means and variances for the underlying normal, do the transform to those, and then back-transform the shifted data. This will retain the lognormal distribution. For some weird distributions (e.g. Johnson), there is not a single translation to the normal, and so it’s often just easiest to discuss translations on the normal scale.\nFor reference, the lognormal transforms to get the desired mean and sd on the normal scale from those on the lognormal (lnmean, lnvar) are\n\n# back-calc normal parameters from lognormal\nnormmu_from_lognorm &lt;- function(lnmean, lnsd) {\n  lnvar &lt;- lnsd^2\n  mu &lt;- log((lnmean^2) / sqrt(lnvar + lnmean^2))\n}\n\nnormsd_from_lognorm &lt;- function(lnmean, lnsd) {\n  lnvar &lt;- lnsd^2\n  sd &lt;- sqrt(log(lnvar / (lnmean^2) +1))\n}",
    "crumbs": [
      "Code Demos",
      "Theory, simulation, statistics, probability",
      "Basic shifts of normal moments"
    ]
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html",
    "href": "tidyprogramming/tidy_programs.html",
    "title": "Tidy programming",
    "section": "",
    "text": "library(tidyverse)",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Programming with dplyr and rlang"
    ]
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#the-issue",
    "href": "tidyprogramming/tidy_programs.html#the-issue",
    "title": "Tidy programming",
    "section": "The issue",
    "text": "The issue\nTidyverse, and particularly dplyr and ggplot, are great for quickly doing very powerful rearrangements and calculations of data and making plots. One of the main way they achieve this is by allowing us to use bare variable names- unquoted, no $ syntax. However, that becomes tricky when programming and we might want to pass variables as an argument. Passing other things as arguments can also be a pain, e.g. functions for summarize. I’ve encountered many different things that trip me up, depending on what I’m trying to pass, but my fixes are typically ad-hoc and scattered around my code. I’ll use this doc as a central place to sort out solutions to various problems as they come up. There’s quite a lot of answers from dplyr itself, but for some reason I always have to figure things out for myself.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Programming with dplyr and rlang"
    ]
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#passing-to-group_by",
    "href": "tidyprogramming/tidy_programs.html#passing-to-group_by",
    "title": "Tidy programming",
    "section": "Passing to group_by",
    "text": "Passing to group_by\nlet’s say we want to allow the user to pass which functions to group_by. The two usual ways I end up doing this are double-embracing or just using character vectors. Let’s demo and test with a grouped mean for mtcars. Embracing allows the user to pass bare names, chars makes them pass characters and we have to use across(all_of()) which is annoying syntax.\n\n# embracing\ngroupbrace &lt;- function(data, groupers) {\n  gm &lt;- data %&gt;%\n    group_by({{groupers}}) %&gt;%\n    summarise(meanmpg = mean(mpg)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n# characters\ngroupchar &lt;- function(data, groupers) {\n  gm &lt;- data %&gt;%\n    group_by(across(all_of(groupers))) %&gt;%\n    summarise(meanmpg = mean(mpg)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nHow do we use those for a single grouping variable?\n\ngroupbrace(mtcars, groupers = gear)\n\n# A tibble: 3 × 2\n   gear meanmpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     3    16.1\n2     4    24.5\n3     5    21.4\n\ngroupchar(mtcars, groupers = 'gear')\n\n# A tibble: 3 × 2\n   gear meanmpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     3    16.1\n2     4    24.5\n3     5    21.4\n\n\nWhat happens when we try to group by more than one column?\n\n# groupbrace(mtcars, groupers = c(gear, carb))\n# \n# groupchar(mtcars, groupers = c('gear', 'carb'))\n\nworks with the characters, but the embracing fails (unsurprisingly).\nThe website says to use …, so we can do that as follows:\n\ngroupdots &lt;- function(data, ...) {\n  gm &lt;- data %&gt;%\n    group_by(...) %&gt;%\n    summarise(meanmpg = mean(mpg)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\ngroupdots(mtcars, gear, carb)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb meanmpg\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1     3     1    20.3\n 2     3     2    17.2\n 3     3     3    16.3\n 4     3     4    12.6\n 5     4     1    29.1\n 6     4     2    24.8\n 7     4     4    19.8\n 8     5     2    28.2\n 9     5     4    15.8\n10     5     6    19.7\n11     5     8    15  \n\n\nThat works, but it becomes an issue if we’re ALSO supplying arguments for other things in the function. See below.\nThe website only uses the dots example, but across() works like it does with summarize. This I think ends up being the answer for bare variable names that don’t get mixed up between grouping and summarizing. See below.\n\ngroupacross &lt;- function(data, groupers) {\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(meanmpg = mean(mpg)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\ngroupacross(mtcars, c(gear, carb))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb meanmpg\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1     3     1    20.3\n 2     3     2    17.2\n 3     3     3    16.3\n 4     3     4    12.6\n 5     4     1    29.1\n 6     4     2    24.8\n 7     4     4    19.8\n 8     5     2    28.2\n 9     5     4    15.8\n10     5     6    19.7\n11     5     8    15",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Programming with dplyr and rlang"
    ]
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#passing-to-summarisemutate",
    "href": "tidyprogramming/tidy_programs.html#passing-to-summarisemutate",
    "title": "Tidy programming",
    "section": "Passing to summarise/mutate",
    "text": "Passing to summarise/mutate\nI’m going to set this up with a simple group_by in all cases because it sets up the combo, and I almost never actually call summarise on a full dataset anyway.\n\nColumns to operate on\nIf we just want one column, but the user supplies its name, we can again embrace or quote.\nNames is an issue here too. They can just be left as a fixed value, but if we want to have the name of the new column reflect what’s being passed in, we handle that in different ways. With the braces we use the glue :=, and the .names argument if characters.\nNow, the dots don’t seem to work to pass multiple bare names, I think probably because of issues with names? But we can modify the simple embraced version to use across(), making it more similar to the character version.\n\n# embracing\nsumbrace &lt;- function(data, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(gear) %&gt;%\n    summarise(\"mean_{{sumcols}}\" := mean({{sumcols}})) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n# characters\nsumchar &lt;- function(data, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(gear) %&gt;%\n    summarise(across(all_of(sumcols), mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n# mulitple bare\nsumbaremulti &lt;- function(data, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(gear) %&gt;%\n    summarise(across({{sumcols}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nWith a single user-supplied column\n\nsumbrace(mtcars, sumcols = mpg)\n\n# A tibble: 3 × 2\n   gear mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     3     16.1\n2     4     24.5\n3     5     21.4\n\nsumchar(mtcars, sumcols = 'mpg')\n\n# A tibble: 3 × 2\n   gear mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     3     16.1\n2     4     24.5\n3     5     21.4\n\n\nMultiple user-supplied cols\n\nsumbaremulti(mtcars, sumcols = c(mpg, hp))\n\n# A tibble: 3 × 3\n   gear mean_mpg mean_hp\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     3     16.1   176. \n2     4     24.5    89.5\n3     5     21.4   196. \n\nsumchar(mtcars, sumcols = c('mpg', 'hp'))\n\n# A tibble: 3 × 3\n   gear mean_mpg mean_hp\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     3     16.1   176. \n2     4     24.5    89.5\n3     5     21.4   196. \n\n\n\n\nCombine with group_by\nI often want to pass a set of variable names to group_by and a set of names to summarize. If we use the dots method, these would get all jumbled together. So the options are embracing or characters, and when embracing we still need the c(bare1, bare2, …, bareN) so each component is a single argument.\n\n# characters\ngsumchar &lt;- function(data, groupers, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(across(all_of(groupers))) %&gt;%\n    summarise(across(all_of(sumcols), mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n# mulitple bare\ngsumbaremulti &lt;- function(data, groupers, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nNow we can feed it multiple grouping columns and multiple summary columns\n\ngsumbaremulti(mtcars, groupers = c(gear, carb), sumcols = c(mpg, hp))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_mpg mean_hp\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     3     1     20.3   104  \n 2     3     2     17.2   162. \n 3     3     3     16.3   180  \n 4     3     4     12.6   228  \n 5     4     1     29.1    72.5\n 6     4     2     24.8    79.5\n 7     4     4     19.8   116. \n 8     5     2     28.2   102  \n 9     5     4     15.8   264  \n10     5     6     19.7   175  \n11     5     8     15     335  \n\ngsumchar(mtcars, groupers = c('gear', 'carb'), sumcols = c('mpg', 'hp'))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_mpg mean_hp\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     3     1     20.3   104  \n 2     3     2     17.2   162. \n 3     3     3     16.3   180  \n 4     3     4     12.6   228  \n 5     4     1     29.1    72.5\n 6     4     2     24.8    79.5\n 7     4     4     19.8   116. \n 8     5     2     28.2   102  \n 9     5     4     15.8   264  \n10     5     6     19.7   175  \n11     5     8     15     335  \n\n\nIt’s really not clear why I’d ever use the dots version, or why we wouldn’t always use the across() wrap to give us generality. I guess if that generality isn’t needed? But while dots can be handy, they’re vague and it’s not like the across() wrap is hard to type.\nWhat this makes very clear is the similarity between the two methods- they’re really just using the select() syntax in the across(), but one has to embrace bare names and the other uses the all_of() modifier we always have to include when we want to select() with a character vector.\n\n\nPassing select syntax\nSince we’re using that across, is it possible to pass other select() syntax than variable names? e.g. is.numeric, starts_with() or b:f? Let’s test it just with the summarize bit.\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = is.numeric)\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(is.numeric, mean, .names = \"mean_{.col}\")`.\nCaused by warning:\n! Use of bare predicate functions was deprecated in tidyselect 1.1.0.\nℹ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %&gt;% select(is.numeric)\n\n  # Now:\n  data %&gt;% select(where(is.numeric))\n\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 11\n    gear  carb mean_mpg mean_cyl mean_disp mean_hp mean_drat mean_wt mean_qsec\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     20.3     5.33     201.    104        3.18    3.05      19.9\n 2     3     2     17.2     8        346.    162.       3.04    3.56      17.1\n 3     3     3     16.3     8        276.    180        3.07    3.86      17.7\n 4     3     4     12.6     8        416.    228        3.22    4.69      16.9\n 5     4     1     29.1     4         84.2    72.5      4.06    2.07      19.2\n 6     4     2     24.8     4        121.     79.5      4.16    2.68      20.0\n 7     4     4     19.8     6        164.    116.       3.91    3.09      17.7\n 8     5     2     28.2     4        108.    102        4.1     1.83      16.8\n 9     5     4     15.8     8        351     264        4.22    3.17      14.5\n10     5     6     19.7     6        145     175        3.62    2.77      15.5\n11     5     8     15       8        301     335        3.54    3.57      14.6\n# ℹ 2 more variables: mean_vs &lt;dbl&gt;, mean_am &lt;dbl&gt;\n\n\nThat works but is angry about missing where(). Just throwing the bare select syntax straight in works though, for the where() type arguments but seems to be general- works for col:col and starts_with() as well.\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = where(is.numeric))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 11\n    gear  carb mean_mpg mean_cyl mean_disp mean_hp mean_drat mean_wt mean_qsec\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     20.3     5.33     201.    104        3.18    3.05      19.9\n 2     3     2     17.2     8        346.    162.       3.04    3.56      17.1\n 3     3     3     16.3     8        276.    180        3.07    3.86      17.7\n 4     3     4     12.6     8        416.    228        3.22    4.69      16.9\n 5     4     1     29.1     4         84.2    72.5      4.06    2.07      19.2\n 6     4     2     24.8     4        121.     79.5      4.16    2.68      20.0\n 7     4     4     19.8     6        164.    116.       3.91    3.09      17.7\n 8     5     2     28.2     4        108.    102        4.1     1.83      16.8\n 9     5     4     15.8     8        351     264        4.22    3.17      14.5\n10     5     6     19.7     6        145     175        3.62    2.77      15.5\n11     5     8     15       8        301     335        3.54    3.57      14.6\n# ℹ 2 more variables: mean_vs &lt;dbl&gt;, mean_am &lt;dbl&gt;\n\n\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = mpg:disp)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 5\n    gear  carb mean_mpg mean_cyl mean_disp\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     20.3     5.33     201. \n 2     3     2     17.2     8        346. \n 3     3     3     16.3     8        276. \n 4     3     4     12.6     8        416. \n 5     4     1     29.1     4         84.2\n 6     4     2     24.8     4        121. \n 7     4     4     19.8     6        164. \n 8     5     2     28.2     4        108. \n 9     5     4     15.8     8        351  \n10     5     6     19.7     6        145  \n11     5     8     15       8        301  \n\n\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = starts_with('d'))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_disp mean_drat\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     201.       3.18\n 2     3     2     346.       3.04\n 3     3     3     276.       3.07\n 4     3     4     416.       3.22\n 5     4     1      84.2      4.06\n 6     4     2     121.       4.16\n 7     4     4     164.       3.91\n 8     5     2     108.       4.1 \n 9     5     4     351        4.22\n10     5     6     145        3.62\n11     5     8     301        3.54\n\n\n\n\nSelect syntax issues\nSometimes we might want to pass a vector of columns to select, but have those that don’t exist get ignored- basically, select however many of this set of columns exist in the dataset. With a character vector, that’s straightforward with any_of. But it fails with bare names, and any_of requires characters.\n\n# These both fail\nmtcars %&gt;% select(c(mpg, fakecolumn))\n\nError in `select()`:\n! Can't select columns that don't exist.\n✖ Column `fakecolumn` doesn't exist.\n\nmtcars %&gt;% select(any_of(mpg, fakecolumn))\n\nError in `select()`:\nℹ In argument: `any_of(mpg, fakecolumn)`.\nCaused by error in `any_of()`:\n! `...` must be empty.\nℹ Did you forget `c()`?\nℹ The expected syntax is `any_of(c(\"a\", \"b\"))`, not `any_of(\"a\", \"b\")`\n\n\nAn obvious solution is to use character vectors.\n\nmtcars %&gt;% select(any_of(c('mpg', 'fakecolumn')))\n\n                     mpg\nMazda RX4           21.0\nMazda RX4 Wag       21.0\nDatsun 710          22.8\nHornet 4 Drive      21.4\nHornet Sportabout   18.7\nValiant             18.1\nDuster 360          14.3\nMerc 240D           24.4\nMerc 230            22.8\nMerc 280            19.2\nMerc 280C           17.8\nMerc 450SE          16.4\nMerc 450SL          17.3\nMerc 450SLC         15.2\nCadillac Fleetwood  10.4\nLincoln Continental 10.4\nChrysler Imperial   14.7\nFiat 128            32.4\nHonda Civic         30.4\nToyota Corolla      33.9\nToyota Corona       21.5\nDodge Challenger    15.5\nAMC Javelin         15.2\nCamaro Z28          13.3\nPontiac Firebird    19.2\nFiat X1-9           27.3\nPorsche 914-2       26.0\nLotus Europa        30.4\nFord Pantera L      15.8\nFerrari Dino        19.7\nMaserati Bora       15.0\nVolvo 142E          21.4\n\n\nBut does that then preclude using other tidyselect syntax such as :, starts_with, etc? Sure, we can swap back and forth if we’re accessing select directly, but not if this is embedded in a function. The answer is sometimes- it works with starts_with but not : (not really shown here because it fails).\n\nmtcars %&gt;% select(any_of(starts_with('d')))\n\n                     disp drat\nMazda RX4           160.0 3.90\nMazda RX4 Wag       160.0 3.90\nDatsun 710          108.0 3.85\nHornet 4 Drive      258.0 3.08\nHornet Sportabout   360.0 3.15\nValiant             225.0 2.76\nDuster 360          360.0 3.21\nMerc 240D           146.7 3.69\nMerc 230            140.8 3.92\nMerc 280            167.6 3.92\nMerc 280C           167.6 3.92\nMerc 450SE          275.8 3.07\nMerc 450SL          275.8 3.07\nMerc 450SLC         275.8 3.07\nCadillac Fleetwood  472.0 2.93\nLincoln Continental 460.0 3.00\nChrysler Imperial   440.0 3.23\nFiat 128             78.7 4.08\nHonda Civic          75.7 4.93\nToyota Corolla       71.1 4.22\nToyota Corona       120.1 3.70\nDodge Challenger    318.0 2.76\nAMC Javelin         304.0 3.15\nCamaro Z28          350.0 3.73\nPontiac Firebird    400.0 3.08\nFiat X1-9            79.0 4.08\nPorsche 914-2       120.3 4.43\nLotus Europa         95.1 3.77\nFord Pantera L      351.0 4.22\nFerrari Dino        145.0 3.62\nMaserati Bora       301.0 3.54\nVolvo 142E          121.0 4.11\n\n# mtcars %&gt;% select(any_of(hp:wt))\n\nIs the trick to pass it the whole any_of expression? that IS a tidyselect call. Try it in the function directly, to get all the across in there correctly. First, this fails if we just pass extra columns:\n\n# gsumbaremulti(mtcars, \n#               groupers = c(gear, carb), \n#               sumcols = c(mpg, fakecol))\n\nIf we know some might not exist, we can instead pass the whole any_of and character names. Is this cleaner? No, now we’re back to characters, but ALSO needing to pass the any_of. So why do it? if we sometimes also need to pass other tidyselect syntax.\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = any_of(c('mpg', 'fakecol')))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\nNow, what if that is in turn buried in a function, so we need to set the argument outside the call? This might happen if we have a user interface where they choose columns. For example, they might set the cols, and then call a function that calls what we have above.\n\nwhichcols &lt;- c('mpg', 'fakecol')\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = any_of(whichcols))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\nThat’s easy enough. But what if whichcols could be tidyselect syntax? That can’t be saved to an object. It can be saved with expr, but then that has to be unpacked with !!.\n\n# Fails\n# whichcols &lt;- starts_with('m')\nwhichcols &lt;- expr(starts_with('m'))\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = !!whichcols)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\nThat allows passing tidyselect, but does it break the any_of situation? Not if we wrap it in expr.\n\nwhichcols &lt;- expr(any_of(c('mpg', 'fakecol')))\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = !!whichcols)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\nThat means that if we might have a character vector and might have tidyselect, we can have a multi-step process to create the expression and pass it to the function. Ie the user can set whichcols directly as an expr-wrapped tidyselect, OR if a character vector it makes it itself. See the next two code blocks.\n\ncolstosum &lt;- c('mpg', 'fakecol')\n# colstosum &lt;- expr(starts_with('d'))\n\nif (is.character(colstosum)) {\n  whichcols &lt;- expr(any_of(colstosum))\n} else {\n  whichcols &lt;- colstosum\n}\n\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = !!whichcols)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\n\ncolstosum &lt;- expr(starts_with('d'))\n\nif (is.character(colstosum)) {\n  whichcols &lt;- expr(any_of(colstosum))\n} else {\n  whichcols &lt;- colstosum\n}\n\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = !!whichcols)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_disp mean_drat\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     201.       3.18\n 2     3     2     346.       3.04\n 3     3     3     276.       3.07\n 4     3     4     416.       3.22\n 5     4     1      84.2      4.06\n 6     4     2     121.       4.16\n 7     4     4     164.       3.91\n 8     5     2     108.       4.1 \n 9     5     4     351        4.22\n10     5     6     145        3.62\n11     5     8     301        3.54\n\n\nBecause that’s ugly, I’m not going to spend more time on it, but it is a workaround for sometimes needing to pass tidyselect syntax and sometimes column names that might not exist. There’s likely a more general way to do this using tidyselect::eval_select, but what I have here will work for now.\n\n\ntidyselect::eval_select\nI’m now running into issues where the approach above isn’t working well, because sometimes the expression ends up including the name of an object (e.g. a passed-in character vector), and by the time we get to the {{}}, we’re too far into the call stack and it ends up failing because it essentially tries to do something like group_by(starts_with(NAME_OF_VECTOR)) instead of group_by(starts_with(VALUES_IN_VECTOR).\nSo, one way to handle this is to in the outer layer use tidyselect::eval_select in the outer layer to get column names and indices. Then we can just pass those around rather than all the promises that get lost doing it other ways. It’s a bit cruder, but i think will involve less gymnastics.\n\nHow does eval_select work?\nFirst, how does eval_select work? What do we need to feed it?\nA bare tidyselect function fails\n\ncolstosum &lt;- starts_with('d')\ntidyselect::eval_select(colstosum, mtcars)\n\nWorks if wrapped in expr\n\ncolstosum &lt;- expr(starts_with('d'))\n\ntidyselect::eval_select(colstosum, mtcars)\n\ndisp drat \n   3    5 \n\n\nWorks with character vectors.\n\ncolstosum &lt;- c('disp', 'mpg')\ntidyselect::eval_select(colstosum, mtcars)\n\ndisp  mpg \n   3    1 \n\n\nDoes not work if there are values in the character vector that aren’t in the data.\n\ncolstosum &lt;- c('disp', 'mpg', 'notinmtcars')\ntidyselect::eval_select(colstosum, mtcars)\n\nSo we likely still need the conditional to use any_of\n\ncolstosum &lt;- c('disp', 'mpg', 'notinmtcars')\n\nif (is.character(colstosum)) {\n  whichcols &lt;- expr(any_of(colstosum))\n} else {\n  whichcols &lt;- colstosum\n}\n\ntidyselect::eval_select(whichcols, mtcars)\n\ndisp  mpg \n   3    1 \n\n\nAnd, what if we pass an argument to a tidyselect? I don’t think this is enough to break the original way without some intervening function calls, but it’s the same idea that’s breaking it as we move down a stack.\n\nstartletter &lt;- 'd'\n\ncolstosum &lt;- expr(starts_with(startletter))\ntidyselect::eval_select(colstosum, mtcars)\n\ndisp drat \n   3    5 \n\n\nWhat actually is that returning? A named vector of indices.\n\ntsout &lt;- tidyselect::eval_select(colstosum, mtcars)\nstr(tsout)\n\n Named int [1:2] 3 5\n - attr(*, \"names\")= chr [1:2] \"disp\" \"drat\"\n\n\nSo, with the conditional in there to guard against grabbing things that don’t exist, that looks like it should work by basically transporting around our selects as character vectors or indices if we evaluate them early enough. In the sort of uses I’m imagining- evaluating this early, and then passing in to further functions- I’d be really nervous about using indices, and so would tend to use the names. How might that work?\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = names(tsout))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_disp mean_drat\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     201.       3.18\n 2     3     2     346.       3.04\n 3     3     3     276.       3.07\n 4     3     4     416.       3.22\n 5     4     1      84.2      4.06\n 6     4     2     121.       4.16\n 7     4     4     164.       3.91\n 8     5     2     108.       4.1 \n 9     5     4     351        4.22\n10     5     6     145        3.62\n11     5     8     301        3.54\n\n\n\n\nA function to parse eval_select\nWhat if I actually make the function do the parsing? So I can pass it the characters, bare names, or expr(selectsyntax)?\n\ngsumtidy &lt;- function(data, groupers, sumcols) {\n  \n  if (is.character(groupers)) {\n    whichg &lt;- expr(any_of(groupers))\n  } else {\n    whichg &lt;- groupers\n  }\n  \n  if (is.character(sumcols)) {\n    whichs &lt;- expr(any_of(sumcols)) \n  } else {\n    whichs &lt;- sumcols\n  }\n  \n  gnames &lt;- whichg %&gt;% \n    tidyselect::eval_select(data) %&gt;% \n    names()\n  snames &lt;- whichs %&gt;% \n    tidyselect::eval_select(data) %&gt;% \n    names()\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{gnames}})) %&gt;%\n    summarise(across({{snames}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n  \n}\n\nTest that with different sorts of things.\n\ngsumtidy(mtcars, \n         groupers = 'cyl', \n         sumcols = expr(starts_with('d')))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nHow about if we include extra cols? works fine.\n\ngsumtidy(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = expr(starts_with('d')))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nThe whole top part of that could be its own function, and get run at any point in a call stack. Returning the names and not the indices, but could return the whole thing I guess, depending on safety of indices.\n\nselectnames &lt;- function(data, selector) {\n  \n  if (is.character(selector)) {\n    whichg &lt;- expr(any_of(selector))\n  } else {\n    whichg &lt;- selector\n  }\n  \n  selnames &lt;- whichg %&gt;% \n    tidyselect::eval_select(data) %&gt;% \n    names()\n  \n  return(selnames)\n}\n\n\ngtidysimple &lt;- function(data, groupers, sumcols) {\n  \n  gnames &lt;- selectnames(data, groupers)\n  snames &lt;- selectnames(data, sumcols)\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{gnames}})) %&gt;%\n    summarise(across({{snames}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n  \n}\n\n\ngtidysimple(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = expr(starts_with('d')))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\n\n\nexpr() vs enquo()\nThe above needs to wrap tidyselect syntax with expr to work- passing the bare starts_with fails\n\ngtidysimple(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = starts_with('d'))\n\nLikewise with bare names\n\ngtidysimple(mtcars, \n         groupers = c(cyl, notinmtcars), \n         sumcols = expr(starts_with('d')))\n\nThat’s because things other than character vectors need to be “defused” (see ?enquo). expr defuses ‘your own local expressions’, while enquo defuses function arguments. So, there are two options- defuse locally when giving the argument to the funciton with expr (as I’ve done above), or defuse internally with enquo.\nIn that case, we re-write the outer function to enquo its arguments.\n\ngtidyquo &lt;- function(data, groupers, sumcols) {\n  \n  gnames &lt;- selectnames(data, enquo(groupers))\n  snames &lt;- selectnames(data, enquo(sumcols))\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{gnames}})) %&gt;%\n    summarise(across({{snames}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n  \n}\n\nNow, that should work without wrapping tidyselect syntax in expr(), and take bare names or character vectors.\n\ngtidyquo(mtcars, \n         groupers = cyl, \n         sumcols = starts_with('d'))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nIt also takes characters\n\ngtidyquo(mtcars, \n         groupers = 'cyl', \n         sumcols = c('disp', 'drat'))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nBut it’s no longer ignoring values not in the data\n\ngtidyquo(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = expr(starts_with('d')))\n\nThat’s because the internal enquo(groupers) in gtidyquo means that selectnames is always seeing selector as language, not character, and so bypassing the any_of() conditional. I don’t want to drop that whole conditional section from selectnames, because that keeps selectnames more general (doesn’t have to be fed enquo’d arguments). Instead, we can use the strict argument in eval_select to decide whether to fail or silently ignore missings. This choice is probably good to have, rather than enforce one or the other- it’s often the case that we should fail if missing columns are called, rather than just ignore silently. The same argument can also be used in the conditional as a switch to make the situation with character selector fail or pass.\n\nselectnames &lt;- function(data, selector, failmissing = TRUE) {\n  \n  if (is.character(selector)) {\n    if (failmissing) {\n      whichg &lt;- expr(all_of(selector))\n    } else {\n      whichg &lt;- expr(any_of(selector))\n    }\n    \n  } else {\n    whichg &lt;- selector\n  }\n  \n  selnames &lt;- whichg %&gt;% \n    tidyselect::eval_select(data, strict = failmissing) %&gt;% \n    names()\n  \n  return(selnames)\n}\n\nWe also need to rewrite gtidyquo to pass failmissing. Could use …, but that’s vague.\n\ngtidyquo &lt;- function(data, groupers, sumcols, failmissing = TRUE) {\n  \n  gnames &lt;- selectnames(data, enquo(groupers), failmissing)\n  snames &lt;- selectnames(data, enquo(sumcols), failmissing)\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{gnames}})) %&gt;%\n    summarise(across({{snames}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n  \n}\n\nNow, does that work with values not in the data?\n\ngtidyquo(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = starts_with('d'),\n         failmissing = FALSE)\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nas bare names\n\ngtidyquo(mtcars, \n         groupers = c(cyl, notinmtcars), \n         sumcols = starts_with('d'),\n         failmissing = FALSE)\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nand it should fail if failmissing = TRUE (or left off, since that’s the default).\n\ngtidyquo(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = starts_with('d'))\n\n\n\nConclusions\nThat seems a bit lame to just translate to characters, but it ends up being a very robust and flexible workaround for situations where passing an object into a tidyselect ends up trying to select the object instead of its contents once we’re further down a call stack, and lets us use characters, bare names, and tidyselect and choose whether or not to fail when columns don’t exist.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Programming with dplyr and rlang"
    ]
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#functions-to-use",
    "href": "tidyprogramming/tidy_programs.html#functions-to-use",
    "title": "Tidy programming",
    "section": "Functions to use",
    "text": "Functions to use\nSometimes we want to tell the function how to summarise the data. Sometimes we want to do this including arguments, e.g. mean with na.rm = TRUE. Sometimes we want to pass multiple functions and have the names appended, and sometimes those functions are user-defined. Further, sometimes they have an argument internal to the data (such as a weighting column) that they need to access.\nWe’ll start simple, though I’ll keep the multi-group and multi-col syntax from above because it keeps things general, and allows testing with multiple summarise cols. I’ll use the bare names and embracing for the grouping and summarise variables, but that shouldn’t affect the way function-passing works if we used the character version instead.\n\nPassing a function by name\nIt’s typically a good idea to name the resulting column with the function when we don’t know what the function will be. And that sets us up for multi-functions.\nIn the simplest case we can just use a FUN argument. While using the all-caps “FUN” as the argument name seems to be a convention, this isn’t a special argument name and it could be whatever we want.\nPreviously, we had defined the function to apply inside our function, and so we had hardcoded the naming, e.g. 'mean_{.col}. But now, we won’t know what it is. We thus need to get the name of the function as well, using as.character(substitute).\n\nfunpass &lt;- function(data, groupers, sumcols,\n                    FUN) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN, .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\nfunpass(mtcars,\n        groupers = gear,\n        sumcols = mpg,\n        FUN = mean)\n\n# A tibble: 3 × 2\n   gear mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     3     16.1\n2     4     24.5\n3     5     21.4\n\n\nWe run into problems as soon as we try to pass arguments to that function, for example when there are NA and we want to use na.rm\n\nnacars &lt;- mtcars %&gt;%\n  mutate(randnum = rnorm(n()),\n         nampg = ifelse(randnum &gt;= 0, mpg, NA))\n\n\n#| error:false\n\n# funpass(nacars,\n#         groupers = gear,\n#         sumcols = nampg,\n#         FUN = mean, na.rm = TRUE)\n\nUsing dots syntax works to allow arguments.\n\nfunpasst &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN,..., .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\nfunpasst(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(nampg, FUN, ..., .names = funcolname)`.\nℹ In group 1: `gear = 3`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 3 × 2\n   gear mean_nampg\n  &lt;dbl&gt;      &lt;dbl&gt;\n1     3       15.6\n2     4       25.4\n3     5       21.7\n\n\nAs usual, dots can be an issue if we’re doing several things. But we’ll get to that. One solution that is also relevant generally is to specify a custom function. In a simple case this could be mean with na.rm = TRUE, but it could be anything.\n\n\nCustom function\nMaybe we want a custom function. That might be as simple as changing the na.rm default, or it might be something complicated with a few arguments. Here, I’ll demo a version with a swapped na.rm default, illustrating a way to avoid passing arguments, and a more complex function that lags values and multiplies them.\n\nmeanna &lt;- function(x) {\n  mean(x, na.rm = TRUE)\n}\n\ncustomfun &lt;- function(x, lag_k = 1, na.rm = TRUE, multiplier) {\n  xl &lt;- lag(x, lag_k)\n  xs &lt;- sum(xl, na.rm = na.rm)*multiplier\n  return(xs)\n}\n\n\nfunpasst(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = meanna)\n\n# A tibble: 3 × 2\n   gear meanna_nampg\n  &lt;dbl&gt;        &lt;dbl&gt;\n1     3         15.6\n2     4         25.4\n3     5         21.7\n\n\n\nfunpasst(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = customfun, lag_k = 0, multiplier = 10)\n\n# A tibble: 3 × 2\n   gear customfun_nampg\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     3            1246\n2     4            1524\n3     5             651\n\n\nand that works with multiple columns and groupers as well\n\nfunpasst(nacars,\n        groupers = c(gear, am),\n        sumcols = c(nampg, hp),\n        FUN = customfun, lag_k = 0, multiplier = 10)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n   gear    am customfun_nampg customfun_hp\n  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1     3     0            1246        26420\n2     4     0             244         4030\n3     4     1            1280         6710\n4     5     1             651         9780\n\n\n\n\nFunction with internal data argument\nSometimes we might want to use a function that relies on multiple columns- for example, the mean of one column using weights in another.\nIn the simplest case, we can hardcode that column. Here in a silly example of finding the mean hp weighted by wt. I’ve removed the dots for now, we’ll get to other arguments next.\n\nfuninternal &lt;- function(data, groupers, sumcols,\n                    FUN) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN, wt, .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\nfuninternal(nacars,\n        groupers = gear,\n        sumcols = mpg,\n        FUN = weighted.mean)\n\n# A tibble: 3 × 2\n   gear weighted.mean_mpg\n  &lt;dbl&gt;             &lt;dbl&gt;\n1     3              15.6\n2     4              23.6\n3     5              19.7\n\n\nand yes, that is weighting- if we just pass mean we get\n\nfunpasst(nacars,\n        groupers = gear,\n        sumcols = mpg,\n        FUN = mean)\n\n# A tibble: 3 × 2\n   gear mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     3     16.1\n2     4     24.5\n3     5     21.4\n\n\nBut what if we need to specify other arguments? We can use dots again.\n\nfuninternald &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN, wt, ..., .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\nfuninternald(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                14.9\n2     4                24.8\n3     5                19.6\n\n\nAnother way to do this that might be a bit clearer, especially as the number of arguments grows is to use tilde function specification. This is nearly the same, but makes it clear what arguments belong to the FUN.\n\nfuninternaldt &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~FUN(., wt, ...), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThat yields the same result, we’ve just specified the summary function differently.\n\nfuninternaldt(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                14.9\n2     4                24.8\n3     5                19.6\n\n\n\n\nPassing internal columns by name\nSo far, the internal columns have been hardcoded, and at a known position in the arguments to the FUN. What if we want to specify them on calling the function?\nCan we just use the dots? Not with a bare name.\n\n# funpasst(nacars,\n#         groupers = gear,\n#         sumcols = nampg,\n#         FUN = weighted.mean, wt, na.rm = TRUE)\n\nDoes it work to use the tilde version?\n\nfuntildedots &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~FUN(., ...), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nNo, that still can’t find the bare name- it looks for an object, not something internal to the data.\n\nfuntildedots(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, wt, na.rm = TRUE)\n\nIf we know that there will be a second data-variable argument to the function, we might be able to embrace.\n\nfuninteralembrace &lt;- function(data, groupers, sumcols,\n                    FUN, arg2, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN, {{arg2}}, ..., .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nfuninteralembraceT &lt;- function(data, groupers, sumcols,\n                    FUN, arg2, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~FUN(., {{arg2}}, ...), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThat works for both the tilde and non-tilde versions.\n\nfuninteralembrace(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, \n        arg2 = wt, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                14.9\n2     4                24.8\n3     5                19.6\n\n\n\nfuninteralembraceT(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, \n        arg2 = wt, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                14.9\n2     4                24.8\n3     5                19.6\n\n\nBut what if we want a function that works with FUNS that may or may not require a second data-variable argument? Do the above functions work with something like mean that won’t have an arg2? No.\n\nfuninteralembrace(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\nfuninteralembraceT(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\nIs there a way to write a function that may have any number from 0 to n internal data arguments, as well as other non-data arguments (e.g. na.rm etc)? It will be tricky, because some unknown number of items will need to be embraced. Usual methods to unpack the ellipses using list(...) won’t work, I don’t think. And if they do, it’s still unclear how many of the items in the list should be embraced. Does it even work if we know how many need to be embraced? Test with a simple case of whether we can even do the list(…).\n\ntestdots &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  dots &lt;- list(...)\n  \n  print(dots)\n  \n  # gm &lt;- data %&gt;%\n  #   group_by(across({{groupers}})) %&gt;%\n  #   summarise(across({{sumcols}}, ~FUN(., {{dots[1]}}, ...), .names = funcolname)) %&gt;%\n  #   ungroup()\n  # return(gm)\n}\n\nEven that doesn’t work- including bare names in the dots and then embracing doesn’t work because list() needs them as objects.\n\ntestdots(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, wt, na.rm = TRUE)\n\nWhat is it I’m actually trying to do here? Write a function that takes an arbitrary number of data-variable arguments and an arbitrary number of passed env-arguments. That’s always going to be tricky, and will get trickier to sort things out like the order of the arguments. Is it possible? Almost certainly. But I think I’ll leave sorting it out for later. We have a version that works for a known number of arguments in a known order, which is enough in some situations. A workaround will become apparent anyway after the next section, where I pass in external vectors.\n\n\nFunction with vector argument passed in\nOne way to get around the issue above is instead of passing the name of a data variable, pass in the vector itself as an object. This also allows passing in vectors unattached to the dataframe being operated on, though since the’ll need to have the same nrows, in most cases they’ll be attached.\nHow does this work? We write the main function to do the grouping and summarizing, and within it define the function to evaluate in the summarize, accounting for the various types of arguments and the grouping. This works because the … are all env-variables (vectors and scalars) instead of bare names of data-variables. This is all based on funpasst above, with the addition of the internal function creation. Because the function we define may be grouped, it needs to be passed the indices for the current group rows so it only operates on those. I’m using tilde notation to keep it clearer how that function gets called in the summarise.\nWe could write the function that creates the function to evaluate inside the main function, or elsewhere. Writing it inside allows us to take some shortcuts because it can access objects in the outer function environment and avoid explicitly passing as many objects around. Though that can be dangerous.\nThe !!! unpacks a list of function arguments.\n\narbvecscal &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  # Define the function to evaluate\n  thisfun &lt;- function(x, indices) {\n    elip &lt;- list(...)\n    \n    # deal with the case of no passed arguments\n    if (length(elip) == 0) {\n      return(rlang::exec(FUN, x))\n    } else {\n      \n      # clip vector ... arguments (e.g. weights) to just the group\n      for (i in 1:length(elip)) {\n        if (length(elip[[i]]) == nrow(data)) {\n          elip[[i]] &lt;- elip[[i]][indices]\n        }\n      }\n      \n      return(rlang::exec(FUN, x, !!!elip))\n    }\n  }\n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~thisfun(., cur_group_rows()), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nSo, for something like the weighted average with an na.rm argument, we specify the vector of weights, rather than their bare name in the dataframe.\n\narbvecscal(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, nacars$wt, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                14.9\n2     4                24.8\n3     5                19.6\n\n\nAnd that also works if we want a function without any data-variables\n\narbvecscal(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear mean_nampg\n  &lt;dbl&gt;      &lt;dbl&gt;\n1     3       15.6\n2     4       25.4\n3     5       21.7\n\n\nIf we don’t want to pass vectors but pass bare names, we might be able to do that with the same approach, but will need to specify which are which. Then we’d create the vectors internal to the function using the same select syntax as before.\nNow the internal function has to be a bit different (simpler) since it doesn’t have to do the checking for length since we’ve specified dataargs.\n\narbdatanames &lt;- function(data, groupers, sumcols,\n                         FUN, dataargs, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  # make a tibble so it doesn't collapse to vector if only one column\n  datavecs &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs}})\n  \n  # Define the function to evaluate\n  thisfun &lt;- function(x, indices) {\n    elip &lt;- list(...)\n    \n    # deal with the case of no passed arguments\n    if (length(elip) == 0 & nrow(datavecs) == 0) {\n      return(rlang::exec(FUN, x))\n    } else {\n      \n      # clip data arguments (e.g. weights) to just the group\n      thisdata &lt;- datavecs[indices, ]\n      \n      # make all the arguments a list so we can call it\n      allargs &lt;- c(as.list(thisdata), elip)\n      \n      return(rlang::exec(FUN, x, !!!allargs))\n    }\n  }\n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~thisfun(., cur_group_rows()), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nNow, that should work for the weighted mean as well. Note that now the data variables have to be part of the dataframe- this function does not accept vectors passed in from elsewhere.\nBUT, it doesn’t work because the names of the arguments need to be the names in the list of arguments following the !!!. And here, wt is the name, but weighted.mean wants w. So we not only need to specify the data-variable name, but the function-argument name for that variable as well. This is getting very in the weeds.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, dataargs = wt, na.rm = TRUE)\n\nFor example, arguments with the wrong names just get ignored. Names are essential, the execution does not just rely on order like if we called a function directly. Which makes sense for safety, but makes things harder here.\n\nvals = rnorm(10)\narglist &lt;- list(x = vals, wt = 1:10, na.rm = TRUE)\nrlang::exec(weighted.mean, !!!arglist)\n\n[1] 0.1166201\n\narglist2 &lt;- list(x = vals, w = 1:10, na.rm = TRUE)\nrlang::exec(weighted.mean, !!!arglist2)\n\n[1] 0.08719472\n\n\nIt would be nice to pass name-value pairs, but the bare names are going to trip us up, I think. Could do it with paired characters I guess, but we’ve just spent quite a lot of time trying to avoid that. Would work though. Kind of a pain to setup- would make most sense as two paired columns or vectors. And if we do that, it’d end up being roughly equivalent to just adding another argument to the function for the matched names.\nSkipping the rename if dataargnames aren’t specified allows ignoring it if the columns have the correct names, and helps it work more smoothly if there aren’t dataargs at all.\n\narbdatanames &lt;- function(data, groupers, sumcols,\n                         FUN, dataargs, dataargnames = NULL, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  # make a tibble so it doesn't collapse to vector if only one column\n  datavecs &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs}})\n  \n  if (!is.null(dataargnames)) {\n    names(datavecs) &lt;- dataargnames\n  }\n  \n  \n  # Define the function to evaluate\n  thisfun &lt;- function(x, indices) {\n    elip &lt;- list(...)\n    \n    # deal with the case of no passed arguments\n    if (length(elip) == 0 & nrow(datavecs) == 0) {\n      return(rlang::exec(FUN, x))\n    } else {\n      \n      # clip data arguments (e.g. weights) to just the group\n      thisdata &lt;- datavecs[indices, ]\n      \n      # make all the arguments a list so we can call it\n      allargs &lt;- c(as.list(thisdata), elip)\n      \n      return(rlang::exec(FUN, x, !!!allargs))\n    }\n  }\n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~thisfun(., cur_group_rows()), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nNow that works. The alternative would be to have a table of matched dataargs and dataargnames, and have that table be a single argument to arbdatanames, but we’d still have to created it and that’d involve more overhead.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, dataargs = wt, dataargnames = 'w', \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                14.9\n2     4                24.8\n3     5                19.6\n\n\nAnd it works in situations without data args.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear mean_nampg\n  &lt;dbl&gt;      &lt;dbl&gt;\n1     3       15.6\n2     4       25.4\n3     5       21.7\n\n\nThat should work for &gt;1 data variable as well. Let’s define a function that needs multiple data variables. This is very contrived with just some division and multiplication, but works as a check.\n\nmultidat &lt;- function(x, w, d, m, na.rm) {\n  preprep &lt;- x/d*m\n  outcome &lt;- weighted.mean(preprep, w, na.rm = na.rm)\n}\n\nThat works. Note that the dependence on argument names means we can specify out of order- we get two very different answers depending on whether we call cyl and hp d and m or m and d.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = multidat, dataargs = c(wt, cyl, hp), dataargnames = c('w', 'd', 'm'), \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear multidat_nampg\n  &lt;dbl&gt;          &lt;dbl&gt;\n1     3           358.\n2     4           466.\n3     5           654.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = multidat, dataargs = c(wt, cyl, hp), dataargnames = c('w', 'm', 'd'), \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear multidat_nampg\n  &lt;dbl&gt;          &lt;dbl&gt;\n1     3          0.643\n2     4          1.39 \n3     5          0.608\n\n\nIs there any reason to specify thisfun externally to the main function? I guess maybe? It forces us to specify arguments, and potentially makes things clearer.\n\n# Define the function to evaluate within the summary\nsumfun &lt;- function(x, indices, FUN, datavecs, ...) {\n  elip &lt;- list(...)\n  \n  # deal with the case of no passed arguments\n  if (length(elip) == 0 & nrow(datavecs) == 0) {\n    return(rlang::exec(FUN, x))\n  } else {\n    \n    # clip data arguments (e.g. weights) to just the group\n    thisdata &lt;- datavecs[indices, ]\n    \n    # make all the arguments a list so we can call it\n    allargs &lt;- c(as.list(thisdata), elip)\n    \n    return(rlang::exec(FUN, x, !!!allargs))\n  }\n}\n\n\nnewfun &lt;- function(data, groupers, sumcols,\n                         FUN, dataargs, dataargnames = NULL, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  # make a tibble so it doesn't collapse to vector if only one column\n  datavecs &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs}})\n  \n  if (!is.null(dataargnames)) {\n    names(datavecs) &lt;- dataargnames\n  }\n  \n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~sumfun(., indices = cur_group_rows(), FUN = FUN, datavecs = datavecs, ...), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThat does work just as above. I’m not sure which will be cleaner in practice, but I like that this relies less on borrowing variables from the creating environment.\n\nnewfun(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, dataargs = wt, dataargnames = 'w', \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                14.9\n2     4                24.8\n3     5                19.6\n\nnewfun(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = multidat, dataargs = c(wt, cyl, hp), dataargnames = c('w', 'm', 'd'), \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear multidat_nampg\n  &lt;dbl&gt;          &lt;dbl&gt;\n1     3          0.643\n2     4          1.39 \n3     5          0.608",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Programming with dplyr and rlang"
    ]
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#multiple-functions--with-appropriate-named-outputs",
    "href": "tidyprogramming/tidy_programs.html#multiple-functions--with-appropriate-named-outputs",
    "title": "Tidy programming",
    "section": "Multiple functions- with appropriate named outputs",
    "text": "Multiple functions- with appropriate named outputs\n\nSimple - hardcoded number of functions\nSometimes we might want to calculate multiple summary or mutate functions for the same set of data, and so rather than repeating the above functions multiple times with different FUN arguments, it would be good to be able to send them all at once for one run-through. The simplest way to do this is to have a known number of functions and write that number of summaries, e.g.\n\nsimplemultifun &lt;- function(data, groupers, sumcols,\n                         FUN1, dataargs1, dataargnames1 = NULL,\n                         FUN2, dataargs2, dataargnames2 = NULL, ...) {\n  # function name as character\n  funname1 &lt;- as.character(substitute(FUN1))\n  # This just avoids clutter in the summarise\n  funcolname1 &lt;- paste0(funname1, '_{.col}')\n  \n    # function name as character\n  funname2 &lt;- as.character(substitute(FUN2))\n  # This just avoids clutter in the summarise\n  funcolname2 &lt;- paste0(funname2, '_{.col}')\n  \n  # make a tibble so it doesn't collapse to vector if only one column\n  datavecs1 &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs1}})\n  \n  if (!is.null(dataargnames1)) {\n    names(datavecs1) &lt;- dataargnames1\n  }\n  \n    # make a tibble so it doesn't collapse to vector if only one column\n  datavecs2 &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs2}})\n  \n  if (!is.null(dataargnames2)) {\n    names(datavecs2) &lt;- dataargnames2\n  }\n  \n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}},\n                     ~sumfun(., indices = cur_group_rows(), \n                             FUN = FUN1, datavecs = datavecs1, ...),\n                     .names = funcolname1),\n              across({{sumcols}},\n                     ~sumfun(., indices = cur_group_rows(),\n                             FUN = FUN2, datavecs = datavecs2, ...),\n                     .names = funcolname2)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThen as an example, let’s do a weighted mean but unweighted sd. note that they need to share the dots.\n\nsimplemultifun(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN1 = weighted.mean, dataargs1 = wt, dataargnames1 = 'w',\n        FUN2 = sd,\n        na.rm = TRUE)\n\n# A tibble: 3 × 3\n   gear weighted.mean_nampg sd_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1     3                14.9     4.01\n2     4                24.8     4.84\n3     5                19.6     7.89\n\n\nThat works, but is really hardcoded in terms of what we can do. It has to have two functions. So, let’s try to say we can pass an arbitrary set of functions from 1 to n.\nNote that a different data structure out the end is likely to be warranted, especially if we calculate these functions on multiple variables.- making this long with a column for the variable name and then the values of the functions might be the way to go if we do this for multiple variables.\n\n\nVariable number of functions\nWhat we really want here is to be able to pass in an arbitrary number of functions. That will get complicated if they have things like different data-variable arguments. In the simplest case, we can make the FUNS a list, and summarise just handles it. However, this breaks the names and the dots for arguments- the list needs to have all the info in it.\n\nfunmulti &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUNS)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nIf the list is named (using lst here, but list(mean = mean, sd = sd) would work too), those names get appended.\n\nfunmulti(nacars,\n        groupers = gear,\n        sumcols = mpg,\n        FUNS = lst(mean, sd))\n\n# A tibble: 3 × 3\n   gear mpg_mean mpg_sd\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1     3     16.1   3.37\n2     4     24.5   5.28\n3     5     21.4   6.66\n\n\nThat approach should work for arbitrary arguments if I use the tilde notation, and even allows data variables. This is a bit messier in the function call than I’d like, and there’s a bit less control over the names, but I think neither of those are major issues. Would be hard to be less verbose, really, and still have argument specification make any sense across multiple functions.\nActually, can I control the names with .names after all?\n\nfunmulti &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n# nameparser &lt;- paste0('prefix_{.fn}_{.col}')\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUNS, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nAs of {dplyr} 1.1, this no longer works- it looks for wt as an object, not a data-variable. We’ll need to find a new solution.\n\nfunmulti(nacars, \n         groupers = gear, \n         sumcols = nampg,\n         FUNS = list(mean = ~mean(., na.rm = TRUE), \n                     sd = ~sd(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE)))\n\nError in `summarise()`:\nℹ In argument: `across(nampg, FUNS, .names = \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_nampg`.\nCaused by error:\n! object 'wt' not found\n\n\nNote that now the function args in the list work with data-variables and scalars, not with vectors passed in. This is not solely because of the grouping needing to be handled as we did above with the sumfun cutting to the correct indices, because even if we don’t group, we get errors about promise evals. This is because the FUNS list is being evaluated inside the summarise, and so thinks everything is a data-variable. There is probably a way to sort that out by using .env[['variablename']] in the specification, but that’ll just get more complex than just adding the column to the dataframe if we hit this situation. Especially since we’d have to pass the vector in so it’s available inside the funmulti environment, not just the global environment.\n\nouterweights &lt;- 1:nrow(nacars)\n\nfunmulti(nacars,\n         # groupers = gear,\n         sumcols = nampg,\n         FUNS = list(mean = ~mean(., na.rm = TRUE),\n                     sd = ~sd(., na.rm = TRUE),\n                     wm = ~weighted.mean(., w = outerweights, \n                                         na.rm = TRUE)))\n\n# A tibble: 1 × 3\n  prefix_mean_nampg prefix_sd_nampg prefix_wm_nampg\n              &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1              20.1            6.59            20.3\n\n\nCould we do something fancy with a list of FUNS and lists of arglists, parallelling how we did things above? Probably. I think in most instances though, this approach will work. I’ll develop that more complex situation only if needed.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Programming with dplyr and rlang"
    ]
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#bringing-it-all-together",
    "href": "tidyprogramming/tidy_programs.html#bringing-it-all-together",
    "title": "Tidy programming",
    "section": "Bringing it all together",
    "text": "Bringing it all together\nNow, let’s choose a couple grouping columns, a selection of cols to summarise, and multiple summary functions, some with data arguments and some that are custom.\nNone of this works as of dplyr 1.1. The issue is that with new behaviour in dplyr, it is looking for the additional arguments not in the column names but as objects. See section below.\n\ncomplexSummary &lt;- funmulti(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = list(mean = ~mean(., na.rm = TRUE), \n                     sd = ~sd(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE),\n                     custom = ~multidat(., w = wt, d = cyl, m = hp,\n                                        na.rm = FALSE)))\n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), FUNS, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3` and `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error:\n! object 'wt' not found\n\ncomplexSummary\n\nError in eval(expr, envir, enclos): object 'complexSummary' not found\n\n\nThis is complex enough it’s probably useful to pivot_longer\n\nlongsums &lt;- complexSummary %&gt;% \n  pivot_longer(cols = -c(gear, carb), \n               names_to = c('variable', 'summary_statistic'),\n               names_sep = '_',\n               values_to = 'value') \n\nError in eval(expr, envir, enclos): object 'complexSummary' not found\n\nlongsums\n\nError in eval(expr, envir, enclos): object 'longsums' not found\n\n\nBut that actually puts a lot of values with different meaning in the same value column. What’s probably better is to give different statistics their own columns, as sort of an intermediate long/wide.\n\nlongwide &lt;- longsums %&gt;% \n  pivot_wider(names_from = summary_statistic, values_from = value) \n\nError in eval(expr, envir, enclos): object 'longsums' not found\n\nlongwide\n\nError in eval(expr, envir, enclos): object 'longwide' not found\n\n\nAnyway, this sort of arrangement isn’t the point of this document, so I’ll stop there.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Programming with dplyr and rlang"
    ]
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#adjusting-to-dplyr-1.1",
    "href": "tidyprogramming/tidy_programs.html#adjusting-to-dplyr-1.1",
    "title": "Tidy programming",
    "section": "Adjusting to dplyr 1.1",
    "text": "Adjusting to dplyr 1.1\nAs of dplyr 1.1, new behaviour means that if we pass multi-argument functions, it looks for the additional arguments not as data-variables (column names), but as objects. E.g., we now get errors for all the weighted.mean calls above, since it cannot find a wt object when wt is a column name.\nThis is discussed as a dplyr github issue, where there is a workaround using rlang::quo, but I really don’t like it for a couple reasons, primarily that it forces a user to wrap their code in rlang::quo, and it matters where in the call stack the function gets defined. I’m not sure I’ll figure anything out that works better for me, since the tidyverse people came up with the workaround, but I need to try.\n\nRe-demoing the issue\nThat workaround uses {{}} around FUNS in the function. Building on funmulti above,\n\nfunbrace &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThat actually works when we define the function to call inside the function argument.\n\nbracecheck &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE)))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\nbracecheck\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nHowever, if we define the functions to call in an object, it fails\n\nfunstocall &lt;- list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., rlang::data_sym('wt'), na.rm = TRUE))\n\nbracecheck2 &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funstocall)\n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), funstocall, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3` and `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `x * w`:\n! non-numeric argument to binary operator\n\nbracecheck2\n\nError in eval(expr, envir, enclos): object 'bracecheck2' not found\n\n\nAnd the ‘solution’ is to use rlang::quo , followed by !! in the call\n\nfunstocallq &lt;- rlang::quo(list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE)))\n\nbracecheckq &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = !!funstocallq)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\nbracecheckq\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nThat works, but it sure requires a lot of fiddling by the user with quosures.\nWe can bring the !! inside the function, which seems to work. I’ve run into issues before where this then requires quosures for everything, but it seems to be working here for mean, which doesn’t need the quosure because it doesn’t reference data-variables.\nThe !! method is\n\nfundefuse &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, !!FUNS, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nAnd so now we don’t have to defuse in the function call.\n\nfunstocallq &lt;- rlang::quo(c(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE)))\n\ndefusecheckb &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funstocallq)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ndefusecheckb\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nand mean works as well, even when it’s not wrapped in rlang::quo because it doesn’t reference data-variables.\n\nfunmean &lt;- list(mean = ~mean(., na.rm = TRUE))\n\ndefusecheckm &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funmean)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ndefusecheckm\n\n# A tibble: 11 × 5\n    gear  carb prefix_mean_disp prefix_mean_drat prefix_mean_nampg\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1     3     1            201.              3.18              21.4\n 2     3     2            346.              3.04              17.8\n 3     3     3            276.              3.07             NaN  \n 4     3     4            416.              3.22              12.4\n 5     4     1             84.2             4.06              27.6\n 6     4     2            121.              4.16              25.4\n 7     4     4            164.              3.91              21  \n 8     5     2            108.              4.1               30.4\n 9     5     4            351               4.22             NaN  \n10     5     6            145               3.62              19.7\n11     5     8            301               3.54              15  \n\n\n\n\nSearching for a solution\nI really don’t want to require quosures. And I want to be able to pass character function names.\n\nMake reference internal to a custom function?\nAttempt 1: can I simply define a function with the data-var internally referenced so it only takes one argument? I doubt it, but that might be the easiest.\n\nweightcars &lt;- function(x) {\n  weighted.mean(x, w = wt, na.rm = TRUE)\n}\n\nThat doesn’t work with either the !! or {{}} method.\n\nfunscustom &lt;- list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weightcars(.))\n\ndefusecheckc &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funscustom)\n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nℹ In group 1: `gear = 3` and `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weightcars()`:\n! object 'wt' not found\n\ndefusecheckc\n\nError in eval(expr, envir, enclos): object 'defusecheckc' not found\n\n\n\nbracecheckc &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funscustom)\n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), funscustom, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3` and `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weightcars()`:\n! object 'wt' not found\n\nbracecheckc\n\nError in eval(expr, envir, enclos): object 'bracecheckc' not found\n\n\nAnd that doesn’t even work with the rlang::quo wrapper (unsurprisingly, I suppose).\n\nfunscustom &lt;- rlang::quo(list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weightcars(.)))\n\ndefusecheckc &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funscustom)\n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nℹ In group 1: `gear = 3` and `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weightcars()`:\n! object 'wt' not found\n\ndefusecheckc\n\nError in eval(expr, envir, enclos): object 'defusecheckc' not found\n\n\n\n\nModify the aggregation function somehow\nMy basic thought here is whether I can auto-build the data referencing. I’ve tried using rlang::data_sym in the weighted mean function, and doing a bunch of other things, but I haven’t come up with anything yet. Maybe rlang::inject?\nEven if I specify the FUNS as a list inside the function, I need the rlang::quo. Which is surprising, since I don’t need it if they’re specified as a function argument. I’m missing something about quoting, I think.\n\nfunbrace &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\n\nDifferent formulat specification\nWhat if instead of using the formula version of anonymous functions, we use \\(x)? I think this will behave like the custom weightcars above, but maybe we can have more control inside the aggregation function?\nFirst, does it work with the quo?\n\nanonq &lt;- rlang::quo(list(mean = \\(x) mean(x, na.rm = TRUE),\n                     wm = \\(x) weighted.mean(x, wt, na.rm = TRUE)))\n\n\ndefusechecka &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = anonq)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ndefusechecka\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nWorks with the !! but not {{}}.\n\nbracechecka &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = anonq)\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'gear', 'carb'. You can override using the\n`.groups` argument.\n\nbracechecka\n\n# A tibble: 22 × 5\n    gear  carb prefix_1_disp prefix_1_drat prefix_1_nampg\n   &lt;dbl&gt; &lt;dbl&gt; &lt;named list&gt;  &lt;named list&gt;  &lt;named list&gt;  \n 1     3     1 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 2     3     1 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 3     3     2 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 4     3     2 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 5     3     3 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 6     3     3 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 7     3     4 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 8     3     4 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 9     4     1 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n10     4     1 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n# ℹ 12 more rows\n\n\nNow, can we get it to work without quo???\n\nanonbare &lt;- list(mean = \\(x) mean(x, na.rm = TRUE),\n                     wm = \\(x) weighted.mean(x, wt, na.rm = TRUE))\n\nanonbare &lt;- list(mean = \\(x) mean(x, na.rm = TRUE),\nNot immediately. but can we modify those functions?\n\ndefusecheckab &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = anonbare)\n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nℹ In group 1: `gear = 3` and `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error:\n! object 'wt' not found\n\ndefusecheckab\n\nError in eval(expr, envir, enclos): object 'defusecheckab' not found\n\nbracecheckab &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = anonbare)\n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), anonbare, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3` and `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error:\n! object 'wt' not found\n\nbracecheckab\n\nError in eval(expr, envir, enclos): object 'bracecheckab' not found\n\n\n\n\nIs the answer to drop dplyr?\nI thought about moving to stats::aggregate, but it seems like that is going to cause just as many problems, especially when we get to passing it arbitrary lists of functions. The syntax is just so clumsy (at least to me).\n\n\nDoes it just work if I give it the vector?\nThis won’t solve the whole problem, and I think it still won’t actually work with the groupings, but should test.\n\nanonbarevec &lt;- list(mean = \\(x) mean(x, na.rm = TRUE),\n                    wm = \\(x) weighted.mean(x, nacars$wt, na.rm = TRUE))\n\nAs expected, that fails because the external vector doesn’t get broken up by the groups.\n\ndefusecheckab &lt;- fundefuse(nacars,  \n                           groupers = c(gear, carb),\n                           sumcols = c(starts_with('d'), nampg),\n                           FUNS = anonbarevec) \n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nℹ In group 1: `gear = 3` `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weighted.mean.default()`:\n! 'x' and 'w' must have the same length\n\ndefusecheckab  \n\nError: object 'defusecheckab' not found\n\nbracecheckab &lt;- funbrace(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = anonbarevec) \n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), anonbarevec, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3` `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weighted.mean.default()`:\n! 'x' and 'w' must have the same length\n\nbracecheckab\n\nError: object 'bracecheckab' not found\n\n\n\n\nBuild and feed a character string\nWe know we need the rlang::quo to get this to work, but we can see the expressions we need in the list inside the function while debugging. So can we build the list wrapped in rlang::quo inside the function? Not very directly, as far as I can tell. But eval(parse(STRING)) seems to be a crude way forward.\nIt works to feed it a character string\n\ncharfuns &lt;- \"rlang::quo(list(mean = function(x) mean(x, na.rm = TRUE), wm = function(x) weighted.mean(x, wt, na.rm = TRUE)))\"\n\n# seems to work. NOW, how can I do that, and do it safely?\n# Likely turn the list into characters, then put rlang::quo on it, and round and round we go. Going to need lots of testing.\n\nAnd a function that parses that\n\nfunchar &lt;- function(data, groupers, sumcols,\n                     FUNS, ...) {\n  \n  FUNS &lt;- eval(parse(text = FUNS))\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\ncharcheck &lt;- funchar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = charfuns) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharcheck\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nSo, that works. This is getting very messy though. We certianly don’t want to make a user send us that string- that’s far worse than just wrapping in rlang::quo.\nBUT, does this allow us to programatically build that string inside the function? Should try without it first, and then if it fails, build the string. Make a function that does that.\n\nfunbracechar &lt;- function(data, groupers, sumcols,\n                     FUNS, ...) {\n  \n  gm &lt;- try(data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup(), silent = TRUE)\n  \n  if (inherits(gm, 'try-error')) {\n    fchar &lt;- paste0(c(\"rlang::quo(\", deparse(FUNS), \")\"), collapse = '')\n    # FUNS2 &lt;- eval(parse(text = fchar)) # base R\n    FUNS3 &lt;- rlang::eval_tidy(rlang::parse_expr(fchar)) # rlang claims to be faster?\n  }\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS3}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  \n  return(gm)\n}\n\nWill need to test this with ~ functions, bare names, and \\(x) anonymous functions. I don’t think I expect it to work with character names. But it might work with character specification of the whole function?\n\nanonbare &lt;- list(mean = \\(x) mean(x, na.rm = TRUE),\n                    wm = \\(x) weighted.mean(x, wt, na.rm = TRUE))\n\nit works with the \\(x) style anonymous function\n\ncharcheck &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = anonbare) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharcheck\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nworks with tilde-style anonymous functions\n\nfunstilde &lt;- list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE))\n\nchartilde &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funstilde) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\nchartilde\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nand unsurprisingly with the long form anonymous\n\nfunsfullanon &lt;- list(mean = function(x) mean(x, na.rm = TRUE),\n                     wm = function(x) weighted.mean(x, wt, na.rm = TRUE))\n\ncharfullanon &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsfullanon) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharfullanon\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nIt works with custom functions with the argument inside. If we look at what the deparse does inside the debugger, we can see that it expands those functions out, and so the thing that gets quoted is actually exactly the same as the previous version in funsfullanon.\n\nweightcustom &lt;- function(x) {\n  weighted.mean(x, w = wt, na.rm = TRUE)\n}\n\nmeancustom &lt;- function(x) {\n  mean(x, na.rm = TRUE)\n}\n\nfunscustom &lt;- list(mean = meancustom,\n                     wm = weightcustom)\n\ncharweightcustom &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funscustom) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharweightcustom\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nIt works when there’s a single function, not a list, too. If we look in the debugger, this does still fail with the simple {{}}, triggers the try loop, and gets deparsed.\n\nfunsnolist &lt;- weightcustom\n\ncharweightcustom &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsnolist) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharweightcustom\n\n# A tibble: 11 × 5\n    gear  carb prefix_1_disp prefix_1_drat prefix_1_nampg\n   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1         208.           3.13           21.4\n 2     3     2         347.           3.03           17.8\n 3     3     3         276.           3.07          NaN  \n 4     3     4         425.           3.19           12.3\n 5     4     1          85.3          4.05           27.5\n 6     4     2         128.           4.05           24.6\n 7     4     4         164.           3.91           21  \n 8     5     2         110.           4.16           30.4\n 9     5     4         351            4.22          NaN  \n10     5     6         145            3.62           19.7\n11     5     8         301            3.54           15  \n\n\nI expect it not to work for a character vector, and it doesn’t.\n\nfunsnolistchar &lt;- 'weightcustom'\n\ncharnolistchar &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsnolistchar) \n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), \"weightcustom\",\n  .names = \"prefix_{.fn}_{.col}\")`.\nCaused by error in `across()`:\n! `.fns` must be a function, a formula, or a list of functions/formulas.\n\ncharnolistchar\n\nError in eval(expr, envir, enclos): object 'charnolistchar' not found\n\n\nBut, does it work if we add an mget line?\n\nfunbracechar &lt;- function(data, groupers, sumcols,\n                     FUNS, ...) {\n  if (is.character(FUNS)) {\n    FUNS &lt;- mget(FUNS, inherits = TRUE)\n  }\n  gm &lt;- try(data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup(), silent = TRUE)\n  \n  if (inherits(gm, 'try-error')) {\n    fchar &lt;- paste0(c(\"rlang::quo(\", deparse(FUNS), \")\"), collapse = '')\n    # FUNS2 &lt;- eval(parse(text = fchar)) # base R\n    FUNS3 &lt;- rlang::eval_tidy(rlang::parse_expr(fchar)) # rlang claims to be faster?\n    gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS3}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  }\n  \n  \n  \n  return(gm)\n}\n\nIt works for a single function\n\nfunsnolistchar &lt;- 'weightcustom'\n\ncharnolistchar &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsnolistchar) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharnolistchar\n\n# A tibble: 11 × 5\n    gear  carb prefix_weightcustom_disp prefix_weightcustom_drat\n   &lt;dbl&gt; &lt;dbl&gt;                    &lt;dbl&gt;                    &lt;dbl&gt;\n 1     3     1                    208.                      3.13\n 2     3     2                    347.                      3.03\n 3     3     3                    276.                      3.07\n 4     3     4                    425.                      3.19\n 5     4     1                     85.3                     4.05\n 6     4     2                    128.                      4.05\n 7     4     4                    164.                      3.91\n 8     5     2                    110.                      4.16\n 9     5     4                    351                       4.22\n10     5     6                    145                       3.62\n11     5     8                    301                       3.54\n# ℹ 1 more variable: prefix_weightcustom_nampg &lt;dbl&gt;\n\n\nAnd for multiple functions if they are in a character vector\n\nfunsmultichar &lt;- c('mean', 'weightcustom')\n\ncharmultichar &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsmultichar) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharmultichar\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_weightcustom_disp prefix_mean_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;                    &lt;dbl&gt;            &lt;dbl&gt;\n 1     3     1            201.                     208.              3.18\n 2     3     2            346.                     347.              3.04\n 3     3     3            276.                     276.              3.07\n 4     3     4            416.                     425.              3.22\n 5     4     1             84.2                     85.3             4.06\n 6     4     2            121.                     128.              4.16\n 7     4     4            164.                     164.              3.91\n 8     5     2            108.                     110.              4.1 \n 9     5     4            351                      351               4.22\n10     5     6            145                      145               3.62\n11     5     8            301                      301               3.54\n# ℹ 3 more variables: prefix_weightcustom_drat &lt;dbl&gt;, prefix_mean_nampg &lt;dbl&gt;,\n#   prefix_weightcustom_nampg &lt;dbl&gt;\n\n\nBut not for a list. This is not unexpected- the mget is in if(is.character(FUNS)) , and so the list won’t get mgot. I think that’s good enough for now. It would be doable obviously to purrr over the list and mget the items that are characters, but that’s not really the focus here. We have figured out an (ugly) workaround for the dplyr 1.1 issue, and that will have to do for now- applying it over all possible organisations of FUNS will have to be for another day.\n\nfunsmulticharl &lt;- list(m = 'mean', wm = 'weightcustom')\n\ncharmulticharl &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsmulticharl) \n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nCaused by error in `across()`:\n! `.fns` must be a function, a formula, or a list of functions/formulas.\n\ncharmulticharl\n\nError in eval(expr, envir, enclos): object 'charmulticharl' not found\n\n\n\n\neval_tidy\nI keep feeling like eval_tidy should work somehow, since it allows the .data pronoun, but I can’t seem to get my head around how it would work here. I’d happily write something like \\(x) eval_tidy(weighted.mean(x, .data$wt, na.rm = TRUE))). Maybe I can get that to work with the right sort of enquoing? I tried for a while and couldn’t figure it out, but maybe come back fresh later on.",
    "crumbs": [
      "Code Demos",
      "Building packages",
      "Programming with dplyr and rlang"
    ]
  },
  {
    "objectID": "website_notes/multiple_render_formats.html",
    "href": "website_notes/multiple_render_formats.html",
    "title": "Multiple render formats",
    "section": "",
    "text": "It’s typically nicest to work with Quarto rendered to html. But sometimes we need other formats (collaborators, publishing, etc). We can render to different formats individually (e.g. word), but we can also produce multiple formats from the same qmd.\nThis multi-render is really nice, in that it provides the html with a link to download the others (word, pdf, etc- see top right of this page). It’s very close to the new Manuscripts project type that is coming in Quarto 1.4, though that keeps breaking for me. As an example, this (the header for this file) will produce an html, docx, and pdf, with links to the docx and pdf files at the top right.\nformat:\n  html:\n    toc: true\n    comments:\n      hypothesis: true\n  docx:\n    toc: true\n    toc-depth: 2\n    prefer-html: true\n    # setting these to the html defaults so they don't get jumbled between format\n    fig-width: 7\n    fig-height: 5\n  pdf: \n    toc: true\n    colorlinks: true\nA nice thing about this approach is that it lets a single page on a website have downloadable word/pdf files, whereas I couldn’t get the manuscript to work as a subset of a website (though I didn’t try very hard). Manuscripts do look like they’ll be nicer for author info and formatting, but I kept breaking the development version, so I’ll try that again later.\nIf working in Rstudio, this also lets us click the arrow next to the render button to choose which format to use for a manual render.\nI’ve also turned on Hypothes.is comments for this page, dealt with in more detail elsewhere. The short story is you can make annotations and highlights on this page, which can be public or private, depending on your settings.\nJust to make that fancier across outputs, I’ll throw a figure on and maybe some math\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt(2\\pi)}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\\]\n\nggplot2::ggplot(iris, ggplot2::aes(x = Sepal.Length, \n                                   y = Petal.Length, \n                                   color = Species)) +\n  ggplot2::geom_point()",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Rendering multiple formats (and serving from web)"
    ]
  },
  {
    "objectID": "website_notes/quarto_images_misc.html",
    "href": "website_notes/quarto_images_misc.html",
    "title": "Images, divs, and similar",
    "section": "",
    "text": "To include an image with a link, use the usual image insertion code, e.g. this is not clickable\n![Surus](../_images/surus.png)\n\n\n\nSurus\n\n\nbut modified with square brackets and parentheses similarly to text links makes it clickable.\n[![Surus](../_images/surus.png)](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\n\n\nSurus\n\n\nIf we want to adjust the size, it needs to go in the square brackets, and not have spaces.\n[![Surus](../_images/surus.png){width=200}](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\n\n\nSurus\n\n\nWe can also use a figure div to make it cross-referenceable.\n::: {#fig-elephant}\n\n[![Surus](../_images/surus.png)](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\nSurus\n\n:::\n\n\n\n\n\n\nSurus\n\n\n\n\nFigure 1: Surus",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Figures in quarto"
    ]
  },
  {
    "objectID": "website_notes/quarto_images_misc.html#links-from-images",
    "href": "website_notes/quarto_images_misc.html#links-from-images",
    "title": "Images, divs, and similar",
    "section": "",
    "text": "To include an image with a link, use the usual image insertion code, e.g. this is not clickable\n![Surus](../_images/surus.png)\n\n\n\nSurus\n\n\nbut modified with square brackets and parentheses similarly to text links makes it clickable.\n[![Surus](../_images/surus.png)](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\n\n\nSurus\n\n\nIf we want to adjust the size, it needs to go in the square brackets, and not have spaces.\n[![Surus](../_images/surus.png){width=200}](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\n\n\nSurus\n\n\nWe can also use a figure div to make it cross-referenceable.\n::: {#fig-elephant}\n\n[![Surus](../_images/surus.png)](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\nSurus\n\n:::\n\n\n\n\n\n\nSurus\n\n\n\n\nFigure 1: Surus",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Figures in quarto"
    ]
  },
  {
    "objectID": "website_notes/quarto_images_misc.html#icons",
    "href": "website_notes/quarto_images_misc.html#icons",
    "title": "Images, divs, and similar",
    "section": "Icons",
    "text": "Icons\nWe can include icon items in the yaml to include icons from bootstrap, but what if we want to include others? The impetus for this is google scholar, but I could also foresee using something like phylopic.\nThe trick it looks like is to install (or write, but let’s leave that for later) a Quarto extension. Academicons comes up early in google, but Iconify seems to contain that set and a bunch more.\nTo install, use quarto add mcanouil/quarto-iconify at the terminal.\nTo use, include \"\". Note that when you find the icon, Iconify gives the name as setname:iconname, and we need to quote it in the yaml.\nThis seems to work here, e.g. the google scholar icon is\n\n\n\nbut I can’t get it to work in the yaml, even with quotes.\nSimply downloading the image and giving a path in the yaml doesn’t work either.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Figures in quarto"
    ]
  },
  {
    "objectID": "website_notes/quarto_website_github.html",
    "href": "website_notes/quarto_website_github.html",
    "title": "Quarto website",
    "section": "",
    "text": "I want to use quarto to build a website hosted on github pages. I have a few goals for it, but step one is to figure out how to do it.\nI’ve already started a github pages repo, and had started putting things in it before I realised I was probably not working in the best way (and some things needed to be private). So before any commits, I moved the work out and want to just start clean and see how to do it. I’ll walk through the process here.\nI’ll start by following the quarto docs, but may diverge. Using the Rstudio version, but will likely use a bit of VS too for python.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Getting started"
    ]
  },
  {
    "objectID": "website_notes/quarto_website_github.html#working-on-setting-up-a-quarto-website",
    "href": "website_notes/quarto_website_github.html#working-on-setting-up-a-quarto-website",
    "title": "Quarto website",
    "section": "",
    "text": "I want to use quarto to build a website hosted on github pages. I have a few goals for it, but step one is to figure out how to do it.\nI’ve already started a github pages repo, and had started putting things in it before I realised I was probably not working in the best way (and some things needed to be private). So before any commits, I moved the work out and want to just start clean and see how to do it. I’ll walk through the process here.\nI’ll start by following the quarto docs, but may diverge. Using the Rstudio version, but will likely use a bit of VS too for python.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Getting started"
    ]
  },
  {
    "objectID": "website_notes/quarto_website_github.html#set-up-github-pages",
    "href": "website_notes/quarto_website_github.html#set-up-github-pages",
    "title": "Quarto website",
    "section": "Set up github pages",
    "text": "Set up github pages\nI did this a while ago, will come back to it.\nClone the repo locally.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Getting started"
    ]
  },
  {
    "objectID": "website_notes/quarto_website_github.html#start-as-a-website-project",
    "href": "website_notes/quarto_website_github.html#start-as-a-website-project",
    "title": "Quarto website",
    "section": "Start as a website project",
    "text": "Start as a website project\nCould I have converted from a normal project? Probably. And I could only get to the ‘quarto website’ option if I made a new directory. So even though I already cloned the repo from github, I put the project in a new dir, and then will copy it into the repo (I guess?).\nI actually made it in a dir, then started a git repo in that dir and set its remote to hit the url of my github.io repo following the instructions on github for starting a local repo and pointing it to github.\nProject seems to work when I click render, though it renders in browser not in Viewer pane (which is actually nicer, just not what the docs say).\nI had my repo in dropbox, as that seems like it usually works fine for other repos and gives another layer of backup. But it was failing here with lots of errors about files being in use by other processes. Moved it to Documents and seems to work fine.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Getting started"
    ]
  },
  {
    "objectID": "website_notes/quarto_website_github.html#setting-up-nav",
    "href": "website_notes/quarto_website_github.html#setting-up-nav",
    "title": "Quarto website",
    "section": "Setting up nav",
    "text": "Setting up nav\nI’m not entirely sure what I want the structure to be, but likely a brief home page, navbar at top with things like ‘Research’, ‘About’, ‘Code examples’, etc. Lots of options here, I guess just cobble something together quickly.\nOne question I have is what happens when I start committing to git. Does it auto-publish? It looks like no, according to quarto, if I set up to render to docs and don’t push master. Or if I publish from a gh-pages branch though that’s not working on windows.\nThe .yaml seems to be where all the website structure goes- nav bars, search, etc.\nI think I’m going to end up with something fairly complex for nav, but for now, maybe try broad categories across the top, then specifics down the side. Add additional nesting later.\nSeems reasonably ok, with ability to have sections within contents in the sidebar (I think).",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Getting started"
    ]
  },
  {
    "objectID": "website_notes/quarto_website_github.html#questions",
    "href": "website_notes/quarto_website_github.html#questions",
    "title": "Quarto website",
    "section": "Questions",
    "text": "Questions\nIf I render a single file, does it render the whole website? seems like yes. If I want to render single pages (like to test them without having to re-render everything), can use the terminal quarto render filename.qmd or a subdir quarto render subdir/. The output ends up in the _site directory.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Getting started"
    ]
  },
  {
    "objectID": "website_notes/quarto_website_github.html#pushing-to-github",
    "href": "website_notes/quarto_website_github.html#pushing-to-github",
    "title": "Quarto website",
    "section": "Pushing to github",
    "text": "Pushing to github\nThe publishing the gh-pages branch seems the nicest, but when I set this up there was a big warning not to do that on Windows. So, I guess I did it the render to docs way. I think I’ll move to the gh-pages way now that the warning is gone, but for posterity:\n\nDocs\nadd output-dir: docs to the _quarto.yml and then create a .nojekyll file. Then quarto render to render to docs. I think I’ll also add a _quarto.yml.local with\nexecute:\n  cache: true\nto cache output and avoid long re-renders ( I hope). Seems to- re-clicking render was much faster.\nTo set to docs, go to repo, then settings –&gt; Pages (on left) –&gt; deploy from a branch, and choose the branch (likely Main) and /docs instead of /root.\nSo, I’ve been developing on dev, I guess I’ll merge main and see what happens.\nDon’t forget to merge and push ‘main’ if using the publish to /docs method. Otherwise no changes will actually appear.\n\n\nSwitching to gh-pages\nI think I’ll just toss all the stuff I made above and follow the instructions for gh-pages.\nIn short I stopped tracking docs/ and _files and other things that happen when the site renders.\nThen I run the code to create a gh-pages branch\ngit checkout --orphan gh-pages\ngit reset --hard # make sure you've committed changes before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\nAnd then go change the source branch to gh-pages as in the docs and delete the .nojekyll file. Note that I had to explicitly push the gh-pages branch after creating it before github saw it.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Getting started"
    ]
  },
  {
    "objectID": "website_notes/render_word.html",
    "href": "website_notes/render_word.html",
    "title": "Quarto word render",
    "section": "",
    "text": "Rendering to word is never as straightfoward as I’d like. In theory, it should be as easy as adding\nor\nSometimes that does work. If you have html output in any figures, e.g. mermaid or graphviz diagrams, it will fail. The solution is to put\nin the _quarto.yml. In theory it could go in the file header yaml, but it seems to not work there. The catch is, this will just skip those items when render, so if you want them, you’ll need to export somehow and read back in. In practice I find this really annoying, because it means we need extra code to make the same plot in different formats, which has to be maintained and never ends up looking the same.\nIf you want figure sizes to match html versions of the document, use\nIf you want to use a word template, (since pandoc’s default is old), put a new file somewhere and link it.\nThe font and similar are included in the template if it’s blank. But things like heading styles and tables of contents actually have to occur in the doc. So put in some headers, table of contents, figs, and tables if you want those formatting structures to work. Adding figs and tables doesn’t actually seem to adjust the way the captioning works, because they’re in boxes. I think this works better in Quarto 1.4, but that’s still in beta and breaks other things for me.\nThis template file can also do useful things like set line numbering, spacing, and page numbers.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Rendering to word"
    ]
  },
  {
    "objectID": "website_notes/render_word.html#multiple-formats",
    "href": "website_notes/render_word.html#multiple-formats",
    "title": "Quarto word render",
    "section": "Multiple formats",
    "text": "Multiple formats\nOne thing that’s very handy is having multiple render targets. I’m not doing that here to keep this doc clean for word–specific issues.",
    "crumbs": [
      "Code Demos",
      "Quarto",
      "Rendering to word"
    ]
  }
]