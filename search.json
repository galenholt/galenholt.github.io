[
  {
    "objectID": "website_notes/updating_website.html",
    "href": "website_notes/updating_website.html",
    "title": "Updating website",
    "section": "",
    "text": "I often work on a bunch of pages and actually merge and push rarely. To identify pages that are present in the repo but not yet in the yaml (and so will not be included in the website), I wrote a little function that finds the files not in the yaml and also qmds in the yml but without a file, as when we change filenames. It’s available in the functions dir and here:\n\nfind_missing_pages &lt;- function() {\n  qmd_in_proj &lt;- list.files(pattern = \"*.qmd\", recursive = TRUE)\n  qmd_in_yml &lt;- readLines(\"_quarto.yml\") |&gt; \n    purrr::keep(\\(x) grepl(\".qmd\", x)) |&gt; \n    gsub('\\\\s', '', x = _) |&gt; \n    gsub('-', '', x = _) |&gt; \n    gsub('href:', '', x = _)\n  # The values in the `render` section are still there, but that's ok\n  \n  not_present &lt;- which(!(qmd_in_proj %in% qmd_in_yml))\n  \n  missing_pages &lt;- qmd_in_proj[not_present]\n  \n  no_file &lt;- which(!(qmd_in_yml %in% qmd_in_proj))\n  \n  broken_links &lt;- qmd_in_yml[no_file]\n\n  \n  return(list(missing_pages = missing_pages, no_file_present = broken_links))\n}\n\n\n\n\nI keep getting dead links, because I think I don’t understand how links of the form\n[words](path/to/quarto_doc.qmd)\nwork. Do I need the path? Should the path be relative to the root, or the particular document being rendered?\nTo test, this doc is in the /website_notes folder, as is quarto_website_github.qmd, while another set of small helpers is in /small_helpers/smallpieces.qmd. I’m going to try to link to both of those in a couple different ways and see what works. Actual text is in code chunks so it’s visible.\nno outer dir, same folder quarto_website_github.qmd : WORKS\n[quarto_website_github.qmd](quarto_website_github.qmd)\nno outer dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](smallpieces.qmd)\nouter dir, same folder quarto_website_github.qmd : FAILS\n[quarto_website_github.qmd](website_notes/quarto_website_github.qmd)\nouter dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](small_helpers/smallpieces.qmd)\nreferenced to file dir, different folder smallpieces.qmd : (doesn’t make sense to do this for the same folder, since the implication is that the reference is to the directory with the qmd in it). WORKS\n[smallpieces.qmd](../small_helpers/smallpieces.qmd)\nSo, LINKS NEED TO BE RELATIVE TO THE LOCATION OF THE FILE. That’s really annoying, and I thought would have been controlled by setting execute-dir: project in the yaml, but that doesn’t seem to be the case.\nIs there a good way to find and fix dead links? Need to look.\n\n\n\nVery frequently pushes to main will still not seem to work- the website will remain in the old state, despite being sure we’ve pushed to main and the local render and preview has all the new content. The issue seems to be caching, and I’m not sure why it’s so aggressive. In chrome, you can open the Inspector, then right-click on the refresh button and then “empty cache and hard reload”. Or incognito window, or clear the cache some other way. It’s very annoying though.\n\n\n\nI had thought that pushing the ‘Render’ button in Rstudio built the whole site- it throws errors for other qmds than the one I currently build. BUT, if that’s all I do and then push and publish, pages that I haven’t specifically rendered are outdated- e.g. the sidebar and content don’t show updates. So it seems that we have to actually quarto render at the terminal first? I think- I’m trying to do that and all the R-python pages have broken.\nAh. I see now that there’s a warning about this in the quarto docs:\n\nAs you preview your site, pages will be rendered and updated. However, if you make changes to global options (e.g. _quarto.yml or included files) you need to fully re-render your site to have all of the changes reflected. Consequently, you should always fully quarto render your site before deploying it, even if you have already previewed changes to some pages with the preview server.\n\n\n\n\nIf there is a folder called ‘website’ in the repo, quarto will throw an error on render ‘unsupported project type ../website’. The solution seems to be to change the folder name.\n\n\n\nI get a lot of errors rendering this site. Often the solution is to trash the _cache and _filesdirectories for the offending files, along with any generated .html or .md . What seems to work the best though, is to delete everything in .Rproj.user. And often restart Rstudio.\nSome common errors are\nError: path for html_dependency not found: C:/Users/username/AppData/Local/Temp/Rtmp...\nAn error about permissions being denied (os error 5) trying to move site_libs: ERROR: PermissionDenied: Access is denied. (os error 5), rename 'path/to/repo/site_libs' -&gt; 'path/to/repo/docs/site_libs'\nPart of the issue seems to be Dropbox, even if the code is only in a backup directory and not a fully-synced directory. It sometimes works to quit dropbox, but sometimes we need to do that and restart."
  },
  {
    "objectID": "website_notes/updating_website.html#website",
    "href": "website_notes/updating_website.html#website",
    "title": "Updating website",
    "section": "",
    "text": "I often work on a bunch of pages and actually merge and push rarely. To identify pages that are present in the repo but not yet in the yaml (and so will not be included in the website), I wrote a little function that finds the files not in the yaml and also qmds in the yml but without a file, as when we change filenames. It’s available in the functions dir and here:\n\nfind_missing_pages &lt;- function() {\n  qmd_in_proj &lt;- list.files(pattern = \"*.qmd\", recursive = TRUE)\n  qmd_in_yml &lt;- readLines(\"_quarto.yml\") |&gt; \n    purrr::keep(\\(x) grepl(\".qmd\", x)) |&gt; \n    gsub('\\\\s', '', x = _) |&gt; \n    gsub('-', '', x = _) |&gt; \n    gsub('href:', '', x = _)\n  # The values in the `render` section are still there, but that's ok\n  \n  not_present &lt;- which(!(qmd_in_proj %in% qmd_in_yml))\n  \n  missing_pages &lt;- qmd_in_proj[not_present]\n  \n  no_file &lt;- which(!(qmd_in_yml %in% qmd_in_proj))\n  \n  broken_links &lt;- qmd_in_yml[no_file]\n\n  \n  return(list(missing_pages = missing_pages, no_file_present = broken_links))\n}\n\n\n\n\nI keep getting dead links, because I think I don’t understand how links of the form\n[words](path/to/quarto_doc.qmd)\nwork. Do I need the path? Should the path be relative to the root, or the particular document being rendered?\nTo test, this doc is in the /website_notes folder, as is quarto_website_github.qmd, while another set of small helpers is in /small_helpers/smallpieces.qmd. I’m going to try to link to both of those in a couple different ways and see what works. Actual text is in code chunks so it’s visible.\nno outer dir, same folder quarto_website_github.qmd : WORKS\n[quarto_website_github.qmd](quarto_website_github.qmd)\nno outer dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](smallpieces.qmd)\nouter dir, same folder quarto_website_github.qmd : FAILS\n[quarto_website_github.qmd](website_notes/quarto_website_github.qmd)\nouter dir, different folder smallpieces.qmd : FAILS\n[smallpieces.qmd](small_helpers/smallpieces.qmd)\nreferenced to file dir, different folder smallpieces.qmd : (doesn’t make sense to do this for the same folder, since the implication is that the reference is to the directory with the qmd in it). WORKS\n[smallpieces.qmd](../small_helpers/smallpieces.qmd)\nSo, LINKS NEED TO BE RELATIVE TO THE LOCATION OF THE FILE. That’s really annoying, and I thought would have been controlled by setting execute-dir: project in the yaml, but that doesn’t seem to be the case.\nIs there a good way to find and fix dead links? Need to look.\n\n\n\nVery frequently pushes to main will still not seem to work- the website will remain in the old state, despite being sure we’ve pushed to main and the local render and preview has all the new content. The issue seems to be caching, and I’m not sure why it’s so aggressive. In chrome, you can open the Inspector, then right-click on the refresh button and then “empty cache and hard reload”. Or incognito window, or clear the cache some other way. It’s very annoying though.\n\n\n\nI had thought that pushing the ‘Render’ button in Rstudio built the whole site- it throws errors for other qmds than the one I currently build. BUT, if that’s all I do and then push and publish, pages that I haven’t specifically rendered are outdated- e.g. the sidebar and content don’t show updates. So it seems that we have to actually quarto render at the terminal first? I think- I’m trying to do that and all the R-python pages have broken.\nAh. I see now that there’s a warning about this in the quarto docs:\n\nAs you preview your site, pages will be rendered and updated. However, if you make changes to global options (e.g. _quarto.yml or included files) you need to fully re-render your site to have all of the changes reflected. Consequently, you should always fully quarto render your site before deploying it, even if you have already previewed changes to some pages with the preview server.\n\n\n\n\nIf there is a folder called ‘website’ in the repo, quarto will throw an error on render ‘unsupported project type ../website’. The solution seems to be to change the folder name.\n\n\n\nI get a lot of errors rendering this site. Often the solution is to trash the _cache and _filesdirectories for the offending files, along with any generated .html or .md . What seems to work the best though, is to delete everything in .Rproj.user. And often restart Rstudio.\nSome common errors are\nError: path for html_dependency not found: C:/Users/username/AppData/Local/Temp/Rtmp...\nAn error about permissions being denied (os error 5) trying to move site_libs: ERROR: PermissionDenied: Access is denied. (os error 5), rename 'path/to/repo/site_libs' -&gt; 'path/to/repo/docs/site_libs'\nPart of the issue seems to be Dropbox, even if the code is only in a backup directory and not a fully-synced directory. It sometimes works to quit dropbox, but sometimes we need to do that and restart."
  },
  {
    "objectID": "website_notes/render_pdf.html",
    "href": "website_notes/render_pdf.html",
    "title": "Quarto render pdfs",
    "section": "",
    "text": "Just like the word version, sometimes we want to render something other than just html. This is working through specific issues with pdf.\nformat:\n  pdf: default\nIt was just working for me to use elsevier-pdf: default in my header, but now it’s not working to just use simple pdf. I get the error\ncompilation failed- no matching packages\nLaTeX Error: File `scrartcl.cls' not found.\nConsensus seems to be to reinstall tinytex. quarto install tinytex says it’s installed but will update.\nThat did get it to work. I think somehow I had tinytex, but not the KOMA Script documentclasses Quarto uses by default."
  },
  {
    "objectID": "website_notes/quarto_profiles.html",
    "href": "website_notes/quarto_profiles.html",
    "title": "Quarto profiles",
    "section": "",
    "text": "Quarto projects with lots of pieces (e.g. websites or books) are great, but once they get big, rendering single documents can be a pain. By default, both the Rstudio ‘Render’ button and quarto render document_name.qmd at the command line render the whole project as defined in _quarto.yml. They use pre-renders for other pages, but always seem to end up re-rendering a lot of other pages. If some of those pages break (e.g. works in progress), the whole process falls down. If we’re working on a single document and want to test its rendering, that can be very annoying. For example, if I want to render this document to check it, I don’t want to render an entire website, including other pages that are still in development.\nQuarto profiles are a partial solution, but need a bit of tweaking to set up.\nThe basic idea of profiles is to keep the common bits in _quarto.yml, and then have changes in _quarto-profilename.yml. The examples are all about rendering different versions of complex projects, with defined sets of pages to be rendered in the different profiles. Here, we have some specific requirements that aren’t obvious from the examples:"
  },
  {
    "objectID": "website_notes/quarto_profiles.html#implementation",
    "href": "website_notes/quarto_profiles.html#implementation",
    "title": "Quarto profiles",
    "section": "Implementation",
    "text": "Implementation\nWe set up a general _quarto.yml file that contains the website headers and overall formatting code. We then have a _quarto-fullsite.yml with the full website build (structure of the pages). Finally, we have a _quarto-singlefile.yml without the structure of the website.\nIn the main _quarto.yml, I set\nprofile:\n  group:\n    - [singlefile, fullsite]\nso that pressing the render button or using quarto render at the terminal defaults to the singlefile.\nThen, to render the whole thing, use quarto render --profile fullsite. And quarto publish gh-pages --profile fullsite works for the github pages publish.\nThe division of what goes in the main _quarto.yml and _quarto-fullsite.yml will be project-dependent. The main issue here is how to specify _quarto-singlefile.yml.\n\nSinglefile - simple\nOne option is for _quarto-singlefile.yml to consist only of\nproject:\n  render:\n    - \"!*.qmd\"\nwhich is a bit surprising- it has all rendering turned off. This takes advantage of quarto rendering the active page even when it is not supposed to be part of a project.\nThe catch is that this approach works by just bypassing the rest of the yaml options. And so the rest of the website structure (color schemes, headers, etc) aren’t there, and more critically, if the execute_dir has been set to project (as I typically do), that gets lost and the working directory reverts to the file directory. Note that it doesn’t work to put execute-dir: project in the _quarto-singlefile.yml , because the workaround here bypasses all the render arguments.\nTo check the working directory as we try different things,\n\ngetwd()\n\n[1] \"C:/Users/galen/Documents/code/web_testing/galen_website\"\n\n\n\n\nSinglefile- extra step\nThese issues go away if we explicitly put the filename in the render: slot of the yaml instead of \"!*.qmd. However, the point is to render the active document, and so hardcoding this isn’t an option.\nA temporary workaround is to write the yaml from R and then render. Make a simple yaml. This can’t be in the notebook as here, because it won’t exist until during the render process, and it’s needed to control that process. So, this would need to be run at the console pre-render. I have this simple version commented out, because the later one is better.\n```{r}\nmake_simpleyml &lt;- function(renderfile) {\n    simple_yaml &lt;- list()\n    simple_yaml$project &lt;- list()\n    simple_yaml$project$render &lt;- list(renderfile)\n  yaml::write_yaml(simple_yaml, '_quarto-singlefile.yml')\n}\n```\nThen, calling this creates the singlefile we want.\n```{r}\nmake_simpleyml('website_notes/quarto_profiles.qmd')\n\n```\nAnd if we use rstudio, we can use it to auto-generate, but only interactively (rstudio is not running when quarto renders). In that case, this works if we run it ad-hoc or if we pre run all before rendering.\n\nmake_simpleyml &lt;- function(renderfile = 'auto') {\n\n  if (renderfile == 'auto') {\n    if (rstudioapi::isAvailable()) {\n      projpath &lt;- rstudioapi::getActiveProject()\n      docpath &lt;- rstudioapi::documentPath()\n      projdir &lt;- sub(\".*/([^/]+)$\", \"\\\\1\", projpath)\n      reldocpath &lt;- sub(paste0(\".*\", projdir, \"/\"), \"\", docpath)\n      renderfile &lt;- reldocpath\n    } else {\n      rlang::inform(\"Rstudio not running, do not want new profiles created while rendering, skipping\")\n      return(invisible())\n    }\n\n\n  }\n\n\n  simple_yaml &lt;- list()\n  simple_yaml$project &lt;- list()\n  simple_yaml$project$render &lt;- list(renderfile)\n  yaml::write_yaml(simple_yaml, '_quarto-singlefile.yml')\n}\n\nAnd now it works to just call that function with a document open. It can be done in a chunk, as here, but that only works while interactively working with the notebook. Rendering fails because it needs Rstudio to be running to use the renderfile = 'auto'. However, it can also be done from the console and still grabs the active file, which is probably fine because this needs to happen pre-render anyway.\n\nmake_simpleyml()\n\nRstudio not running, \n                    do not want new profiles created while rendering, \n                    so skipping\n\n\nNote that using full vs relative paths for renderfile matters. If we use full paths (e.g. C://…) It overrides what _quarto.yml has in execute_dir, and sets it back to the file.\n\n# Works\n# make_simpleyml(reldocpath)\n\n# Sets the working directory to the file\n# make_simpleyml(fulldocpath)"
  },
  {
    "objectID": "website_notes/quarto_profiles.html#other-approaches-that-dont-work-yet",
    "href": "website_notes/quarto_profiles.html#other-approaches-that-dont-work-yet",
    "title": "Quarto profiles",
    "section": "Other approaches that don’t work (yet)",
    "text": "Other approaches that don’t work (yet)\nCan we use lua? It will print to markdown, but will it work in an R chunk? First, install lua-env with quarto add mcanouil/quarto-lua-env, then put\n# filters: \n#   - lua-env\nin the header. Then\n\n# {{&lt; lua-env quarto.doc.input_file &gt;}}\n\ngives the input_file. But I can’t get it to work in an R chunk and it’s REALLY buggy even in markdown so I’ve had to put it in an R block and comment it out to even run this file without errors. Ideally, we want to auto-detect the active script, and generate the needed yml when the Render button is pressed.\nPre-render scripts look like they have some useful environment variables, but in trying them I can’t seem to access those variables, and they seem to run after the yaml setup and before the render, so also don’t work for yaml modification."
  },
  {
    "objectID": "website_notes/quarto_change_yml.html",
    "href": "website_notes/quarto_change_yml.html",
    "title": "Switching quarto ymls",
    "section": "",
    "text": "Quarto projects with lots of pieces (e.g. this website) are great, but once they get big, rendering single documents can be a pain. By default, both the Rstudio ‘Render’ button and quarto render document_name.qmd at the command line render the whole project as defined in _quarto.yml. They supposedly use pre-renders for other pages, but always seem to end up re-rendering a bunch of things. If we’re working on a single document and want to test its rendering, that can be very annoying. For example, if I want to render this document to check it, I don’t want to render the entire website. I’m surprised quarto itself doesn’t have a way to switch behaviour. (I think profiles are that way. I can sort of get them to work about as well as what’s below, but it needs more testing.\nThe workaround developed below does a file rename/swap with _quarto.yml- have one simple version and one complex version, and trade them out.\nWe want to have a directory _yml to store various .yaml files, and then have a simple call to choose between them. To get there, we need to\nCould this be done most cleanly inside Quarto itself? Yes. It would be really nice to be able to do something like quarto render filename.qmd -simple or quarto render filename.qmd -website. But until that’s the case, maybe this will work."
  },
  {
    "objectID": "website_notes/quarto_change_yml.html#setup",
    "href": "website_notes/quarto_change_yml.html#setup",
    "title": "Switching quarto ymls",
    "section": "Setup",
    "text": "Setup\nFirst, set up a new structure programatically. You could always just add files to this manually, if you want a bunch of different yamls. This assumes a single _quarto_project.yml with the complete project definition. In the simplest case, make_multi_yaml just makes the _yml directory. But it can also copy over an existing _quarto.yml and make a simple version.\nNote- this shouldn’t be dangerous to run again- the overwrite argument to file.copy is FALSE by default, so it shouldn’t overwrite the _quarto_project.yml with a simpler version later, but still makes me nervous.\n\nmake_multi_yaml &lt;- function(yamdir = '_yml', \n                            copy_orig = TRUE, \n                            make_simple = TRUE,\n                            leave_orig = TRUE) {\n  \n  \n  if (!dir.exists(yamdir)) {dir.create(yamdir)}\n  \n  if (copy_orig) {\n      file.copy('_quarto.yml', file.path(yamdir, '_quarto_project.yml'))\n      if (!leave_orig) {file.remove('_quarto.yml')}\n  }\n\n  if (make_simple) {\n    make_simple_yaml(file.path(yamdir, '_quarto_project.yml'), yamdir = yamdir)\n  }\n}\n\nThis function just makes a simple yaml from the main one.\n\nmake_simple_yaml &lt;- function(proj_yaml_file = NULL,\n                             yamdir = getwd(), \n                             simple_file = NULL) {\n  # By default, assume yaml is in working directory/_quarto.yml\n  if (is.null(proj_yaml_file)) {\n    proj_yaml_file &lt;- file.path(yamdir, '_quarto.yml')\n  }\n  \n  proj_yaml &lt;- yaml::read_yaml(proj_yaml_file)\n  \n  simple_yaml &lt;- list() \n  simple_yaml$project &lt;- proj_yaml$project\n  # kill the type in case it's complex (e.g. website, book)\n  simple_yaml$project$type &lt;- NULL\n  # kill render options- only rendering a single doc should allow rendering that doc and no others\n  simple_yaml$project$render &lt;- NULL\n  \n  if (is.null(simple_file)) {\n    simple_file &lt;- file.path(yamdir, '_quarto_simple.yml')\n  }\n  \n  yaml::write_yaml(simple_yaml, simple_file)\n  \n}"
  },
  {
    "objectID": "website_notes/quarto_change_yml.html#switching",
    "href": "website_notes/quarto_change_yml.html#switching",
    "title": "Switching quarto ymls",
    "section": "Switching",
    "text": "Switching\nThe main functionality here is to simply copy over a desired .yaml file to the project directory and name it _quarto.yaml so it gets used on render.\n\nuse_quarto_yaml &lt;- function(yamfile = 'project', yamdir = '_yml') {\n  yamfiles &lt;- list.files(yamdir)\n  whichyam &lt;- grepl(yamfile, yamfiles)\n  if (sum(whichyam) &gt; 1) {stop('too many matching yaml files')}\n \n  file.copy(file.path(yamdir, yamfiles[whichyam]), '_quarto.yml', overwrite = TRUE)\n  \n  return(invisible())\n}\n\nThat uses partial matching, so we can run use_quarto_yaml('simple') to switch to that version prior to rendering. This will work whether or not we use the earlier functions to set up the directory- as long as we have unique names for the various yaml options, calling use_quarto_yaml switches which is active, e.g. \n```{r}\nuse_quarto_yaml('simple')\n```\nThis is not run here, because it would reset the yaml on the fly during render, which would be bad. It should be run interactively, just prior to rendering."
  },
  {
    "objectID": "website_notes/commenting_quarto.html",
    "href": "website_notes/commenting_quarto.html",
    "title": "Commenting quarto",
    "section": "",
    "text": "I’d like to be able to comment quarto documents (or at least the html output). The commenting section of the quarto docs suggests we can use Hypothes.is, Utterances, and Giscus. It’s unclear which is better, but all their other demos use Hypothes.is so I think I’ll start there.\nBased on a bit of testing, it looks like putting\ncomments:\n  hypothesis: true\nin the main yml file is all it takes to turn it on across a project. It also works in the yaml headers for single files. I’ve added it to the yaml header in this file, enabling commenting here.\nformat:\n  html:\n    comments:\n      hypothesis: true\nWhat’s nice is that turning on hypothes.is works on local renders, private repos, and public. I’m not entirely sure how it works on local renders, but it does. However it’s working, it persists across renders- if I render this file, comment on it, delete it, and re-render, the comments are still there. They go away if all the commented text is deleted, but come back if it’s re-added. Comments can always be deleted though."
  },
  {
    "objectID": "website_notes/commenting_quarto.html#adding-commenting-ability",
    "href": "website_notes/commenting_quarto.html#adding-commenting-ability",
    "title": "Commenting quarto",
    "section": "",
    "text": "I’d like to be able to comment quarto documents (or at least the html output). The commenting section of the quarto docs suggests we can use Hypothes.is, Utterances, and Giscus. It’s unclear which is better, but all their other demos use Hypothes.is so I think I’ll start there.\nBased on a bit of testing, it looks like putting\ncomments:\n  hypothesis: true\nin the main yml file is all it takes to turn it on across a project. It also works in the yaml headers for single files. I’ve added it to the yaml header in this file, enabling commenting here.\nformat:\n  html:\n    comments:\n      hypothesis: true\nWhat’s nice is that turning on hypothes.is works on local renders, private repos, and public. I’m not entirely sure how it works on local renders, but it does. However it’s working, it persists across renders- if I render this file, comment on it, delete it, and re-render, the comments are still there. They go away if all the commented text is deleted, but come back if it’s re-added. Comments can always be deleted though."
  },
  {
    "objectID": "website_notes/commenting_quarto.html#issues",
    "href": "website_notes/commenting_quarto.html#issues",
    "title": "Commenting quarto",
    "section": "Issues",
    "text": "Issues\nYou do need to sign up for Hypothes.is. There are free and education accounts, but it’s unclear what the differences are. Free seems to be working for me so far, but it hasn’t been very long.\nThe public/private annotations are confusing and pointless, and ‘Post to only me’ in Public doesn’t seem to work. It does seem to work to make a Private Group and just annotate there. In the dropdown in the annotation area, choose ‘New private group’ and then post in that (e.g. in the below, I’d select the JustMe group).\n\n\n\nHypothes.is dropdown\n\n\nGiscus looks like it’s more about leaving comments at the bottom of blog posts, not annotating documents. And it says it requires a public repo. I wonder if that’s actually true, or if commenting is just limited to people with repo access? Either way, it might be good at what it does, but I’m looking for highlights, comments on manuscripts, and so Hypothes.is looks like the only game in town."
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html",
    "href": "vicwater/vicwater_api_howtocall.html",
    "title": "Vicwater api crude testing",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#access-victoria-water-data-through-api",
    "href": "vicwater/vicwater_api_howtocall.html#access-victoria-water-data-through-api",
    "title": "Vicwater api crude testing",
    "section": "Access Victoria water data through API",
    "text": "Access Victoria water data through API\nWe want to access victorian water data for a set of sites. That requires using the api at https://data.water.vic.gov.au/cgi/webservice.exe?[JSON_request] , but it’s poorly documented, and I’ve maybe done one API call ever. Time to figure this out. Will start by piggybacking on the mdba-gauge-getter python that gets water levels as a starting point and then try to get other data.\nFirst, how do we make an API request? Most tutorials use twitter or github, which are well-documented. But let’s try something similar.\npurrr conflicts with jsonlite::flatten, so don’t load tidyverse.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(httr)\nlibrary(jsonlite)\n\n# Actually end up using\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.2.2\n\n\nLooks like the first thing is the base url. The web says it’s this, and the mdba-gauge-getter uses the same, and then appends json_data\n\nvicurl &lt;- \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\n\nI guess I need to specify something to get. But there is no documentation I can find for what the parameters are. The gauge-getter has a few, so I guess start picking things apart.\n\nparams &lt;- list(\"site_list\" = '232202')\n\n\nresponse &lt;- GET(vicurl, query = params)\nresponse\n\nResponse [https://data.water.vic.gov.au/cgi/webservice.exe?site_list=232202]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: text/html\n  Size: 103 B\n\n\nFollowing the R api vignette,\n\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Missing top-level \\\"version\\\" item\\r\\nMissing top-level \\\"params\\\" item\"\n\n\nInteresting. It looked like it returned 200 (good) when I printed response and when I look at it in the View, but actually had errors. Where ARE these results?\nso, can we add those missing ‘top-level’ items? I see now that the gauge-getter has a two-level dict\n\nparams &lt;- list(\"version\" = '2')\nresponse &lt;- GET(vicurl, params = params)\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\nparsed\n\nTry the example. can’t get it to even be a character vector\n\n# demourl &lt;- https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}\n\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\nresponse &lt;- GET(vicurl, query = params)\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 122\n\n$error_msg\n[1] \"Parameter error(s) for function get_db_info:Missing: table_name\"\n\n\nWell, that’s a start. at least I’m not getting the top-level errors. Can i just smash that whole demo into a single params list? without the sublists of ‘params’ and ‘filter_values’?\n\n# params &lt;- list(\"function\" = 'get_db_info',\n#                \"version\" = \"3\",\n#                \"table_name\" = \"site\",\n#                \"station\" = \"221001\")\n# response &lt;- GET(vicurl, query = params)\n# \n# # The parsed barfs\n# # parsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n# # parsed\n# \n# response\n\nWhat am I actually asking for here? GET is using modify_url\n\nmodify_url(vicurl, query = params)\n\n[1] \"https://data.water.vic.gov.au/cgi/webservice.exe?function=get_db_info&version=3\"\n\n\nSo that’s using the ’conventional parameter pairs’ option here, not the json . How do I generate some json so I can see if I’m matching the format? auto_unbox = TRUE is needed to not wrap the second values in brackets.\n\ntoJSON(params, auto_unbox = TRUE)\n\n{\"function\":\"get_db_info\",\"version\":\"3\"} \n\n\nOK, so that looks vaguely right, but not leveled. Can we do lists of lists?\n\nnestparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"hash\",\n                               \"filter_values\" = list(\"station\" = \"221001\")))\ntoJSON(nestparams, auto_unbox = TRUE)\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}} \n\n\nWhoa! that looks right. Now, let’s try it. It immediately fails to just use GET. try looking at modify_url to see why.\n\njsonbit &lt;- toJSON(nestparams, auto_unbox = TRUE)\n\n\nmodify_url(vicurl, query = jsonbit)\n\n[1] \"https://data.water.vic.gov.au/cgi/webservice.exe?{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\n\nmodify_url(vicurl, path = jsonbit)\n\n[1] \"https://data.water.vic.gov.au/{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\nGetting a lot of slashes. does it matter? Maybe?\n\nmodify_url(vicurl, scheme = nestparams)\n\n[1] \"get_db_info://data.water.vic.gov.au/cgi/webservice.exe\"                                                                                    \n[2] \"3://data.water.vic.gov.au/cgi/webservice.exe\"                                                                                              \n[3] \"list(table_name = \\\"site\\\", return_type = \\\"hash\\\", filter_values = list(station = \\\"221001\\\"))://data.water.vic.gov.au/cgi/webservice.exe\"\n\n\nIs httpbin a way to test?\n\nbinurl &lt;- \"http://httpbin.org/get\"\n\nbinr &lt;- GET(binurl, query = jsonbit)\nbinr\n\nResponse [http://httpbin.org/get?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: application/json\n  Size: 689 B\n{\n  \"args\": {\n    \"{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name...\n  }, \n  \"headers\": {\n    \"Accept\": \"application/json, text/xml, application/xml, */*\", \n    \"Accept-Encoding\": \"deflate, gzip\", \n    \"Host\": \"httpbin.org\", \n    \"User-Agent\": \"libcurl/7.64.1 r-curl/4.3.3 httr/1.4.4\", \n    \"X-Amzn-Trace-Id\": \"Root=1-638ffc12-7dab7950495d406217e5071c\"\n...\n\n\n\nparsedB &lt;- fromJSON(content(binr, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsedB\n\n$args\n$args$`{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}`\n[1] \"\"\n\n\n$headers\n$headers$Accept\n[1] \"application/json, text/xml, application/xml, */*\"\n\n$headers$`Accept-Encoding`\n[1] \"deflate, gzip\"\n\n$headers$Host\n[1] \"httpbin.org\"\n\n$headers$`User-Agent`\n[1] \"libcurl/7.64.1 r-curl/4.3.3 httr/1.4.4\"\n\n$headers$`X-Amzn-Trace-Id`\n[1] \"Root=1-638ffc12-7dab7950495d406217e5071c\"\n\n\n$origin\n[1] \"180.222.17.154\"\n\n$url\n[1] \"http://httpbin.org/get?{\\\"function\\\":\\\"get_db_info\\\",\\\"version\\\":\\\"3\\\",\\\"params\\\":{\\\"table_name\\\":\\\"site\\\",\\\"return_type\\\":\\\"hash\\\",\\\"filter_values\\\":{\\\"station\\\":\\\"221001\\\"}}}\"\n\n\nThat call looks right.\n\nresponse &lt;- GET(vicurl, query = jsonbit, encode = 'json')\nresponse\n\nResponse [https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: text/html\n  Size: 99 B\n\n\n\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Request is not well-formed JSON\\r\\nInput request was not valid JSON\"\n\n\nI pasted it in to notebook++ and it’s exactly the same as the example. So, why isn’t it working?\n\nmodurl &lt;- modify_url(vicurl, query = jsonbit)\nresponse &lt;- GET(modurl)\nresponse\n\nResponse [https://data.water.vic.gov.au/cgi/webservice.exe?{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}]\n  Date: 2022-12-07 02:36\n  Status: 200\n  Content-Type: text/html\n  Size: 99 B\n\n\n\nparsed &lt;- fromJSON(content(response, 'text'), simplifyVector = FALSE)\n\nNo encoding supplied: defaulting to UTF-8.\n\nparsed\n\n$error_num\n[1] 120\n\n$error_msg\n[1] \"Request is not well-formed JSON\\r\\nInput request was not valid JSON\""
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#httr2",
    "href": "vicwater/vicwater_api_howtocall.html#httr2",
    "title": "Vicwater api crude testing",
    "section": "httr2",
    "text": "httr2\nHmmm. I see Hadley has released a v2. And it has a req_body_json. See if that works\n\nlibrary(httr2)\n\nThe req_dry_run lets us see what it’s passing. THat looks right? I think?\n\nreq &lt;- request(vicurl)\nreq %&gt;% \n  req_body_json(nestparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 129\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"filter_values\":{\"station\":\"221001\"}}}\n\n\n\nresp &lt;- req %&gt;% \n  req_body_json(nestparams) %&gt;% \n  req_perform()\n\n\nresp %&gt;% resp_raw()\n\nHTTP/1.1 200 OK\nDate: Wed, 07 Dec 2022 02:36:05 GMT\nContent-Type: text/html\nContent-Length: 972\nConnection: keep-alive\nContent-Encoding: gzip\nVary: Accept-Encoding\nServer: Microsoft-IIS/10.0\nContent: \nAccess-Control-Allow-Origin: *\n\n{\"error_num\":0,\"return\":{\"rows\":{\"221001\":{\"category20\":\"\",\"category19\":\"\",\"category18\":\"\",\"category17\":\"\",\"category16\":\"\",\"category15\":\"\",\"category14\":\"\",\"category13\":\"\",\"category12\":\"\",\"category11\":\"\",\"category10\":\"N\\/A\",\"active\":false,\"northing\":\"5887218.000\",\"timezone\":\"10.0\",\"shortname\":\"GENOA R @ ROCKTON\",\"datecreate\":18991230,\"elevdatum\":\"\",\"stname\":\"GENOA RIVER @ ROCKTON\",\"category9\":\"N\\/A\",\"category8\":\"G(S)\",\"category7\":\"G\",\"category6\":\"2WD\\/4WD\",\"category5\":\"10\",\"category4\":\"150\",\"category3\":\"GRAVEL\",\"category2\":\"V_70D3\",\"category1\":\"23\",\"elevacc\":\"1\",\"dbver47\":false,\"quarter\":\"Y\",\"section\":0,\"commence\":19930526,\"parent\":\"\",\"mapname\":\"MAFFRA\",\"meridian\":\"\",\"spare5\":\"\",\"spare4\":\"\",\"spare3\":\"\",\"spare2\":\"N\\/A\",\"spare1\":\"N\\/A\",\"posacc\":\"9\",\"timemod\":1642,\"region\":\"221\",\"grdatum\":\"UTM\",\"township\":\"\",\"longitude\":\"149.317296100\",\"comment\":\" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011\\/07\\/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\",\"lldatum\":\"WGS84\",\"station\":\"221001\",\"datemod\":20221117,\"timecreate\":0,\"orgcode\":\"DSE\",\"barcode\":\"Bombala\",\"zone\":55,\"elev\":\"429.000\",\"cease\":18991230,\"local_map\":\"CRAIGIE\",\"latitude\":\"-37.138654990\",\"range\":\"\",\"qquarter\":\"Y\",\"easting\":\"705825.000\",\"stntype\":\"VIR\"}}}}\n\n\nI thought it was json? but resp_body_json fails with defaults\n\nrj &lt;- resp %&gt;% resp_body_json(check_type = FALSE)\nrj\n\n$error_num\n[1] 0\n\n$return\n$return$rows\n$return$rows$`221001`\n$return$rows$`221001`$category20\n[1] \"\"\n\n$return$rows$`221001`$category19\n[1] \"\"\n\n$return$rows$`221001`$category18\n[1] \"\"\n\n$return$rows$`221001`$category17\n[1] \"\"\n\n$return$rows$`221001`$category16\n[1] \"\"\n\n$return$rows$`221001`$category15\n[1] \"\"\n\n$return$rows$`221001`$category14\n[1] \"\"\n\n$return$rows$`221001`$category13\n[1] \"\"\n\n$return$rows$`221001`$category12\n[1] \"\"\n\n$return$rows$`221001`$category11\n[1] \"\"\n\n$return$rows$`221001`$category10\n[1] \"N/A\"\n\n$return$rows$`221001`$active\n[1] FALSE\n\n$return$rows$`221001`$northing\n[1] \"5887218.000\"\n\n$return$rows$`221001`$timezone\n[1] \"10.0\"\n\n$return$rows$`221001`$shortname\n[1] \"GENOA R @ ROCKTON\"\n\n$return$rows$`221001`$datecreate\n[1] 18991230\n\n$return$rows$`221001`$elevdatum\n[1] \"\"\n\n$return$rows$`221001`$stname\n[1] \"GENOA RIVER @ ROCKTON\"\n\n$return$rows$`221001`$category9\n[1] \"N/A\"\n\n$return$rows$`221001`$category8\n[1] \"G(S)\"\n\n$return$rows$`221001`$category7\n[1] \"G\"\n\n$return$rows$`221001`$category6\n[1] \"2WD/4WD\"\n\n$return$rows$`221001`$category5\n[1] \"10\"\n\n$return$rows$`221001`$category4\n[1] \"150\"\n\n$return$rows$`221001`$category3\n[1] \"GRAVEL\"\n\n$return$rows$`221001`$category2\n[1] \"V_70D3\"\n\n$return$rows$`221001`$category1\n[1] \"23\"\n\n$return$rows$`221001`$elevacc\n[1] \"1\"\n\n$return$rows$`221001`$dbver47\n[1] FALSE\n\n$return$rows$`221001`$quarter\n[1] \"Y\"\n\n$return$rows$`221001`$section\n[1] 0\n\n$return$rows$`221001`$commence\n[1] 19930526\n\n$return$rows$`221001`$parent\n[1] \"\"\n\n$return$rows$`221001`$mapname\n[1] \"MAFFRA\"\n\n$return$rows$`221001`$meridian\n[1] \"\"\n\n$return$rows$`221001`$spare5\n[1] \"\"\n\n$return$rows$`221001`$spare4\n[1] \"\"\n\n$return$rows$`221001`$spare3\n[1] \"\"\n\n$return$rows$`221001`$spare2\n[1] \"N/A\"\n\n$return$rows$`221001`$spare1\n[1] \"N/A\"\n\n$return$rows$`221001`$posacc\n[1] \"9\"\n\n$return$rows$`221001`$timemod\n[1] 1642\n\n$return$rows$`221001`$region\n[1] \"221\"\n\n$return$rows$`221001`$grdatum\n[1] \"UTM\"\n\n$return$rows$`221001`$township\n[1] \"\"\n\n$return$rows$`221001`$longitude\n[1] \"149.317296100\"\n\n$return$rows$`221001`$comment\n[1] \" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011/07/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\"\n\n$return$rows$`221001`$lldatum\n[1] \"WGS84\"\n\n$return$rows$`221001`$station\n[1] \"221001\"\n\n$return$rows$`221001`$datemod\n[1] 20221117\n\n$return$rows$`221001`$timecreate\n[1] 0\n\n$return$rows$`221001`$orgcode\n[1] \"DSE\"\n\n$return$rows$`221001`$barcode\n[1] \"Bombala\"\n\n$return$rows$`221001`$zone\n[1] 55\n\n$return$rows$`221001`$elev\n[1] \"429.000\"\n\n$return$rows$`221001`$cease\n[1] 18991230\n\n$return$rows$`221001`$local_map\n[1] \"CRAIGIE\"\n\n$return$rows$`221001`$latitude\n[1] \"-37.138654990\"\n\n$return$rows$`221001`$range\n[1] \"\"\n\n$return$rows$`221001`$qquarter\n[1] \"Y\"\n\n$return$rows$`221001`$easting\n[1] \"705825.000\"\n\n$return$rows$`221001`$stntype\n[1] \"VIR\"\n\n\nIf I hack it together to check, first note that the resp_body is raw, as we see in the str of resp_raw and in resp_body_raw\n\nresp %&gt;% resp_raw() %&gt;% str()\n\nHTTP/1.1 200 OK\nDate: Wed, 07 Dec 2022 02:36:05 GMT\nContent-Type: text/html\nContent-Length: 972\nConnection: keep-alive\nContent-Encoding: gzip\nVary: Accept-Encoding\nServer: Microsoft-IIS/10.0\nContent: \nAccess-Control-Allow-Origin: *\n\n{\"error_num\":0,\"return\":{\"rows\":{\"221001\":{\"category20\":\"\",\"category19\":\"\",\"category18\":\"\",\"category17\":\"\",\"category16\":\"\",\"category15\":\"\",\"category14\":\"\",\"category13\":\"\",\"category12\":\"\",\"category11\":\"\",\"category10\":\"N\\/A\",\"active\":false,\"northing\":\"5887218.000\",\"timezone\":\"10.0\",\"shortname\":\"GENOA R @ ROCKTON\",\"datecreate\":18991230,\"elevdatum\":\"\",\"stname\":\"GENOA RIVER @ ROCKTON\",\"category9\":\"N\\/A\",\"category8\":\"G(S)\",\"category7\":\"G\",\"category6\":\"2WD\\/4WD\",\"category5\":\"10\",\"category4\":\"150\",\"category3\":\"GRAVEL\",\"category2\":\"V_70D3\",\"category1\":\"23\",\"elevacc\":\"1\",\"dbver47\":false,\"quarter\":\"Y\",\"section\":0,\"commence\":19930526,\"parent\":\"\",\"mapname\":\"MAFFRA\",\"meridian\":\"\",\"spare5\":\"\",\"spare4\":\"\",\"spare3\":\"\",\"spare2\":\"N\\/A\",\"spare1\":\"N\\/A\",\"posacc\":\"9\",\"timemod\":1642,\"region\":\"221\",\"grdatum\":\"UTM\",\"township\":\"\",\"longitude\":\"149.317296100\",\"comment\":\" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011\\/07\\/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\",\"lldatum\":\"WGS84\",\"station\":\"221001\",\"datemod\":20221117,\"timecreate\":0,\"orgcode\":\"DSE\",\"barcode\":\"Bombala\",\"zone\":55,\"elev\":\"429.000\",\"cease\":18991230,\"local_map\":\"CRAIGIE\",\"latitude\":\"-37.138654990\",\"range\":\"\",\"qquarter\":\"Y\",\"easting\":\"705825.000\",\"stntype\":\"VIR\"}}}}\nList of 5\n $ method     : chr \"POST\"\n $ url        : chr \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\n $ status_code: int 200\n $ headers    :List of 9\n  ..$ Date                       : chr \"Wed, 07 Dec 2022 02:36:05 GMT\"\n  ..$ Content-Type               : chr \"text/html\"\n  ..$ Content-Length             : chr \"972\"\n  ..$ Connection                 : chr \"keep-alive\"\n  ..$ Content-Encoding           : chr \"gzip\"\n  ..$ Vary                       : chr \"Accept-Encoding\"\n  ..$ Server                     : chr \"Microsoft-IIS/10.0\"\n  ..$ Content                    : chr \"\"\n  ..$ Access-Control-Allow-Origin: chr \"*\"\n  ..- attr(*, \"class\")= chr \"httr2_headers\"\n $ body       : raw [1:1460] 7b 22 65 72 ...\n - attr(*, \"class\")= chr \"httr2_response\"\n\n\n\nresp %&gt;% resp_body_raw()\n\n   [1] 7b 22 65 72 72 6f 72 5f 6e 75 6d 22 3a 30 2c 22 72 65 74 75 72 6e 22 3a\n  [25] 7b 22 72 6f 77 73 22 3a 7b 22 32 32 31 30 30 31 22 3a 7b 22 63 61 74 65\n  [49] 67 6f 72 79 32 30 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 39 22 3a\n  [73] 22 22 2c 22 63 61 74 65 67 6f 72 79 31 38 22 3a 22 22 2c 22 63 61 74 65\n  [97] 67 6f 72 79 31 37 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 36 22 3a\n [121] 22 22 2c 22 63 61 74 65 67 6f 72 79 31 35 22 3a 22 22 2c 22 63 61 74 65\n [145] 67 6f 72 79 31 34 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 33 22 3a\n [169] 22 22 2c 22 63 61 74 65 67 6f 72 79 31 32 22 3a 22 22 2c 22 63 61 74 65\n [193] 67 6f 72 79 31 31 22 3a 22 22 2c 22 63 61 74 65 67 6f 72 79 31 30 22 3a\n [217] 22 4e 5c 2f 41 22 2c 22 61 63 74 69 76 65 22 3a 66 61 6c 73 65 2c 22 6e\n [241] 6f 72 74 68 69 6e 67 22 3a 22 35 38 38 37 32 31 38 2e 30 30 30 22 2c 22\n [265] 74 69 6d 65 7a 6f 6e 65 22 3a 22 31 30 2e 30 22 2c 22 73 68 6f 72 74 6e\n [289] 61 6d 65 22 3a 22 47 45 4e 4f 41 20 52 20 40 20 52 4f 43 4b 54 4f 4e 22\n [313] 2c 22 64 61 74 65 63 72 65 61 74 65 22 3a 31 38 39 39 31 32 33 30 2c 22\n [337] 65 6c 65 76 64 61 74 75 6d 22 3a 22 22 2c 22 73 74 6e 61 6d 65 22 3a 22\n [361] 47 45 4e 4f 41 20 52 49 56 45 52 20 40 20 52 4f 43 4b 54 4f 4e 22 2c 22\n [385] 63 61 74 65 67 6f 72 79 39 22 3a 22 4e 5c 2f 41 22 2c 22 63 61 74 65 67\n [409] 6f 72 79 38 22 3a 22 47 28 53 29 22 2c 22 63 61 74 65 67 6f 72 79 37 22\n [433] 3a 22 47 22 2c 22 63 61 74 65 67 6f 72 79 36 22 3a 22 32 57 44 5c 2f 34\n [457] 57 44 22 2c 22 63 61 74 65 67 6f 72 79 35 22 3a 22 31 30 22 2c 22 63 61\n [481] 74 65 67 6f 72 79 34 22 3a 22 31 35 30 22 2c 22 63 61 74 65 67 6f 72 79\n [505] 33 22 3a 22 47 52 41 56 45 4c 22 2c 22 63 61 74 65 67 6f 72 79 32 22 3a\n [529] 22 56 5f 37 30 44 33 22 2c 22 63 61 74 65 67 6f 72 79 31 22 3a 22 32 33\n [553] 22 2c 22 65 6c 65 76 61 63 63 22 3a 22 31 22 2c 22 64 62 76 65 72 34 37\n [577] 22 3a 66 61 6c 73 65 2c 22 71 75 61 72 74 65 72 22 3a 22 59 22 2c 22 73\n [601] 65 63 74 69 6f 6e 22 3a 30 2c 22 63 6f 6d 6d 65 6e 63 65 22 3a 31 39 39\n [625] 33 30 35 32 36 2c 22 70 61 72 65 6e 74 22 3a 22 22 2c 22 6d 61 70 6e 61\n [649] 6d 65 22 3a 22 4d 41 46 46 52 41 22 2c 22 6d 65 72 69 64 69 61 6e 22 3a\n [673] 22 22 2c 22 73 70 61 72 65 35 22 3a 22 22 2c 22 73 70 61 72 65 34 22 3a\n [697] 22 22 2c 22 73 70 61 72 65 33 22 3a 22 22 2c 22 73 70 61 72 65 32 22 3a\n [721] 22 4e 5c 2f 41 22 2c 22 73 70 61 72 65 31 22 3a 22 4e 5c 2f 41 22 2c 22\n [745] 70 6f 73 61 63 63 22 3a 22 39 22 2c 22 74 69 6d 65 6d 6f 64 22 3a 31 36\n [769] 34 32 2c 22 72 65 67 69 6f 6e 22 3a 22 32 32 31 22 2c 22 67 72 64 61 74\n [793] 75 6d 22 3a 22 55 54 4d 22 2c 22 74 6f 77 6e 73 68 69 70 22 3a 22 22 2c\n [817] 22 6c 6f 6e 67 69 74 75 64 65 22 3a 22 31 34 39 2e 33 31 37 32 39 36 31\n [841] 30 30 22 2c 22 63 6f 6d 6d 65 6e 74 22 3a 22 20 5c 72 5c 6e 54 75 72 6e\n [865] 20 72 69 67 68 74 20 66 72 6f 6d 20 4d 6f 6e 61 72 6f 20 48 69 67 68 77\n [889] 61 79 20 6f 6e 74 6f 20 49 6d 6c 61 79 20 72 6f 61 64 20 73 69 67 6e 70\n [913] 6f 73 74 65 64 20 38 32 20 6b 6d 20 74 6f 20 45 64 65 6e 2e 20 20 54 72\n [937] 61 76 65 6c 20 6f 76 65 72 20 6c 6f 77 20 6c 65 76 65 6c 20 62 72 69 64\n [961] 67 65 20 20 74 6f 20 61 20 73 68 61 72 70 20 6c 65 66 74 20 62 65 6e 64\n [985] 20 69 6e 20 74 68 65 20 72 6f 61 64 2e 20 20 41 20 70 69 6e 65 20 70 6c\n[1009] 61 6e 74 61 74 69 6f 6e 20 20 77 69 6c 6c 20 62 65 20 64 69 72 65 63 74\n[1033] 6c 79 20 6f 6e 20 74 68 65 20 72 69 67 68 74 2e 20 20 54 75 72 6e 20 64\n[1057] 6f 77 6e 20 69 6e 74 6f 20 70 6c 61 6e 74 61 74 69 6f 6e 20 61 6e 64 20\n[1081] 74 72 61 76 65 6c 20 61 70 70 72 6f 78 2e 20 31 30 30 6d 32 30 31 31 5c\n[1105] 2f 30 37 5c 2f 31 37 20 56 69 72 74 75 61 6c 20 73 69 74 65 20 65 6e 74\n[1129] 72 79 20 67 65 6e 65 72 61 74 65 64 20 62 79 20 44 53 45 56 49 52 54 55\n[1153] 41 4c 53 49 54 45 2e 48 53 43 20 66 72 6f 6d 20 64 65 74 61 69 6c 73 20\n[1177] 69 6e 20 32 32 31 30 30 31 41 5c 72 5c 6e 22 2c 22 6c 6c 64 61 74 75 6d\n[1201] 22 3a 22 57 47 53 38 34 22 2c 22 73 74 61 74 69 6f 6e 22 3a 22 32 32 31\n[1225] 30 30 31 22 2c 22 64 61 74 65 6d 6f 64 22 3a 32 30 32 32 31 31 31 37 2c\n[1249] 22 74 69 6d 65 63 72 65 61 74 65 22 3a 30 2c 22 6f 72 67 63 6f 64 65 22\n[1273] 3a 22 44 53 45 22 2c 22 62 61 72 63 6f 64 65 22 3a 22 42 6f 6d 62 61 6c\n[1297] 61 22 2c 22 7a 6f 6e 65 22 3a 35 35 2c 22 65 6c 65 76 22 3a 22 34 32 39\n[1321] 2e 30 30 30 22 2c 22 63 65 61 73 65 22 3a 31 38 39 39 31 32 33 30 2c 22\n[1345] 6c 6f 63 61 6c 5f 6d 61 70 22 3a 22 43 52 41 49 47 49 45 22 2c 22 6c 61\n[1369] 74 69 74 75 64 65 22 3a 22 2d 33 37 2e 31 33 38 36 35 34 39 39 30 22 2c\n[1393] 22 72 61 6e 67 65 22 3a 22 22 2c 22 71 71 75 61 72 74 65 72 22 3a 22 59\n[1417] 22 2c 22 65 61 73 74 69 6e 67 22 3a 22 37 30 35 38 32 35 2e 30 30 30 22\n[1441] 2c 22 73 74 6e 74 79 70 65 22 3a 22 56 49 52 22 7d 7d 7d 7d\n\n\nSo, if we get the raw, convert to char, then pass to JSON, it looks the same as what I’m getting out of resp_body_json.\n\nresp %&gt;% resp_body_raw() %&gt;% rawToChar() %&gt;% fromJSON()\n\n$error_num\n[1] 0\n\n$return\n$return$rows\n$return$rows$`221001`\n$return$rows$`221001`$category20\n[1] \"\"\n\n$return$rows$`221001`$category19\n[1] \"\"\n\n$return$rows$`221001`$category18\n[1] \"\"\n\n$return$rows$`221001`$category17\n[1] \"\"\n\n$return$rows$`221001`$category16\n[1] \"\"\n\n$return$rows$`221001`$category15\n[1] \"\"\n\n$return$rows$`221001`$category14\n[1] \"\"\n\n$return$rows$`221001`$category13\n[1] \"\"\n\n$return$rows$`221001`$category12\n[1] \"\"\n\n$return$rows$`221001`$category11\n[1] \"\"\n\n$return$rows$`221001`$category10\n[1] \"N/A\"\n\n$return$rows$`221001`$active\n[1] FALSE\n\n$return$rows$`221001`$northing\n[1] \"5887218.000\"\n\n$return$rows$`221001`$timezone\n[1] \"10.0\"\n\n$return$rows$`221001`$shortname\n[1] \"GENOA R @ ROCKTON\"\n\n$return$rows$`221001`$datecreate\n[1] 18991230\n\n$return$rows$`221001`$elevdatum\n[1] \"\"\n\n$return$rows$`221001`$stname\n[1] \"GENOA RIVER @ ROCKTON\"\n\n$return$rows$`221001`$category9\n[1] \"N/A\"\n\n$return$rows$`221001`$category8\n[1] \"G(S)\"\n\n$return$rows$`221001`$category7\n[1] \"G\"\n\n$return$rows$`221001`$category6\n[1] \"2WD/4WD\"\n\n$return$rows$`221001`$category5\n[1] \"10\"\n\n$return$rows$`221001`$category4\n[1] \"150\"\n\n$return$rows$`221001`$category3\n[1] \"GRAVEL\"\n\n$return$rows$`221001`$category2\n[1] \"V_70D3\"\n\n$return$rows$`221001`$category1\n[1] \"23\"\n\n$return$rows$`221001`$elevacc\n[1] \"1\"\n\n$return$rows$`221001`$dbver47\n[1] FALSE\n\n$return$rows$`221001`$quarter\n[1] \"Y\"\n\n$return$rows$`221001`$section\n[1] 0\n\n$return$rows$`221001`$commence\n[1] 19930526\n\n$return$rows$`221001`$parent\n[1] \"\"\n\n$return$rows$`221001`$mapname\n[1] \"MAFFRA\"\n\n$return$rows$`221001`$meridian\n[1] \"\"\n\n$return$rows$`221001`$spare5\n[1] \"\"\n\n$return$rows$`221001`$spare4\n[1] \"\"\n\n$return$rows$`221001`$spare3\n[1] \"\"\n\n$return$rows$`221001`$spare2\n[1] \"N/A\"\n\n$return$rows$`221001`$spare1\n[1] \"N/A\"\n\n$return$rows$`221001`$posacc\n[1] \"9\"\n\n$return$rows$`221001`$timemod\n[1] 1642\n\n$return$rows$`221001`$region\n[1] \"221\"\n\n$return$rows$`221001`$grdatum\n[1] \"UTM\"\n\n$return$rows$`221001`$township\n[1] \"\"\n\n$return$rows$`221001`$longitude\n[1] \"149.317296100\"\n\n$return$rows$`221001`$comment\n[1] \" \\r\\nTurn right from Monaro Highway onto Imlay road signposted 82 km to Eden.  Travel over low level bridge  to a sharp left bend in the road.  A pine plantation  will be directly on the right.  Turn down into plantation and travel approx. 100m2011/07/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 221001A\\r\\n\"\n\n$return$rows$`221001`$lldatum\n[1] \"WGS84\"\n\n$return$rows$`221001`$station\n[1] \"221001\"\n\n$return$rows$`221001`$datemod\n[1] 20221117\n\n$return$rows$`221001`$timecreate\n[1] 0\n\n$return$rows$`221001`$orgcode\n[1] \"DSE\"\n\n$return$rows$`221001`$barcode\n[1] \"Bombala\"\n\n$return$rows$`221001`$zone\n[1] 55\n\n$return$rows$`221001`$elev\n[1] \"429.000\"\n\n$return$rows$`221001`$cease\n[1] 18991230\n\n$return$rows$`221001`$local_map\n[1] \"CRAIGIE\"\n\n$return$rows$`221001`$latitude\n[1] \"-37.138654990\"\n\n$return$rows$`221001`$range\n[1] \"\"\n\n$return$rows$`221001`$qquarter\n[1] \"Y\"\n\n$return$rows$`221001`$easting\n[1] \"705825.000\"\n\n$return$rows$`221001`$stntype\n[1] \"VIR\"\n\n\nNow can I clean that up? THere’s a lot of nesting but some of it is pointless?\n\nnames(rj)\n\n[1] \"error_num\" \"return\"   \n\n\nand we know error_num is a single int from the str above. And $return is length 1 with only rows and rows only has the gauge number. After that there’s actually some data.\n\nnames(rj$return)\n\n[1] \"rows\"\n\nnames(rj$return$rows)\n\n[1] \"221001\"\n\nnames(rj$return$rows$`221001`)\n\n [1] \"category20\" \"category19\" \"category18\" \"category17\" \"category16\"\n [6] \"category15\" \"category14\" \"category13\" \"category12\" \"category11\"\n[11] \"category10\" \"active\"     \"northing\"   \"timezone\"   \"shortname\" \n[16] \"datecreate\" \"elevdatum\"  \"stname\"     \"category9\"  \"category8\" \n[21] \"category7\"  \"category6\"  \"category5\"  \"category4\"  \"category3\" \n[26] \"category2\"  \"category1\"  \"elevacc\"    \"dbver47\"    \"quarter\"   \n[31] \"section\"    \"commence\"   \"parent\"     \"mapname\"    \"meridian\"  \n[36] \"spare5\"     \"spare4\"     \"spare3\"     \"spare2\"     \"spare1\"    \n[41] \"posacc\"     \"timemod\"    \"region\"     \"grdatum\"    \"township\"  \n[46] \"longitude\"  \"comment\"    \"lldatum\"    \"station\"    \"datemod\"   \n[51] \"timecreate\" \"orgcode\"    \"barcode\"    \"zone\"       \"elev\"      \n[56] \"cease\"      \"local_map\"  \"latitude\"   \"range\"      \"qquarter\"  \n[61] \"easting\"    \"stntype\"   \n\n\nNow, can we put that in a dataframe? Is each one length 1?\n\nactualdata &lt;- rj$return$rows$`221001`\n\nall(purrr::map_int(actualdata, length) == 1)\n\n[1] TRUE\n\n\n\ntibout &lt;- as_tibble(actualdata)\ntibout\n\n# A tibble: 1 × 62\n  category20 category19 catego…¹ categ…² categ…³ categ…⁴ categ…⁵ categ…⁶ categ…⁷\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1 \"\"         \"\"         \"\"       \"\"      \"\"      \"\"      \"\"      \"\"      \"\"     \n# … with 53 more variables: category11 &lt;chr&gt;, category10 &lt;chr&gt;, active &lt;lgl&gt;,\n#   northing &lt;chr&gt;, timezone &lt;chr&gt;, shortname &lt;chr&gt;, datecreate &lt;int&gt;,\n#   elevdatum &lt;chr&gt;, stname &lt;chr&gt;, category9 &lt;chr&gt;, category8 &lt;chr&gt;,\n#   category7 &lt;chr&gt;, category6 &lt;chr&gt;, category5 &lt;chr&gt;, category4 &lt;chr&gt;,\n#   category3 &lt;chr&gt;, category2 &lt;chr&gt;, category1 &lt;chr&gt;, elevacc &lt;chr&gt;,\n#   dbver47 &lt;lgl&gt;, quarter &lt;chr&gt;, section &lt;int&gt;, commence &lt;int&gt;, parent &lt;chr&gt;,\n#   mapname &lt;chr&gt;, meridian &lt;chr&gt;, spare5 &lt;chr&gt;, spare4 &lt;chr&gt;, spare3 &lt;chr&gt;, …\n\n\nand toss cols with no data\n\ntibout %&gt;% select(where(~!all(. == '')))\n\n# A tibble: 1 × 44\n  catego…¹ active north…² timez…³ short…⁴ datec…⁵ stname categ…⁶ categ…⁷ categ…⁸\n  &lt;chr&gt;    &lt;lgl&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1 N/A      FALSE  588721… 10.0    GENOA …  1.90e7 GENOA… N/A     G(S)    G      \n# … with 34 more variables: category6 &lt;chr&gt;, category5 &lt;chr&gt;, category4 &lt;chr&gt;,\n#   category3 &lt;chr&gt;, category2 &lt;chr&gt;, category1 &lt;chr&gt;, elevacc &lt;chr&gt;,\n#   dbver47 &lt;lgl&gt;, quarter &lt;chr&gt;, section &lt;int&gt;, commence &lt;int&gt;, mapname &lt;chr&gt;,\n#   spare2 &lt;chr&gt;, spare1 &lt;chr&gt;, posacc &lt;chr&gt;, timemod &lt;int&gt;, region &lt;chr&gt;,\n#   grdatum &lt;chr&gt;, longitude &lt;chr&gt;, comment &lt;chr&gt;, lldatum &lt;chr&gt;,\n#   station &lt;chr&gt;, datemod &lt;int&gt;, timecreate &lt;int&gt;, orgcode &lt;chr&gt;,\n#   barcode &lt;chr&gt;, zone &lt;int&gt;, elev &lt;chr&gt;, cease &lt;int&gt;, local_map &lt;chr&gt;, …\n\n\nCool, I have data and can do stuff with it. NOW, how do I get the data I actually want, vs the data that happens to be in the one demo on the website?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#querying-options",
    "href": "vicwater/vicwater_api_howtocall.html#querying-options",
    "title": "Vicwater api crude testing",
    "section": "Querying options",
    "text": "Querying options\nBefore we go get data, we need to figure out what we can ask for. First, sort out those functions.\n\nDatasources\nLet’s try get_datasources_by_site. Takes site_list params. Dunno what versions it works for? Tried 2, gave error says has to be 1. I assume the “A” datasource means archive here, since that’s what it means in QLD.\n\nds_s_params &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217\"))\n\n# req &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 84\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217\"}}\n\nresp_ds_s &lt;- req %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_perform()\n\nrbody_ds_s &lt;- resp_ds_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 1\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\n\n\nSitelist\nOk, I could make that into a tibble easily enough. It tells me what that site has for datasources, how about another? Can I figure out how to use a sitelist? That’d be really nice, and applies in a lot of places. Cool. I had tried to do \"sitelist\" = c('site', 'site') , and that failed. But it works to have \"site, site\"\n\nds_s_params &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\"))\n\n# req &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 92\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328\"}}\n\nresp_ds_s &lt;- req %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_perform()\n\nrbody_ds_s &lt;- resp_ds_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 2\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405328\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\n\n\nVariables\nNow, for a given site, we want to know what variables are available. (and I also eventually want to know what all possible variables are, and what happens if we ask for variables that aren’t there). Let’s start with the same two sites. I’m\n\nv_s_params &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\",\n                               \"datasource\" = \"A\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 103\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328\",\"datasource\":\"A\"}}\n\nresp_v_s &lt;- req %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_perform()\n\nrbody_v_s &lt;- resp_v_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 2\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ variables   :List of 6\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"19610306171500\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"210.00\"\n  .. .. .. .. ..$ units       : chr \"pH\"\n  .. .. .. .. ..$ name        : chr \"Acidity/Alkalinity (pH)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"215.00\"\n  .. .. .. .. ..$ units       : chr \"ppm\"\n  .. .. .. .. ..$ name        : chr \"Dissolved Oxygen (ppm)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"233217\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221111094500\"\n  .. .. .. .. ..$ period_start: chr \"20091119170800\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. ..$ site        : chr \"405328\"\n\n\nSo, that gives me the number and name of each variable at each site. But it does not give derived variables (discharge being the main one)."
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#location-etc",
    "href": "vicwater/vicwater_api_howtocall.html#location-etc",
    "title": "Vicwater api crude testing",
    "section": "Location etc",
    "text": "Location etc\nSo, get_db_info seems like it should be useful, but kind of isn’t (see above). Maybe I’ll come back to that. It does let us do geofilters, but they seem both crude and complex. I think I’d probably rather do geofiltering myself and then turn that into a site_list. But might come back to this. The complex_filter might be useful if we can use it to choose sites based on something. But again, I’d probably do that myself? Again, come back to this maybe?\n\nCan we get a list of all sites and all variables?\nMaybe? Do we want to?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#getting-timeseries",
    "href": "vicwater/vicwater_api_howtocall.html#getting-timeseries",
    "title": "Vicwater api crude testing",
    "section": "Getting timeseries",
    "text": "Getting timeseries\nNow, let’s go back to get timeseries, now we know what the variables are. Just for the Barwon at first, and way fewer days. There’s a var_list option, or varfrom and varto. It’s unclear whether the from and to version is numerivally inclusive- ie if we have 100 and 10000, does it get everything? I’ll try with varto = 820, since that’s the highest number avail at the barwon. Gives cryptic error. Try 210? Also, why isn’t 141 available in teh lsit above? Again, cryptic “Assumed fail to reload varcon for 233217: 100.00-&gt; 210.00, as we failed loading it last time”. does it work for 141? Yes.\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"100\",\n                               \"interval\" = \"day\",\n                               \"varto\" = \"141\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"100\",\"interval\":\"day\",\"varto\":\"141\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 150: chr \"Rating extrapolated above 1.5x maximum flow gauged.\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"15.93\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"14.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"11.23\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.81\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.75\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.09\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"4.13\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"2.48\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.88\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"12.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"9.87\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.42\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"6.45\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"9.74\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Discharge (Ml/d)\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"141.00\"\n  .. .. .. ..$ units     : chr \"megalitres/day\"\n  .. .. .. ..$ name      : chr \"Stream Discharge (Ml/d)\"\n\n\nDo the other vars work if we ask for them separately? Try pH (which failed above)\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"210\",\n                               \"interval\" = \"day\",\n                               \"varto\" = \"210\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"210\",\"interval\":\"day\",\"varto\":\"210\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nInteresting. How about a var_list?\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nThat works, seems to set the varfrom and varto to each value in the list. I wonder if things like 141 are done in the from/to way because they are derived from 100. But how do we find them when they don’t appear in the get_variable_list? Can I include them in var_list? Hmm. No. what’s going on? see table 3 in qld doc- they are derived, and it gives numbers. Is there a get_available_varcons or similar?\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,141,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\",\n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 227\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,141,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nHow about asking for variables that don’t exist- ie can we just ask for all of them, and it just gives us whatever’s available? The other site (Steavenson, 405328) only has variable 100, so ask for some others. Just gives 100.\n\nsparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"405328\",\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(sparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"405328\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nresps &lt;- req %&gt;% \n  req_body_json(sparams) %&gt;% \n  req_perform()\n\nrbodys &lt;- resps %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodys)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ longitude : chr \"145.773503100\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. .. ..$ latitude  : chr \"-37.525797590\"\n  .. .. .. ..$ org_name  : chr \"Victorian Rural Water Corporation\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.738\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.736\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.734\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.755\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.739\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.735\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.759\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.742\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.757\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"405328\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\n\nCan we get derived (141, etc) with varlist\nIt looks like it’s really just 140 (cumecs) and 141 (ML/d) we’d want, if Vic matches QLD. There are a couple other varcons, but they’re about groundwater.\nLet’s see what get_varcon gives us. I can’t get this not to error, and the examples online have square brackets mixed in the json. I think some combo of c and list might do it but not worth it.\n\nvc_params &lt;- list(\"function\" = 'get_varcon',\n               \"version\" = \"2\",\n               \"params\" = list(\"varcons\" = list(\"varfrom\" = \"100\",\n                               \"varto\" = \"141\",\n                               \"site_list\" = \"233217, 405328\",\n                               \"datasource\" = \"A\",\n                               \"requests\" = list(\"qf1\" = \"1\", \n                                                 \"t1\" = \"20200101000000\",\n                                                 \"t2\" = \"20200131000000\"))))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(vc_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 205\n\n{\"function\":\"get_varcon\",\"version\":\"2\",\"params\":{\"varcons\":{\"varfrom\":\"100\",\"varto\":\"141\",\"site_list\":\"233217, 405328\",\"datasource\":\"A\",\"requests\":{\"qf1\":\"1\",\"t1\":\"20200101000000\",\"t2\":\"20200131000000\"}}}}\n\nresp_vc &lt;- req %&gt;% \n  req_body_json(vc_params) %&gt;% \n  req_perform()\n\nrbody_vc &lt;- resp_vc %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_vc)\n\nList of 3\n $ error_num: int 124\n $ error_msg: chr \"Access violation at address 005203ED in module 'webservice.exe'. Read of address 00000008\"\n $ return   :List of 1\n  ..$ varcons: list()\n\n\nThis isn’t worth it. If we ask for 141 or 140, just do another round with varfrom and varto. Or always get 100, 140, 141, then only sometimes get the others if asked?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#geolocation",
    "href": "vicwater/vicwater_api_howtocall.html#geolocation",
    "title": "Vicwater api crude testing",
    "section": "Geolocation",
    "text": "Geolocation\nSo, get_db_info seems to have a way to get sites by radius or bounding box.. And the flipside is we might want to get sites within a polygon, and so need their locations, which should be available as geoJSON. Try to figure both those out.\nI think get_site_geojson is going to be simpler. start there. Not sure why the site_list can’t have a c(), but the fields has to use it. Works though, gives a feature list. I think those are readable by sf, so that’s good. Not sure what the fields even are though. The help says “Any field that is part of the site table”. So I guess we need to sort that out. On to get_db_info.\n\ng_j_params &lt;- list(\"function\" = 'get_site_geojson',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = \"233217, 405328\",\n                               \"get_elev\" = \"1\",\n                               \"fields\" = c(\"zone\",\"region\")))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(g_j_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 127\n\n{\"function\":\"get_site_geojson\",\"version\":\"2\",\"params\":{\"site_list\":\"233217, 405328\",\"get_elev\":\"1\",\"fields\":[\"zone\",\"region\"]}}\n\nresp_g_j &lt;- req %&gt;% \n  req_body_json(g_j_params) %&gt;% \n  req_perform()\n\nrbody_g_j &lt;- resp_g_j %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_g_j)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 2\n  ..$ features:List of 2\n  .. ..$ :List of 4\n  .. .. ..$ properties:List of 2\n  .. .. .. ..$ region: chr \"233\"\n  .. .. .. ..$ zone  : int 55\n  .. .. ..$ geometry  :List of 2\n  .. .. .. ..$ coordinates:List of 3\n  .. .. .. .. ..$ : num 144\n  .. .. .. .. ..$ : num -38.2\n  .. .. .. .. ..$ : int 0\n  .. .. .. ..$ type       : chr \"Point\"\n  .. .. ..$ id        : chr \"233217\"\n  .. .. ..$ type      : chr \"Feature\"\n  .. ..$ :List of 4\n  .. .. ..$ properties:List of 2\n  .. .. .. ..$ region: chr \"405\"\n  .. .. .. ..$ zone  : int 55\n  .. .. ..$ geometry  :List of 2\n  .. .. .. ..$ coordinates:List of 3\n  .. .. .. .. ..$ : num 146\n  .. .. .. .. ..$ : num -37.5\n  .. .. .. .. ..$ : int 0\n  .. .. .. ..$ type       : chr \"Point\"\n  .. .. ..$ id        : chr \"405328\"\n  .. .. ..$ type      : chr \"Feature\"\n  ..$ type    : chr \"FeatureCollection\""
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#db-info",
    "href": "vicwater/vicwater_api_howtocall.html#db-info",
    "title": "Vicwater api crude testing",
    "section": "DB info",
    "text": "DB info\nI was using get_db_info to test above, so let’s just go back to that as a start and think a bit more about what we want. look at the barown. Cannot feed it a list of sites for fitler_values. It does give lat/long/northing, etc, so could use this instead of geoJSON, but geoJSON probably better if we want geo. Using return_type = hash is not noticably different than return_type = array. All examples use hash, so I guess keep using that moving forward. I think we can filter on lots of things in this list, both here and in the geojson.\n\ndbparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"station\" = \"233217\")))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 130\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"station\":\"233217\"}}}\n\nrespdb &lt;- req %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_perform()\n\nrbodydb &lt;- respdb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodydb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 1\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5772691.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON @ GEELONG\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"0\"\n  .. .. ..$ category4 : chr \"150\"\n  .. .. ..$ category3 : chr \"SEALED\"\n  .. .. ..$ category2 : chr \"V_93G4\"\n  .. .. ..$ category1 : chr \"0\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19601118\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"BW\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. ..$ comment   : chr \"\\r\\n\\r\\n\\r\\nBarwon Water flood monitoring stationFrom the intersection of the Fyans St and La Trobe Terrace, he\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233217\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Geelong\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"267562.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n\n\n\nGeofiltering the db to select sites\nOK, so there are lots of ways to filter (sitename, date, name, etc). Some of those like Name or region or active might be useful, but for now let’s try the geo filters (boudning box and radius).\nTry circle (lat, long, radius in degrees)- use the Barwon lat/long.\nKeeps crashing with timeouts. is it just too much to ask for? Or is the json not right?\n\n# dbparams &lt;- list(\"function\" = 'get_db_info',\n#                \"version\" = \"3\",\n#                \"params\" = list(\"table_name\" = \"site\",\n#                                \"return_type\" = \"hash\",\n#                                \"geo_filter\" = list(\"circle\" = c(\"-38.16\", \"144.35\", \"0.25\"))))\n# \n# req &lt;- request(vicurl)\n# \n# req %&gt;% \n#   req_body_json(dbparams) %&gt;% \n#   req_dry_run()\n# \n# respdb &lt;- req %&gt;% \n#   req_body_json(dbparams) %&gt;% \n#   req_perform()\n# \n# rbodydb &lt;- respdb %&gt;% resp_body_json(check_type = FALSE)\n# \n# str(rbodydb)\n\nLet’s try one of the other geofilters. Otherwise this will work better to write my own if I can et the geojson of all the sites.\nUgh. the rectangle (and region) need nested square brackets. I can make one with c(),\n\ndbparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"hash\",\n                               \"geo_filter\" = list(\"rectangle\" = \n                                                     c(c(\"-38.126\", \"144.282\"),\n                                                       c(\"-38.223\", \"144.406\")))))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 161\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"hash\",\"geo_filter\":{\"rectangle\":[\"-38.126\",\"144.282\",\"-38.223\",\"144.406\"]}}}\n\n# \n# respdb &lt;- req %&gt;% \n#   req_body_json(dbparams) %&gt;% \n#   req_perform()\n# \n# rbodydb &lt;- respdb %&gt;% resp_body_json(check_type = FALSE)\n# \n# str(rbodydb)\n\nthe locations of all guages it’d be\nOK, generating the json for these geo selections is horrible. If I can pull faster to do it myself.\nCan I get a complete gaugelist, nad then pull geojson?"
  },
  {
    "objectID": "vicwater/vicwater_api_howtocall.html#get-all-gauges",
    "href": "vicwater/vicwater_api_howtocall.html#get-all-gauges",
    "title": "Vicwater api crude testing",
    "section": "Get all gauges",
    "text": "Get all gauges\nWhat’s the best way? with get_db_info? With get_sites_by_datasource? The latter would assume we know all datasources. We probably do just want those in ‘A’ but not positive.\nSo, how about db_info, but maybe not all columns? Tempted to get lat/long or easting/northing.\nTakes a while, but it does run. 189,464 sites??? Yikes. WHY? Clearly i need to filter on something.\n\ndbparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"field_list\" = c(\"station\", \"stname\", \"shortname\")))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 139\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"field_list\":[\"station\",\"stname\",\"shortname\"]}}\n\nrespdb &lt;- req %&gt;% \n  req_body_json(dbparams) %&gt;% \n  req_perform()\n\nrbodydb &lt;- respdb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodydb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 189491\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"0\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"044079\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"044387\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045407\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045621\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045652\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"045717\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"092825\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"097421\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"097962\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100000\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100001\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100002\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100003\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100004\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100005\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100006\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100007\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100008\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100009\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100010\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100011\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100012\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100013\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100014\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100015\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100016\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100017\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100018\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100019\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100020\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100021\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100022\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100023\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100024\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100025\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100026\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100028\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100029\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100030\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100031\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100032\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100033\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100034\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100035\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100036\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100037\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100038\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100039\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100040\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100041\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100042\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100043\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100044\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100045\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100046\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100047\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100048\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100049\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100050\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100051\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100054\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100055\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100056\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100057\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100058\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100059\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100060\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100061\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100062\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100063\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100064\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100065\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100066\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100067\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100068\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100069\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100070\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100071\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100072\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100073\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100074\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100075\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100076\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100077\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100078\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100079\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100080\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100081\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100082\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100083\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100084\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100085\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100086\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100087\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100088\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100089\"\n  .. ..$ :List of 3\n  .. .. ..$ shortname: chr \"\"\n  .. .. ..$ stname   : chr \"\"\n  .. .. ..$ station  : chr \"100090\"\n  .. .. [list output truncated]\n\n\nwhat are the variables at some of those sites? Can we figure out what’s up that way? I have a feeling some are groundewater, but there’s no obvious field for that.\n\nv_s_params &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = \"100089, 100079\",\n                               \"datasource\" = \"A\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 103\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"100089, 100079\",\"datasource\":\"A\"}}\n\nresp_v_s &lt;- req %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_perform()\n\nrbody_v_s &lt;- resp_v_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 2\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"\"\n  .. .. .. ..$ name      : chr \"\"\n  .. .. ..$ variables   : list()\n  .. .. ..$ site        : chr \"100089\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"\"\n  .. .. .. ..$ name      : chr \"\"\n  .. .. ..$ variables   : list()\n  .. .. ..$ site        : chr \"100079\"\n\n\nUhhh, those have no variables? WHat’s going on here?"
  },
  {
    "objectID": "stats_probability/shifting_normals.html",
    "href": "stats_probability/shifting_normals.html",
    "title": "Shifting distributions",
    "section": "",
    "text": "Shifting normal distributions to have desired properties is straightforward, but I have to look it up every time.\nI typically need to do it for two reasons (which are really the same reason, when it comes down to it), plus a third that is also the same but requires transforms and backtransforms."
  },
  {
    "objectID": "stats_probability/shifting_normals.html#setup",
    "href": "stats_probability/shifting_normals.html#setup",
    "title": "Shifting distributions",
    "section": "Setup",
    "text": "Setup\nTo have some common references, let’s get a set of random numbers from a standard normal\n\nstdnorm &lt;- rnorm(1000)\n\nAnd set example mean and sd we want.\n\nmean_want &lt;- 5\nsd_want &lt;- 2"
  },
  {
    "objectID": "stats_probability/shifting_normals.html#mean-shifts",
    "href": "stats_probability/shifting_normals.html#mean-shifts",
    "title": "Shifting distributions",
    "section": "Mean shifts",
    "text": "Mean shifts\n\nFrom 0\nMean shifts are just arithmetic. So, if we have a standard normal, we can just add the new mean we want.\n\nnorm5 &lt;- stdnorm + mean_want\n\nmean(norm5)\n\n[1] 4.961001\n\n\n\n\nFrom x to y\nIf we have a distribution with a nonzero mean (say, the one we just made) and want to shift it, we need to get the difference. This is often how we deal with correcting a set of generated numbers to have the right mean, but here I use a more extreme example.\nI’m also writing this as a function with variables to see the generality.\n\nshift_mean &lt;- function(sample_vals, mean_want) {\n  mean_sample &lt;- mean(sample_vals)\n\n  mean_shift &lt;- mean_want - mean_sample\n\n  shiftsample &lt;- sample_vals + mean_shift\n  \n  return(shiftsample)\n}\n\n\nnorm2 &lt;- shift_mean(norm5, 2)\nmean(norm2)\n\n[1] 2\n\n\nThis works for the simple case of the standard normal too, and unlike above, where we add 5 and get some error from the particular set of values, here the mean ends up exact because the shift isn’t 5, it’s the difference between 5 and the realised mean of the vector.\n\nmean(shift_mean(stdnorm, 5))\n\n[1] 5\n\n\nI can’t be bothered making this a tibble. Though maybe I should, I really am bad at plot .\n\nplot(density(stdnorm), type = 'lines')\n\nWarning in plot.xy(xy, type, ...): plot type 'lines' will be truncated to first\ncharacter\n\nlines(density(norm5), col = 'firebrick')\nlines(density(norm2), col = 'dodgerblue')"
  },
  {
    "objectID": "stats_probability/shifting_normals.html#standard-deviation-shifts",
    "href": "stats_probability/shifting_normals.html#standard-deviation-shifts",
    "title": "Shifting distributions",
    "section": "Standard deviation shifts",
    "text": "Standard deviation shifts\nThe standard deviation requires a multiplicative shift. So if we want an sd of 2, we need to multiply the standard normal by 2.\n\nnormsd2 &lt;- stdnorm*sd_want\nsd(normsd2)\n\n[1] 2.058442\n\n\nAs with the mean, to shift from arbitrary sd to desired, we can write a function that finds the relative sds and shift.\n\nshift_sd &lt;- function(sample_vals, sd_want) {\n  sd_sample &lt;- sd(sample_vals)\n\n  sd_shift &lt;- sd_want/sd_sample\n\n  shiftsample &lt;- sample_vals * sd_shift\n  \n  return(shiftsample)\n}\n\nSo to move from the vector with sd 2 to one with sd 5\n\nnormsd5 &lt;- shift_sd(normsd2, 5)\nsd(normsd5)\n\n[1] 5\n\n\nAnd like the mean example above, we can use this to correct the realised mean of a set of random numbers, since it’s comparative to that realised mean.\n\nsd(shift_sd(stdnorm, sd_want))\n\n[1] 2\n\n\nQuick plots\n\nplot(density(stdnorm), type = 'lines')\n\nWarning in plot.xy(xy, type, ...): plot type 'lines' will be truncated to first\ncharacter\n\nlines(density(normsd5), col = 'firebrick')\nlines(density(normsd2), col = 'dodgerblue')"
  },
  {
    "objectID": "stats_probability/shifting_normals.html#both-mean-and-sd",
    "href": "stats_probability/shifting_normals.html#both-mean-and-sd",
    "title": "Shifting distributions",
    "section": "Both mean and sd",
    "text": "Both mean and sd\nTo shift both mean and sd together, we first shift the sd, and then the mean. Again, we can make this a function.\n\nshift_mean_sd &lt;- function(sample_vals, \n                          sd_want, mean_want) {\n  \n  shiftedsd &lt;- shift_sd(sample_vals, sd_want)\n  \n  shiftedboth &lt;- shift_mean(shiftedsd, mean_want)\n}\n\nNow, we can use that to create a new distribution\n\nnorm52 &lt;- shift_mean_sd(stdnorm, sd_want, mean_want)\nmean(norm52)\n\n[1] 5\n\nsd(norm52)\n\n[1] 2\n\nplot(density(norm52))\n\n\n\n\nAnd we can shift that to something different again- we don’t need to start with a standard normal.\n\nnorm25 &lt;- shift_mean_sd(norm52, sd_want = 0.5, mean_want = 2)\nmean(norm25)\n\n[1] 2\n\nsd(norm25)\n\n[1] 0.5\n\nplot(density(norm25))\n\n\n\n\nSo, now we have a function that can shift any arbitrary set of numbers to have a desired mean and sd.\n\nThe wrong way- order of operations\nAs a quick aside, it does not work to shift the mean first.\n\nshift_mean_sd_BACKWARDS &lt;- function(sample_vals, \n                          sd_want, mean_want) {\n  \n  shiftedmean &lt;- shift_mean(sample_vals, mean_want)\n  \n  shiftedboth &lt;- shift_sd(shiftedmean, sd_want)\n}\n\nThis produces the right sd, but the mean is wrong, because the mulitplication happens after the mean shift, and so shifts the mean again.\n\nbacknorm25 &lt;- shift_mean_sd_BACKWARDS(norm52, sd_want = 0.5, mean_want = 5)\nmean(backnorm25)\n\n[1] 1.25\n\nsd(backnorm25)\n\n[1] 0.5\n\nplot(density(backnorm25))"
  },
  {
    "objectID": "stats_probability/shifting_normals.html#other-distributions",
    "href": "stats_probability/shifting_normals.html#other-distributions",
    "title": "Shifting distributions",
    "section": "Other distributions",
    "text": "Other distributions\nFor other distributions based on the normal (e.g. lognormal), these equations shouldn’t be used directly to set means and sds on those scales. E.g. if we have data that is lognormal and we want it to have mean 5 and sd 10 and we naively apply the function above, it will have those moments, but the distribution will be nonlinearly altered and no longer really be lognormal.\nInstead, we need to find the desired means and variances for the underlying normal, do the transform to those, and then back-transform the shifted data. This will retain the lognormal distribution. For some weird distributions (e.g. Johnson), there is not a single translation to the normal, and so it’s often just easiest to discuss translations on the normal scale.\nFor reference, the lognormal transforms to get the desired mean and sd on the normal scale from those on the lognormal (lnmean, lnvar) are\n\n# back-calc normal parameters from lognormal\nnormmu_from_lognorm &lt;- function(lnmean, lnsd) {\n  lnvar &lt;- lnsd^2\n  mu &lt;- log((lnmean^2) / sqrt(lnvar + lnmean^2))\n}\n\nnormsd_from_lognorm &lt;- function(lnmean, lnsd) {\n  lnvar &lt;- lnsd^2\n  sd &lt;- sqrt(log(lnvar / (lnmean^2) +1))\n}"
  },
  {
    "objectID": "stats_probability/fitting_lognormals.html",
    "href": "stats_probability/fitting_lognormals.html",
    "title": "Fitting lognormals",
    "section": "",
    "text": "I need to fit some truncated lognormals, and want to work through how to do that with known distributions. Ignoring the truncated nature, I’m also getting weird outcomes where if I fit the data as lognormal, the distribution is way off, but when I manually log and then fit a normal, it’s fine.\nI’ll use fitdistr from MASS, but may move on.\nlibrary(MASS)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()"
  },
  {
    "objectID": "stats_probability/fitting_lognormals.html#the-data",
    "href": "stats_probability/fitting_lognormals.html#the-data",
    "title": "Fitting lognormals",
    "section": "The data",
    "text": "The data\nRather than use the real data, at least at the outset, I’ll generate data with known distribution. I’ll make it somewhere off the standard normal.\n\ntestdata &lt;- rlnorm(10000, meanlog = 5, sdlog = 1.5)\n\nDoes that look right? Yes, at least nothing obviously wrong.\n\nggplot(tibble(testdata), aes(x = testdata)) + geom_density()\n\n\n\nggplot(tibble(testdata), aes(x = log(testdata))) + geom_density()"
  },
  {
    "objectID": "stats_probability/fitting_lognormals.html#fitting",
    "href": "stats_probability/fitting_lognormals.html#fitting",
    "title": "Fitting lognormals",
    "section": "Fitting",
    "text": "Fitting\nI’ll fit the raw data using lognormal, and the logged data using normal, then plot the pdfs and cdfs\n\nfit_log &lt;- fitdistr(testdata, densfun = 'lognormal')\nfit_n &lt;- fitdistr(log(testdata), densfun = 'normal')\n\nfit_log\n\n    meanlog       sdlog   \n  5.01252077   1.49363174 \n (0.01493632) (0.01056157)\n\nfit_n\n\n      mean          sd    \n  5.01252077   1.49363174 \n (0.01493632) (0.01056157)\n\n\nThat is clearly working the same, so that’s good.\nNow let’s make some plots of the CDF and PDF of the fit curves, along with the empirical fits.\nWe need to make some dataframes. Ideally we’d combine them and pivot_longer, but I’m not going to bother.\n\ndf_log &lt;- tibble(x = 0:10000, \n                 cdf = plnorm(x, fit_log$estimate[1], fit_log$estimate[2]),\n                 pdf = dlnorm(x, fit_log$estimate[1], fit_log$estimate[2]))\n\ndf_n &lt;- tibble(x = seq(0,10, by = 0.01), \n                 cdf = pnorm(x, fit_n$estimate[1], fit_n$estimate[2]),\n                 pdf = dnorm(x, fit_n$estimate[1], fit_n$estimate[2]))\n\nPlot the fits done with *lnorm on the raw data-\nFirst, pdf on the linear scale. Let’s zoom in, too. Use xlim instead of coord_cartesian or it doesn’t have enough points in the geom_density. That seems a bit shifted.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = testdata), color = 'black') +\n  geom_line(data = df_log, aes(x = x, y = pdf), color = 'firebrick') +\n  xlim(c(-1, 1000))\n\nWarning: Removed 1025 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 9000 rows containing missing values (`geom_line()`).\n\n\n\n\n\nPDF on the log scale (but this is still the fit done without pre-logging the data. This is dramatically shifted down. Same thing happened with my real data, so I’m trying to understand why. The mean should be at 4.97, and this clearly isn’t.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_log, aes(x = log(x), y = pdf*50), color = 'firebrick')\n\n\n\n\nPDF of the pre-logged and fit as normal. That fits great. So, why is there a shift in the lognormal? They’re using the same functions and parameters.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_n, aes(x = x, y = pdf), color = 'firebrick')\n\n\n\n\nIs the weirdness with the log possibly because of some strange discrepancy with the testdata vs evenly-spaced x? It shouldn’t be- it just gets P(x=X) at each x. But using testdata as x and getting the fit should remove that as an issue and focus just on the fit.\n\ndf_log2 &lt;- tibble(x = testdata, \n                 cdf = plnorm(x, fit_log$estimate[1], fit_log$estimate[2]),\n                 pdf = dlnorm(x, fit_log$estimate[1], fit_log$estimate[2]))\n\nSame issue.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_log2, aes(x = log(x), y = pdf*50), color = 'firebrick')\n\n\n\n\nDo we see this issue in the CDFs (plnorm)? or is it really just an issue with dlnorm? This doesn’t look obviously shifted down 2.\n\nggplot() + \n  stat_ecdf(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_log, aes(x = log(x), y = cdf), color = 'firebrick')\n\n\n\n\nWould we notice if it were shifted down? Build one on the normal scale where we know the parameters are working how we think. That is behaving how it should. SO. Why is dlnorm not producing the densities we expect? Especially when plnorm does produce the CDFs we expect?\n\ndf_n2 &lt;- tibble(x = seq(0,10, by = 0.01), \n                 cdf = pnorm(x, 2.5, fit_n$estimate[2]),\n                 pdf = dnorm(x, 2.5, fit_n$estimate[2]))\n\nggplot() + \n  stat_ecdf(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_log, aes(x = log(x), y = cdf), color = 'firebrick') +\n  geom_line(data = df_n2, aes(x = x, y = cdf), color = 'dodgerblue')\n\n\n\n\nWhat if we go back to the linear scale- would we see a shift there? e.g. is the issue in the translation from linear PDF to log, and I’m forgetting something about calculations around f(g(x))? Now, we use the df_log, but change the mean. This time I’ll shift UP to try to see if the resulting blue line gets closer than the red line. Nope. With that amount of shift, the fit is obviously different (and much worse). I had a bit of a play, and the closest I can get is with a mean of about 5.1, but this is only part of the distribution, so the values from the fit do seem to be getting the whole PDF as close as possible on this scale.\n\ndf_log2 &lt;- tibble(x = 0:10000, \n                 cdf = plnorm(x, 7.5, fit_log$estimate[2]),\n                 pdf = dlnorm(x, 7.5, fit_log$estimate[2]))\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = testdata), color = 'black') +\n  geom_line(data = df_log, aes(x = x, y = pdf), color = 'firebrick') +\n  geom_line(data = df_log2, aes(x = x, y = pdf), color = 'dodgerblue') + \n xlim(c(0, 1000))\n\nWarning: Removed 1025 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 9000 rows containing missing values (`geom_line()`).\nRemoved 9000 rows containing missing values (`geom_line()`).\n\n\n\n\n\nThe pre-logged and fit as normal works, as we expect.\n\nggplot() + \n  stat_ecdf(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_line(data = df_n, aes(x = x, y = cdf), color = 'firebrick')\n\n\n\n\nI think I’m being naive in my translation because I’m trying to get the PDF on a transformed variable, and so not accounting for nonlinear spacing. E.g. the f(g(x)) issue."
  },
  {
    "objectID": "stats_probability/fitting_lognormals.html#random-numbers",
    "href": "stats_probability/fitting_lognormals.html#random-numbers",
    "title": "Fitting lognormals",
    "section": "Random numbers",
    "text": "Random numbers\nIf I find random numbers from rlnorm or rnorm, do they match the distribution? In both directions.\n\ndf_rand &lt;- tibble(r_lnorm = rlnorm(10000, \n                                   fit_log$estimate[1], fit_log$estimate[2]),\n                  r_norm = rnorm(10000,\n                                 fit_n$estimate[1], fit_n$estimate[2]),\n                  r_lnormlog = log(r_lnorm),\n                  r_normexp = exp(r_norm))\n\nOn the linear scale, r_lnorm and r_normexp should match the testdata, and they do.\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = testdata), color = 'black') +\n  geom_density(data = df_rand, aes(x = r_lnorm), color = 'firebrick') +\n  geom_density(data =  df_rand, aes(x = r_normexp), color = 'dodgerblue') + \n xlim(c(0, 1000))\n\nWarning: Removed 1025 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 1017 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 977 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nOn the log scale, r_norm and rlnormlog should match log(testdata)\n\nggplot() + \n  geom_density(data = tibble(testdata), aes(x = log(testdata)), color = 'black') +\n  geom_density(data = df_rand, aes(x = r_norm), color = 'firebrick') +\n  geom_density(data =  df_rand, aes(x = r_lnormlog), color = 'dodgerblue')\n\n\n\n\nThat quite clearly works in both directions, further demonstrating that the naive logging of x for the PDF is just mathematically not appropriate. I should do the math to figure it out, but I need to move on now."
  },
  {
    "objectID": "small_helpers/zip_downloading.html",
    "href": "small_helpers/zip_downloading.html",
    "title": "Download Zip helper",
    "section": "",
    "text": "When we download files from the internet, we often feed in a url, and it returns a zip, which we then want to unzip to access. There’s a fairly simple way to do that, but we can write a quicky function to do it and clean up the directory afterwards."
  },
  {
    "objectID": "small_helpers/zip_downloading.html#the-issue",
    "href": "small_helpers/zip_downloading.html#the-issue",
    "title": "Download Zip helper",
    "section": "",
    "text": "When we download files from the internet, we often feed in a url, and it returns a zip, which we then want to unzip to access. There’s a fairly simple way to do that, but we can write a quicky function to do it and clean up the directory afterwards."
  },
  {
    "objectID": "small_helpers/zip_downloading.html#the-function",
    "href": "small_helpers/zip_downloading.html#the-function",
    "title": "Download Zip helper",
    "section": "The function",
    "text": "The function\nwe want to give it the dirname for the file(s), the datadir that contains our data, and the URL. Then it checks if it exists, and downloads, unzips, and cleans up.\n\nzip_load &lt;- function(dirname, datadir, sourceurl,  \n                      existing_dirs = list.files(datadir)) {\n  print(existing_dirs)\n  if (!(dirname %in% existing_dirs)) {\n    \n    zippath &lt;- file.path(datadir, paste0(dirname, '.zip'))\n    download.file(sourceurl, destfile = zippath)\n    \n    unzip(zippath, exdir = file.path(datadir, dirname))\n    \n    file.remove(zippath)\n  }\n}"
  },
  {
    "objectID": "small_helpers/zip_downloading.html#an-example",
    "href": "small_helpers/zip_downloading.html#an-example",
    "title": "Download Zip helper",
    "section": "An example",
    "text": "An example\nGet the Murray-Darling basin boundary\n\nzip_load('mdb_boundary', 'data', \"https://data.gov.au/data/dataset/4ede9aed-5620-47db-a72b-0b3aa0a3ced0/resource/8a6d889d-723b-492d-8c12-b8b0d1ba4b5a/download/sworkingadhocjobsj4430dataoutputsmdb_boundarymdb_boundary.zip\")\n\ncharacter(0)\n\n\nWarning in download.file(sourceurl, destfile = zippath): URL\nhttps://data.gov.au/data/dataset/4ede9aed-5620-47db-a72b-0b3aa0a3ced0/resource/8a6d889d-723b-492d-8c12-b8b0d1ba4b5a/download/sworkingadhocjobsj4430dataoutputsmdb_boundarymdb_boundary.zip:\ncannot open destfile 'data/mdb_boundary.zip', reason 'No such file or directory'\n\n\nWarning in download.file(sourceurl, destfile = zippath): download had nonzero\nexit status\n\n\nWarning in unzip(zippath, exdir = file.path(datadir, dirname)): error 1 in\nextracting from zip file\n\n\nWarning in file.remove(zippath): cannot remove file 'data/mdb_boundary.zip',\nreason 'No such file or directory'\n\n\n[1] FALSE\n\n\nI’ve saved this in functions/ so I have easy access to it everywhere."
  },
  {
    "objectID": "small_helpers/quarto_notes.html",
    "href": "small_helpers/quarto_notes.html",
    "title": "Quarto notes",
    "section": "",
    "text": "library(ggplot2)"
  },
  {
    "objectID": "small_helpers/quarto_notes.html#quarto-root-directory",
    "href": "small_helpers/quarto_notes.html#quarto-root-directory",
    "title": "Quarto notes",
    "section": "Quarto root directory",
    "text": "Quarto root directory\nIf there’s not a Quarto project (which is not the same as an Rproject), i.e. the .qmds are standalone, then the above rmarkdown method works to set the root to the Rproject. But if there is a Quarto project (I’ve moved to almost always doing this), then we can set the Quarto root directory in the _quarto.yml for the project. There are actually two useful settings we can make there- a project-wide output directory, and what to use to execute. There are (to my knowledge) two options for execute-dir- execute-dir: file and execute-dir: project, which set the root for rendering as the file location or the project. It almost always makes most sense to use project, because then everything uses the same reference for relative paths. For this website, that yaml is\n\nproject:\n  type: website\n  output-dir: docs\n  execute-dir: project\n\nLinking pages\nThis is fairly specific to websites (and I guess books?). Sorted out in the website-specific page. Takehome is links need to be relative to the file. And so those in different directories often have to use ../other_dir/other_file.qmd to get up and over.\n\n\nNested Rprojects\nThe only exception that I’ve run into so far is a weird situation where I have a Quarto project with an Rproject in a subdirectory because I want the Quarto to have access to several different code projects. The catch is that if I use paths relative to the R project and ask Quarto to render, all the paths are wrong. They are also wrong if I Run or run the code cells interactively. If I add the Rmarkdown setup chunk above knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()), then Run and interactive stuff works, but when I try to quarto render, it can’t find the Rproject because it’s looking at the same level and up, and the Rproj is in a directory down.\nSo, the workaround I’ve come up with is to set execute-dir: file, so the dir is set to the file dir, which is inside the Rproject. Then, use an Rmarkdown-style setup chunk with knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) to reset the directory to the Rproject root dir.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nThen the code both Runs and renders, and we don’t have to keep track of paths relative to two different nested projects manually. This approach works if the internal projects are self-contained (and we often want them to be). But if we need to render something across several, we’ll need to do something different.\nOther options that might work but I haven’t tried are parameterised notebooks (with conditional params?) Or using freeze and virtual environments (see docs)\nWhat I really want is a way to set the execute-dir on a file-by-file basis in the header yaml, but that doesn’t seem to work."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#headers-in-vs-code-vs-rstudio",
    "href": "small_helpers/quarto_notes.html#headers-in-vs-code-vs-rstudio",
    "title": "Quarto notes",
    "section": "Headers in VS code vs Rstudio",
    "text": "Headers in VS code vs Rstudio\nI had a few notebooks that ran fine in Rstudio, but wouldn’t render in VS (or from command line). I got the very cryptic error YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key with a line and col number that didn’t seem to correspond to anything with YAML in the project- not the file header, not the _quarto.yml, not _quarto.yaml.local, and nothing particularly useful showed up on google.\nThe solution seems to be to add an explicit line for editor: visual in the file header, e.g.\ntitle: Test\nauthor: Galen Holt\nformat:\n  html:\n    df-print: paged\neditor: visual\nIt doesn’t seem to matter if the title and author are wrapped in double-quotes (which was one suggestion online).\nI think what’s happening is that Rstudio’s visual editor is adding some hidden formatting syntax that gets exposed and looks like YAML to VS and the quarto CLI."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#python-paths",
    "href": "small_helpers/quarto_notes.html#python-paths",
    "title": "Quarto notes",
    "section": "Python paths",
    "text": "Python paths\nIf we use {reticulate}, R wants to know where the python lives, and gets grumpy if it can’t find it. Sometimes it silently points somewhere we don’t want, and other times we see errors like\nError creating conda environment 'C:/Users/galen/Documents/Website/galen_website/renv/python/r-reticulate' [exit code 1]\n\nRprofile\nThe main solution is to edit .Rprofile to set the RETICULATE_PYTHON environment variable,\nSys.setenv(RETICULATE_PYTHON = '../werptoolkitpy/.venv/Scripts/python.exe')\nNote that this can either be a full path, or relative to the R project directory- in the situation above, I have the R project nested in a Quarto project that also contains a py project, so we need to go up and over to get to the .venv.\nThis is needed to get any of the reticulate code to work (though you can set it on a file-by-file basis.\nAdditional issues can come up with Quarto though.\n\n\nQuarto environment\nWhen we run Quarto inside Rstudio in parallel with an R project, everything works fine. But, if Quarto is run through VScode, for some reason it doesn’t hit the .Rprofile and so doesn’t run the line we just added, and we’re back to using the wrong python and errors about conda. To make it more complex, Quarto has its own python environment variable QUARTO_PYTHON. This is all more complicated when the Quarto project directory doesn’t match the R project directory.\nThe solution is to set up a _environment file in the Quarto project directory to set those variables according to the docs (and maybe PY_PYTHON too, just to be safe- I can’t find the docs to know how these differ). For my case, my _environment file looks like\nRETICULATE_PYTHON='../werptoolkitpy/.venv/Scripts/python.exe'\nQUARTO_PYTHON='werptoolkitpy/.venv/Scripts/python.exe'\nPY_PYTHON='werptoolkitpy/.venv/Scripts/python.exe'\nHere, again, I have Rproject nested in Quarto proj, but they access this file differently so the paths differ even though they point to the same place. RETICULATE_PYTHON needs to get up and out of the R subdir and over to the py side of things, while the Quarto project is the outer dir and so QUARTO_PYTHON can go straight in to the py subdir.\nAs far as I can tell, this does NOT supersede the necessity of setting .Rprofile, which is needed for the R code to run. This stuff in _environment is in addition so Quarto (especially in VS) can access the right info."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#table-printing",
    "href": "small_helpers/quarto_notes.html#table-printing",
    "title": "Quarto notes",
    "section": "Table printing",
    "text": "Table printing\n\nStyle\nUse the yaml header to declare style, one of paged, kable, tibble, and default as in documentation. In a twist, default seems to be the way to do some customisation using S3, see https://debruine.github.io/quarto_demo/table.html, though I haven’t played with that.\nSo, typically the yaml header would be something like\nformat:\n  html:\n    df-print: paged\nCan I change that for a single chunk? Work on that later. Putting it in as #| df-print: option isn’t recognized.\n\n\nRow number\nThe different df-print options have different defaults of how much they print (and with paged it doesn’t matter so much). As far as I can tell, tibble prints the whole thing, and kable prints 10 rows. Sometimes we want to control that though - maybe we have a df with 13 rows, and we want to just print the whole thing.\nThe default is 10, though that’s not working when I render to the web for some reason\n\n```{r}\niris\n```\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\n\nTo print more rows, we can use the rows.print option. NOTE- this used to work, and now does not. Will need to sort out a new solution.\n\n```{r}\n#| rows.print: 15\n\niris\n```\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\n\n\n\nFigure captions with variables\nSometimes we want figure captions to auto-update, e.g. maybe we want them to provide some parameter values. In that case, we can provide !expr, for example\n\nparam_val &lt;- 17\n\n\n```{r}\n#| fig-cap: !expr glue::glue(\"Iris petals multiplied by {param_val}\")\n\nggplot(iris, aes(x = Petal.Length * param_val, y = Petal.Width * param_val, color = Species)) + geom_point()\n```\n\n\n\n\nIris petals multiplied by 17\n\n\n\n\nThat’s a bit annoying that we have to build the whole cap as an R expression, but we should be able to use glue, paste, or whatever syntax (will need to sort something out for math and greek, maybe expression or bquote? latex2exp isn’t just working out of the box).\n\n```{r}\n#| fig-cap: !expr latex2exp::TeX(\"$\\\\alpha$\")\n\nggplot(iris, aes(x = Petal.Length * param_val, y = Petal.Width * param_val, color = Species)) + geom_point()\n```\n\n\n\n\nalpha"
  },
  {
    "objectID": "small_helpers/quarto_notes.html#website",
    "href": "small_helpers/quarto_notes.html#website",
    "title": "Quarto notes",
    "section": "Website",
    "text": "Website\nStarting and updating quarto websites has a few tricks, written up in more detail at those links."
  },
  {
    "objectID": "small_helpers/quarto_notes.html#rendering",
    "href": "small_helpers/quarto_notes.html#rendering",
    "title": "Quarto notes",
    "section": "Rendering",
    "text": "Rendering\nThe error\nError: The process cannot access the file because it is being used by another process. (os error 32),\nSeems to be due to dropbox or onedrive even when they are not connected to the folder with the code. Quit them, and it usually goes away. A restart is sometimes required."
  },
  {
    "objectID": "small_helpers/list_names.html",
    "href": "small_helpers/list_names.html",
    "title": "List names as variables",
    "section": "",
    "text": "I often end up wanting to build a list with named items, and the names come in as variables.\nFor example, instead of the typical\nlist(a = 'first', b = 'second', d = c(3,4))\n\n$a\n[1] \"first\"\n\n$b\n[1] \"second\"\n\n$d\n[1] 3 4\nI might have the names defined elsewhere. This is particularly common inside functions.\nname1 &lt;- 'a'\nname2 &lt;- 'b'\nname3 &lt;- 'd'\nWe can’t pass those in as usual- this uses name# as the name, not the value of the variable.\nlist(name1 = 'first', name2 = 'second', name3 = c(3,4))\n\n$name1\n[1] \"first\"\n\n$name2\n[1] \"second\"\n\n$name3\n[1] 3 4"
  },
  {
    "objectID": "small_helpers/list_names.html#various-solutions",
    "href": "small_helpers/list_names.html#various-solutions",
    "title": "List names as variables",
    "section": "Various solutions",
    "text": "Various solutions\nWe can use setNames\n\nsetNames(list('first', 'second',c(3,4)), c(name1, name2, name3))\n\n$a\n[1] \"first\"\n\n$b\n[1] \"second\"\n\n$d\n[1] 3 4\n\n\nwe can do it in two steps with names , which I think is what setNames wraps, and is just extra verbose and requires carrying data copies around.\n\nbarelist &lt;- list('first', 'second',c(3,4))\nnames(barelist) &lt;- c(name1, name2, name3)\nbarelist\n\n$a\n[1] \"first\"\n\n$b\n[1] \"second\"\n\n$d\n[1] 3 4\n\n\nCan we unquote/eval?\nI can almost never get !! or !!! to work. this doesn’t, as usual.\n\nlist(!!name1 = 'first', !!name2 = 'second', !!name3 = c(3,4))\n\nNor this\n\nlist(eval(name1) = 'first', eval(name2) = 'second', eval(name3) = c(3,4))\n\nNor this, despite the eval working\n\nrlang::eval_bare(name1)\n\n[1] \"a\"\n\n\n\nlist(rlang::eval_bare(name1) = 'first', rlang::eval_bare(name2) = 'second', rlang::eval_bare(name3) = c(3,4))\n\nHow about tibble::lst ? I often use it for lists of variables because it self-names them, so maybe it’s the answer here. Yep. That’s just cleaner.\n\ntibble::lst(name1 = 'first', name2 = 'second', name3 = c(3,4))\n\n$name1\n[1] \"first\"\n\n$name2\n[1] \"second\"\n\n$name3\n[1] 3 4\n\n\nAnd, that self-naming I was describing, which solves a different problem- having to write list(name = name, age = age).\n\nname &lt;- c('David', 'Susan')\nage &lt;- c(1,2)\n\ntibble::lst(name, age)\n\n$name\n[1] \"David\" \"Susan\"\n\n$age\n[1] 1 2"
  },
  {
    "objectID": "small_helpers/json_api_construction.html",
    "href": "small_helpers/json_api_construction.html",
    "title": "JSON API coding",
    "section": "",
    "text": "I’m working on an API that uses JSON in the body, but getting it to come out right with square brackets, curly brackets, commas, etc where they’re supposed to be has been trial and error. I’m going to put what I’ve figured out here. I’m using examples from the {vicwater} package, but the main point is to show how to get different sorts of output.\nUsing the httr2 examples with req_dry_run to see what the request looks like and check the formats.\n\nlibrary(httr2)\nreq &lt;- request(\"http://httpbin.org/post\")\n\n\n\nTo pass simple one to one key-value pairs, wrapped in {} use a list.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\n\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 40\n\n{\"function\":\"get_db_info\",\"version\":\"3\"}\n\n\n\n\n\nTo pass nested key-value pairs, use nested lists\n\nparams &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '123abc',\n                               \"datasource\" = \"A\"))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 95\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"123abc\",\"datasource\":\"A\"}}\n\n\n\n\n\nThese cannot be created with c(), because that does something else (square brackets- see below).\n\nparams &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '233217, 405328, 405331'))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 100\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331\"}}\n\n\n\n\n\nTo get square brackets, we need a vector. So, typically c() the bits together in the call (or previously).\n\nparams &lt;- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = c('A', 'TELEM')))\n\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}\n\n\n\n\n\nTo get patterns like [['a', 'b'],['c', 'd']], use a matrix (and I think maybe a df). Which makes sense if we think of that as a group of vectors. The pattern is [[row1], [row2], [row_n]].\n\ntopleft &lt;- c('-35', '148')\nbottomright &lt;- c('-36', '149')\n\nrectbox &lt;- rbind(topleft, bottomright)\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"148\"],[\"-36\",\"149\"]]}}}\n\n\n\n\nGives some horrible combination of curly and square braces including column and row names.\n\nrectdf &lt;- data.frame(rectbox)\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 208\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"X1\":\"-35\",\"X2\":\"148\",\"_row\":\"topleft\"},{\"X1\":\"-36\",\"X2\":\"149\",\"_row\":\"bottomright\"}]}}}\n\n\nTibbles aren’t really any different, but the names are a bit cleaner\n\nrectdf &lt;- tibble::as_tibble(rectbox)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}\n\n\n\n\n\n\nThere are arguments to toJSON that alter how matrices and dfs get parsed. Matrices are by default row-wise, but we can change to cols (e.g. [['col1'], ['col2']] with matrix = 'columnmajor'.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %&gt;%\n  req_body_json(params, matrix = 'columnmajor') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"-36\"],[\"148\",\"149\"]]}}}\n\n\nSimilarly, we can alter how dfs work, which might actually be fairly useful in the way it handles named columns especially. The default (above) is dataframe = 'rows' , which is kind of a mess (or at least not how my brain parses what a dataframe means). But dataframe = 'columns' ends up with named vectors. I don’t currently need that, but it sure makes more sense.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params, dataframe = 'columns') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 160\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":{\"V1\":[\"-35\",\"-36\"],\"V2\":[\"148\",\"149\"]}}}}\n\n\nUsing dataframe = 'values' is again a confusing list.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params, datafraem = 'values') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}\n\n\n\n\n\nTo get square brackets around multiple sets of curlies, e.g. `[{‘key’: ‘value’}, {‘key2’: ‘value2’}], use a list of lists.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \n                               \"complex_filter\" = list(list('fieldname' = 'stntype', \n                                                       'value' = 'HYD'),\n                                                   list('combine' = 'OR',\n                                                        'fieldname' = 'stntype',\n                                                        'value' = 'VIR'))))\nreq %&gt;%\n  req_body_json(params, datafraem = 'values') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 203\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"complex_filter\":[{\"fieldname\":\"stntype\",\"value\":\"HYD\"},{\"combine\":\"OR\",\"fieldname\":\"stntype\",\"value\":\"VIR\"}]}}"
  },
  {
    "objectID": "small_helpers/json_api_construction.html#simple-key-value",
    "href": "small_helpers/json_api_construction.html#simple-key-value",
    "title": "JSON API coding",
    "section": "",
    "text": "To pass simple one to one key-value pairs, wrapped in {} use a list.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\")\n\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 40\n\n{\"function\":\"get_db_info\",\"version\":\"3\"}"
  },
  {
    "objectID": "small_helpers/json_api_construction.html#nested-key-value",
    "href": "small_helpers/json_api_construction.html#nested-key-value",
    "title": "JSON API coding",
    "section": "",
    "text": "To pass nested key-value pairs, use nested lists\n\nparams &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '123abc',\n                               \"datasource\" = \"A\"))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 95\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"123abc\",\"datasource\":\"A\"}}"
  },
  {
    "objectID": "small_helpers/json_api_construction.html#comma-separated-strings",
    "href": "small_helpers/json_api_construction.html#comma-separated-strings",
    "title": "JSON API coding",
    "section": "",
    "text": "These cannot be created with c(), because that does something else (square brackets- see below).\n\nparams &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = '233217, 405328, 405331'))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 100\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331\"}}"
  },
  {
    "objectID": "small_helpers/json_api_construction.html#square-brackets",
    "href": "small_helpers/json_api_construction.html#square-brackets",
    "title": "JSON API coding",
    "section": "",
    "text": "To get square brackets, we need a vector. So, typically c() the bits together in the call (or previously).\n\nparams &lt;- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = c('A', 'TELEM')))\n\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}"
  },
  {
    "objectID": "small_helpers/json_api_construction.html#double-square-brackets",
    "href": "small_helpers/json_api_construction.html#double-square-brackets",
    "title": "JSON API coding",
    "section": "",
    "text": "To get patterns like [['a', 'b'],['c', 'd']], use a matrix (and I think maybe a df). Which makes sense if we think of that as a group of vectors. The pattern is [[row1], [row2], [row_n]].\n\ntopleft &lt;- c('-35', '148')\nbottomright &lt;- c('-36', '149')\n\nrectbox &lt;- rbind(topleft, bottomright)\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"148\"],[\"-36\",\"149\"]]}}}\n\n\n\n\nGives some horrible combination of curly and square braces including column and row names.\n\nrectdf &lt;- data.frame(rectbox)\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 208\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"X1\":\"-35\",\"X2\":\"148\",\"_row\":\"topleft\"},{\"X1\":\"-36\",\"X2\":\"149\",\"_row\":\"bottomright\"}]}}}\n\n\nTibbles aren’t really any different, but the names are a bit cleaner\n\nrectdf &lt;- tibble::as_tibble(rectbox)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params) %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}"
  },
  {
    "objectID": "small_helpers/json_api_construction.html#orientation-of-dfs-and-matrices",
    "href": "small_helpers/json_api_construction.html#orientation-of-dfs-and-matrices",
    "title": "JSON API coding",
    "section": "",
    "text": "There are arguments to toJSON that alter how matrices and dfs get parsed. Matrices are by default row-wise, but we can change to cols (e.g. [['col1'], ['col2']] with matrix = 'columnmajor'.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\nreq %&gt;%\n  req_body_json(params, matrix = 'columnmajor') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 150\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[[\"-35\",\"-36\"],[\"148\",\"149\"]]}}}\n\n\nSimilarly, we can alter how dfs work, which might actually be fairly useful in the way it handles named columns especially. The default (above) is dataframe = 'rows' , which is kind of a mess (or at least not how my brain parses what a dataframe means). But dataframe = 'columns' ends up with named vectors. I don’t currently need that, but it sure makes more sense.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params, dataframe = 'columns') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 160\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":{\"V1\":[\"-35\",\"-36\"],\"V2\":[\"148\",\"149\"]}}}}\n\n\nUsing dataframe = 'values' is again a confusing list.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectdf)))\nreq %&gt;%\n  req_body_json(params, datafraem = 'values') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 170\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"geo_filter\":{\"rectangle\":[{\"V1\":\"-35\",\"V2\":\"148\"},{\"V1\":\"-36\",\"V2\":\"149\"}]}}}"
  },
  {
    "objectID": "small_helpers/json_api_construction.html#square-brackets-around-curly",
    "href": "small_helpers/json_api_construction.html#square-brackets-around-curly",
    "title": "JSON API coding",
    "section": "",
    "text": "To get square brackets around multiple sets of curlies, e.g. `[{‘key’: ‘value’}, {‘key2’: ‘value2’}], use a list of lists.\n\nparams &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \n                               \"complex_filter\" = list(list('fieldname' = 'stntype', \n                                                       'value' = 'HYD'),\n                                                   list('combine' = 'OR',\n                                                        'fieldname' = 'stntype',\n                                                        'value' = 'VIR'))))\nreq %&gt;%\n  req_body_json(params, datafraem = 'values') %&gt;%\n  req_dry_run()\n\nPOST /post HTTP/1.1\nHost: httpbin.org\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 203\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"complex_filter\":[{\"fieldname\":\"stntype\",\"value\":\"HYD\"},{\"combine\":\"OR\",\"fieldname\":\"stntype\",\"value\":\"VIR\"}]}}"
  },
  {
    "objectID": "small_helpers/breaking_whiles.html",
    "href": "small_helpers/breaking_whiles.html",
    "title": "‘Break’-ing ‘while’ loops in debug",
    "section": "",
    "text": "I’m getting unexpected behaviour with break in a while loop. Basically, it isn’t breaking out of the while, or even out of that iteration, but returning control to an earlier point in the same iteration. I think it might have to do with nested if statements?\nOutcome: break itself works fine. The issue is that it does not work the same while debugging. For some reason break in a debug doesn’t break out of the loop, but just reiterates. So the behaviour was weird and unexpected while stepping in debug, but worked fine when the code just ran."
  },
  {
    "objectID": "small_helpers/breaking_whiles.html#testing--this-all-works",
    "href": "small_helpers/breaking_whiles.html#testing--this-all-works",
    "title": "‘Break’-ing ‘while’ loops in debug",
    "section": "Testing- this all works",
    "text": "Testing- this all works\nThis behaves as expected\n\ncount &lt;- 1\nwhile(count &lt; 100) {\n  print(glue::glue(\"Count is {count}\"))\n  if (count == 10) {\n    break()\n  }\n  \n  count &lt;- count + 1\n}\n\nCount is 1\nCount is 2\nCount is 3\nCount is 4\nCount is 5\nCount is 6\nCount is 7\nCount is 8\nCount is 9\nCount is 10\n\n\nThis is the situation I have that’s failing, but it seems to work here.\n\ncount &lt;- 1\nwhile(count &lt; 100) {\n  print(glue::glue(\"Count is {count}\"))\n  if ((count %% 2) == 0) {\n    print(glue::glue(\"{count} is an even number\"))\n    if (count == 10) {\n      break()\n    }\n  }\n  \n  \n  count &lt;- count + 1\n}\n\nCount is 1\nCount is 2\n2 is an even number\nCount is 3\nCount is 4\n4 is an even number\nCount is 5\nCount is 6\n6 is an even number\nCount is 7\nCount is 8\n8 is an even number\nCount is 9\nCount is 10\n10 is an even number\n\n\nJust making the while more complicated doesn’t change it. Still works as expected.\n\ncount &lt;- 1\nwhile(count &lt; 100) {\n  print(glue::glue(\"Count is {count}\"))\n  if ((count %% 2) == 0) {\n    print(glue::glue(\"{count} is an even number\"))\n    \n    if (count == 10) {\n      print('10')\n    }\n    \n    if (count == 10) {\n      print('breaking')\n      break()\n    }\n    \n    \n  }\n  \n  if ((count %% 2) == 1) {\n    print(glue::glue(\"{count} is an odd number\"))\n  }\n  \n  count &lt;- count + 1\n}\n\nCount is 1\n1 is an odd number\nCount is 2\n2 is an even number\nCount is 3\n3 is an odd number\nCount is 4\n4 is an even number\nCount is 5\n5 is an odd number\nCount is 6\n6 is an even number\nCount is 7\n7 is an odd number\nCount is 8\n8 is an even number\nCount is 9\n9 is an odd number\nCount is 10\n10 is an even number\n[1] \"10\"\n[1] \"breaking\"\n\n\nBut if I put those in a function, and step in with a debug, it doesn’t break, it keeps iterating. Just a quirk, I guess that I need to be aware of."
  },
  {
    "objectID": "setup/rstudio_themes.html",
    "href": "setup/rstudio_themes.html",
    "title": "Editing Rstudio themes",
    "section": "",
    "text": "I really don’t like how faint the selection colours are for all of the dark themes. When I do a ‘find’, I don’t want to hunt around for dark grey on black. So to fix that, I need to edit the theme.\n\n\nI went to this massive list of themes, clicked ‘gallery’, chose the one I wanted (just a simple edit of Tomorrow Night, will save a more complex hunt for another day). Then the pane in the middle lets us change all the colours. I clicked the ‘General’ tab, and changed the lineHighlight and selection to a nice blue. I also changed the comment colour to green, not sure if I like that or not.\n\nThen, go to the ‘Info’ tab and change the name before downloading. Otherwise Rstudio won’t find it under a new name.\nDownload. This saves as a .tmTheme file, which I think might just be able to be used directly (see new Posit documentation, but I was looking at something old and so used rstudioapi::convertTheme('setup/Tomorrow Night HL.tmTheme', outputLocation = 'setup') to create a .rstheme file.\nThen global options, add, and select the theme. I had to restart Rstudio a couple times for it to take. The edited theme is available in the git for this website."
  },
  {
    "objectID": "setup/rstudio_themes.html#creating-a-new-theme",
    "href": "setup/rstudio_themes.html#creating-a-new-theme",
    "title": "Editing Rstudio themes",
    "section": "",
    "text": "I went to this massive list of themes, clicked ‘gallery’, chose the one I wanted (just a simple edit of Tomorrow Night, will save a more complex hunt for another day). Then the pane in the middle lets us change all the colours. I clicked the ‘General’ tab, and changed the lineHighlight and selection to a nice blue. I also changed the comment colour to green, not sure if I like that or not.\n\nThen, go to the ‘Info’ tab and change the name before downloading. Otherwise Rstudio won’t find it under a new name.\nDownload. This saves as a .tmTheme file, which I think might just be able to be used directly (see new Posit documentation, but I was looking at something old and so used rstudioapi::convertTheme('setup/Tomorrow Night HL.tmTheme', outputLocation = 'setup') to create a .rstheme file.\nThen global options, add, and select the theme. I had to restart Rstudio a couple times for it to take. The edited theme is available in the git for this website."
  },
  {
    "objectID": "research/management_modeling.html",
    "href": "research/management_modeling.html",
    "title": "Ecological modelling projects",
    "section": "",
    "text": "The majority of my work on ecological modelling addresses water management in the Murray-Darling Basin, with a focus on ecological responses to water delivery. I develop tools and models that capture those responses at ecologically-relevant scales and scale up to the basin."
  },
  {
    "objectID": "research/management_modeling.html#md-werp",
    "href": "research/management_modeling.html#md-werp",
    "title": "Ecological modelling projects",
    "section": "MD-WERP",
    "text": "MD-WERP\n\n\n\n\n\nThe Murray-Darling Water & Environment Research Program is an initiative of the Murray-Darling Basin Authority to improve knowledge and management of the Murray-Darling Basin across a range of outcomes.\nI am a member of the Climate Adaptation Theme, where I lead development of a toolkit to incorporate disparate models of response to environmental condition and synthesize their outcomes into usable information for decisionmaking. Most scenario modelling for water management stops at hydrology; this toolkit targets the subsequent responses to that hydrology across a range of values. The primary target of this toolkit is the assesment of climate scenarios and scenarios representing adaptation to those scenarios, but its capabilities extend to assessment of any hydrologic scenario.\nA beta version of a website documenting the toolkit and its capabilities is now live, with updated documentation and publicly available R package to come shortly."
  },
  {
    "objectID": "research/management_modeling.html#flow-mer",
    "href": "research/management_modeling.html#flow-mer",
    "title": "Ecological modelling projects",
    "section": "Flow-MER",
    "text": "Flow-MER\n\n\n\n\n\nThe Flow-MER project is a large, multi-institution, collaborative project initiated by the Australian Commonwealth Environmental Water Holder designed to better understand how environmental water delivery in the Murray-Darling Basin affects ecological outcomes across the basin.\nI was a member of the Modelling Cross-Cutting Theme, where we developed a framework for ecological modelling from local scales to the scale of the Murray-Darling Basin, designed to assess the impacts of environmental water. Key to this framework was the ability to use data available at the basin scale (often remotely-sensed), while modelling close to the scale at which ecological processes occur. The outcomes of these processes can then be scaled up in time and space. Our framework provides the capacity for this modelling in a consistent way across a range of ecological outcomes, and is designed to flexibly incorporate new models."
  },
  {
    "objectID": "research/management_modeling.html#ewkr",
    "href": "research/management_modeling.html#ewkr",
    "title": "Ecological modelling projects",
    "section": "EWKR",
    "text": "EWKR\n\n\n\n\n\nThe EWKR project was a precursor to Flow-MER, designed to improve the science underpinning water management in the Murray-Darling Basin.\nI was a member of the foodwebs theme, where I developed models of foodweb response to different watering scenarios. These models included producer production as a consequence of environmental watering actions, models of consumer diet sources derived from Bayesian mixing models, and assessment of uncertainty."
  },
  {
    "objectID": "publishing/private_github_figshare.html",
    "href": "publishing/private_github_figshare.html",
    "title": "Private github and figshare",
    "section": "",
    "text": "While open-access from the get-go is nice, we often are in a situation where we are working in private git repos until publication or other forms of release. The catch is that when the code is needed for journal submission, we need to provide it to the reviewers without a full public release until acceptance. How can we do this?\nMy university uses Figshare as the backend to our mandated data repo, so this will address using figshare, though Dryad etc are hopefully similar.\nFigshare has the ability to generate a private embargoed dataset with pre-allocated DOI and private link, which can be made public on acceptance, as outlined in their docs. So one option would just be to do that with a raw dump of the source code. That’s less than ideal- once publication happens, we want to point people to github, not a single snapshot of copied code.\nThe solution is to link figshare with github. The docs suggest it will only link with public repos, but it can see all of them. One potential catch is that my institutional figshare simply does not have the button to connect to github (or gitlab, or bitbucket …). I have no idea why, but the only solution I’ve found is to just create a personal figshare account, where the login works fine.\nThen, select the repo, and create a new figshare item. The docs linked above for making something private until publication would work, but at the initial submission stage, we may not want to actually reserve DOIs for this particular version- it’s likely there will be changes in revision and those are what we will want to actually have linked to the paper with a DOI. It doesn’t look like we can update the data, so I’m assuming that what will end up happening is we will create a new Figshare item with whatever the code is at acceptance (and make a Github ‘release’ for that)."
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html",
    "href": "plotting/setting_colors_for_groups.html",
    "title": "Setting colors for groups",
    "section": "",
    "text": "I often want the same colors to map to the same levels across many plots within a project, even if those levels aren’t included in a plot. E.g. if we plot the iris dataset with and without ‘setosa’, the colors change.\n\nlibrary(ggplot2)\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_brewer(palette = 'Dark2')\n\n\n\n\nvs\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_brewer(palette = 'Dark2')\n\n\n\n\nNow, virginica and versicolor have changed colors, and that’s confusing."
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#assign-colors",
    "href": "plotting/setting_colors_for_groups.html#assign-colors",
    "title": "Setting colors for groups",
    "section": "Assign colors",
    "text": "Assign colors\nNow use setNames to match\n\nsppal &lt;- setNames(iriscols, unique(iris$Species))"
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#use-manual-scale-and-check",
    "href": "plotting/setting_colors_for_groups.html#use-manual-scale-and-check",
    "title": "Setting colors for groups",
    "section": "Use manual scale and check",
    "text": "Use manual scale and check\nand re-do the above two plots to demonstrate that dropping levels keeps colors the same\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_manual(values = sppal)\n\n\n\n\nand drop setosa\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_manual(values = sppal)\n\n\n\n\nNow they match."
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#creating-a-scale_-function",
    "href": "plotting/setting_colors_for_groups.html#creating-a-scale_-function",
    "title": "Setting colors for groups",
    "section": "Creating a scale_ function",
    "text": "Creating a scale_ function\nWe should also probably provide a function to create the standard color-level matching. And maybe that’s the solution to the above. If a palette isn’t passed in, create one in-function.\nFirst, can I make a silly wrapper that just takes the values? It’s no different than scale_color_manual at this point\n\nscale_color_custom &lt;- function(pal) {\n  scale_color_manual(values = pal)\n}\n\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = sppal)\n\n\n\n\nNow, can we catch times when that’s not a named vector and if not pass something else? Do we want to? It starts getting infinite what we want to handle (e.g. palette names? from which packages?)\nI suppose as long as it’s a vector, use it but warn. I think otherwise just fail- it’s too hard to catch everything. Something like\n\nscale_color_custom &lt;- function(pal) {\n  if (!is.character(pal)) {\n    stop(\"pal needs to be a character vector, ideally named\")\n  }\n  if (is.null(names(pal))) {\n    warning(\"unnamed vector, colors may not be consistent between plots.\")\n  }\n  \n  scale_color_manual(values = pal)\n}"
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#palette-generation",
    "href": "plotting/setting_colors_for_groups.html#palette-generation",
    "title": "Setting colors for groups",
    "section": "Palette generation",
    "text": "Palette generation\nHow about a palette generating function?\nThis will be easiest if I enforce a package, though I’m sure it can be made more flexible. For the sake of this quick demo, let’s just use {paletteer}, since it has access to lots of options. And we at least are limited to discrete scales, since we’re level-matching.\nOne thing I often want to do is set specific colors to specific levels (e.g. a reference). So make that possible.\n\nmake_pal &lt;- function(levels, palette, refvals = NULL, refcols = NULL) {\n  if (is.factor(levels)) {levels &lt;- as.character(levels)}\n  nonrefs &lt;- levels[!(levels %in% refvals)]\n  cols &lt;- paletteer::paletteer_d(palette, length(nonrefs))\n  \n  namedcols &lt;- setNames(c(refcols, cols), c(refvals, nonrefs))\n}"
  },
  {
    "objectID": "plotting/setting_colors_for_groups.html#workflow",
    "href": "plotting/setting_colors_for_groups.html#workflow",
    "title": "Setting colors for groups",
    "section": "Workflow",
    "text": "Workflow\nFirst, we set colors with make_pal\n\nircols &lt;- make_pal(unique(iris$Species), palette = 'calecopal::kelp2')\n\nThen we plot with scale_color_custom\nFirst with all three species\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircols)\n\n\n\n\nAnd removing setosa\n\nggplot(dplyr::filter(iris, Species != 'setosa'), aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircols)\n\n\n\n\n\nReference level\nLet’s make setosa a reference level that stands out like purple.\n\nircolsS &lt;- make_pal(unique(iris$Species), palette = 'calecopal::kelp2', refvals = 'setosa', refcols = 'purple')\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS)\n\n\n\n\nNote that now the colors of the others have shifted, because the reference-levelling happened at the start. If a reference is always a reference, that might be fine. But it might be the case that we only sometimes want to call attention to a particular level, and so we want to assign colors including it, so we can switch between an accentuated and unaccentuated palette. Let’s put another argument in make_pal (which requires some reframing of how we assign things). We also might want to return both the unreferenced and the referenced palettes at the same time, rather than calling the function twice.\n\nmake_pal &lt;- function(levels, palette, refvals = NULL, refcols = NULL, includeRef = FALSE, returnUnref = FALSE) {\n  \n  if (returnUnref) {\n    if (!includeRef) {\n      stop(\"does not make sense to return a reffed and unreffed palette that don't match\")\n    }\n    }\n  \n  if (is.factor(levels)) {levels &lt;- as.character(levels)}\n  \n  if (!includeRef) {levels &lt;- levels[!(levels %in% refvals)]}\n  \n  # nonrefs &lt;- levels[!(levels %in% refvals)]\n  cols &lt;- paletteer::paletteer_d(palette, length(levels))\n  \n  if (returnUnref & includeRef) {unref &lt;- setNames(cols, levels)}\n  \n  # delete the reference levels out of the vectors\n  whichlevs &lt;- which(!(levels %in% refvals))\n  nonrefs &lt;- levels[whichlevs]\n  nonrefcols &lt;- cols[whichlevs]\n  \n  namedcols &lt;- setNames(c(refcols, nonrefcols), c(refvals, nonrefs))\n  \n  if (returnUnref & includeRef) {\n    return(list(refcols = namedcols, unrefcols = unref))\n    } else {\n      return(namedcols)\n  }\n\n}\n\nNow test that- setting includeref = TRUE should keep these the same as earlier plots except for setosa (i.e. the light blue that would go to setosa just drops out\n\nircolsS_include &lt;- make_pal(unique(iris$Species), palette = 'calecopal::kelp2', refvals = 'setosa', refcols = 'purple', includeRef = TRUE)\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS_include)\n\n\n\n\nAnd if I want to get both palettes (reffed and unreffed) so I can accentuate sometimes,\n\nircolsS_both &lt;- make_pal(unique(iris$Species), palette = 'calecopal::kelp2', refvals = 'setosa', refcols = 'purple', includeRef = TRUE, returnUnref = TRUE)\n\nWith setosa as a reference- should match above\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS_both$refcols)\n\n\n\n\nWithout setosa as a reference, others retain same colors.\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  scale_color_custom(pal = ircolsS_both$unrefcols)"
  },
  {
    "objectID": "plotting/rayshader.html",
    "href": "plotting/rayshader.html",
    "title": "Rayshading",
    "section": "",
    "text": "I want to figure out how to use rayshader. It has the potential to be really good for heatmaps, maps, etc.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rayshader)\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\nlibrary(stars)\n\nLoading required package: abind\nMostly I just want to play around with both making heatmaps and maps."
  },
  {
    "objectID": "plotting/rayshader.html#plots",
    "href": "plotting/rayshader.html#plots",
    "title": "Rayshading",
    "section": "Plots",
    "text": "Plots\nLet’s first see about using it to make 3d plots. The 2-d autocorr should be a good example. From that notebook, let’s generate some data.\n\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galen_website\n\n\n\nacmatrix_7_9 &lt;- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is 0.022\"\n[1] \"Var of all points is 0.997\"\n[1] \"Mean y AC is 0.891\"\n[1] \"Mean x AC is 0.696\"\n\nactib_7_9 &lt;- tibble::as_tibble(acmatrix_7_9)  |&gt; \n  mutate(y = row_number())  |&gt; \n  pivot_longer(cols = starts_with('V'))  |&gt; \n  mutate(x = as.numeric(str_remove(name, 'V')))  |&gt; \n  select(-name)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\nand the flat ggplot\n\ngg2d &lt;- ggplot(filter(actib_7_9, x &gt; 100 & x &lt;= 200 & y &gt; 300 & y &lt; 400), \n               aes(x = x, y = y, fill = value)) + \n  geom_tile() +\n  viridis::scale_fill_viridis(option = 'viridis')\n\ngg2d\n\n\n\n\n\nRayshade version\nThere are a lot of arguments we can play with, but a simple default set works. Just have to remember to render_snapshot to see it.\n\nplot_gg(gg2d)\nrender_snapshot(clear = TRUE)\n\n\n\n\nA few tweaks, but basically, that’s what we want- the tweaks needed for any particular plot will be plot-specific.\nThe flat_plot_render option is interesing, as it builds the typical heatmap as comparison.\n\nplot_gg(gg2d, width = 10, height = 10, flat_plot_render = TRUE,\n        # These are arguments passed to plot_3d, just playing around\n        solid = FALSE,\n        theta = 60,\n        phi = 30)\nrender_snapshot()\n\n\n\n\nThere’s also a render_highquality too, Though that seems to need some tweaks to look right, both in the notebook or the Rstudio viewer.\n\nplot_gg(gg2d)\nrender_highquality(clear = TRUE)"
  },
  {
    "objectID": "plotting/rayshader.html#maps",
    "href": "plotting/rayshader.html#maps",
    "title": "Rayshading",
    "section": "Maps",
    "text": "Maps\nI found some huge DEMs, but figure I should start smaller, so I chose a small area of the 10m Vic DEM.\nI’d typically use stars, but the examples all use raster. Not sure it’ll work with stars.\n\nvicdem &lt;- read_stars(file.path('data', 'DATA_362071', 'VIC Government', 'DEM', '10 Metre', 'vmelev_dem10m.tif'))\n\nPlot that with plot.stars\n\nplot(vicdem)\n\ndownsample set to 3\n\n\n\n\n\nNow, can I make a map? The examples use raster, but I think we need a matrix. Each sheet in the stars is a matrix, so just get it directly.\n\nvicdem[[1]] %&gt;%\n  sphere_shade(texture = \"imhof2\") %&gt;%\n  plot_map()\n\n\n\n\nMake it 3d- this is interesting. It takes the DEM as the second argument too, because the arguments are hillshade, which is generated by sphere_shade and heightmap, which is just the DEM.\n\nvicdem[[1]] %&gt;%\n  sphere_shade(texture = \"imhof3\") %&gt;%\n  plot_3d(vicdem[[1]], zscale = 10)\n\nrender_snapshot()\n\n\n\n\nCan we use geom_stars to generate a {rayshader} fig from plot_gg?\n\ndemgg &lt;- ggplot() +\n  geom_stars(data = vicdem)\ndemgg\n\n\n\n\nTry the plot_gg. It does have some hillshade- easier to confirm if we move it around in the window that pops up.\n\nplot_gg(demgg, width = 10, height = 10)\nrender_snapshot()\n\n\n\n\nNow I want to overlay stream lines and municipalities on there. Will need to get those shapefiles.\nThe BOM geofabric is in the ANAE, and I have that already. So\n\nstreams &lt;- sf::read_sf(file.path('data', 'ANAE_Rivers_v3_23mar2021', 'ANAE_Rivers_v3_23mar2021', 'Waterways_ANAE_Geofabric3.shp'))\n\nThat makes a nice plot just on its own\n\nstreams |&gt; \n  dplyr::mutate(slope = ifelse(slope &lt; 0, 0, slope)) |&gt; \n  ggplot() +\n  geom_sf(mapping = aes(color = log(slope + 0.01))) +\n  scale_color_viridis_c()\n\n\n\n\nGet towns and roads- Marysville is about the only town in the chosen area of the victorian DEM?\n\ntowns &lt;- sf::read_sf(dsn = file.path('data', 'MDB_ANAE_Aug2017/MDB_ANAE.gdb'), layer = 'MajorTowns')\nroads &lt;- sf::read_sf(dsn = file.path('data', 'MDB_ANAE_Aug2017/MDB_ANAE.gdb'), layer = 'MajorRoads')\n\nClip to the vicdem. This is a bit funny, because the streams_mville dataset from the ANAE is clipped to the Murray-Darling Basin, but the DEM is only partially in the basin, and so we lose some of it.\n\nvicbb &lt;- st_bbox(vicdem) |&gt; st_as_sfc()\n\nstreams_mville &lt;- streams |&gt; \n  st_transform(st_crs(vicdem)) |&gt; \n  st_intersection(vicbb)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\ntowns &lt;- towns |&gt; \n  st_transform(st_crs(vicdem)) |&gt; \n  st_intersection(vicbb)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nroads &lt;- roads |&gt; \n  st_transform(st_crs(vicdem)) |&gt; \n  st_intersection(vicbb)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nQuick gg\n\ndemgg_streams_mville &lt;- ggplot() +\n  geom_stars(data = vicdem) +\n  geom_sf(data = streams_mville, color = 'dodgerblue', linewidth = 2) +\n  geom_sf(data = roads, color = 'black', linewidth = 2) +\n  #geom_sf(data = towns, color = 'firebrick') +\n  colorspace::scale_fill_continuous_sequential(palette = 'Terrain 2')\n#demgg_streams_mville\n\nUse plot_gg\n\nplot_gg(demgg_streams_mville, width = 10, height = 10)\nrender_snapshot()\n\n\n\n\nThe streams don’t really show up well. Do they with a flat map? is it just that they’re too narrow to render well in 3d?\n\ndemgg_streams_mville\n\n\n\n\nCan I make that with plot3d? I can’t get the extent argument to work without passing heightmap as well. Not sure why- they seem to both define the cropping extent.\nCan I get it to work at all? Using almost exactly the example code, just modified for this dataset. Those are really wonky, and it’s not because of an issue with height- the roads were just given a length and width. I have to make the streams have huge linewidth to see them.\n\nvicdem[[1]] |&gt; \n  height_shade() |&gt; \n  add_overlay(generate_line_overlay(streams_mville, color = 'dodgerblue',\n                                    extent = st_bbox(vicdem),\n                                    heightmap = vicdem[[1]],\n                                    linewidth = 10)) |&gt; \n  add_overlay(generate_line_overlay(roads, color = 'black',\n                                     extent = st_bbox(vicdem),\n                                    width = 1080, height = 1080)) |&gt; \n  plot_map()\n\n\n\n\nSo does this not work because the streams_mville just get lost? e.g. are they there, they just don’t show up?\n\nvicdem[[1]] %&gt;%\n  sphere_shade(texture = \"imhof3\") |&gt; \n  # add_overlay(generate_line_overlay(roads, color = 'black',\n  #                                   extent = attr(vicdem[[1]], 'extent'))) |&gt; \n  # add_overlay(generate_point_overlay(towns, color = 'firebrick',\n  #                                   extent = attr(vicdem[[1]], 'extent'))) |&gt; \n    add_overlay(generate_line_overlay(streams_mville, color = 'dodgerblue',\n                                    extent = st_bbox(vicdem),\n                                    heightmap = vicdem[[1]],\n                                    linewidth = 10)) |&gt; \n  plot_3d(vicdem[[1]], zscale = 10)\n\n\nrender_snapshot()\n\n\n\n\nThe streams layer itself isn’t so bad when looked at alone. So somehow it’s sort of disintegrating when used as an overlay, whether with the gg method or not.\n\nggplot(streams_mville) + geom_sf()"
  },
  {
    "objectID": "plotting/hcl_exploration.html",
    "href": "plotting/hcl_exploration.html",
    "title": "hcl exploration",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\nUsing knitr::inline_expr(r message = FALSE, warning = FALSE) hopefully stops it printing all the package messages\nlibrary(tidyverse) # Overkill, but easier than picking and choosing\nlibrary(colorspace)\nFinding colors to use for a given plot can be a pain. I’m trying to find some good color ramps for a project, and also sort out manipulating those colors to allow fading. This is me playing around to try to understand how to do those manipulations and looking at the various potential color palettes.\nColorspace (https://colorspace.r-forge.r-project.org/index.html) is a particularly useful package (though it is not the only color package I use).\nColorspace uses a hue-chroma-luminance specification for colors that is really powerful. It also has built-in palettes. For some other work, I was interested in exploring moving along those dimensions and generating color palettes for plotting.\nPreviously (for the project that gave rise to looking at fading colors), I was using purples and emerald, so let’s start there. But for simplicity switch to greens so constant hue.\nswatchplot(\n  'Purples' = sequential_hcl(8, 'Purples'),\n  'Greens' = sequential_hcl(8, 'Greens'))\nI actually like those single-hue fades a lot for showing more or less of something. But it SHOULD be possible to do a hue shift from green to purple for one axis? will that make sense?"
  },
  {
    "objectID": "plotting/hcl_exploration.html#hue-sequences",
    "href": "plotting/hcl_exploration.html#hue-sequences",
    "title": "hcl exploration",
    "section": "Hue sequences",
    "text": "Hue sequences\nI’d like to be able to specify the endpoints of a hue sequence and just shift along that axis. I’ll try it out with the purple and green above.\nFirst, I want to try to get the hue values (and the L and C as well) to make the endpoints. I can’t find a straightforward extraction in colorspace to get the HCLs though. So, since I know the endpoints are coming from those palettes above, I want their values. Make the palette, turn it into RGB, then turn the RGB into polarLUV to get the three axis values. Here, rows are the 8 fades in the palettes above.\n\nrgbpurps &lt;- hex2RGB(sequential_hcl(8, 'Purples'))\n\nluvpurps &lt;- as(rgbpurps, 'polarLUV')\nluvpurps\n\n            L         C        H\n[1,] 19.88570 55.128356 274.8415\n[2,] 34.37280 69.304529 274.3131\n[3,] 47.99202 56.799744 273.3506\n[4,] 60.90031 43.200021 272.3221\n[5,] 72.77975 31.772302 271.4182\n[6,] 83.46538 21.076091 271.6285\n[7,] 92.78865 10.863733 268.7090\n[8,] 98.79258  2.985742 276.3941\n\n\nThat’s sure roundabout, going palette that’s polarLUV under the hood but returns in hex to rgb and then back to polarLUV. Seems to work though.\n\nswatchplot(hex(luvpurps))\n\n\n\n\n\nrgbgrns &lt;- hex2RGB(sequential_hcl(8, 'Greens'))\n\nluvgrns &lt;- as(rgbgrns, 'polarLUV')\nluvgrns\n\n            L         C        H\n[1,] 25.06952 33.792199 132.8916\n[2,] 40.15678 49.456834 132.0640\n[3,] 54.06676 63.854764 129.4059\n[4,] 66.47833 62.340742 126.5380\n[5,] 77.49000 47.581607 123.8001\n[6,] 86.86700 33.248323 120.6451\n[7,] 93.95644 19.112933 117.4570\n[8,] 98.08100  5.367478 116.7639\n\n\nI can swatchplot them up together.\n\nswatchplot(hex(luvpurps), hex(luvgrns))\n\n\n\n\nNow, the goal is actually to identify those dark colors and transition between them. Now, can I get from purple to green? The L and C are quite different, unfortunately. Pick something the middle?\nHardcode numbers for now, though ideally we’ll get to a function that takes a start and end value.\n\npg &lt;- polarLUV(L = 20, C = 40, H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\nWarning in max(nchar(rnam) - 1L): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\nThat fails. So, now we learned the ranges of the other axes matter. Likely chroma?\n\n# Fails\nmax_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20)\n\n[1] 29.55000 19.90429 16.62571 16.05429 17.74000 23.18000 41.07429 66.11000\n\n\nCan I just use the minimum max_chroma? Not really…\n\n# Guessing I can't just go with 16, but let's try\npg &lt;- polarLUV(L = 20, C = 16, H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\n\n\n\nIf I try to fix how dark that is with chroma, it doesn’t work very well and I still lose one.\n\npg &lt;- polarLUV(L = 20, \n               C = max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20),\n               H = seq(from = 130, to = 275, length.out = 8))\nswatchplot(hex(pg))\n\n\n\n\nUsing a matrix isn’t the answer- same thing, though a floor argument puts the missing color back\n\nhclmat &lt;- cbind(20, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 20, floor = TRUE),\n      seq(from = 130, to = 275, length.out = 8))\n\npg &lt;- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\nGuessing I don’t want to just turn up luminance, but let’s see what that does to get a better sense how this all works.\n\nhclmat &lt;- cbind(80, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 80, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg &lt;- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\nLower luminance does work OK, but it’s still ‘darker’ in the middle and the shift to blue on the right is abrupt. The darker middle is likely why a lot of the colorspace palettes have triangular luminance. I don’t particularly want to get so fine-tuned here. I was looking for a way to programatically define these sequences, and getting into tweaking luminance in a nonlinear and nonmonotonic way could get very bespoke very quickly. Likely better to just use the built-in palettes where someone who understands color theory has already done that.\n\nhclmat &lt;- cbind(50, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 50, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg &lt;- polarLUV(hclmat)\nswatchplot(hex(pg))"
  },
  {
    "objectID": "plotting/hcl_exploration.html#fading",
    "href": "plotting/hcl_exploration.html#fading",
    "title": "hcl exploration",
    "section": "Fading",
    "text": "Fading\nI also want to make faded versions of palettes, and control levels of fade. The particular use I have in mind is to illustrate levels of uncertainty, but it could be any bivariate outcomes.\nI originally thought that I would need to manually adjust the chroma and luminance manually. But the exploration above suggests they interact and so it’s unlikely to just shift one or the other. Still, colorspace provides lighten, darken (which both shift luminance), and desaturate, which shifts chroma. I should be able to play with these to see how they work using either a homebrew base palette as above or the inbuilt ones.\nIn either case, we need the hex values\n\nhexcols &lt;- hex(pg)\n\nLighten (increase luminance)\n\nswatchplot('orig' = hexcols,\n           '25' = lighten(hexcols, amount = 0.25),\n           '50' = lighten(hexcols, amount = 0.5),\n           '75' = lighten(hexcols, amount = 0.75),\n           '100' = lighten(hexcols, amount = 1))\n\n\n\n\nDarken (decrease luminance)\n\nswatchplot('orig' = hexcols,\n           '25' = darken(hexcols, amount = 0.25),\n           '50' = darken(hexcols, amount = 0.5),\n           '75' = darken(hexcols, amount = 0.75),\n           '100' = darken(hexcols, amount = 1))\n\n\n\n\nDesaturate (adjust chroma)\n\nswatchplot('orig' = hexcols,\n           '25' = desaturate(hexcols, amount = 0.25),\n           '50' = desaturate(hexcols, amount = 0.5),\n           '75' = desaturate(hexcols, amount = 0.75),\n           '100' = desaturate(hexcols, amount = 1))\n\n\n\n\nFor my particular use, I like desaturating better, in that it implies less information. But it also makes the values look more similar across the range, and we don’t want that. That gets captured better by lightening.\nAs a bit of an aside, the ends of the lightened versions are effectively ‘Purples’ and ‘Greens’, reading down instead of across. What does it look like if I desaturate those built-in palettes?\n\npurp8 &lt;- sequential_hcl(8, 'Purples')\nswatchplot('orig' = purp8,\n           '25' = desaturate(purp8, amount = 0.25),\n           '50' = desaturate(purp8, amount = 0.5),\n           '75' = desaturate(purp8, amount = 0.75),\n           '100' = desaturate(purp8, amount = 1))\n\n\n\n\nIt does remove color, but it perceptually darkens as well, which is NOT what I want.\nWhat about choosing a pre-built set of colors and lightening/darkening? Start with viridis, we know it has good properties in greyscale, etc.\n\nvir8 &lt;- sequential_hcl(8, 'Viridis')\nswatchplot('orig' = vir8,\n                 '25' = lighten(vir8, amount = 0.25),\n                 '50' = lighten(vir8, amount = 0.5),\n                 '75' = lighten(vir8, amount = 0.75),\n                 '100' = lighten(vir8, amount = 1))\n\n\n\n\nThat actually works pretty well, even though the original had a luminance ramp on it already (https://colorspace.r-forge.r-project.org/articles/approximations.html), this just shifts it each time, I think. We can compare using specplot.\n\nspecplot(vir8, lighten(vir8, amount = 0.75))\n\n\n\n\nWhat does a desaturated viridis look like?\n\nswatchplot('orig' = vir8,\n           '25' = desaturate(vir8, amount = 0.25),\n           '50' = desaturate(vir8, amount = 0.5),\n           '75' = desaturate(vir8, amount = 0.75),\n           '100' = desaturate(vir8, amount = 1))\n\n\n\n\nAgain, makes them more similar, though the underlying luminance ramp helps. I don’t like that the first level still ends up darker though.\n\nInteracting chroma and luminance\nSo, changing luminance makes colors brighter or darker, while adjusting chroma removes color but tends to make them darker. Neither is exactly what I want- a color ramp that look the same, just “faded”. Is the answer to control this interaction? Does a simultaneous lighten and desaturate give me what I want by avoiding the perceptual darkening from the desaturation?\n\nswatchplot('orig' = vir8,\n           '25' = desaturate(vir8, amount = 0.25) %&gt;%\n             lighten(amount = 0.25),\n           '50' = desaturate(vir8, amount = 0.5) %&gt;%\n             lighten(amount = 0.5),\n           '75' = desaturate(vir8, amount = 0.75) %&gt;%\n             lighten(amount = 0.75),\n           '100' = desaturate(vir8, amount = 1) %&gt;%\n             lighten(amount = 1))\n\n\n\n\nThat works really well, actually. Does the order of operations matter? No:\n\nswatchplot('orig' = vir8,\n           '25' = lighten(vir8, amount = 0.25) %&gt;%\n             desaturate(amount = 0.25),\n           '50' = lighten(vir8, amount = 0.5) %&gt;%\n             desaturate(amount = 0.5),\n           '75' = lighten(vir8, amount = 0.75) %&gt;%\n             desaturate(amount = 0.75),\n           '100' = lighten(vir8, amount = 1) %&gt;%\n             desaturate(amount = 1))\n\n\n\n\nDid I just get lucky with viridis, or does it work with other palettes too? how about my ramp that I made from green to purple? Seems to:\n\nswatchplot('orig' = hexcols,\n           '25' = lighten(hexcols, amount = 0.25) %&gt;%\n             desaturate(amount = 0.25),\n           '50' = lighten(hexcols, amount = 0.5) %&gt;%\n             desaturate(amount = 0.5),\n           '75' = lighten(hexcols, amount = 0.75) %&gt;%\n             desaturate(amount = 0.75),\n           '100' = lighten(hexcols, amount = 1) %&gt;%\n             desaturate(amount = 1))\n\n\n\n\nDoes the lighten and desat work for the single-hue scales? Seems like it shouldn’t because they’re already changing along those axes.\n\nswatchplot('orig' = purp8,\n           '25' = lighten(purp8, amount = 0.25) %&gt;%\n             desaturate(amount = 0.25),\n           '50' = lighten(purp8, amount = 0.5) %&gt;%\n             desaturate(amount = 0.5),\n           '75' = lighten(purp8, amount = 0.75) %&gt;%\n             desaturate(amount = 0.75),\n           '100' = lighten(purp8, amount = 1) %&gt;%\n             desaturate(amount = 1))\n\n\n\n\nNot really. It basically does what it should, but the light end is just always light and so doesn’t contain info in the faded dimension and very similar colors appear in both dimensions- values at row n and col m are frequently very similar to row n + 1 and col m - 1.\nI suppose that might be OK for particular situations, but still not ideal. Might work ok though if we limited that lower end? ie don’t let it fall all the way to white in the original? Getting pretty hacky at that point and the diagonals are still too similar.\n\nswatchplot('orig' = purp8[1:6],\n           '25' = lighten(purp8[1:6], amount = 0.25) %&gt;%\n             desaturate(amount = 0.25),\n           '50' = lighten(purp8[1:6], amount = 0.5) %&gt;%\n             desaturate(amount = 0.5),\n           '75' = lighten(purp8[1:6], amount = 0.75) %&gt;%\n             desaturate(amount = 0.75),\n           '100' = lighten(purp8[1:6], amount = 1) %&gt;%\n             desaturate(amount = 1))\n\n\n\n\n\nTesting with other palettes\nViridis and the one I made are both fine, but look at a couple other palettes too. This is not comprehensive, mostly looking at those that have greens and purples for the use I have in mind.\nWrite a little function to do the fade and make this less cut and paste\n\npalcheck &lt;- function(palname, n = 8) {\n pal8 &lt;- sequential_hcl(n, palname)\n \n swatchplot('orig' = pal8,\n            '25' = lighten(pal8, amount = 0.25) %&gt;%\n              desaturate(amount = 0.25),\n            '50' = lighten(pal8, amount = 0.5) %&gt;%\n              desaturate(amount = 0.5),\n            '75' = lighten(pal8, amount = 0.75) %&gt;%\n              desaturate(amount = 0.75),\n            '100' = lighten(pal8, amount = 1) %&gt;%\n              desaturate(amount = 1))\n \n}\n\nPlasma\n\npalcheck('Plasma')\n\n\n\n\nGreen-based\nag_GrnYl is OK, but does get a bit of the diagonal issue\n\npalcheck('ag_GrnYl')\n\n\n\n\nditto Emrld, but might work?\n\npalcheck('Emrld')\n\n\n\n\nTerrains might be OK? 2 is less gaudy\n\npalcheck('Terrain')\n\n\n\npalcheck('Terrain2')\n\n\n\n\nmints and TealGrn fail diagonal test\n\npalcheck('Dark Mint')\n\n\n\npalcheck('Mint')\n\n\n\npalcheck('TealGrn')\n\n\n\n\nYlGn is pretty good, actually.\n\npalcheck('YlGn')\n\n\n\n\nFor the specific use, keep in mind that it will be two levels of fade, and so I can do something like orig and 75% and it’ll be pretty different. But here I’m trying to be fairly general.\nMeh\n\npalcheck('BluGrn')\n\n\n\n\nas expected, batlow and Hawaii are extreme, though might be OK?\n\npalcheck('Batlow')\n\n\n\npalcheck('Hawaii')\n\n\n\n\nPurple-based\nsingle hue doesn’t work\n\npalcheck('Purples')\n\n\n\npalcheck('Purples 3')\n\n\n\n\nthese are all maybes with tricky diagonals\n\npalcheck('Purple-Blu')\n\n\n\npalcheck('Purple-Ora')\n\n\n\npalcheck('Purp')\n\n\n\npalcheck('PurpOr')\n\n\n\npalcheck('Sunset')\n\n\n\npalcheck('Magenta')\n\n\n\npalcheck('SunsetDark')\n\n\n\n\npretty good, but have a fair amount of green in, so could be confusing\n\npalcheck('Purple-Yellow')\n\n\n\npalcheck('Viridis')\n\n\n\npalcheck('Mako')\n\n\n\n\nPlasma pretty good\n\npalcheck('Plasma')\n\n\n\n\nInferno might actually be pretty good if I cut off the first one\n\npalcheck('Inferno')\n\n\n\n\nag_Sunset is better on the diagonals than similar hue sequences\n\npalcheck('ag_Sunset')\n\n\n\n\nGood, but would need to cut the last one; too white. It is less gaudy/ more obviously a hue ramp than ag sunset. Diagonals are tricky too\n\npalcheck('RdPu')\n\n\n\n\nPretty good, but blue could be an issue getting confused with water for this project.\n\npalcheck('BuPu')\n\n\n\n\n\n\n\nContinuous hue from specified palettes\nIf I want to map values to colors continously, that gets tricky using the specified palettes because sequential_hcl takes an n argument.\nCan I get the endpoints and make my own (as I did above with green and purple?)\ndoes the one I’m using use a linear hue scale\n\nspecplot(sequential_hcl(8, 'ag_Sunset'))\n\n\n\n\nIt does, but doesn’t use linear chroma. and it has luminance shift too.\nCan I extract the hue from the ends? The same way I did right at the beginning for the greens and purples.\n\nspecplot(sequential_hcl(2, 'ag_Sunset'))\n\n\n\nrgbsun &lt;- hex2RGB(sequential_hcl(8, 'ag_Sunset'))\n\nluvsun &lt;- as(rgbsun, 'polarLUV')\nluvsun\n\n            L         C          H\n[1,] 25.00933  69.80052 274.922758\n[2,] 33.57582  78.49556 296.995075\n[3,] 42.09671  87.16488 318.944488\n[4,] 50.70304  96.81962 341.141446\n[5,] 59.33484 102.07413   3.730076\n[6,] 67.89723  89.83472  25.626328\n[7,] 76.47041  74.80664  47.677726\n[8,] 84.95182  45.16493  69.593540\n\n\nThis generates the wrong thing (roughly, viridis) because the hue crosses 0\n\nsunmat &lt;- cbind(seq(from = 85, to = 25, length.out = 8), \n                max_chroma(h = seq(from = 69, to = 275, length.out = 8), \n                           l = seq(from = 85, to = 25, length.out = 8), \n                           floor = TRUE),\n                seq(from = 69, to = 275, length.out = 8))\n\npgsun &lt;- polarLUV(sunmat)\nswatchplot(hex(pgsun))\n\n\n\n\nCan I fix the zero-crossing? I’m sure there’s a polar coord package, but for now, add a 360 and take it off\n\nhvec &lt;- seq(from = luvsun@coords[1, 3], to = 360+luvsun@coords[8,3], length.out = 8)\nhvec[hvec &gt; 360] &lt;- hvec[hvec&gt;360]-360\n\nlvec &lt;- seq(from = luvsun@coords[1, 1], to = luvsun@coords[8, 1], length.out = 8)\n\nThe max_chroma is intense, but not sure how else to choose the chromas if we’re trying to build a continuous ramp. Could just use n = 1000 or something to get pseudo-continuous\n\nsunmat &lt;- cbind(lvec, \n                max_chroma(h = hvec, \n                           l = lvec, \n                           floor = TRUE),\n                hvec)\n\npgsun &lt;- polarLUV(sunmat)\nswatchplot(hex(pgsun))\n\n\n\n\nSo, one option is just treating the built-in palettes as their endmembers like that and then doing it as I did before. But it does lose the actual built-in palettes, especially chroma or nonlinearity. Likely better to just use a large n for now and call it good."
  },
  {
    "objectID": "plotting/fonts.html",
    "href": "plotting/fonts.html",
    "title": "Fonts",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\nUsing knitr::inline_expr(r message = FALSE, warning = FALSE) hopefully stops it printing all the package messages\n\nlibrary(tidyverse) # Overkill, but easier than picking and choosing\n\nThese are mostly little plot tweaks and small things that I find and forget all the time.\n\nAccessing fonts\nIn the past, I’ve used extrafonts to use fonts within figures, but it’s failing for me (‘No FontName, skipping’ error as in https://github.com/wch/extrafont/issues/88).\nTry sysfonts. Actually, showtext on top of sysfonts. First, look at how it finds the fonts.\n\nlibrary(showtext)\n\nWarning: package 'showtext' was built under R version 4.2.2\n\n\nLoading required package: sysfonts\n\n\nWarning: package 'sysfonts' was built under R version 4.2.2\n\n\nLoading required package: showtextdb\n\n\nWarning: package 'showtextdb' was built under R version 4.2.2\n\nfontsIhave &lt;- font_files()\nfontsIhave\n\n\n\n  \n\n\nstr(fontsIhave)\n\n'data.frame':   349 obs. of  6 variables:\n $ path   : chr  \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" \"C:/Windows/Fonts\" ...\n $ file   : chr  \"AGENCYB.TTF\" \"AGENCYR.TTF\" \"ALGER.TTF\" \"ANTQUAB.TTF\" ...\n $ family : chr  \"Agency FB\" \"Agency FB\" \"Algerian\" \"Book Antiqua\" ...\n $ face   : chr  \"Bold\" \"Regular\" \"Regular\" \"Bold\" ...\n $ version: chr  \"Version 1.01\" \"Version 1.01\" \"Version 1.57\" \"Version 2.35\" ...\n $ ps_name: chr  \"AgencyFB-Bold\" \"AgencyFB-Reg\" \"Algerian\" \"BookAntiqua-Bold\" ...\n\n\nI should be able to use font_add\nFirst, what fonts are CURRENTLY available in R?\n\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\nfont_families()\n\n[1] \"sans\"         \"serif\"        \"mono\"         \"wqy-microhei\"\n\n\nTest with one of the\n\nfont_add('Bookman Old Style', regular = 'BOOKOS.TTF', \n         italic = 'BOOKOSI.TTF', \n         bold = 'BOOKOSB.TTF', \n         bolditalic = 'BOOKOSBI.TTF')\n\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\nfont_families()\n\n[1] \"sans\"              \"serif\"             \"mono\"             \n[4] \"wqy-microhei\"      \"Bookman Old Style\"\n\n\nI’m not quite understanding how this object is organised. What is that last line? are the $xxxx the defaults?\nTo test, let’s make a plot and try to change font.\nThe help (https://cran.rstudio.com/web/packages/showtext/vignettes/introduction.html) says we need to tell R to use showtext for text.\n\nshowtext_auto()\n\n\nbaseiris &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\nbaseiris + theme(axis.title = element_text(family = 'Bookman Old Style'),\n                 legend.title = element_text(family = 'Bookman Old Style', face = 'bold.italic'))\n\n\n\n\nThat seems to work. Can I feed in all the fonts on my system automaticallly? Is that a bad idea? Might be if it takes a while and we only want one.\nFirst, though, can I give it a font name as a character and it load all of the faces automatically?\nNote: some of the fonts I have have weird faces. For now, just fail on those and stick with the ones supported by showtext. That should be fine.\n\nunique(fontsIhave$face)\n\n [1] \"Bold\"            \"Regular\"         \"Bold Italic\"     \"Italic\"         \n [5] \"Demibold\"        \"Demibold Italic\" \"Demibold Roman\"  \"Bold Oblique\"   \n [9] \"Oblique\"         \"Light\"          \n\n\nThis is a) a useful thing to simplify adding single fonts, and b) precursor to loading them all.\n\n# chosen more or less at random for testing\nfamilyname &lt;- 'Candara'\n\n# I could use dplyr but this seems better to just use base logical indexing.\n# fontsIhave %&gt;%\n#   filter(family == familyname & face == 'Regular') %&gt;%\n#   select(file) %&gt;%\n#   pull()\n\n# Could do all the indexing in the function call to font_add(), but it just gets ridiculous\nregfile &lt;- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Regular'), 'file']\n\nitalfile &lt;- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Italic'), 'file']\n\nboldfile &lt;- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Bold'), 'file']\n\nbifile &lt;- fontsIhave[which(fontsIhave$family == familyname &\n                   fontsIhave$face == 'Bold Italic'), 'file']\n\n\n# NEED TO TEST WHEN THE FACE DOESN'T EXIST AND THROW NULL\n  # If not there, the value will be character(0). testing for that and returning NULL (which is what the function needs) is a bit tricky:\nnoface &lt;- function(x) {ifelse(rlang::is_empty(x), return(NULL), return(x))}\n\nfont_add(familyname, \n         regular = noface(regfile), \n         italic = noface(italfile), \n         bold = noface(boldfile), \n         bolditalic = noface(bifile))\n\nTest that with a figure\n\nbaseiris + theme(axis.title.x = element_text(family = familyname, face = 'italic'),\n                 axis.title.y = element_text(family = familyname, face = 'bold'),\n                 legend.text = element_text(family = familyname),\n                 legend.title = element_text(family = familyname, face = 'bold.italic'))\n\n\n\n\nHow bad an idea is it to just read them ALL in at once?\nEasy enough to feed the font_add above in a loop. Probably vectorizable too, but why bother?\nWrite it as a function, then it will work for all fonts or a subset if that’s a bad idea. Either feed it a dataframe of fonts or just use font_files() directly. It can also take NULL for fontvec, in which case it loads all the fonts.\n\nloadfonts &lt;- function(fontvec = NULL, fontframe = NULL) {\n  \n  # Get all fonts if no fontframe\n  if (is.null(fontframe)) {\n    fontframe &lt;- font_files()\n  }\n  \n  # Load all fonts if no fontvec\n  if (is.null(fontvec)) {\n    fontvec &lt;- unique(fontframe$family)\n  }\n  \n  # Loop over the font families\n  for (i in 1:length(fontvec)) {\n    familyname &lt;- fontvec[i]\n    regfile &lt;- fontframe[which(fontframe$family == familyname &\n                   fontframe$face == 'Regular'), 'file']\n\n    italfile &lt;- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Italic'), 'file']\n    \n    boldfile &lt;- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Bold'), 'file']\n    \n    bifile &lt;- fontframe[which(fontframe$family == familyname &\n                       fontframe$face == 'Bold Italic'), 'file']\n    \n## TODO: THROW A TRYCATCH ON HERE TO BYPASS AND ALERT FOR FAILURES\n    # For example, Berlin Sans FB Demi has no 'regular' and so fails. let's just skip those, this isn't supposed to be the most robust thing ever that handles all cases flawlessly.\n    try(font_add(fontvec[i], \n         regular = noface(regfile), \n         italic = noface(italfile), \n         bold = noface(boldfile), \n         bolditalic = noface(bifile)))\n    \n    # To avoid unforeseen carryover through the loop\n    rm(familyname, regfile, italfile, boldfile, bifile)\n  }\n  \n}\n\nLet’s try just reading everything in. Use try in the function above because there are failures for a few reasons, and instead of dealing with them I just want to quickly read in what’s easy to read in. I don’t have a ton of interest here in figuring out corner cases for weird fonts.\nTest the function with a vector of fontnames first, because can’t do that after try with everything\n\nloadfonts(fontvec = c('Consolas', 'Comic Sans MS', 'Tahoma'))\nfont_families()\n\n[1] \"sans\"              \"serif\"             \"mono\"             \n[4] \"wqy-microhei\"      \"Bookman Old Style\" \"Candara\"          \n[7] \"Consolas\"          \"Comic Sans MS\"     \"Tahoma\"           \n\n\nNow, go for it with everything. There are a million errors, so I’ve turned error reporting off for this code chunk.\n\nsystem.time(loadfonts())\n\nThat was pretty quick. What do I have?\n\nfont_families()\n\n  [1] \"sans\"                            \"serif\"                          \n  [3] \"mono\"                            \"wqy-microhei\"                   \n  [5] \"Bookman Old Style\"               \"Candara\"                        \n  [7] \"Consolas\"                        \"Comic Sans MS\"                  \n  [9] \"Tahoma\"                          \"Agency FB\"                      \n [11] \"Algerian\"                        \"Book Antiqua\"                   \n [13] \"Arial\"                           \"Arial Narrow\"                   \n [15] \"Arial Black\"                     \"Arial Rounded MT Bold\"          \n [17] \"Bahnschrift\"                     \"Baskerville Old Face\"           \n [19] \"Bauhaus 93\"                      \"Bell MT\"                        \n [21] \"Bernard MT Condensed\"            \"Bodoni MT\"                      \n [23] \"Bodoni MT Black\"                 \"Bodoni MT Condensed\"            \n [25] \"Bodoni MT Poster Compressed\"     \"Bradley Hand ITC\"               \n [27] \"Britannic Bold\"                  \"Berlin Sans FB\"                 \n [29] \"Broadway\"                        \"Bookshelf Symbol 7\"             \n [31] \"Calibri\"                         \"Calibri Light\"                  \n [33] \"Californian FB\"                  \"Calisto MT\"                     \n [35] \"Cambria\"                         \"Candara Light\"                  \n [37] \"Cascadia Code\"                   \"Cascadia Mono\"                  \n [39] \"Castellar\"                       \"Century Schoolbook\"             \n [41] \"Centaur\"                         \"Century\"                        \n [43] \"Chiller\"                         \"Colonna MT\"                     \n [45] \"Constantia\"                      \"Cooper Black\"                   \n [47] \"Copperplate Gothic Bold\"         \"Copperplate Gothic Light\"       \n [49] \"Corbel\"                          \"Corbel Light\"                   \n [51] \"Courier New\"                     \"Curlz MT\"                       \n [53] \"Dubai\"                           \"Dubai Light\"                    \n [55] \"Dubai Medium\"                    \"Ebrima\"                         \n [57] \"Elephant\"                        \"Engravers MT\"                   \n [59] \"Eras Bold ITC\"                   \"Eras Demi ITC\"                  \n [61] \"Eras Light ITC\"                  \"Eras Medium ITC\"                \n [63] \"Felix Titling\"                   \"Forte\"                          \n [65] \"Franklin Gothic Book\"            \"Franklin Gothic Demi\"           \n [67] \"Franklin Gothic Demi Cond\"       \"Franklin Gothic Heavy\"          \n [69] \"Franklin Gothic Medium\"          \"Franklin Gothic Medium Cond\"    \n [71] \"Freestyle Script\"                \"French Script MT\"               \n [73] \"Footlight MT Light\"              \"Gabriola\"                       \n [75] \"Gadugi\"                          \"Garamond\"                       \n [77] \"Georgia\"                         \"Gigi\"                           \n [79] \"Gill Sans MT\"                    \"Gill Sans MT Condensed\"         \n [81] \"Gill Sans Ultra Bold Condensed\"  \"Gill Sans Ultra Bold\"           \n [83] \"Gloucester MT Extra Condensed\"   \"Gill Sans MT Ext Condensed Bold\"\n [85] \"Century Gothic\"                  \"Goudy Old Style\"                \n [87] \"Goudy Stout\"                     \"Harrington\"                     \n [89] \"Haettenschweiler\"                \"Microsoft Himalaya\"             \n [91] \"HoloLens MDL2 Assets\"            \"HP Simplified\"                  \n [93] \"HP Simplified Light\"             \"HP Simplified Hans Light\"       \n [95] \"HP Simplified Hans\"              \"HP Simplified Jpan Light\"       \n [97] \"HP Simplified Jpan\"              \"High Tower Text\"                \n [99] \"Impact\"                          \"Imprint MT Shadow\"              \n[101] \"Informal Roman\"                  \"Ink Free\"                       \n[103] \"Blackadder ITC\"                  \"Edwardian Script ITC\"           \n[105] \"Kristen ITC\"                     \"Javanese Text\"                  \n[107] \"Jokerman\"                        \"Juice ITC\"                      \n[109] \"Kunstler Script\"                 \"Lucida Sans Unicode\"            \n[111] \"Wide Latin\"                      \"Lucida Bright\"                  \n[113] \"Leelawadee UI\"                   \"Leelawadee UI Semilight\"        \n[115] \"Lucida Fax\"                      \"Lucida Sans\"                    \n[117] \"Lucida Sans Typewriter\"          \"Lucida Console\"                 \n[119] \"Maiandra GD\"                     \"Malgun Gothic\"                  \n[121] \"Malgun Gothic Semilight\"         \"Marlett\"                        \n[123] \"Matura MT Script Capitals\"       \"Microsoft Sans Serif\"           \n[125] \"MingLiU-ExtB\"                    \"Mistral\"                        \n[127] \"Myanmar Text\"                    \"Modern No. 20\"                  \n[129] \"Mongolian Baiti\"                 \"MS Gothic\"                      \n[131] \"Microsoft JhengHei\"              \"Microsoft JhengHei Light\"       \n[133] \"Microsoft YaHei\"                 \"Microsoft YaHei Light\"          \n[135] \"Microsoft Yi Baiti\"              \"Monotype Corsiva\"               \n[137] \"MT Extra\"                        \"MV Boli\"                        \n[139] \"Niagara Engraved\"                \"Niagara Solid\"                  \n[141] \"Nirmala UI\"                      \"Nirmala UI Semilight\"           \n[143] \"Microsoft New Tai Lue\"           \"OCR A Extended\"                 \n[145] \"Old English Text MT\"             \"Onyx\"                           \n[147] \"MS Outlook\"                      \"Palatino Linotype\"              \n[149] \"Palace Script MT\"                \"Papyrus\"                        \n[151] \"Parchment\"                       \"Perpetua\"                       \n[153] \"Microsoft PhagsPa\"               \"Playbill\"                       \n[155] \"Poor Richard\"                    \"Pristina\"                       \n[157] \"Rage Italic\"                     \"Ravie\"                          \n[159] \"MS Reference Sans Serif\"         \"MS Reference Specialty\"         \n[161] \"Rockwell Condensed\"              \"Rockwell\"                       \n[163] \"Rockwell Extra Bold\"             \"Sans Serif Collection\"          \n[165] \"Script MT Bold\"                  \"Segoe MDL2 Assets\"              \n[167] \"Segoe Fluent Icons\"              \"Segoe Print\"                    \n[169] \"Segoe Script\"                    \"Segoe UI\"                       \n[171] \"Segoe UI Light\"                  \"Segoe UI Semilight\"             \n[173] \"Segoe UI Black\"                  \"Segoe UI Emoji\"                 \n[175] \"Segoe UI Historic\"               \"Segoe UI Semibold\"              \n[177] \"Segoe UI Symbol\"                 \"Segoe UI Variable\"              \n[179] \"Showcard Gothic\"                 \"SimSun\"                         \n[181] \"SimSun-ExtB\"                     \"Sitka Text\"                     \n[183] \"Snap ITC\"                        \"Stencil\"                        \n[185] \"Sylfaen\"                         \"Symbol\"                         \n[187] \"Microsoft Tai Le\"                \"Tw Cen MT\"                      \n[189] \"Tw Cen MT Condensed\"             \"Tw Cen MT Condensed Extra Bold\" \n[191] \"Tempus Sans ITC\"                 \"Times New Roman\"                \n[193] \"Trebuchet MS\"                    \"Verdana\"                        \n[195] \"Viner Hand ITC\"                  \"Vladimir Script\"                \n[197] \"Webdings\"                        \"Wingdings\"                      \n[199] \"Wingdings 2\"                     \"Wingdings 3\"                    \n[201] \"Yu Gothic\"                       \"Yu Gothic Light\"                \n[203] \"Yu Gothic Medium\"                \"ZWAdobeF\"                       \n\n\nI’m sure if there were something that got bypassed that I really needed I could get it directly with font_add(), but this is sure quick to get them all. Test a couple of the new ones.\n\nbaseiris + theme(axis.title.x = element_text(family = 'Poor Richard', face = 'italic'),\n                 axis.title.y = element_text(family = 'Stencil', face = 'bold'),\n                 legend.text = element_text(family = 'Papyrus'),\n                 legend.title = element_text(family = 'Onyx', face = 'bold.italic'))\n\n\n\n\nI have also put loadfonts in the functions folder so I can use it elsewhere."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html",
    "href": "pix4d/read_pix4d_outputs.html",
    "title": "Pix4d outputs",
    "section": "",
    "text": "Trying to read in pix4d outputs- specifically point clouds, orthophotos, dsm_xyz, and dtm.\nI’m going to assume I need stars and sf.\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\nlibrary(stars)\n\nLoading required package: abind\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rayshader)\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galen_website\nTypically the projdir would have a better name (likely similar/same as the project name)\nprojname &lt;- '4m_upandback'\nprojdir &lt;- file.path('data', 'pix4dout')"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#raster-dsm",
    "href": "pix4d/read_pix4d_outputs.html#raster-dsm",
    "title": "Pix4d outputs",
    "section": "Raster DSM",
    "text": "Raster DSM\nThe settings for this seem to be in the DSM and orthomosaic tab and resolution matches the ortho, as far as I can tell.\n\ndsm_rpath &lt;- file.path(projdir, projname, '3_dsm_ortho', '1_dsm')\n\nread it in?\n\ndsm_r &lt;- read_stars(file.path(dsm_rpath, paste0(projname, '_dsm.tif')))\n\n\ndsm_r\n\nstars object with 2 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                           Min.   1st Qu.    Median      Mean   3rd Qu.\n4m_upandback_dsm.tif  -24.73554 -24.70843 -24.68753 -24.68809 -24.67387\n                           Max.  NA's\n4m_upandback_dsm.tif  -24.62663 96962\ndimension(s):\n  from   to  offset    delta                refsys point x/y\nx    1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny    1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\n\n\nReally not super clear why useRaster = TRUE isn’t working for these, but whatever.\n\nplot(dsm_r, useRaster = FALSE)\n\ndownsample set to 1\n\n\n\n\n\nThose are funny z-values. What is the reference- clearly not elevation.\nHow many pixels? 1.0983136^{7}. 11 million is a lot."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#grid-dsm",
    "href": "pix4d/read_pix4d_outputs.html#grid-dsm",
    "title": "Pix4d outputs",
    "section": "Grid DSM",
    "text": "Grid DSM\nThis is an xyz point file.The settings are in the Additional Outputs tab, and we can set the grid spacing there. This was done with a spacing of 100cm. That implies these are centers, and this is a much coarser grid than in the dsm raster, which is at GSD scale.\n\ndsm_xyzpath &lt;- file.path(projdir, projname, '3_dsm_ortho', '1_dsm')\n\n\ndsm_xyz &lt;- readr::read_csv(file.path(dsm_xyzpath, paste0(projname, '_dsm_1cm.xyz')),\n                                 col_names = c('x', 'y', 'z', 'r', 'g', 'b'))\n\nRows: 1490037 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): x, y, z, r, g, b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nassume the last 3 cols are rgb, use my function\n\ndsm_xyz &lt;- dsm_xyz %&gt;% \n  mutate(hexcol = rgb2hex(r,g,b))\n\nIgnore spatial for a minute, can I just slam it in ggplot? using z as fill first. If we want the rgb, it’ll either be an orthomosiac or we’ll need to do something 3d with color overlay\n\ndsm_xyzgg &lt;- ggplot(dsm_xyz, aes(x = x, y = y, fill = z)) + geom_raster() + coord_equal()\ndsm_xyzgg\n\n\n\n\nThat was really weirdly fast.\nCan I use the rgb to see the orthomosiac?\n\ndsm_xyzorthogg &lt;- ggplot(dsm_xyz, aes(x = x, y = y, fill = hexcol)) + geom_raster() + coord_equal() + scale_fill_identity()\ndsm_xyzorthogg\n\n\n\n\nWould be cool to do a 3d map with color, maybe {rayshader}? Would really be nice to be able to rotate it etc.\nThat has color and height- might be good to feed to a ML algorithm to ID rocks using both sets of info and their relationships to each other.\nThis is supposedly a 100cm grid, but it has 1.5 million pixels, which is a lot less than the dsm raster at GSD resolution, but is nowhere near the difference I’d expect. It’s approximately 10x less, but GSD is &lt; 1cm. And even at 1cm, I’d expect 10,000x fewer pixels. And those pictures are clearly not 1m pixels. What IS the spacing? Iterates fastest on x\n\ndsm_xyz[2, 'x'] - dsm_xyz[1, 'x']\n\n     x\n1 0.01\n\n# dsm_xyz[2, 'y'] - dsm_xyz[1, 'y']\n\nSo, what is 0.01? That’s a cm, isn’t it?\nAssuming the same crs as the raster dsm, the LENGTHUNIT is 1 meter. So, 0.01 would be 1cm, and that makes more sense.\n\nst_crs(dsm_r)\n\nCoordinate Reference System:\n  User input: WGS 84 / UTM zone 55S \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 55S\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 55S\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",147,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",10000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 144°E and 150°E, southern hemisphere between 80°S and equator, onshore and offshore. Australia. Papua New Guinea.\"],\n        BBOX[-80,144,0,150]],\n    ID[\"EPSG\",32755]]\n\n\nIt’s not exactly a translation from the raster, because the raster has all the NA off the edges, and a step of\n\ndiff(st_get_dimension_values(dsm_r, 'x')[1:2])\n\n[1] 0.00556\n\n\nSo, the raster should have about 4x, but also includes the NAs.\n\nsum(is.na(dsm_r[[1]]))\n\n[1] 5896746\n\n\nThat’s not entirely it. Still about 266k more pixels in the rnn than expected once we drop the NA and adjust by pixel size.\n\n#non-na in raster\nrnn  &lt;- (2416*4546)-sum(is.na(dsm_r[[1]]))\n#pixel difference- how many dsm_r pixels per dsm_xyz pixels?\npixdif &lt;- (0.01/diff(st_get_dimension_values(dsm_r, 'x')[1:2]))^2\n\nexpected_rnn &lt;- nrow(dsm_xyz)*pixdif\n\nexpected_rnn-rnn\n\n[1] -266385.2\n\n\n\nMake geographic\n\nsf\nCan I make it geographic (sf)? Assume the same crs as the dsm_r\n\ndsm_xyzsf &lt;- st_as_sf(dsm_xyz, coords = c('x', 'y'), crs = st_crs(dsm_r))\n\nuse the sf plot method? bad idea. takes forever. The points (next) are points, but it might make more sense to make these stars anyway- it is gridded.\n\nplot(dsm_xyzsf[,'z'])\n\n\n\n\n\n\nstars\nTry stars- first, confirm it is gridded- what’s the step?\nNeed to do a bit of cleanup- the x’s have negatives when they turnover y’s, and the y’s have a ton of zeros because they iterate slower.\n\nrawx &lt;- diff(pull(dsm_xyz[, 'x']))\ncleanx &lt;- rawx[rawx &gt; 0]\n# rounding error\ndifclean &lt;- abs(cleanx-cleanx[1])\n\n\n# huh- some are still out. by a lot. It seems systematic and strange\nall(difclean &lt; 1e-8)\n\n[1] FALSE\n\nwhich(difclean &gt; 1e-8)\n\n  [1]  191679  192281  192883  343775  344443  345109  375364  378772  378773\n [10]  380145  381514  382200  443113  443822  483744  484458  485170  485881\n [19]  486590  487296  527316  528041  530943  764429  765245  766058  766870\n [28]  767680  768486  773330  774145  774961  775777  822605  823429  824254\n [37]  825076  825896  877470  878310  879147  879982  884988  999784 1000608\n [46] 1001431 1002252 1002256 1003070 1003881 1004687 1070957 1071726 1136369\n [55] 1137116 1137859 1143776 1144521 1145270 1146022 1146781 1162799 1163554\n [64] 1205080 1205808 1206534 1207256 1207976 1212992 1333223 1384324 1385469\n [73] 1389993 1390552 1429954 1430434 1457519 1457948 1458373 1458795 1459216\n [82] 1459636 1460053 1460467 1460877 1461283 1461685 1462082 1462475 1462863\n [91] 1463247 1463626 1464003 1464376 1464744 1465107 1465466 1465821 1466172\n[100] 1466520 1466861 1467199 1467534 1467865 1468192 1468513 1468831 1469145\n[109] 1469455 1469760 1470062 1470360 1470655 1470946 1471233 1471516 1471796\n[118] 1472073 1472346 1472616 1472881 1473144 1473402 1473656 1473905 1474150\n[127] 1474392 1474629 1474862 1475092 1475316 1475537 1475754 1475966 1476174\n\ncleanx[which(difclean &gt; 1e-8)]\n\n  [1] 0.06 0.04 0.03 0.02 0.05 0.09 0.04 0.02 0.02 0.02 0.02 0.02 0.02 0.04 0.02\n [16] 0.06 0.08 0.10 0.13 0.15 0.03 0.04 0.04 0.02 0.04 0.07 0.08 0.11 0.14 0.03\n [31] 0.05 0.07 0.09 0.02 0.04 0.05 0.08 0.11 0.02 0.03 0.07 0.10 0.02 0.02 0.03\n [46] 0.05 0.06 0.03 0.14 0.15 0.20 0.03 0.06 0.02 0.03 0.06 0.29 0.25 0.22 0.17\n [61] 0.10 0.03 0.05 0.03 0.04 0.07 0.09 0.10 0.02 0.03 0.02 0.02 0.03 0.05 0.02\n [76] 0.04 0.02 0.05 0.08 0.10 0.10 0.11 0.11 0.14 0.17 0.21 0.24 0.27 0.30 0.33\n [91] 0.38 0.39 0.42 0.45 0.49 0.53 0.55 0.58 0.61 0.65 0.69 0.70 0.73 0.76 0.80\n[106] 0.84 0.86 0.90 0.93 0.97 1.00 1.01 1.05 1.09 1.12 1.15 1.17 1.21 1.25 1.28\n[121] 1.31 1.32 1.37 1.40 1.43 1.46 1.48 1.52 1.55 1.59 1.62 1.64 1.68 1.71 1.74\n\n\nIf I plot that does it become clear? Because I took diffs and cut out the big steps, I can’t just attach it. Will need to modify in place.\n\ndsm_xyz &lt;- dsm_xyz |&gt; \n  mutate(xdifs = x - lag(x, 1),\n         ydifs = y - lag(y, 1)) |&gt; \n  mutate(xdifs = ifelse(ydifs == 0, xdifs, NA),\n         ydifs = ifelse(ydifs == 0, NA, ydifs))\n\nThe steps are where there are weird things happening along angled edges.\n\ndsm_xyz_x &lt;- ggplot(dsm_xyz, aes(x = x, y = y, fill = xdifs)) + geom_raster() + coord_equal() + scale_fill_gradient(low = 'white', high = 'red')\ndsm_xyz_x\n\n\n\n\nCan we see it better with size?\n\ndsm_xyz_xp &lt;- ggplot(dsm_xyz, aes(x = x, y = y, color = xdifs, size = xdifs)) + geom_point() + coord_equal() + scale_color_gradient(low = 'white', high = 'red')\ndsm_xyz_xp\n\nWarning: Removed 2370 rows containing missing values (`geom_point()`).\n\n\n\n\n\nYeah, it’s also centered on corners.\nCheck y in the same way\n\ndsm_xyz_yp &lt;- ggplot(dsm_xyz, aes(x = x, y = y, color = ydifs, size = ydifs)) + geom_point() + coord_equal() + scale_color_gradient(low = 'white', high = 'red')\ndsm_xyz_yp\n\nWarning: Removed 1487668 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe ydifs are all 0.01, and we can see the rows stepover on the left edge.\nI think it’s safe to ignore the few discontinuities.\nNow, actually make it stars\n\ndsm_xyzstars &lt;- st_as_stars(dsm_xyz, crs = st_crs(dtm))\n\n\nplot(dsm_xyzstars['z'])\n\n\n\n\n\nplot(dsm_xyzstars['hexcol'])\n\n\n\n\nggplot version- orthomosaic\n\ndsm_xyzstarsgg &lt;- ggplot() + \n  geom_stars(data = dsm_xyzstars, aes(fill = hexcol)) +\n  scale_fill_identity() +\n  theme(legend.position = 'none') + \n  coord_equal()\ndsm_xyzstarsgg\n\n\n\n\nggplot version- elevation\n\ndsm_xyzstarsgg_z &lt;- ggplot() + \n  geom_stars(data = dsm_xyzstars, aes(fill = z)) +\n  colorspace::scale_fill_continuous_sequential(palette = 'Terrain 2') +\n  theme(legend.position = 'none') + \n  coord_equal()\ndsm_xyzstarsgg_z\n\n\n\n\nIs it faster to just use geom_raster? Would need to have a non-spatial df as we did above. This just errors when I try it.\n\ndsm_xyzstarsgg_z_raster &lt;- ggplot() + \n  geom_raster(data = dsm_xyzstars, aes(fill = z)) +\n  colorspace::scale_fill_continuous_sequential(palette = 'Terrain 2') +\n  theme(legend.position = 'none') + \n  coord_equal()\ndsm_xyzstarsgg_z_raster\n\nWhat about geom_sf on the point version? I plotted it above, but haven’t tried ggplot. This is more specifically what the point cloud is, below. Change everything to color from fill.\n\ndsm_xyzsfgg_z &lt;- ggplot() + \n  geom_sf(data = dsm_xyzsf, aes(color = z)) +\n  colorspace::scale_color_continuous_sequential(palette = 'Terrain 2') +\n  theme(legend.position = 'none') + \n  coord_sf()\ndsm_xyzsfgg_z"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#check-pixels-dsm-ortho",
    "href": "pix4d/read_pix4d_outputs.html#check-pixels-dsm-ortho",
    "title": "Pix4d outputs",
    "section": "Check pixels dsm & ortho",
    "text": "Check pixels dsm & ortho\n\nst_dimensions(dsm_r)\n\n  from   to  offset    delta                refsys point x/y\nx    1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny    1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\n\nst_dimensions(ortho)\n\n     from   to  offset    delta                refsys point x/y\nx       1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny       1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\nband    1    4      NA       NA                    NA    NA    \n\n\nConfirm they match\n\nall(st_get_dimension_values(dsm_r, 'x') == st_get_dimension_values(ortho, 'x'))\n\n[1] TRUE\n\nall(st_get_dimension_values(dsm_r, 'y') == st_get_dimension_values(ortho, 'y'))\n\n[1] TRUE\n\n\nCan we combine those into one stars? the catch is that the rgb vals aren’t on the same scale as the z. For the ortho, we have a pixel value for each of 4 bands (r,g,b, alpha). But z isn’t a band. So how would we do it? Make the bands attributes, probably. Or just call z a band?\nTo make the bands attributes and add z, this works. Will need to better define what’s needed later before deciding whether to then merge this back or what. And maybe we should be using hex (e.g. merging colmos and dsm_r).\n\northosplit &lt;- split(ortho, 'band') |&gt; \n  setNames(c('r', 'g','b', 'alpha'))\n\ndsm_r &lt;- setNames(dsm_r, 'z')\n\northodsm &lt;- c(orthosplit, dsm_r)\n\nMerging into a dim- takes a while. and it’s unclear what to call this dimension or its values- ‘band’ isn’t right, and the values aren’t 0-255 for the z. I think don’t do this unless we’re really sure we want to.\n\northodsm_dim &lt;- orthodsm |&gt; \n  merge()\n\nDoes it actually make more sense to add a z dimension? Maybe? we can’t just merge the dsm_r, because then it has no attributes. Can we control the merge above? Maybe, but it’ll be a bit tricky. ignore until we have a clearer definition of what we need. The few obvious things I’ve tried have failed.\nWhatever we do about the combination, the pixels match for the DSM raster and the orthophoto"
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#check-pixels-dtm-ortho",
    "href": "pix4d/read_pix4d_outputs.html#check-pixels-dtm-ortho",
    "title": "Pix4d outputs",
    "section": "Check pixels dtm & ortho",
    "text": "Check pixels dtm & ortho\nThese are unlikely to match-the dtm’s settings are done elsewhere and unless we were really particular to set them the same as the ortho, (which would require downgrading the ortho), they won’t match. AND, it’s unnecessary, since there’s a raster dsm that does match.\nLooking at the from-to, they clearly don’t match, and looking at dimensions confirms it\n\nst_dimensions(ortho)\n\n     from   to  offset    delta                refsys point x/y\nx       1 2416  263755  0.00556 WGS 84 / UTM zone 55S FALSE [x]\ny       1 4546 5774223 -0.00556 WGS 84 / UTM zone 55S FALSE [y]\nband    1    4      NA       NA                    NA    NA    \n\nst_dimensions(dtm)\n\n  from   to  offset    delta                refsys point x/y\nx    1 1932  263755  0.00695 WGS 84 / UTM zone 55S FALSE [x]\ny    1 3636 5774223 -0.00695 WGS 84 / UTM zone 55S FALSE [y]\n\n\nBut, the DTM is some sort of smoothed thing, and is set in the Additional Outputs tab. We could make the ortho matched, but it’d be contrived (and pointless, given the existence of the raster dsm."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#convert-dsm_xyz-to-image-with-z",
    "href": "pix4d/read_pix4d_outputs.html#convert-dsm_xyz-to-image-with-z",
    "title": "Pix4d outputs",
    "section": "Convert dsm_xyz to image with z?",
    "text": "Convert dsm_xyz to image with z?\nAgain, I don’t really see the point of this turning the dsm_xyz into an image (raster) is already done in the raster dsm + ortho. We can merge if we want. Just not sure exactly how we’d want to present that."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#can-i-get-rayshader-to-work",
    "href": "pix4d/read_pix4d_outputs.html#can-i-get-rayshader-to-work",
    "title": "Pix4d outputs",
    "section": "Can I get rayshader to work?",
    "text": "Can I get rayshader to work?\nWould be cool to do height with z and color with hexcol to actually map the stream in 3d with photo overlay. Pix4d does it, but would be nice to do here too.\n\nplot_gg(dsm_xyzstarsgg, ggobj_height = dsm_xyzstarsgg_z, scale = 50)\nrender_snapshot()\n\n\n\n\nGetting it to work directly isn’t happening for some reason, but gg is."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#why-does-the-point-cloud-not-include-the-water",
    "href": "pix4d/read_pix4d_outputs.html#why-does-the-point-cloud-not-include-the-water",
    "title": "Pix4d outputs",
    "section": "Why does the point cloud not include the water?",
    "text": "Why does the point cloud not include the water?\nIt can’t do the photo point matching on the moving surface. So the images are there but it can’t get parallax and so no z."
  },
  {
    "objectID": "pix4d/read_pix4d_outputs.html#to-do-make-a-package.",
    "href": "pix4d/read_pix4d_outputs.html#to-do-make-a-package.",
    "title": "Pix4d outputs",
    "section": "To do: make a package.",
    "text": "To do: make a package.\nHave funs for xyz and funs for tifs. But call those within specific funs for each output (e.g. dsm_xyz and point cloud should have their own funs, and the dsm_xyz should allow returning a raster in addition to points, but pc shouldn’t). Similar for a standard set of plot returns"
  },
  {
    "objectID": "parallelism/plans_and_hpc.html",
    "href": "parallelism/plans_and_hpc.html",
    "title": "Testing plans and systems",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tibble)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nregisterDoFuture()\nI want to test how tasks get split up in different plans (e.g. multisession, multicore, cluster, future.batchtools, future.callr). In a local case on Windows, I think it’s fairly clear we want plan(multisession) and on unix/mac, plan(multicore). But when we have access to something bigger, like an HPC, I want to know how the plans work where there are multiple nodes, each with several cores. Can we access more than one node? How? What’s the difference between cluster, batchtools and callr?\nI’ve done some haphazard poking at this, and at that time, if we just had a slurm script that requested multiple nodes, but then called a single R script, I couldn’t use workers on more than one node. My workaround was to use array jobs to do the node-parallelisation, and then {future} for the cores. But this splits the parallelisation into some being handled by shell scripts and slurm, and some handled by R and futures. And that makes it harder to control and load-balance. And in fact, I had an extra level, where I had shell scripts that started a set of SLURM array jobs, which then split into cores. This approach worked, but it’s very hardcodey, even when we auto-generate the batch and SLURM scripts. It tends to not be well-balanced without a lot of manual intervention every time anything changes. And it’s not portable- we have to restructure the code to run locally vs on the HPC.\nWhat would be nice is to auto-break-up the set of work into evenly-sized chunks, and then fire off the SLURM commands to start an appropriate number of nodes with some number of cores. And have that also work locally, where it would just parallelise over those things. Can I figure that out?\nI’m not sure how I’m going to test this in a quarto doc, since I need to run things on the HPC and return to stdout. Might have to copy-paste and treat this like onenote or something. But I should be able to set up the desired structure here, anyway.\nI think I’ll start similarly to some of my local parallel testing, where I just returned process IDs to see what resources are being used.\nAnd I think I’ll start by requesting resources with sbatch and slurm scripts that call Rscript and see what that gets me and whether we can access all the resources, and then move on to trying to get R to generate the SLURM calls, likely with future.batchtools."
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#setting-up-tests",
    "href": "parallelism/plans_and_hpc.html#setting-up-tests",
    "title": "Testing plans and systems",
    "section": "Setting up tests",
    "text": "Setting up tests\nI want to be able to see if I’m getting nodes and cores\nAs a first pass, I don’t particularly care about speed- deal with that after I figure out how it’s actually working."
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#structure",
    "href": "parallelism/plans_and_hpc.html#structure",
    "title": "Testing plans and systems",
    "section": "Structure",
    "text": "Structure\nI’ll write a slurm script to start an sbatch job that requests &gt; 1 node and all cores on those nodes. That will call a test R script that calls a plan and runs some foreach loops that return PIDs, as I did previously. These loops need to iterate over at least nodes X cores so we can see what gets used.\nThe easiest thing to do will be to just use print statements to print to stdout, I think, though I guess I could save rds files or something.\nCan I automate? Or can I call all the plans one after each other in the same script? Probably yes to both. What’s the best way to get the code over to the HPC? I usually use git/github to sync changes, but I don’t really want to git this whole website over there. It’s a hassle, but I might set up a template slurm testing repo and use that.\n\nThe R code\nBasically, I want to call plan(PLAN_NAME), and then run a loop, checking PID as before. That loop will be like the simple nested loop with %:% I built before at least to start. I might do more complex nesting and try plan(list(PLAN1, PLAN2) later. Why am I looping at all? In case there’s some built-in capacity to throw one set of loops on nodes and the other on cores. This is more likely to come in later, but might as well set it up now.\nThat nested function is\n\nnest_test &lt;- function(outer_size, inner_size, planname) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %:% \n    foreach(j = 1:inner_size,\n                       .combine = bind_rows) %dopar% {\n    \n                         thisproc &lt;- tibble(plan = planname,\n                                            outer_iteration = i,\n                                            inner_iteration = j, \n                                            pid = Sys.getpid())\n                       }\n  \n  return(outer_out)\n}\n\nAnd I also want to know availableWorkers() and availableCores(). Also try availableCores(methods = 'Slurm') to see how that differs and which matches the resources we actually use.\nCan I make the stdout auto-generate something in markdown I can copy-paste in here? That’d be nice.\nI think as a first pass, I’ll try this for the single-machine plans- sequential, multicore, and multiprocess. I have a feeling I’ll encounter more difficulty with the cluster, callr, and batchtools, so get the basics figured out first.\nSo, what should that R script look like? I think I’ll use a for over the plans.\nThe test HPC I’ll use has 20 nodes with 12 cores. I’ll request 2 nodes and 24 cores for testing so I can see if it uses multiple nodes. That means I’ll need at least 24 loops, and ideally more. Might as well go 50- this won’t actually take any time.\nI can run this locally too, so I guess do that?\n\nplannames &lt;- c('sequential', 'multisession', 'multicore')\n\n# The loopings\nnest_test &lt;- function(outer_size, inner_size, planname) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %:% \n    foreach(j = 1:inner_size,\n            .combine = bind_rows) %dopar% {\n              \n              thisproc &lt;- tibble(plan = planname,\n                                 outer_iteration = i,\n                                 inner_iteration = j, \n                                 pid = Sys.getpid())\n            }\n  \n  return(outer_out)\n}\n\n\nfor (planname in plannames) {\n  print(paste0(\"# \", planname))\n  plan(planname)\n  \n  print('## available Workers:')\n  print(availableWorkers())\n  \n  print('## available Cores:')\n  print(\"### non-slurm\")\n  print(availableCores())\n  print(\"### slurm method\")\n  print(availableCores(methods = 'Slurm'))\n  \n  # base R process id\n  print('## Main PID:')\n  print(Sys.getpid())\n  \n  looptib &lt;- nest_test(25, 25, planname)\n  \n  print('## Unique processes')\n  print(length(unique(looptib$pid)))\n  print(\"This should be the IDs of all cores used\")\n  print(unique(looptib$pid))\n  \n  print('## Full loop data')\n  print(looptib)\n  \n  \n}\n\n[1] \"# sequential\"\n[1] \"## available Workers:\"\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n[1] \"## available Cores:\"\n[1] \"### non-slurm\"\nsystem \n    20 \n[1] \"### slurm method\"\ncurrent \n      1 \n[1] \"## Main PID:\"\n[1] 24368\n[1] \"## Unique processes\"\n[1] 1\n[1] \"This should be the IDs of all cores used\"\n[1] 24368\n[1] \"## Full loop data\"\n# A tibble: 625 × 4\n   plan       outer_iteration inner_iteration   pid\n   &lt;chr&gt;                &lt;int&gt;           &lt;int&gt; &lt;int&gt;\n 1 sequential               1               1 24368\n 2 sequential               1               2 24368\n 3 sequential               1               3 24368\n 4 sequential               1               4 24368\n 5 sequential               1               5 24368\n 6 sequential               1               6 24368\n 7 sequential               1               7 24368\n 8 sequential               1               8 24368\n 9 sequential               1               9 24368\n10 sequential               1              10 24368\n# … with 615 more rows\n[1] \"# multisession\"\n[1] \"## available Workers:\"\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n[1] \"## available Cores:\"\n[1] \"### non-slurm\"\nsystem \n    20 \n[1] \"### slurm method\"\ncurrent \n      1 \n[1] \"## Main PID:\"\n[1] 24368\n[1] \"## Unique processes\"\n[1] 20\n[1] \"This should be the IDs of all cores used\"\n [1] 26432 22348 26260 24924 21440 25436 11104  4544 18052 12440 18076 22024\n[13] 25728 20768  7728 12632  6628 25368 23212 24668\n[1] \"## Full loop data\"\n# A tibble: 625 × 4\n   plan         outer_iteration inner_iteration   pid\n   &lt;chr&gt;                  &lt;int&gt;           &lt;int&gt; &lt;int&gt;\n 1 multisession               1               1 26432\n 2 multisession               1               2 26432\n 3 multisession               1               3 26432\n 4 multisession               1               4 26432\n 5 multisession               1               5 26432\n 6 multisession               1               6 26432\n 7 multisession               1               7 26432\n 8 multisession               1               8 26432\n 9 multisession               1               9 26432\n10 multisession               1              10 26432\n# … with 615 more rows\n[1] \"# multicore\"\n[1] \"## available Workers:\"\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n[1] \"## available Cores:\"\n[1] \"### non-slurm\"\nsystem \n    20 \n[1] \"### slurm method\"\ncurrent \n      1 \n[1] \"## Main PID:\"\n[1] 24368\n[1] \"## Unique processes\"\n[1] 1\n[1] \"This should be the IDs of all cores used\"\n[1] 24368\n[1] \"## Full loop data\"\n# A tibble: 625 × 4\n   plan      outer_iteration inner_iteration   pid\n   &lt;chr&gt;               &lt;int&gt;           &lt;int&gt; &lt;int&gt;\n 1 multicore               1               1 24368\n 2 multicore               1               2 24368\n 3 multicore               1               3 24368\n 4 multicore               1               4 24368\n 5 multicore               1               5 24368\n 6 multicore               1               6 24368\n 7 multicore               1               7 24368\n 8 multicore               1               8 24368\n 9 multicore               1               9 24368\n10 multicore               1              10 24368\n# … with 615 more rows\n\n\n\n\nThe slurm script\nwe need a batch script that calls the R script, requests resources (here, 2 nodes with 12 cores each so we can see node utilisation- or not).\n#!/bin/bash\n\n# # Resources on test system: 20 nodes, each with 12 cores. 70GB RAM\n\n#SBATCH --time=0:05:00 # request time (walltime, not compute time)\n#SBATCH --mem=8GB # request memory. 8 should be more than enough to test\n#SBATCH --nodes=2 # number of nodes. Need &gt; 1 to test utilisation\n#SBATCH --ntasks-per-node=12 # Cores per node\n\n#SBATCH -o node_core_%A_%a.out # Standard output\n#SBATCH -e node_core_%A_%a.err # Standard error\n\n# timing\nbegin=`date +%s`\n\nmodule load R\n\nRscript testing_plans.R\n\n\nend=`date +%s`\nelapsed=`expr $end - $begin`\n\necho Time taken for code: $elapsed\nThat’s then called with\nsbatch filename.sh\nFrom within the directory."
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#sequential",
    "href": "parallelism/plans_and_hpc.html#sequential",
    "title": "Testing plans and systems",
    "section": "sequential",
    "text": "sequential\n\navailable workers:\ngandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04\n\n\ntotal workers:\n24\n\n\nunique workers:\ngandalf-vm03 gandalf-vm04\n\n\navailable Cores:\n\nnon-slurm\n12\n\n\nslurm method\n12\n\n\n\nMain PID:\n1224603\n\n\nUnique processes\n1\nIDs of all cores used\n1224603"
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#multisession",
    "href": "parallelism/plans_and_hpc.html#multisession",
    "title": "Testing plans and systems",
    "section": "multisession",
    "text": "multisession\nℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2) ℹ Using R 4.0.3 (lockfile was generated with R 4.2.2)\n\navailable workers:\ngandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04\n\n\ntotal workers:\n24\n\n\nunique workers:\ngandalf-vm03 gandalf-vm04\n\n\navailable Cores:\n\nnon-slurm\n12\n\n\nslurm method\n12\n\n\n\nMain PID:\n1224603\n\n\nUnique processes\n12\nIDs of all cores used\n1224723 1224717 1224715 1224716 1224718 1224725 1224724 1224721 1224720 1224722 1224719 1224726"
  },
  {
    "objectID": "parallelism/plans_and_hpc.html#multicore",
    "href": "parallelism/plans_and_hpc.html#multicore",
    "title": "Testing plans and systems",
    "section": "multicore",
    "text": "multicore\n\navailable workers:\ngandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm03 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04 gandalf-vm04\n\n\ntotal workers:\n24\n\n\nunique workers:\ngandalf-vm03 gandalf-vm04\n\n\navailable Cores:\n\nnon-slurm\n12\n\n\nslurm method\n12\n\n\n\nMain PID:\n1224603\n\n\nUnique processes\n12\nIDs of all cores used\n1225561 1225564 1225567 1225570 1225573 1225576 1225581 1225586 1225591 1225596 1225601 1225606\nTime taken for code: 15 :::\nSo, in all cases, the workers were seen across all nodes ({parallely} says as much in the help- this uses scontrol to find workers), while the cores are local. And only 12 cores ever end up getting used here, even with multisession and multicore."
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html",
    "href": "parallelism/nested_parallel_hpc.html",
    "title": "Nested futures with batchtools",
    "section": "",
    "text": "Sometimes it makes sense to just send all parallel jobs to single cpus each. But there are other situations, like when we have some high-level parallelization over parameters, followed by additional parallelization over something else, where we can take advantage of the structure of HPC clusters to send biggish jobs to a node, and then further parallelize within it onto CPUS. Which is faster is going to be highly context-dependent, I think, but let’s figure out how and then set up some scripts that can be modified for speed testing.\nI’ll use the parallel inner and outer functions developed in testing nested paralellism, but modified as in most of the slurm_r_tests repo to just return PIDs and other diagnostics so we can see what resources get used.\nThe approach here is learning a lot from this github issue."
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html#creating-a-nested-plan",
    "href": "parallelism/nested_parallel_hpc.html#creating-a-nested-plan",
    "title": "Nested futures with batchtools",
    "section": "Creating a nested plan",
    "text": "Creating a nested plan\n\nUnnested template\nIf I was using a plan like this to assign jobs to CPUS\n\nplan(tweak(batchtools_slurm,\n           template = \"batchtools.slurm.tmpl\",\n           resources = list(time = 5,\n                            ntasks.per.node = 1, \n                            mem = 1000,\n                            job.name = 'NewName')))\n\n\n\nNested with list()\nI now need to do two things- wrap it in a list with a second plan type, and ask for more tasks per node. (It should also work to do this with a focus on tasks and cpus per tasks), but that should work similarly.\nSo, we can wrap that in a list\n\nplan(list(tweak(batchtools_slurm,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 5,\n                                 ntasks.per.node = 12, \n                                 mem = 1000,\n                                 job.name = 'NewName')),\n          multicore))"
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html#nested-functions-to-return-resource-use",
    "href": "parallelism/nested_parallel_hpc.html#nested-functions-to-return-resource-use",
    "title": "Nested futures with batchtools",
    "section": "Nested functions to return resource use",
    "text": "Nested functions to return resource use\nAnd set up nested functions that return the outer and inner PIDs, as well as some other stuff:\n\ninner_par &lt;- function(inner_size, outer_it, outer_pid) {\n  inner_out &lt;- foreach(j = 1:inner_size,\n                       .combine = bind_rows) %dorng% {\n                         thisproc &lt;- tibble(all_job_nodes = paste(Sys.getenv(\"SLURM_JOB_NODELIST\"),\n                                                                  collapse = \",\"),\n                                            node = Sys.getenv(\"SLURMD_NODENAME\"),\n                                            loop = \"inner\",\n                                            outer_iteration = outer_it,\n                                            outer_pid = outer_pid,\n                                            inner_iteration = j, \n                                            inner_pid = Sys.getpid(),\n                                            taskid = Sys.getenv(\"SLURM_LOCALID\"),\n                                            cpus_avail = Sys.getenv(\"SLURM_JOB_CPUS_PER_NODE\"))\n                         \n                       }\n  return(inner_out)\n}\n\n# The outer loop calls the inner one\nouter_par &lt;- function(outer_size, inner_size) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %dorng% {\n                         \n                         # do some stupid work so this isn't trivially nested\n                         a &lt;- 1\n                         b &lt;- 1\n                         d &lt;- a+b\n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- inner_par(inner_size = inner_size,\n                                                outer_it = i, outer_pid = Sys.getpid())\n                         \n                         inner_out\n                       }\n  \n  return(outer_out)\n}\n\nAnd it WORKS!--\nI did this in nested_plan.R in the slurm_r_tests repo, and running it for 25 loops in each of the nested functions shows that we’re getting and using 12 inner PIDS per outer PID\n## Nodes and pids simple\n# A tibble: 25 × 2\n   outer_pid n_inner\n       &lt;int&gt;   &lt;int&gt;\n 1    549832      12\n 2    549960      12\n 3    550097      12\n 4    550238      12\n 5   1507878      12\n 6   1508006      12\n 7   1508130      12\n 8   1508254      12\n 9   1508380      12\n10   1508502      12\n11   1508624      12\n12   3494670      12\n13   3494799      12\n14   3494921      12\n15   3495044      12\n16   3495168      12\n17   3495292      12\n18   3495415      12\n19   4189058      12\n20   4189200      12\n21   4189329      12\n22   4189455      12\n23   4189580      12\n24   4189702      12\n25   4189829      12\nFor more detail, run the code in the slurm_r_tests repo. But it shows that we get 25 nodes and 12 cores each (because of ntasks.per.node = 12 in resources), each of which gets used 2-3 times, which makes sense for a 25 outer x 25 inner looping."
  },
  {
    "objectID": "parallelism/nested_parallel_hpc.html#controlling-chunks-and-workers",
    "href": "parallelism/nested_parallel_hpc.html#controlling-chunks-and-workers",
    "title": "Nested futures with batchtools",
    "section": "Controlling chunks and workers",
    "text": "Controlling chunks and workers\nI’m not going to bother with trying to figure out array jobs (sounds like not easy or supported, and the batchtools framework basically works by spitting out sbatch calls instead of arrays. That’s not ideal, but it’s how it works at least for now. .\nIt probably is worth addressing chunking. One reason is just to not overload the cluster- if I ask for a million batch futures, that’s not going to make anyone happy, and it’s not efficient anyway. It looks like we can set this with a workers argument (which has default 100). That means however many futures we ask for, they get chunked into workers chunks, and that many jobs get submitted. So, we also might want to modify this if we have a job that naturally wants to be nested, and we know we want a node per outer loop, then we could say workers = OUTER_LOOP_LENGTH.\nAs a test, we can modify the plan above to have workers = 5 , which should only grab 5 nodes, and send 5 of the 25 outer loops to each of them. I’m going to up the time.\n\nplan(list(tweak(batchtools_slurm,\n                workers = 5,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 15,\n                                 ntasks.per.node = 12, \n                                 mem = 1000,\n                                 job.name = 'NewName')),\n          multicore))\n\nAnd that does what it’s supposed to- keep everything on 5 nodes\n## Nodes and pids simple\n# A tibble: 5 × 2\n  outer_pid n_inner\n      &lt;int&gt;   &lt;int&gt;\n1    552072      60\n2   1510060      60\n3   3496848      60\n4   4191978      60\n5   4192353      60"
  },
  {
    "objectID": "parallelism/nested_dependent_iterations.html",
    "href": "parallelism/nested_dependent_iterations.html",
    "title": "Nested dependencies with %:%",
    "section": "",
    "text": "I did a lot of nesting checking, but here I’m specifically interested in dependencies in the iterations in the nesting of foreach loops using %:%."
  },
  {
    "objectID": "parallelism/nested_dependent_iterations.html#packages-and-setup",
    "href": "parallelism/nested_dependent_iterations.html#packages-and-setup",
    "title": "Nested dependencies with %:%",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nlibrary(foreach)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nregisterDoFuture()\nplan(multisession)"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html",
    "href": "parallelism/interrogating_parallel.html",
    "title": "Interrogating parallel functions",
    "section": "",
    "text": "In testing parallel functions, especially on new machines, or if we’re trying to get a granular understanding of what they’re doing, we will want to do more than benchmark. We might, for example, want to know what cores and processes they’r using to make sure they’re taking full advantage of resources, or to better understand how resources get divided up, or to better understand the differences between plans.\nI’ve done some basic speed testing of parallel functions as well as some testing of nested functions Here, I’ll build on that to better understand how the workers get divided up in those nested functions, and use that to jump off into testing different plans."
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#packages-and-setup",
    "href": "parallelism/interrogating_parallel.html#packages-and-setup",
    "title": "Interrogating parallel functions",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}, because I tend to like writing for loops (there’s a reason I’ll try to write up sometime later). I test other packages in the {future} family (furrr, future_apply) where I try to better understand when they do and don’t give speed advantages.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nlibrary(foreach)\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tibble)\nlibrary(patchwork)\n\nregisterDoFuture()\nplan(multisession)"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#workers-and-cores",
    "href": "parallelism/interrogating_parallel.html#workers-and-cores",
    "title": "Interrogating parallel functions",
    "section": "Workers and cores",
    "text": "Workers and cores\nWe get assigned workers and cores when we call plan . We can also get the outer process id\nnote on HPC, we need to use availableCores(methods = 'Slurm') .\n\navailableWorkers()\n\n [1] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n [7] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[13] \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\" \"localhost\"\n[19] \"localhost\" \"localhost\"\n\navailableCores()\n\nsystem \n    20 \n\nSys.getpid()\n\n[1] 21416"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#the-setup--nested-functions",
    "href": "parallelism/interrogating_parallel.html#the-setup--nested-functions",
    "title": "Interrogating parallel functions",
    "section": "The setup- nested functions",
    "text": "The setup- nested functions\nI’ll use the inner and outer parallel functions similar to my tests of nested functions, but instead of doing anything, they’ll just track the processes.\nWhat do I want to interrogate? The process id, for one- I can use Sys.getpid(). I think I might just skip all the actual processing and just get the IDs\nWhat do I want to check? How the processes get divvied up. Are the inner loops getting different processes? Are the outer, and then the inner all use that one? Does it change through the outer loop? Does it depend on the number of iterations?\n\ninner_par &lt;- function(outer_it, size) {\n  inner_out &lt;- foreach(j = 1:size,\n                       .combine = bind_rows) %dorng% {\n    \n                         thisproc &lt;- tibble(loop = \"inner\",\n                                            outer_iteration = outer_it,\n                                            inner_iteration = j, \n                                            pid = Sys.getpid())\n    # d &lt;- rnorm(size, mean = j)\n    # \n    # f &lt;- matrix(rnorm(size*size), nrow = size)\n    # \n    # g &lt;- d %*% f\n    # \n    # mean(g)\n    \n  }\n}\n\nFor the outer loop, let’s check the PID both before and after the inner loop runs.\n\nouter_par &lt;- function(outer_size, innerfun, inner_size) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %dorng% {\n                         \n                        outerpre &lt;- tibble(loop = 'outer_pre',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(outer_it = i, size = inner_size)\n                         \n                         outerpost &lt;- tibble(loop = 'outer_post',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         bind_rows(outerpre, inner_out, outerpost)\n                         \n                         \n                       }\n  \n  return(outer_out)\n}"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#sequential-outer-loop",
    "href": "parallelism/interrogating_parallel.html#sequential-outer-loop",
    "title": "Interrogating parallel functions",
    "section": "sequential outer loop",
    "text": "sequential outer loop\nI assume if we make the outer loop sequential, the inner will then get PIDs\n\nouter_seq &lt;- function(outer_size, innerfun, inner_size) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %do% {\n                         \n                        outerpre &lt;- tibble(loop = 'outer_pre',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(outer_it = i, size = inner_size)\n                         \n                         outerpost &lt;- tibble(loop = 'outer_post',\n                                           outer_iteration = i,\n                                           inner_iteration= NA,\n                                           pid = Sys.getpid())\n                         \n                         bind_rows(outerpre, inner_out, outerpost)\n                         \n                         \n                       }\n  \n  return(outer_out)\n}\n\n\ntestseq10 &lt;- outer_seq(outer_size = 10, innerfun = inner_par, inner_size = 10)\n\n\nggplot(testseq10, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0)) +\n  scale_x_continuous(breaks = 0:10) +\n  scale_color_binned(breaks = 0:10)\n\n\n\n\nAt least that- if we have a sequential outer, it lets the inner parallelise."
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#nested-with",
    "href": "parallelism/interrogating_parallel.html#nested-with",
    "title": "Interrogating parallel functions",
    "section": "Nested with %:%",
    "text": "Nested with %:%\nThe ‘proper’ way to nest foreach loops is with %:%. That’s not always possible, but I think we can here, to check what they’re getting.\n\nouter_nest &lt;- function(outer_size, innerfun, inner_size) {\n  outer_out &lt;- foreach(i = 1:outer_size,\n                       .combine = bind_rows) %:% \n    foreach(j = 1:inner_size,\n                       .combine = bind_rows) %dopar% {\n    \n                         thisproc &lt;- tibble(loop = \"\",\n                                            outer_iteration = i,\n                                            inner_iteration = j, \n                                            pid = Sys.getpid())\n                       }\n  \n  return(outer_out)\n}\n\nnote- inner_par isn’t doing anything here, since it’s not a function anymore\n\ntestnest &lt;- outer_nest(10, inner_par, 10)\n\n\nggplot(testnest, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0)) +\n  scale_x_continuous(breaks = 0:10) +\n  scale_color_binned(breaks = 0:10)\n\n\n\n\nNow, each outer loop is getting two PIDs to split up with its inner loop.\nSo that makes sense- this way foreach knows what’s coming and can split up workers. It looks like the outer loop is favored, though that shouldn’t matter when they’re specified this way.\nWe can check though\n\ntestnest50_10 &lt;- outer_nest(50, inner_par, 10)\ntestnest10_50 &lt;- outer_nest(10, inner_par, 50)\n\n\nplotnest50_10 &lt;- ggplot(testnest50_10, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplotnest10_50 &lt;- ggplot(testnest10_50, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplotnest50_10 + plotnest10_50\n\n\n\n\nThose look more similar than they are because of different axes. I think that is giving more PIDs to the outer when it has more iterations."
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#list-plans",
    "href": "parallelism/interrogating_parallel.html#list-plans",
    "title": "Interrogating parallel functions",
    "section": "List-plans",
    "text": "List-plans\n\nNaive- just defaults\nThere is information out there, largely related to using future.batchtools, that a list of plans lets us handle nested futures. Does that work with multisession?\nplan(\"list\") tells us what the plan is. This is super helpful for checking what’s going on.\n\nplan(list(multisession, multisession))\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(list(multisession, multisession))\n2. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(list(multisession, multisession))\n\n\nThen let’s use the same outer_par we tried earlier\nLet’s check an outer loop with 1, and inner with 10, and vice versa\n\ntest1_10_double &lt;- outer_par(outer_size = 1, \n                             innerfun = inner_par, inner_size = 10)\ntest10_1_double &lt;- outer_par(outer_size = 10, \n                             innerfun = inner_par, inner_size = 1)\n\n\nplot1_10_double &lt;- ggplot(test1_10_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot10_1_double &lt;- ggplot(test10_1_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot1_10_double + plot10_1_double\n\n\n\n\nThat didn’t work. BUT, plan(\"list\") shows that the first one uses workers = availableCores(). Does that eat all the cores?\n\n\nTweak outer plan\nIf we limit the outer plan, do the ‘leftover’ cores go to the inner?\n\nplan(list(tweak(multisession, workers = 2), multisession))\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = 2, envir = parent.frame())\n   - tweaked: TRUE\n   - call: plan(list(tweak(multisession, workers = 2), multisession))\n2. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(list(tweak(multisession, workers = 2), multisession))\n\n\nLet’s check an outer loop with 1, and inner with 10, and vice versa\n\ntest1_10_double &lt;- outer_par(outer_size = 1, \n                             innerfun = inner_par, inner_size = 10)\ntest10_1_double &lt;- outer_par(outer_size = 10, \n                             innerfun = inner_par, inner_size = 1)\n\n\nplot1_10_double &lt;- ggplot(test1_10_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot10_1_double &lt;- ggplot(test10_1_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot1_10_double + plot10_1_double\n\n\n\n\nThat restricted the outer, but didn’t give any to the inner. Can I get them there?\n\n\nTweak both plans\nNow, we’re explicitly telling each plan how many workers it gets.\n\nplan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 5)))\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = 2, envir = parent.frame())\n   - tweaked: TRUE\n   - call: plan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 5)))\n2. multisession:\n   - args: function (..., workers = 5, envir = parent.frame())\n   - tweaked: TRUE\n   - call: plan(list(tweak(multisession, workers = 2), tweak(multisession, workers = 5)))\n\n\nLet’s check an outer loop with 1, and inner with 10, and vice versa\n\ntest1_10_double &lt;- outer_par(outer_size = 1, \n                             innerfun = inner_par, inner_size = 10)\ntest10_1_double &lt;- outer_par(outer_size = 10, \n                             innerfun = inner_par, inner_size = 1)\n\n\nplot1_10_double &lt;- ggplot(test1_10_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot10_1_double &lt;- ggplot(test10_1_double, aes(x = outer_iteration, \n                   y = as.factor(pid), \n                   color = inner_iteration)) + \n  geom_point(position = position_jitter(width = 0.2, height = 0))\n\nplot1_10_double + plot10_1_double\n\n\n\n\nThat worked! So, if we explicitly give each plan workers, we can manage nestedness.\nTurn the plan back to multisession\n\nplan(multisession)\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(multisession)"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#inner-loop",
    "href": "parallelism/interrogating_parallel.html#inner-loop",
    "title": "Interrogating parallel functions",
    "section": "Inner loop",
    "text": "Inner loop\n\nParallel version\n\ninner_par &lt;- function(in_vec, size, outer_it) {\n  inner_out &lt;- foreach(j = in_vec,\n                       .combine = bind_rows) %dorng% {\n    d &lt;- rnorm(size, mean = j)\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    h &lt;- mean(g)\n    \n    thisproc &lt;- tibble(loop = \"inner\",\n                       outer_iteration = outer_it,\n                       inner_iteration = j, \n                       pid = Sys.getpid())\n    \n                       }\n  return(inner_out)\n}\n\n\n\nSequential version\n\ninner_seq &lt;- function(in_vec, size, outer_it) {\n  inner_out &lt;- foreach(j = in_vec,\n                       .combine = bind_rows) %do% {\n    d &lt;- rnorm(size, mean = j)\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    h &lt;- mean(g)\n    \n    thisproc &lt;- tibble(loop = \"inner\",\n                       outer_iteration = outer_it,\n                       inner_iteration = j, \n                       pid = Sys.getpid())\n    \n                       }\n  \n  return(inner_out)\n}\n\n\n\nUsing preallocated for\nThis is likely to be faster than the sequential. Preallocate both the vector and the new tibble output.\n\ninner_for &lt;- function(in_vec, size, outer_it) {\n  inner_out &lt;- vector(mode = 'numeric', length = size)\n  \n  thisproc &lt;- tibble(loop = \"inner\",\n                       outer_iteration = outer_it,\n                       inner_iteration = 1:length(in_vec), \n                       pid = Sys.getpid())\n  \n  for(j in 1:length(in_vec)) {\n    d &lt;- rnorm(size, mean = in_vec[j])\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    inner_out[j] &lt;- mean(g)\n    \n    thisproc$pid[j] &lt;- Sys.getpid()\n    thisproc$inner_iteration[j] &lt;- j\n    \n  }\n  \n  return(thisproc)\n}"
  },
  {
    "objectID": "parallelism/interrogating_parallel.html#outer-loop",
    "href": "parallelism/interrogating_parallel.html#outer-loop",
    "title": "Interrogating parallel functions",
    "section": "Outer loop",
    "text": "Outer loop\nI cant divide by inner_out now that it’s not a matrix, so just get a cv.\n\nparallel\n\nouter_par &lt;- function(size, innerfun) {\n  outer_out &lt;- foreach(i = 1:size,\n                       .combine = bind_rows) %dorng% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a &lt;- rnorm(size, mean = i)\n                         \n                         b &lt;- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec &lt;- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(in_vec = cvec, \n                                               size = size, \n                                               outer_it = i)\n                         \n                         h &lt;- sd(cvec)/mean(cvec)\n                         \n                         inner_out\n                         \n                       }\n  \n  return(outer_out)\n}\n\n\n\nsequential\n\nouter_seq &lt;- function(size, innerfun) {\n  outer_out &lt;- foreach(i = 1:size,\n                       .combine = bind_rows) %do% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a &lt;- rnorm(size, mean = i)\n                         \n                         b &lt;- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec &lt;- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(in_vec = cvec, \n                                               size = size, outer_it = i)\n                         \n                         h &lt;- sd(cvec)/mean(cvec)\n                         \n                         inner_out\n                       }\n  \n  return(outer_out)\n}\n\n\n\nUn-preallocated for\nBecause this would need to replace chnks in the tibble, it’s hard to preallocate. Just don’t bother- the point isn’t speed, it’s testing pids.\n\nouter_for &lt;- function(size, innerfun) {\n  outer_out &lt;- matrix(nrow = size, ncol = size)\n  \n  thisproc &lt;- tibble(loop = \"inner\",\n                       outer_iteration = 1,\n                       inner_iteration = 1, \n                       pid = Sys.getpid(),\n                     .rows = 0)\n  \n  for(i in 1:size) {\n    \n    # Do a matrix mult on a vector specified with i\n    a &lt;- rnorm(size, mean = i)\n    \n    b &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    cvec &lt;- a %*% b\n    \n    # Now iterate over the values in c to do somethign else\n    inner_out &lt;- innerfun(in_vec = cvec, size = size, outer_it = i)\n    \n    outer_out[, i] &lt;- sd(cvec)/mean(cvec)\n    \n    thisproc &lt;- bind_rows(thisproc, inner_out)\n    \n    \n  }\n  outer_out &lt;- c(outer_out) \n  \n  \n  return(thisproc)\n}"
  },
  {
    "objectID": "parallelism/hpc_ephemera.html",
    "href": "parallelism/hpc_ephemera.html",
    "title": "HPC ephemera",
    "section": "",
    "text": "To see characteristics of the system (nodes, CPUs, memory, etc) and state:\nsinfo --Node --long"
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#slurm-commands",
    "href": "parallelism/hpc_ephemera.html#slurm-commands",
    "title": "HPC ephemera",
    "section": "",
    "text": "To see characteristics of the system (nodes, CPUs, memory, etc) and state:\nsinfo --Node --long"
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#common-workflow",
    "href": "parallelism/hpc_ephemera.html#common-workflow",
    "title": "HPC ephemera",
    "section": "Common workflow:",
    "text": "Common workflow:\ngit clone repo, cd into it\nset up environment with renv - usually just module load R then R for an interactive session\nwhen things break because the R version is too old, it sometimes works to force it to update using {rig}\ndo most dev on local computer, push\ngit pull to HPC- if on branch\ngit pull origin BRANCHNAME\nrun code with sbatch\nsomething breaks\nfix locally, push, pull, re-run\nuse scripts to copy data down"
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#transferring-files",
    "href": "parallelism/hpc_ephemera.html#transferring-files",
    "title": "HPC ephemera",
    "section": "Transferring files",
    "text": "Transferring files\n\nThe best solution- WinSCP\nUnless we want to batch-transfer a lot of stuff automatically, USE WINSCP- it’s way easier, and we can open docs with notepad++, etc.\n\n\nScripting\nI have better ones, but simply, if we are on a local terminal in a directory we want to put the file (or maybe we want it in a subdir)\nscp user@remote.address:~/path/to/file/filename.txt /subdir/filename.txt\nThat’s annoying because we need to start local, and so have an scp terminal running alongside the one that’s sshed. Otherwise we have to treat local as remote from inside the ssh session."
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#running-any-file",
    "href": "parallelism/hpc_ephemera.html#running-any-file",
    "title": "HPC ephemera",
    "section": "Running any file",
    "text": "Running any file\nThe how-tos for using SLURM all have a line that is Rscript filename.R to run that file. But that means we have to have a different shell script for each R script we want to run. THat’s really annoying, especially when prototyping or with a lot of similar R scripts. Instead, we want to build a shell script that takes the R script name as input in the sbatch call. E.g.\nsbatch shellname.sh\nWith the R script hardcoded in sh.\nInstead, we want\nsbatch shellname.sh rname.R\nthat can take an arbitrary R script.\nThis then will get to other arguments, but this is the first step and super useful.\nIt’s relatively easy- just use Rscript $1 in the shell script, and then the command above works."
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#arguments-in-sbatch",
    "href": "parallelism/hpc_ephemera.html#arguments-in-sbatch",
    "title": "HPC ephemera",
    "section": "Arguments in sbatch",
    "text": "Arguments in sbatch\nSometimes we want to pass arguments to R scripts in the sbatch call so we don’t need fully-new Rscripts just to change a parameter value. In that case, we can pass the args, and they’re available in R with commandArgs(). There are a couple things to be aware of to use this. Primarily, they boil down to The shell SLURM script must match the argument extraction in the R script- ie they need to know the argument order and meaning.\n\nSlots for them MUST be available in the slurm script. e.g.\nRscript $1 $2\nwill work to pass sbatch any_R.sh rscriptname.R argument1 in, with rscriptname.R being $1(the first argument to the slurm script) and argument1 being $2. (The first additional argument, intended for R). If you send sbatch any_R.sh rscriptname.R argument1 argument2 in to a slurm script as above, argument2 will just disappear into the ether.\nAlternatively, the slurm script itself can define arguments-\nRscript $1 \"argument1\" \"argument2\"\nmakes whatever $1 is coming in from command line, as well as \"argument1\" and \"argument2\" available in R via commandArgs()\nWe can use arbitrary numbers of arguments as well with $* , which accesses all the arguments.\nRscript $*\nNote that now we’ve dropped the $1- it’s included as the first item in the $* . This is a bit dangerous- we need to make sure we use the right order, but it is flexible.\nAccessing the arguments is tricky- they are numbered in order, but there are some initial invisible ones. It seems, but may not always be true, that the first four are set, with the fourth being --file=filename.R for the file called by Rscript, then 5 is --args and the subsequent args are 6-n.\nNumbers come through as characters\n${SLURM_ARRAY_TASK_ID} is a particularly useful variable to include, as it lets us manually divide tasks among a slurm array."
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#naming-jobs",
    "href": "parallelism/hpc_ephemera.html#naming-jobs",
    "title": "HPC ephemera",
    "section": "Naming jobs",
    "text": "Naming jobs\nAnd especially putting the name on the stdout and err. Creating a new dir for them would be even nicer, but I’ll leave that for later.\nthe produced stdout and err get hard to find after a lot of jobs. If we use %x in their names in the slurm script, it appends the jobname\nThe jobname by default is the name of the shell script, e.g. sbatch shellscript.sh has jobname “shellscript.sh”. That’s actually pretty useful if we have individual shell scripts. But if we’re using a script that takes R script names as arguments, it’s not. In that case, we can set the job with --job-name or -J flags, e.g. sbatch -J test_job shellscript.sh.\nNote that the jobname flag has to happen before the script, and does NOT affect the args returned by commandArgs() (thankfully)."
  },
  {
    "objectID": "parallelism/hpc_ephemera.html#testing-future-plans",
    "href": "parallelism/hpc_ephemera.html#testing-future-plans",
    "title": "HPC ephemera",
    "section": "Testing future plans",
    "text": "Testing future plans\nplan(\"list\") tells us what the plan is. This is super helpful for checking what’s going on.\n\nlibrary(future)\nplan(multisession)\nplan(\"list\")\n\nList of future strategies:\n1. multisession:\n   - args: function (..., workers = availableCores(), lazy = FALSE, rscript_libs = .libPaths(), envir = parent.frame())\n   - tweaked: FALSE\n   - call: plan(multisession)"
  },
  {
    "objectID": "parallelism/conditional_plans.html",
    "href": "parallelism/conditional_plans.html",
    "title": "Conditional futures",
    "section": "",
    "text": "library(future)\n\nIt’s likely that we want to write code that just runs either locally or on an HPC without having to change a bunch of things inside the relevant scripts. This is useful for local prototyping, as well as just sometimes needing to run things locally.\nThe portability of {futures} is a major benefit for this reason- futures should all work the same, no matter what plan is used to resolve them. So, we can write code that runs both locally and on the HPC by changing the plan. And we can automate this process by making the plan conditional.\nDifferent HPCs define themselves differently, but in the simple case where the code only runs on Linux on an HPC, something like\n\nif (grepl('^Windows', Sys.info()[\"sysname\"])) {\n  inpath &lt;- file.path('path', 'to', 'local', 'inputs')\n  outpath &lt;- file.path('path', 'to', 'local', 'outputs')\n  plan(multisession)\n}\n\nif (grepl('^Linux', Sys.info()[\"sysname\"])) {\n  inpath &lt;- file.path('path', 'to', 'HPC', 'inputs')\n  outpath &lt;- file.path('path', 'to', 'HPC', 'outputs')\n  plan(list(tweak(batchtools_slurm,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 5,\n                                 ntasks.per.node = 12, \n                                 mem = 1000)),\n          multicore))\n}\n\nObviously if you run on Linux that’s NOT an HPC, that second if needs to ask about something else. Sys.info()$user and login are often the same, but I’ve had success with something like\n\nif (grepl('^nameofcluster', Sys.info()[\"nodename\"]) | \n    grepl('^c', Sys.info()[\"nodename\"])) {\n  inpath &lt;- file.path('path', 'to', 'HPC', 'inputs')\n  outpath &lt;- file.path('path', 'to', 'HPC', 'outputs')\n  plan(list(tweak(batchtools_slurm,\n                template = \"batchtools.slurm.tmpl\",\n                resources = list(time = 5,\n                                 ntasks.per.node = 12, \n                                 mem = 1000)),\n          multicore))\n}\n\nwhere that second grepl is because the working nodes have a different nodename than the login node, which tends to have the same name as the whole HPC. But I have encountered HPCs where there a many different formats for that nodename, so there’s just some trial and error involved, often involving some jobs like those in slurm_r_tests that query and return HPC resources used, including the names of the nodes."
  },
  {
    "objectID": "package/package_creation.html",
    "href": "package/package_creation.html",
    "title": "Creating a package",
    "section": "",
    "text": "I’ve always meant to build packages, but never quite have the time, and often end up with very convoluted projects that are not ideal for shoehorning into a typical package structure, particularly as a first try.\nIn part, I think, that is because my code is often a combination of package-type-things (functions, tests, other software flow) and analyses. It’s unclear what the best approach to this sort of flow is, where we absolutely want functions, but they are very specific to the analyses, of which there are many. Do the analyses go in the package? In two projects, but that’s a hassle? Anyway, that’s a topic for a longer post.\nHere, I have a self-contained, broadly usable bit of code I’m working on to extract information from the Victoria (Australia) waterdata network API. It’s more interesting than a Hello World type package, but also constrained in scope and the analyses can clearly go elsewhere.\nThis doc will be developed as I go, and so like most docs on this site isn’t a tutorial per se, but a sequence of steps, including pitfalls and recoveries (hopefully).\n\n\nFirst, created a repo in git.\nFor the main package development, I’m largely going to follow https://r-pkgs.org/, though I’m hoping I don’t have to read the whole thing (I know I should, but time is time).\nOpened a new Rstudio session (I use renv, but want to adjust some things globally- particularly {devtools}).\ninstall_packages(\"devtools\"), then devtools::dev_sitrep() and install any requested updates (in my case, {roxygen2} was out of date.\n\ndevtools::dev_sitrep()\n\n── R ───────────────────────────────────────────────────────────────────────────\n• version: 4.2.1\n• path: 'C:/Program Files/R/R-4.2.1/'\n\n\n• R is out of date (4.2.1 vs 4.2.2)\n\n\n── Rtools ──────────────────────────────────────────────────────────────────────\n• path: 'C:/rtools42/usr/bin/'\n── devtools ────────────────────────────────────────────────────────────────────\n• version: 2.4.5\n\n\n• devtools or its dependencies out of date:\n  'jsonlite', 'stringr', 'openssl', 'whisker', 'gert'\n  Update them with `devtools::update_packages(\"devtools\")`\n\n\n── dev package ─────────────────────────────────────────────────────────────────\n• package: &lt;unset&gt;\n• path: &lt;unset&gt;\n\n\nR is also out of date (at the time of writing). Fix it with rig, then re-run and update the packages.\n\ndevtools::update_packages('devtools')\n\nCheck the name I used works.\n\navailable::available('vicwater')\n\nWarning: package 'tidytext' was built under R version 4.2.2\n\n\n── vicwater ────────────────────────────────────────────────────────────────────\nName valid: ✔\nAvailable on CRAN: ✔ \nAvailable on Bioconductor: ✔\nAvailable on GitHub:  ✔ \nAbbreviations: http://www.abbreviations.com/vicwater\nWikipedia: https://en.wikipedia.org/wiki/vicwater\nWiktionary: https://en.wiktionary.org/wiki/vicwater\nSentiment:???\n\n\nLooks good.\nQuestion- I typically use Rprojects and renv to manage dependencies and sandbox projects. I also know that I can just devtools::create() (which I think just wraps usethis::create_package(). Can I start with the Rproj and then turn it into a package? Should I want to?\nAnswer- I just needed to read a bit further. Rstudio has devtools and Rprojects working together. So calling usethis::create_package() builds the project and puts all the scaffolding where it needs to be. I’ll need to cross the existing complex Rproj –&gt; package bridge with another project later, but this is fairly straightforward here.\nSo, let’s create the package.\n\nusethis::create_package('~Galen/Documents/vicwater')\n\nAnd that worked with an existing directory. Was kind of worried about that. And it auto-opens a new Rstudio session.\nNow I’m mostly moving over there, but I ran usethis::use_mit_license() to set the license. Looks like description and namespace need work, but do that later.\nLet’s start building.\n\n\n\nI’ve been testing and poking at the API in some qmds here. I expect a lot of that ends up as vignettes in the package, and some is ready to become functions. I’ll likely maintain that flow- test in the qmd, make into functions there, repeat.\nI’m going to go write a function, and then figure out how to use it.\nSwitching to the native pipe |&gt; to see how it goes and reduce dependencies.\n\n\nFor dependencies, I used usethis::use_package(), which installs and auto-populates the DESCRIPTION file. But I think I’m going to try using renv in here too, so I don’t always overwrite system-wide libraries. Hope it doesn’t screw anything up. Usual renv::init().\npackages that are nice to have (e.g. to allow parallelisation) are usethis::use_package('packagename', type = \"suggests\"). And if we want to import a function and not use package::function, use_import_from()- see below for the %dopar%.\n\n\n\nSo, I think usually the thing to do is run devtools::load_all() within the package project. I’m sure I’ll end up doing that. But it is also be possible to run it here, just passing the path, e.g. devtools::load_all(\"path/to/package/dir\"). That lets me work on test and development qmds and scripts here. For a bit. But why? For one, seems like vignettes have to use rmd at least at present. And it keeps all the trial and error out of that repo.\nI got hung up here for a while trying to pre-figure out how I’d install it once it was on github. Turns out it’s super straightforward (see below). It ends up just working as long as the thing on github has basic package structure.\n\n\n\nI’m using roxygen comments, as in the package dev book and roxygen docs for things like inheriting parameters and sections. Running devtools::document() builds the .rd files and means ?function works. There’s a lot of fancy stuff we could do there, but keeping it simple at first.\n\n\n\nI like having actual demonstrations of the code, rather than just function docs, so using usethis::use_vignette to start building some. They have to be in rmd, not qmd. But the visual editor still works, which is nice. Just going to have to re-remember rmd chunk headers.\nI can’t get df_print: paged to work. I think it might be a difference between html and html_vignette, but it is listed as an option in the help. For now using kable even though it’s huge for tables.\nI ended up using the main vignette as an example in the primary github readme. To do that, I did usethis::use_readme_rmd(). Would be good to sort out {pkgdown}, or maybe there’s a streamlined quarto version that builds a website?\n\n\n\nUsing usethis::use_testthat(3) and writing tests was fairly straighforward, but I think there will be a learning curve about what and how to test. I tend to look very granularly at ad-hoc tests, i.e. scanning for weird NA, types, etc. But testthat and the expect_* functions lend themselves to simpler checks.\nIt gets sort of cumbersome if a function takes a while and generates something complex. In that case, I built tests that run the function (and so are fragile to the function just erroring out), and then run multiple different expect_* tests against it to make sure the output is right. As an example,\n\ntest_that(\"derived variables work for ts\", {\n  s3 &lt;- get_response(\"https://data.water.vic.gov.au/cgi/webservice.exe?\",\n                     paramlist = list(\"function\" = 'get_ts_traces',\n                                      \"version\" = \"2\",\n                                      \"params\" = list(\"site_list\" = '233217',\n                                                      \"start_time\" = 20200101,\n                                                      \"varfrom\" = \"100\",\n                                                      \"varto\" = \"140\",\n                                                      \"interval\" = \"day\",\n                                                      \"datasource\" = \"A\",\n                                                      \"end_time\" = 20200105,\n                                                      \"data_type\" = \"mean\",\n                                                      \"multiplier\" = 1)))\n  expect_equal(class(s3), 'list')\n  expect_equal(s3[[1]], 0)\n\n})\n\nAnd then, if I want to hit the function with edge cases, etc, I have to do that over and over. There’s likely a better way, but I’ll need to experiment.\n\n\n\n\nTrying to use %dopar%, but can’t get foreach::%dopar% to work, or with backticks. Putting it in a roxygen comment as @importFrom foreach %dopar% failed too. Seems to have worked to do usethis::use_import_from('foreach', '%dopar%'), which built some new files.\nHaving a hard time testing with doFuture, since it can’t find this package. pause that for a while\n\n\n\nOnce it’s pushed to github, it’s fairly straightforward to install- just\n\ndevtools::install_github(\"galenholt/vicwater\")\n\n\n\n\nIt ended up being pretty straightforward to use devtools::check() and using continuous integration with github to run the checks and put the little badges on, as described in the book.\nIt is easy to end up with funny missing pieces and issues if you forget to run devtools::check() and just push to github followed by devtools::install_github or even more likely if you just devtools::install_local from the directory with the code in it. In general, I think the github actions should take care of the check, but I never seem to get the emails that say it’s happened.\nI often forget these steps. But to actually make the package usable other than with load_all(), we seem to need to devtools::check() and if there’s an rmd readme, knit that.\nThe readme ends up being hard to build when it gets updated without reinstalling the package. I ended up in a weird loop once where I couldn’t build the package with a broken readme, but couldn’t update the readme without package updates. The solution is devtools::build_readme() to install a temp package and build the readme from that, and then devtools::check().\n\n\n\n\n\nI use {renv} for package management and reproducibility, which usually (in a non-package Rproject) puts symlinks to the package in a projdir/renv/library/R-4.x/CPUtype/ directory. But interestingly, in a package project, it puts the symlinked library/R4.x/... in a central location (in my case, ~/AppData/Local/R/cache/R/renv/library/PACKAGENAME-HASH/R4.x/….\n\n\n\nI need to make some standard figure functions as part of the package. To test them, I’ve found the {vdiffr} package. It saves a figure if one doesn’t exist and if one does exist, it checks against the saved version. It seems to work well, the only trick is to remember to usethis::use_package('vdiffr', 'Suggests'), or it won’t be available to use by devtools::check().\n\n\n\nWhen I devtools::check() on a package using dplyr, I get a million errors about ‘no visible binding for global variable ’variable_name’’. The issue is that R CMD CHECK is interpreting the bare variable names in mutate, summarise, etc as variables and can’t find them. The code runs fine, but it’s annoying.\nThe answer, unfortunately, is to use the .data[['variable_name']] or .data$variable_name convention everywhere and usethis::use_import_from('rlang', '.data'). That works to get rid of the errors, but now we’ve lost one of the really nice things about writing dplyr code- the simplicity of bare data variable names.\n\n\n\nData in /data needs to be included with the package (unsurprisingly- it obviously can’t be accessed if it’s not there). This is easy to have bite though, since my typical default is to gitignore data. Then everything works locally (where the package is originally built and data created), but fails when, for example, we install from github. The obvious answer is to not gitignore data (and so don’t change it very often). We do not seem to need to export the data like we do functions to have access to it in an installed package, (though documenting data is clearly ideal).\n\n\n\nSetting up badges and github actions sounds relatively straightforward, but there are tricks that either aren’t well-documented or I couldn’t figure out at all.\nFor example, if we set up automated coverage and checking,\n\nusethis::use_github_action('test-coverage')\nusethis::use_github_action('check-standard')\n\nThe first thing we need to do is run\n\ndevtools::build_readme()\n\nTo put the badges in.\nGetting the coverage to work was really unclear to me. After cobbling it together, what seems to work is go to the codecov website and sign up for an account. Link it to your github, and then choose the repo you want and click ‘setup’. That tells you to put in a “repository secret”. Do that. It also says to ‘add codecov to your github actions’, which I didn’t do and it seems to work, I think because the use_github_action has already set that bit up (though it doesn’t have the particular text they say to add). That action seems to only run on master or main, so if nothing happens on a branch, try pushing there.\n\n\n\nI have a lot of projects that I want to make into packages, largely to be able to have cleaner testing structure and be able to use devtools::load_all() to access functions instead of source in the head of everything. Pretty simple, just call\n{r}} usethis::create_package(getwd(), check_name = FALSE)\nafter you double check the working directory is right (should be, if you’re already in a project), and the check_name = FALSE because we likely don’t care if it’s a name that makes CRAN happy (if we do, obviously set that to TRUE).\nThis did cause a series of cascading issues that should have been expected. First, since {renv} puts the library in a different place for packages vs projects, it wanted to reinstall all packages. That took a while because I hadn’t been keeping up. Then it took a few restarts of Rstudio to get it to register the structure and make things like shortcuts to devtools::load_all() available.\nIt sure is nice to be able to load functions in though."
  },
  {
    "objectID": "package/package_creation.html#getting-started",
    "href": "package/package_creation.html#getting-started",
    "title": "Creating a package",
    "section": "",
    "text": "First, created a repo in git.\nFor the main package development, I’m largely going to follow https://r-pkgs.org/, though I’m hoping I don’t have to read the whole thing (I know I should, but time is time).\nOpened a new Rstudio session (I use renv, but want to adjust some things globally- particularly {devtools}).\ninstall_packages(\"devtools\"), then devtools::dev_sitrep() and install any requested updates (in my case, {roxygen2} was out of date.\n\ndevtools::dev_sitrep()\n\n── R ───────────────────────────────────────────────────────────────────────────\n• version: 4.2.1\n• path: 'C:/Program Files/R/R-4.2.1/'\n\n\n• R is out of date (4.2.1 vs 4.2.2)\n\n\n── Rtools ──────────────────────────────────────────────────────────────────────\n• path: 'C:/rtools42/usr/bin/'\n── devtools ────────────────────────────────────────────────────────────────────\n• version: 2.4.5\n\n\n• devtools or its dependencies out of date:\n  'jsonlite', 'stringr', 'openssl', 'whisker', 'gert'\n  Update them with `devtools::update_packages(\"devtools\")`\n\n\n── dev package ─────────────────────────────────────────────────────────────────\n• package: &lt;unset&gt;\n• path: &lt;unset&gt;\n\n\nR is also out of date (at the time of writing). Fix it with rig, then re-run and update the packages.\n\ndevtools::update_packages('devtools')\n\nCheck the name I used works.\n\navailable::available('vicwater')\n\nWarning: package 'tidytext' was built under R version 4.2.2\n\n\n── vicwater ────────────────────────────────────────────────────────────────────\nName valid: ✔\nAvailable on CRAN: ✔ \nAvailable on Bioconductor: ✔\nAvailable on GitHub:  ✔ \nAbbreviations: http://www.abbreviations.com/vicwater\nWikipedia: https://en.wikipedia.org/wiki/vicwater\nWiktionary: https://en.wiktionary.org/wiki/vicwater\nSentiment:???\n\n\nLooks good.\nQuestion- I typically use Rprojects and renv to manage dependencies and sandbox projects. I also know that I can just devtools::create() (which I think just wraps usethis::create_package(). Can I start with the Rproj and then turn it into a package? Should I want to?\nAnswer- I just needed to read a bit further. Rstudio has devtools and Rprojects working together. So calling usethis::create_package() builds the project and puts all the scaffolding where it needs to be. I’ll need to cross the existing complex Rproj –&gt; package bridge with another project later, but this is fairly straightforward here.\nSo, let’s create the package.\n\nusethis::create_package('~Galen/Documents/vicwater')\n\nAnd that worked with an existing directory. Was kind of worried about that. And it auto-opens a new Rstudio session.\nNow I’m mostly moving over there, but I ran usethis::use_mit_license() to set the license. Looks like description and namespace need work, but do that later.\nLet’s start building."
  },
  {
    "objectID": "package/package_creation.html#building",
    "href": "package/package_creation.html#building",
    "title": "Creating a package",
    "section": "",
    "text": "I’ve been testing and poking at the API in some qmds here. I expect a lot of that ends up as vignettes in the package, and some is ready to become functions. I’ll likely maintain that flow- test in the qmd, make into functions there, repeat.\nI’m going to go write a function, and then figure out how to use it.\nSwitching to the native pipe |&gt; to see how it goes and reduce dependencies.\n\n\nFor dependencies, I used usethis::use_package(), which installs and auto-populates the DESCRIPTION file. But I think I’m going to try using renv in here too, so I don’t always overwrite system-wide libraries. Hope it doesn’t screw anything up. Usual renv::init().\npackages that are nice to have (e.g. to allow parallelisation) are usethis::use_package('packagename', type = \"suggests\"). And if we want to import a function and not use package::function, use_import_from()- see below for the %dopar%.\n\n\n\nSo, I think usually the thing to do is run devtools::load_all() within the package project. I’m sure I’ll end up doing that. But it is also be possible to run it here, just passing the path, e.g. devtools::load_all(\"path/to/package/dir\"). That lets me work on test and development qmds and scripts here. For a bit. But why? For one, seems like vignettes have to use rmd at least at present. And it keeps all the trial and error out of that repo.\nI got hung up here for a while trying to pre-figure out how I’d install it once it was on github. Turns out it’s super straightforward (see below). It ends up just working as long as the thing on github has basic package structure.\n\n\n\nI’m using roxygen comments, as in the package dev book and roxygen docs for things like inheriting parameters and sections. Running devtools::document() builds the .rd files and means ?function works. There’s a lot of fancy stuff we could do there, but keeping it simple at first.\n\n\n\nI like having actual demonstrations of the code, rather than just function docs, so using usethis::use_vignette to start building some. They have to be in rmd, not qmd. But the visual editor still works, which is nice. Just going to have to re-remember rmd chunk headers.\nI can’t get df_print: paged to work. I think it might be a difference between html and html_vignette, but it is listed as an option in the help. For now using kable even though it’s huge for tables.\nI ended up using the main vignette as an example in the primary github readme. To do that, I did usethis::use_readme_rmd(). Would be good to sort out {pkgdown}, or maybe there’s a streamlined quarto version that builds a website?\n\n\n\nUsing usethis::use_testthat(3) and writing tests was fairly straighforward, but I think there will be a learning curve about what and how to test. I tend to look very granularly at ad-hoc tests, i.e. scanning for weird NA, types, etc. But testthat and the expect_* functions lend themselves to simpler checks.\nIt gets sort of cumbersome if a function takes a while and generates something complex. In that case, I built tests that run the function (and so are fragile to the function just erroring out), and then run multiple different expect_* tests against it to make sure the output is right. As an example,\n\ntest_that(\"derived variables work for ts\", {\n  s3 &lt;- get_response(\"https://data.water.vic.gov.au/cgi/webservice.exe?\",\n                     paramlist = list(\"function\" = 'get_ts_traces',\n                                      \"version\" = \"2\",\n                                      \"params\" = list(\"site_list\" = '233217',\n                                                      \"start_time\" = 20200101,\n                                                      \"varfrom\" = \"100\",\n                                                      \"varto\" = \"140\",\n                                                      \"interval\" = \"day\",\n                                                      \"datasource\" = \"A\",\n                                                      \"end_time\" = 20200105,\n                                                      \"data_type\" = \"mean\",\n                                                      \"multiplier\" = 1)))\n  expect_equal(class(s3), 'list')\n  expect_equal(s3[[1]], 0)\n\n})\n\nAnd then, if I want to hit the function with edge cases, etc, I have to do that over and over. There’s likely a better way, but I’ll need to experiment."
  },
  {
    "objectID": "package/package_creation.html#issues",
    "href": "package/package_creation.html#issues",
    "title": "Creating a package",
    "section": "",
    "text": "Trying to use %dopar%, but can’t get foreach::%dopar% to work, or with backticks. Putting it in a roxygen comment as @importFrom foreach %dopar% failed too. Seems to have worked to do usethis::use_import_from('foreach', '%dopar%'), which built some new files.\nHaving a hard time testing with doFuture, since it can’t find this package. pause that for a while"
  },
  {
    "objectID": "package/package_creation.html#installing",
    "href": "package/package_creation.html#installing",
    "title": "Creating a package",
    "section": "",
    "text": "Once it’s pushed to github, it’s fairly straightforward to install- just\n\ndevtools::install_github(\"galenholt/vicwater\")"
  },
  {
    "objectID": "package/package_creation.html#checking-building-etc",
    "href": "package/package_creation.html#checking-building-etc",
    "title": "Creating a package",
    "section": "",
    "text": "It ended up being pretty straightforward to use devtools::check() and using continuous integration with github to run the checks and put the little badges on, as described in the book.\nIt is easy to end up with funny missing pieces and issues if you forget to run devtools::check() and just push to github followed by devtools::install_github or even more likely if you just devtools::install_local from the directory with the code in it. In general, I think the github actions should take care of the check, but I never seem to get the emails that say it’s happened.\nI often forget these steps. But to actually make the package usable other than with load_all(), we seem to need to devtools::check() and if there’s an rmd readme, knit that.\nThe readme ends up being hard to build when it gets updated without reinstalling the package. I ended up in a weird loop once where I couldn’t build the package with a broken readme, but couldn’t update the readme without package updates. The solution is devtools::build_readme() to install a temp package and build the readme from that, and then devtools::check()."
  },
  {
    "objectID": "package/package_creation.html#ephemera",
    "href": "package/package_creation.html#ephemera",
    "title": "Creating a package",
    "section": "",
    "text": "I use {renv} for package management and reproducibility, which usually (in a non-package Rproject) puts symlinks to the package in a projdir/renv/library/R-4.x/CPUtype/ directory. But interestingly, in a package project, it puts the symlinked library/R4.x/... in a central location (in my case, ~/AppData/Local/R/cache/R/renv/library/PACKAGENAME-HASH/R4.x/….\n\n\n\nI need to make some standard figure functions as part of the package. To test them, I’ve found the {vdiffr} package. It saves a figure if one doesn’t exist and if one does exist, it checks against the saved version. It seems to work well, the only trick is to remember to usethis::use_package('vdiffr', 'Suggests'), or it won’t be available to use by devtools::check().\n\n\n\nWhen I devtools::check() on a package using dplyr, I get a million errors about ‘no visible binding for global variable ’variable_name’’. The issue is that R CMD CHECK is interpreting the bare variable names in mutate, summarise, etc as variables and can’t find them. The code runs fine, but it’s annoying.\nThe answer, unfortunately, is to use the .data[['variable_name']] or .data$variable_name convention everywhere and usethis::use_import_from('rlang', '.data'). That works to get rid of the errors, but now we’ve lost one of the really nice things about writing dplyr code- the simplicity of bare data variable names.\n\n\n\nData in /data needs to be included with the package (unsurprisingly- it obviously can’t be accessed if it’s not there). This is easy to have bite though, since my typical default is to gitignore data. Then everything works locally (where the package is originally built and data created), but fails when, for example, we install from github. The obvious answer is to not gitignore data (and so don’t change it very often). We do not seem to need to export the data like we do functions to have access to it in an installed package, (though documenting data is clearly ideal).\n\n\n\nSetting up badges and github actions sounds relatively straightforward, but there are tricks that either aren’t well-documented or I couldn’t figure out at all.\nFor example, if we set up automated coverage and checking,\n\nusethis::use_github_action('test-coverage')\nusethis::use_github_action('check-standard')\n\nThe first thing we need to do is run\n\ndevtools::build_readme()\n\nTo put the badges in.\nGetting the coverage to work was really unclear to me. After cobbling it together, what seems to work is go to the codecov website and sign up for an account. Link it to your github, and then choose the repo you want and click ‘setup’. That tells you to put in a “repository secret”. Do that. It also says to ‘add codecov to your github actions’, which I didn’t do and it seems to work, I think because the use_github_action has already set that bit up (though it doesn’t have the particular text they say to add). That action seems to only run on master or main, so if nothing happens on a branch, try pushing there.\n\n\n\nI have a lot of projects that I want to make into packages, largely to be able to have cleaner testing structure and be able to use devtools::load_all() to access functions instead of source in the head of everything. Pretty simple, just call\n{r}} usethis::create_package(getwd(), check_name = FALSE)\nafter you double check the working directory is right (should be, if you’re already in a project), and the check_name = FALSE because we likely don’t care if it’s a name that makes CRAN happy (if we do, obviously set that to TRUE).\nThis did cause a series of cascading issues that should have been expected. First, since {renv} puts the library in a different place for packages vs projects, it wanted to reinstall all packages. That took a while because I hadn’t been keeping up. Then it took a few restarts of Rstudio to get it to register the structure and make things like shortcuts to devtools::load_all() available.\nIt sure is nice to be able to load functions in though."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I am a quantitative community ecologist with a focus on aquatic systems. I address questions of biodiversity maintenance and climate change responses using theoretical and empirical approaches, and also apply these skills to developing models for ecological and water management. I do quite a lot of coding in R and Matlab (and dabble in Python), including data cleaning, statistical analysis, and developing packages and software.\nI am a Research Fellow in the QAEL Lab at Deakin University, where I pursue these research directions and supervise Honours, Masters, and PhD students.\n\n\n\n\nBear creek, California"
  },
  {
    "objectID": "drones/overlaps.html",
    "href": "drones/overlaps.html",
    "title": "Overlaps",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "drones/overlaps.html#calculating-photo-separation-for-desired-overlaps",
    "href": "drones/overlaps.html#calculating-photo-separation-for-desired-overlaps",
    "title": "Overlaps",
    "section": "Calculating photo separation for desired overlaps",
    "text": "Calculating photo separation for desired overlaps\nThere are a lot of variables that go into this. My goal here is to start by making a plot of how this varies with height. And then sort out how to calculate photo frequency given speed (or vice versa). Though Litchi allows photos by distance interval, which is nice."
  },
  {
    "objectID": "drones/overlaps.html#gsd",
    "href": "drones/overlaps.html#gsd",
    "title": "Overlaps",
    "section": "GSD",
    "text": "GSD\nGround sampling distance. Lots of people determine heights to fly for a desired ground sampling distance (GSD) in cm/pixel. That’s useful to know, but we’re almost certainly always going to be flying low enough for really good GSD. Our main need here is that the overlap calculations rely on GSD. So, let’s find it. The equation is \\[GSD_w=\\frac{HS_w}{FI_w}\\]\nand\n\\[GSD_h=\\frac{HS_h}{FI_h}\\]\nwhere the subscripts h and w are the dimensions height (along path) and width (across path), H is flight altitude in meters, S is sensor width or height (ie physical size of the sensor in mm), F is focal length (true, in mm. not 35mm equivalent), and I is image width or height in pixels (be careful here- often the default photo settings are 16:9 but the sensor dims are given for 4:3, and the 16:9 crops to get there. Easiest to change to shoot in 4:3, and get better coverage to boot. This gives GSD in m/px. We typically want it in cm/px, so we adjust H in the calculations below.\n\nWhere to get those values?\n\nSensor size I\nDJI sensor sizes are given here https://www.djzphoto.com/blog/2018/12/5/dji-drone-quick-specs-amp-comparison-page. For the drones we have, the Mini 2 is 6.3w x 4.7h (called 1/2.3” sensor), and the Phantom 4 Pro V2 is 13.2w x 8.8h (called 1” sensor).\nParameterise for Mini 2\n\nsw &lt;- 6.3\nsh &lt;- 4.7\n\n\n\nImage size\nImage sizes depend on photo settings, and are given for the Mini 2 and Phantom 4 Pro V2 by DJI. You can also get them from the photo’s EXIF data (either in Properties, or more completely from https://jimpl.com/. It’s a good idea to check, because the set aspect ratio can change. Mini can be 16:9 or 4:3 (best), and Phantom can be 16:9, 4:3, or 3:2 (best).\nMini 2\n\n4:3: 4000×3000\n16:9: 4000×2250\n\nPhantom 4 Pro V2\n\n3:2 Aspect Ratio: 5472×3648\n4:3 Aspect Ratio: 4864×3648\n16:9 Aspect Ratio: 5472×3078\n\nParameterise for Mini 2 at 4:3\n\nIw &lt;- 4000\nIh &lt;- 3000\n\n\n\nFocal length\nI found the true focal length from the EXIF data, though it can be back-calculated from the 35mm format equivalent on the specs pages above.\nThe EXIF had Mini 2 at 4.5, Phantom at 8.8.\nTo back-calculate, the expression is \\(Focal*Scale=35mmequiv\\) where the scale factor is found in the EXIF, or is known and can be looked up for the sensor size. Both drones have a 24mm 35mm equivalent, but scale factors differ.\nParameterise for Mini 2\n\nFocal &lt;- 4.5 \n\n\n\n\nCalculate GSD\nWe now have what we need to get the two GSDs. Let’s get one as a function of height, and another to solve for height given GSD\n\nget_gsd &lt;- function(H_m, focal_mm, sensor_mm, image_px, h_units = 'm') {\n  # make cm\n  if (h_units == 'm') {H_m = H_m * 100} else if (h_units != 'cm') {stop(\"units not supported\")}\n  gsd &lt;- (H_m * sensor_mm)/(focal_mm * image_px)\n}\n\nAnd typically, the recommendation is to use the maximum of the height and width GSDs, as that’s the worst resolution.\n\nworst_gsd &lt;- function(H_m, focal_mm, sensor_w, sensor_h, image_w, image_h, h_units = 'm') {\n  gsd_w &lt;- get_gsd(H_m, focal_mm, sensor_w, image_w)\n  gsd_h &lt;- get_gsd(H_m, focal_mm, sensor_h, image_h)\n  \n  gsd &lt;- pmax(gsd_w, gsd_h)\n}\n\nWhat is that for the mini 2 for a range of heights? I’ll look at all of them, even if that’s not typically what we’d do.\n\nheights &lt;- seq(0, 100, by = 0.1)\nmini_gsd_w &lt;- get_gsd(heights, Focal, sw, Iw)\nmini_gsd_h &lt;- get_gsd(heights, Focal, sh, Ih)\n\nmini_gsd_worst &lt;- worst_gsd(heights, Focal, sw, sh, Iw, Ih)\n\nPlot\n\nmini_gsd &lt;- tibble(altitude_m = heights, width_gsd = mini_gsd_w, height_gsd = mini_gsd_w, worst_gsd = mini_gsd_worst)\n\n\nmini_gsd |&gt; \n  tidyr::pivot_longer(cols = ends_with('gsd'), names_to = 'gsd_type', values_to = 'GSD_cmperpx') |&gt; \nggplot(aes(x = altitude_m, y = GSD_cmperpx, color = gsd_type)) + geom_line()\n\n\n\n\nNot much difference there between h and w.\nDoes it match the Pix4d calculator?\nThat gives a GSD of 0.07 for a height of 2m, I get\n\nformat(mini_gsd_w[which(heights == 2)], scientific = FALSE)\n\n[1] \"0.07\""
  },
  {
    "objectID": "drones/overlaps.html#distance-and-overlap",
    "href": "drones/overlaps.html#distance-and-overlap",
    "title": "Overlaps",
    "section": "Distance and overlap",
    "text": "Distance and overlap\nWhat we really want is to get the flight distance we need to get a desired overlap. That will depend on height and GSD (from which we get the ground area covered per photo). And if we want photo timings, we need speed as well. Using the equations from pix4d, but rearranged so the order of operations makes sense to me.\nThe image size on the ground in meters (again using h and w for height (along path) and width (across path), given image height in px and GSD in cm/px is \\[D_h=(I_hGSD)/100\\]\nThen the flight distance needed for an overlap % O_p expressed in 0-1 is \\[d = D_h-O_pD_h = D_h(1-O_p)\\]\nThe pix4d then goes on to back that back out to the definition of D_h, \\[d=((I_hGSD)/100)(1-O_p)\\]\nIf we can’t set a travel distance d for the drone, we will need to adjust it’s velocity v in m/s and the photo interval t in seconds. In practice, well want to adjust them in tandem (and for a given height). To get the photo interval for a given velocity, it’s simply the desired distance divided by velocity, \\[t = d/v\\]\nand so the velocity for a given interval is\n\\[\nv=d/t\n\\]\nWe can obviously break this down into the equation for d, e.g. \\[t = D_h(1-O_p)/v\\].\n\nFunctions\n\nGiven GSD\nFirst, the ground distance in m\n\nground_dist &lt;- function(image_px, gsd_cmpx) {\n  D &lt;- image_px*gsd_cmpx/100\n}\n\nNext, the distance the drone should travel, given overlap\n\ndrone_dist &lt;- function(ground_dist_m, overlap_prop) {\n  d &lt;- ground_dist_m * (1-overlap_prop)\n}\n\nThe time interval, given velocity\n\nphoto_interval &lt;- function(drone_dist_m, v_ms) {\n  t &lt;- drone_dist_m/v_ms\n}\n\nThe velocity, given interval (not typical, but we might want it since we can only set intervals down to 2 seconds when hand-flying.\n\nvelocity &lt;- function(drone_dist_m, p_s) {\n  v &lt;- drone_dist_m/p_s\n}\n\n\n\nWrap those up\nDrone dist is just one level\n\ndrone_dist_from_gsd &lt;- function(image_px, gsd_cmpx, overlap_prop) {\n  D &lt;- ground_dist(image_px, gsd_cmpx)\n  dd &lt;- drone_dist(D, overlap_prop)\n}\n\nPhoto interval and velocity need to depend on that\n\nphoto_interval_from_gsd &lt;- function(image_px, gsd_cmpx, overlap_prop, v_ms) {\n  dd &lt;- drone_dist_from_gsd(image_px, gsd_cmpx, overlap_prop)\n  t &lt;- photo_interval(dd, v_ms)\n}\n\n\nvelocity_from_gsd &lt;- function(image_px, gsd_cmpx, overlap_prop, p_s) {\n  dd &lt;- drone_dist_from_gsd(image_px, gsd_cmpx, overlap_prop)\n  v &lt;- velocity(dd, p_s)\n}"
  },
  {
    "objectID": "drones/overlaps.html#values-from-h-and-overlap",
    "href": "drones/overlaps.html#values-from-h-and-overlap",
    "title": "Overlaps",
    "section": "Values from H and overlap",
    "text": "Values from H and overlap\nDo this separately for h and w, I guess?\n\nDrone distance\nThis might be all we need for litchi, and regardless, it will tell us about how close flightpaths need to be.\n\ndrone_dist_H_o &lt;- function(H_m, overlap_prop, \n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  gsd &lt;- get_gsd(H_m, focal_mm, \n                   sensor_mm, \n                   image_px, \n                   h_units = 'm')\n  \n  gD &lt;- ground_dist(image_px, gsd)\n  \n  dd &lt;- drone_dist(gD, overlap_prop)\n  \n  return(dd)\n}\n\n\n\nVelocity and photo intervals\n\nv_H_o &lt;- function(H_m, overlap_prop, p_s,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  dd &lt;- drone_dist_H_o(H_m, overlap_prop,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm')\n  v &lt;- dd/p_s # could use velocity(dd/ps), but not worth it here.\n  return(v)\n}\n\n\nt_H_o &lt;- function(H_m, overlap_prop, v_ms,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm') {\n  dd &lt;- drone_dist_H_o(H_m, overlap_prop,\n                           focal_mm, sensor_mm, image_px, \n                          h_units = 'm')\n  t &lt;- dd/v_ms # could use velocity(dd/ps), but not worth it here.\n  \n  return(t)\n}"
  },
  {
    "objectID": "drones/overlaps.html#plots",
    "href": "drones/overlaps.html#plots",
    "title": "Overlaps",
    "section": "Plots",
    "text": "Plots\n\nDrone distance\nHow does the drone distance depend on height and overlap?\nFor a fixed overlap of 80%, and the same heights sequences as above, for the h dimension (along flight path) and w (across),\n\ndd_h &lt;- drone_dist_H_o(heights, 0.8,\n                       Focal, sh, Ih)\n\ndd_w &lt;- drone_dist_H_o(heights, 0.8,\n                       Focal, sw, Iw)\n\nThose are fairly different, actually\n\ndd_hw &lt;- tibble(altitude_m = heights, width_dd = dd_w, height_dd = dd_h) |&gt; \n  tidyr::pivot_longer(cols = ends_with('dd'), names_to = 'dd_direction', values_to = 'drone_photo_distance')\n\n\nggplot(dd_hw, aes(x = altitude_m, y = drone_photo_distance, color = dd_direction)) + geom_line()\n\n\n\n\nCould use plotly here to have mouseover. Or use observable or shiny.\nWhat if we zoom in on the lower altitudes (&lt; 10m)?\n\ndd_hw |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \nggplot(aes(x = altitude_m, y = drone_photo_distance, color = dd_direction)) + geom_line()\n\n\n\n\nWe could make a heatmap with overlaps, but I’m not sure we really care that much? We’d really only be interested in maybe 75, 80, 85 or something, and this is for rule of thumb. Do that later.\nWhat are those at 2 and 4 m?\n\ndd_hw |&gt; \n  dplyr::filter(altitude_m %in% c(2,4))\n\n# A tibble: 4 × 3\n  altitude_m dd_direction drone_photo_distance\n       &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;\n1          2 width_dd                    0.56 \n2          2 height_dd                   0.418\n3          4 width_dd                    1.12 \n4          4 height_dd                   0.836\n\n\n\n\nVelocity from interval\nHere, let’s say we have a fixed overlap, and want to know the velocity we need to fly to get that at a given height and photo interval. This sounds contrived, but is pretty much exactly our situation when hand flying- the shortest interval we have is 2seconds, so how fast/slow do we need to fly to get 80% overlap at a range of heights?\nHere, we’ll focus on the h dimension. While we could fly sideways, we usually will fly with forward velocity.\nLet’s say 2 seconds, and then look at a heatmap.\n\nv_h &lt;- v_H_o(heights, 0.8, 2,\n                       Focal, sh, Ih)\n\nv_tib &lt;- tibble(altitude_m = heights, velocity_ms = v_h)\n\n\nggplot(v_tib, aes(x = altitude_m, y = velocity_ms)) + geom_line()\n\n\n\n\nAnd again zoom in\n\nv_tib |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \nggplot(aes(x = altitude_m, y = velocity_ms)) + geom_line()\n\n\n\n\nSo, to get 80% overlap if we’re limited to intervals of 2 seconds, we’d need to fly at about 0.2m/s at 2m or 0.4 at 4m. Should do a tooltip kinda thing. But for now\n\nv_tib |&gt; \n  dplyr::filter(altitude_m %in% c(2, 4)) \n\n# A tibble: 2 × 2\n  altitude_m velocity_ms\n       &lt;dbl&gt;       &lt;dbl&gt;\n1          2       0.209\n2          4       0.418\n\n\nHow about a heatmap of heights and intervals?\n\nintervals &lt;- seq(0.1, 5, by = 0.1)\n\nvel_map &lt;- tidyr::expand_grid(altitude_m = heights, photo_intervals = intervals) |&gt; \n  mutate(velocity_ms = v_H_o(altitude_m, 0.8, photo_intervals,\n                       Focal, sh, Ih))\n\n\nvel_map |&gt; \nggplot(aes(x = altitude_m, y = photo_intervals, fill = velocity_ms)) + geom_raster()\n\n\n\n\nThat’s a dumb scale. Obviously we can fly super fast at 100m and super fast photo intervals\n\nvel_map |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \nggplot(aes(x = altitude_m, y = photo_intervals, fill = velocity_ms)) + geom_raster()\n\n\n\n\nStill not particularly useful. Moving on.\n\n\nInterval from velocity\nHow often do we need to take photos given a velocity and height? Let’s start by saying velocity of 1m/s (3.6km/h).\n\nt_h &lt;- t_H_o(heights, 0.8, 1,\n                       Focal, sh, Ih)\n\nt_tib &lt;- tibble(altitude_m = heights, photo_interval = t_h)\n\n\nt_tib |&gt; \n  ggplot(aes(x = altitude_m, y = photo_interval)) + geom_line()\n\n\n\n\n\nt_tib |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \n  ggplot(aes(x = altitude_m, y = photo_interval)) + geom_line()\n\n\n\n\nSo, at that velocity, the photo interval can’t be as long as 2 seconds without flying at 10m. Clearly that would change as we fly slower.\nMake a heatmap again, I guess, even if it wasn’t very useful before and we know this is just flipping axes and colors.\n\nspeeds &lt;- seq(0.1, 5, by = 0.1)\n\ninterval_map &lt;- tidyr::expand_grid(altitude_m = heights, velocity_m = speeds) |&gt; \n  mutate(photo_interval = t_H_o(altitude_m, 0.8, velocity_m,\n                       Focal, sh, Ih))\n\n\ninterval_map |&gt; \n  dplyr::filter(altitude_m &lt;= 10) |&gt; \nggplot(aes(x = altitude_m, y = velocity_m, fill = photo_interval)) + geom_raster()"
  },
  {
    "objectID": "drones/overlaps.html#next-steps",
    "href": "drones/overlaps.html#next-steps",
    "title": "Overlaps",
    "section": "Next steps",
    "text": "Next steps\nThe obvious thing to do here is to make an observable quarto where we can select drone type, desired overlap, things we want to set, and it makes the plot and returns values we want. For now though, these plots get us most of the way there.\nWhat do those distances look like for the phantom?"
  },
  {
    "objectID": "data_acquisition/bom_gauges.html",
    "href": "data_acquisition/bom_gauges.html",
    "title": "Bom reference stations",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.2.2\n\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nTrying to find BOM gauge locations. Found reference stations. It’s a simple link, but have to use httr2 to download because there’s an error with the user_agent if we try to just download.file.\nMostly including this here as an example of changing user_agent.\nhttp://www.bom.gov.au/waterdata/ has a clickable link to what I want, but the data is buried in a frame so can’t scrape.\nThe below is because I found a link to reference stations and wanted to see what they were.\n\nIs there a url for BOM?\nIt’s just a csv, but have to faff about with httr2 and deparsing back to csv because need to pass a user agent or get a 403 error.\n\nbom2 &lt;- httr2::request(\"http://www.bom.gov.au/water/hrs/content/hrs_station_details.csv\") |&gt;\n  httr2::req_user_agent(\"md-werp\") |&gt; \n  httr2::req_perform() |&gt; \n  httr2::resp_body_string() |&gt; \n  readr::read_csv(skip = 11) |&gt; \n  dplyr::select(site = `Station Name`, \n                gauge = `AWRC Station Number`,  \n                owner = `Data Owner Name`, \n                Latitude, Longitude)\n\nRows: 467 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): AWRC Station Number, Station Name, Jurisdiction, Data Owner Name, D...\ndbl (3): Latitude, Longitude, Catchment Area (km2)\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nbasin &lt;- read_sf(file.path('data','mdb_boundary', 'mdb_boundary.shp'))\n\n\nbom2 &lt;- bom2 |&gt;\n    # lat an long come in as chr because there is a line for 'undefined'\n    dplyr::filter(site != 'undefined') |&gt;\n  st_as_sf(coords = c('Longitude', 'Latitude'), crs = 4326) |&gt; \n  st_transform(crs = st_crs(basin))\n\nThey’re not the gauges I’m looking for. Only 457, instead of 6500, and around the edges of the basin.\n\nggplot() + \n  geom_sf(data = basin) +\n  geom_sf(data = bom2)"
  },
  {
    "objectID": "code_demos.html",
    "href": "code_demos.html",
    "title": "Code Demos",
    "section": "",
    "text": "This section has code and demos that cover many topics and serves several purposes. The pages here are organised thematically, though it will likely take me some iterating on the quarto website yaml to get there.\nThe goal in many of these examples and demos is NOT clean, efficient coding, but exploring HOW the code works and how to accomplish something. That often means creating LOTS of extra variables, copy-paste, and being extremely verbose."
  },
  {
    "objectID": "code_demos.html#clarify-thinking-and-testing",
    "href": "code_demos.html#clarify-thinking-and-testing",
    "title": "Code Demos",
    "section": "Clarify thinking and testing",
    "text": "Clarify thinking and testing\nClarify what I’m actually trying to do, and what the expected outcomes are. Then figuring out a) how do get those, and b) why I sometimes don’t, which can be just as important. Doing this sort of testing here instead of in-project can be very helpful as using minimal examples forces me to isolate the issue I’m trying to solve from all the particulars of a given dataset or project structure."
  },
  {
    "objectID": "code_demos.html#central-location-for-useful-bits",
    "href": "code_demos.html#central-location-for-useful-bits",
    "title": "Code Demos",
    "section": "Central location for useful bits",
    "text": "Central location for useful bits\nA central point for (relatively) clean, complete things that I want to be able to use across many projects (e.g. 2d autocorrelation, the Johnson distribution, how to use certain packages, fonts, colours and other plotting things etc). Having one central reference point keeps me from having to either reinvent the wheel or remember which project I put the wheel in, and having many slightly different variations. And improvements/extensions can then be accessed across projects."
  },
  {
    "objectID": "code_demos.html#understanding-code-testing-beyond-standard-uses",
    "href": "code_demos.html#understanding-code-testing-beyond-standard-uses",
    "title": "Code Demos",
    "section": "Understanding code, testing beyond standard uses",
    "text": "Understanding code, testing beyond standard uses\nI spend quite a lot of time figuring out how to do things in code, understanding how code works, and double-checking everything is working correctly. There are a lot of good demos and tutorials out there (e.g. stackoverflow, some package vignettes and websites), but I often end up needing to figure out weird edge cases. And I often end up doing something similar later, but needing not the final answer, but some intermediate step along the way."
  },
  {
    "objectID": "code_demos.html#the-process-of-coding",
    "href": "code_demos.html#the-process-of-coding",
    "title": "Code Demos",
    "section": "The process of coding",
    "text": "The process of coding\nI also think there can be value in seeing how I’ve solved a problem and tested the various avenues, both for my own future reference and others. For one, if I do later have a need for one of those side avenues, they’re available. For another, it exposes the actual process of coding a bit more than the usual tutorial that has cleaned everything up start to finish. And it gives a better starting point for additional development potentially much later if I can see what I’ve already tried. Maybe most importantly, there are few tutorials/walkthroughs I’ve followed that don’t end up with some sort of error, especially as soon as I try to modify them for my purposes. Seeing where I’ve hit errors, what caused them, and how I solved it can be incredibly helpful, rather than only seeing what worked."
  },
  {
    "objectID": "RpyEnvs/rig.html",
    "href": "RpyEnvs/rig.html",
    "title": "Managing R versions",
    "section": "",
    "text": "I’ve update to R 4.2, but have projects that were built with 4.0 and even 3.x. Most new versions of packages for R 4.x don’t work in 3.x, and 4.2 seems to have broken quite a bit compared to 4.0. So I can’t just renv::restore(), and would need to update packages one at a time, but I know doing that will break things, and I don’t have time to do a full update of the project.\nI use renv to manage the packages, but not currently anything to switch/manage R versions itself. In python, there’s pyenv to manage python versions. I’ve run across rig (https://github.com/r-lib/rig)."
  },
  {
    "objectID": "RpyEnvs/rig.html#issue",
    "href": "RpyEnvs/rig.html#issue",
    "title": "Managing R versions",
    "section": "",
    "text": "I’ve update to R 4.2, but have projects that were built with 4.0 and even 3.x. Most new versions of packages for R 4.x don’t work in 3.x, and 4.2 seems to have broken quite a bit compared to 4.0. So I can’t just renv::restore(), and would need to update packages one at a time, but I know doing that will break things, and I don’t have time to do a full update of the project.\nI use renv to manage the packages, but not currently anything to switch/manage R versions itself. In python, there’s pyenv to manage python versions. I’ve run across rig (https://github.com/r-lib/rig)."
  },
  {
    "objectID": "RpyEnvs/rig.html#install",
    "href": "RpyEnvs/rig.html#install",
    "title": "Managing R versions",
    "section": "Install",
    "text": "Install\nclick on windows installer. Restart terminal. Type rig list to see what R is available."
  },
  {
    "objectID": "RpyEnvs/rig.html#using-it",
    "href": "RpyEnvs/rig.html#using-it",
    "title": "Managing R versions",
    "section": "Using it",
    "text": "Using it\nGo to the project I want to run, and figure out what version of R it was using. The one I’m testing on uses 4.0.2, which still is different enough from 4.2 that a lot of the packages fail when I renv::restore() in a session with 4.2. To avoid any issues, I will downgrade to that to run the project because I really just need things to work. Then, once I know it works, I can start updating and testing.\nSo, try rig add 4.0.2. Seems to have worked. Set the default to current, though.\nrig default 4.2.1.\n\nChoosing for a project\nWhat if I just open the project file by double clicking? There’s no obvious way to change the R version just by opening Rstudio- it uses the default.\nI think there’s probably a way to use the CLI to change the R version and then double click, but what seems to be easiest is cd path/to/repo and then rig rstudio renv.lock to open with the version in the lockfile.\nNote: this does not work with the new version of Rstudio (2022.12.0 Build 353 and others): https://github.com/r-lib/rig/issues/134 and https://github.com/rstudio/rstudio/issues/12545\nAnd do I keep using other R versions elsewhere? Seem to. For now, this should do what I need."
  },
  {
    "objectID": "RpyEnvs/rig.html#installing-rtools",
    "href": "RpyEnvs/rig.html#installing-rtools",
    "title": "Managing R versions",
    "section": "Installing rtools",
    "text": "Installing rtools\nWe need rtools to install packages with compiled components. R 4.2 has updated to Rtools 42 (from 40), and so using previous versions of R need older Rtools. The telltale is when trying to install a package, we get errors about ‘make’ not being found. The rig documents imply that rig system update-rtools40 should work, but I get “Error: the system cannot find the path specified”. I’m not sure what path that is, so hard to fix. So, I seem to be OK until I need something that needs ‘make’, and then I’m out of luck.\n\nThe solution\nTo install Rtools40, needed for R 4.0- 4.1, run rig add rtools40. Seems to be all it took, now I can compile. I assume there’s a similar command for even older Rtools if need to downgrade to R 3.x, but haven’t tried."
  },
  {
    "objectID": "RpyEnvs/python_setup.html",
    "href": "RpyEnvs/python_setup.html",
    "title": "Python setup",
    "section": "",
    "text": "I’m working on a Python project, and trying to figure out how to set up and get started. I’m used to R, where most simply, all I have to do is download R, Rstudio, and then start coding. R doesn’t need any environment manager to get going, but I do tend to use renv to manage packages, but that’s pretty lightweight and straightforward. And I can start coding without it.\nPython, on the other hand, is more opaque. In part it’s because I’m new to it, but a bit of googling suggests I’m not the only one. It’s likely also because there’s no one dedicated IDE/workflow that almost everyone uses, a la Rstudio (maybe that will change with the Rstudio–&gt; Posit move?).\nSo, I’m going to work out getting setup to code in Python (I sorta did it before, but I’m trying a new way with fewer black boxes). And using this as a place to write notes/what I did as I go. That means this might end up being less a tutorial and more a series of pitfalls, but we’ll see how it goes.\n\n\nI’m trying to get set up to manage Python versions themselves with pyenv and packages with poetry. As far as I can tell, poetry does approximately similar things to renv (but more complicated because python). And I haven’t used something similar to pyenv to manage R versions themselves, but I am about to have to figure that out too because a lot of packages broke when I moved to R 4.2. Will probably try rig, and write another one of these. I’m assuming I’ll code primarily in VS Code, unless Posit suddenly runs python like R (without reticulate). Even then, remote work will all use VS Code for the time being. I’m loosely following https://briansunter.com/blog/python-setup-pyenv-poetry and https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/, and doing all the actual setup in VS Code, not Rstudio.\nRealising after I wrote this that I probably could have actually done all of this inside quarto- I think I can run powershell/system code in code blocks?"
  },
  {
    "objectID": "RpyEnvs/python_setup.html#the-issue",
    "href": "RpyEnvs/python_setup.html#the-issue",
    "title": "Python setup",
    "section": "",
    "text": "I’m working on a Python project, and trying to figure out how to set up and get started. I’m used to R, where most simply, all I have to do is download R, Rstudio, and then start coding. R doesn’t need any environment manager to get going, but I do tend to use renv to manage packages, but that’s pretty lightweight and straightforward. And I can start coding without it.\nPython, on the other hand, is more opaque. In part it’s because I’m new to it, but a bit of googling suggests I’m not the only one. It’s likely also because there’s no one dedicated IDE/workflow that almost everyone uses, a la Rstudio (maybe that will change with the Rstudio–&gt; Posit move?).\nSo, I’m going to work out getting setup to code in Python (I sorta did it before, but I’m trying a new way with fewer black boxes). And using this as a place to write notes/what I did as I go. That means this might end up being less a tutorial and more a series of pitfalls, but we’ll see how it goes.\n\n\nI’m trying to get set up to manage Python versions themselves with pyenv and packages with poetry. As far as I can tell, poetry does approximately similar things to renv (but more complicated because python). And I haven’t used something similar to pyenv to manage R versions themselves, but I am about to have to figure that out too because a lot of packages broke when I moved to R 4.2. Will probably try rig, and write another one of these. I’m assuming I’ll code primarily in VS Code, unless Posit suddenly runs python like R (without reticulate). Even then, remote work will all use VS Code for the time being. I’m loosely following https://briansunter.com/blog/python-setup-pyenv-poetry and https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/, and doing all the actual setup in VS Code, not Rstudio.\nRealising after I wrote this that I probably could have actually done all of this inside quarto- I think I can run powershell/system code in code blocks?"
  },
  {
    "objectID": "RpyEnvs/python_setup.html#getting-started--systemwide-installations",
    "href": "RpyEnvs/python_setup.html#getting-started--systemwide-installations",
    "title": "Python setup",
    "section": "Getting started- systemwide installations",
    "text": "Getting started- systemwide installations\nBoth those websites are working on Unix and Mac, so while step 1 is install pyenv, we aren’t going to apt-get or brew install. In fact, the pyenvgithub says we need to use a windows fork. Things already getting nonstandard. Should I just run everything in Windows Subsystem for Linux? Maybe, but I’d like to just use windows if possible, as much as I like WSL.\n\npyenv\n\nWindows\nGuess I’ll just start at Quick start. Going to use powershell directly rather than inside vs code here, because vs code likes to open in recent directories instead of globally, and I think I want pyenv system-wide.\nStep 1- install in powershell with Invoke-WebRequest -UseBasicParsing -Uri \"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\" -OutFile \"./install-pyenv-win.ps1\"; &\"./install-pyenv-win.ps1\"\nCan’t run scripts (new computer). Sends me to https:/go.microsoft.com/fwlink/?LinkID=135170.\n\nSetting the policy to just the running process. Will probably regret that when I next try to run a script, but for now I don’t really want universal unrestricted powershell scripts.\n\n\nPowershell script permissions\nAside- it starts to get really annoying because pyenv runs scripts, so will need to fix. Get an error when I try to change Scope to CurrentUser because of a group policy. Setting it to Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope LocalMachine seems to work, despite apparently being a larger set than my User.\nIt says it wasn’t installed successfully, but when I try again it says it’s there. I guess push on?\n\nStep 2 was just to shut down and reopen powershell\nStep 3- Run pyenv --version. If you haven’t changed policy to something larger than Process, this will fail because of the ExecutionPolicy. Guess I need to turn it back on. I kept doing one-offs for a while until I got annoyed and then set it to LocalMachine (see above).\nStep 4, pyenv install -l lists a million python versions. Seems like a good thing. I’m going to need older versions in projects, but for now, let’s install the latest.\nStep 5- the install of python. There’s a 3.12.a1, which I’m assuming means alpha, so I’ll go with pyenv install 3.11.0, which is the most recent without the .a1.\nThe install seems to have worked.\nStep 6- set global pyenv global 3.11.0\nStep 7- check it worked pyenv version \nStep 8- check python with python -c \"import sys; print(sys.executable)\"\nworks, gives me the path, \\.pyenv\\pyenv-win\\versions\\3.11.0\\python.exe\nNow it says to validate by closing and reopening- do that. Now pyenv --version gives the version, while pyenv alone tells us the commands. They’re also all at the github page I’m following.\nThen try in a vs code terminal, also works. Note that using the git bash terminal instead of powershell bypasses the script permissions issue if it hasn’t been set larger than Process.\n\n\nLinux\nThis is much easier. Install with curl https://pyenv.run | bash. That tells you to add somethign to .bashrc, do that. Then close and restart bash, and run pyenv install 3.11.0 or whatever version we’re using. I needed to sudo apt-get install libffi-dev to get it to compile some C bits.\nNow, we have Python 3.11 as the global python version, but should be able to install other versions and use them in local projects. Assuming I’ll get there once I set up poetry.\n\n\n\npoetry\n\nWindows\nOnce again, the websites I’m following have commands for mac/unix, so back to the main poetry page to sort this out on windows.\nAgain, powershell command on windows. Could I use the git bash in vs? Maybe? Just stick with powershell. (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -.\nThe instructions say that last bit should be py instead of python if python not installed from Microsoft Store, but had to use python anyway.\nseems to have worked- \nThe instructions then have an advanced section I’m skipping, but that install message above seems to match step 3, where we add Poetry to the PATH in order to run Poetry by just running poetry and not the full path.\nI could change it with powershell, but the instructions I found involved a bunch of regex. Instead, search for “Advanced System Settings”, then in the bottom right, click Environment Variables, then in the System Variables box, click on Path, then Edit button, then New. That creates a blank line, paste in the path from poetry install, or use the one from their website, %APPDATA%\\Python\\Scripts. OK out of all the system settings boxes.\nShut down powershell, then fire it back up and run poetry --version. If the path setting failed, it won’t be able to find poetry, if it worked, it’ll give the version number.\nThat worked for me once, but now isn’t working the next day. Actually, it works in powershell, but not vs code powershell. It wasn’t unpacking the %APPDATA% correctly - running (Get-ChildItem env:path).Value lists %APPDATA% instead of the expanded directory. I stuffed the full path as in the screenshot above in the PATH, restarted VS Code and it works. Interestingly though, I left the %APPDATA% version there too, and it’s now unpacked when I run (Get-ChildItem env:path).Value.\n\n\nLinux\nAs at the docs, install with curl -sSL https://install.python-poetry.org | python3 -\nadd export PATH=\"/home/azureuser/.local/bin:$PATH\" to ~/.bashrc, start a new shell and make sure it worked with poetry --version."
  },
  {
    "objectID": "RpyEnvs/python_setup.html#setting-up-a-project",
    "href": "RpyEnvs/python_setup.html#setting-up-a-project",
    "title": "Python setup",
    "section": "Setting up a project",
    "text": "Setting up a project\nLet’s first say we’re going to use a different-than-standard version of python, so install that with pyenv. For this test, let’s just use 3.8.9. Not really any particular reason. So, run pyenv install 3.8.9\nNow, pyenv versions (NO FLAGS- the “--” flag will give the version of pyenv) shwos two versions with an asterisk by global.\n\n\nCreate the project\nJust need a directory and cd inside it, I think. I’ll make it inside the directory with this qmd. mkdir pytesting, cd pytesting. Actually, this yields too much nesting. poetry builds a directory for the project, and another directory in that, so this just yields annoying levels of nesting. Call poetry new (see below) from the directory you want to contain the main project directory.\n\n\nSet the python version\nI think just pyenv local 3.8.9. That creates a .python-version file in the directory, which seems to be the idea.\n\n\nSet poetry\nAm I going to completely screw up my R project having this inside it? Guess we’ll find out.\npoetry new pytesting then creates another directory and returns “Created package pytesting in pytesting”. It builds the outer directory, so don’t make one first or the nesting gets silly.\nThat directory seems to be establishing a standard package structure and the lockfiles etc. Opening the .toml looks like it didn’t pick up the python version though- it’s using 3.11. Hmmm. Tried killing and restarting powershell and it’s still doing that. Not sure why it’s not picking up the local python.\nIf I move up a directory, the pyenv versions returns back to 3.11. So pyenv seems to be working, but poetry’s not picking it up. I guess I can change in manually, but that’s annoying.\nSeems to be a long-running known issue- recent posts here https://github.com/python-poetry/poetry/issues/651. Ignore for now, maybe fix manually if it becomes an issue. I tried the solution in the last post (poetry config virtualenvs.prefer-active-python true), and it didn’t fix it. Tried completely starting over a few times. No luck. Worry about that later. I guess that means the pyenv stuff might be useless for the moment- will need to use the version poetry thinks it has in the directory. It does look like can reinstall poetry, but that seems like a pain. (No one else seems to have issues with the config above).\nSo, I’ve just set the pyenv to the global for now, and moving ahead with poetry for the project. I guess I could change pyenv global each time I switch projects as an annoying workaround if it becomes an issue. An answer might be poetry env use , see https://python-poetry.org/docs/managing-environments/.\n\n\nUsing poetry for dependencies\nMuch like renv can install all dependencies from info in renv.lock, we could build the project with dependencies from pyproject.toml. Or, also like renv, as we’re developing a project, we can add iteratively. Let’s do that, since that’s what we’re doing.\nThere seems to be an intermediate step here though, running poetry install to initialise a virtual environment from the .toml. I assume this would install whatever’s in the toml, but we don’t have anything at present. That apparently creates a virtual environment somewhere globally (.cache/, according to https://www.adaltas.com/en/2021/06/09/pyrepo-project-initialization/).\nAnd now I have poetry.lock . According to the docs, this takes precedence over the .toml, though I doubt that’s true for python version itself. This is what gets committed to share the project.\nTyping poetry shell at the command line activates the environment. But how do we activate it for a VS code session?\nSeems to just be active once we open a .py file in that directory (e.g. if we open the file, then a powershell at that location, it appears with pyenv shell already going.\nTo test adding a dependency, i’ll try numpy. First, I can run simply python code- a = 1 etc. But import numpy as np fails (as expected)- ModuleNotFoundError: No module named ‘numpy’. So, click back to the powershell terminal, and try poetry add numpy. It resolves dependencies and writes a lock file.\nAnd yet, if I try import numpy as np, same error. The poetry show command lists it as installed.\nSo, it’s because VScode doesn’t know where to find the venvs. On a one-off basis, can use poetry env info --path to get the path, then in VS code select interpreter (ctrl-shift-p for the search thingy), then paste in that path. But that’s annoying.\nI’m trying getting to the search thing, then User settings, then adding C:\\Users\\galen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ to the venv Folders. That seems to work, but it may just be remembering from last time.\nThe most robust option might be to not keep the venvs in .cache but instead local to the project, as described here. Looks like that’s done with poetry config virtualenvs.in-project true at the very outset, and then rebuilding the project (and it should persist for future projects).\nAnd that’s what we’re doing in the project I’m doing this for. So let’s do that. First, run poetry env list to get the name, then poetry env remove NAME to delete. But it failed, seemingly partway. So also go to AppData\\Local\\pypoetry\\Cache\\virtualenvs and delete the folder. Restart vs and now poetry env list doesn’t return anything.\nTo set the config, poetry config virtualenvs.in-project true . I believe that’s global (I ran it outside the project).\nNow rebuild from .toml in the project with poetry install. The .venv is there now, but VS still can’t find it.\nOK, shut everything down, and instead of opening VS again as usual, I opened it and then opened a new window, and now it works. It was somehow setting the root based on where VS happened to open, rather than based on where files were. That probably makes sense for a git repo with just python, but broke here. I assume we need to add the venv directory to the gitignore.\nJust did poetry add pandas and it works and I immediately have access to it in a python script.\n\n\nComplications with add\nTo add a specific version of a package, use\npoetry add packagename==2.0.5\nThere are a number of other ways to specify version ranges, installing from git, etc.\nTo install from git, dpending on https or ssh or different branches\n# For the main branch\npoetry add git+https://github.com/sdispater/pendulum.git\npoetry add git+ssh://git@github.com/sdispater/pendulum.git\n\n# For other branches\npoetry add git+https://github.com/sdispater/pendulum.git#develop\npoetry add git+https://github.com/sdispater/pendulum.git#2.0.5\n\npoetry add git+ssh://github.com/sdispater/pendulum.git#develop\npoetry add git+ssh://github.com/sdispater/pendulum.git#2.0.5\n\n\nVS code note\nSometimes VS seems to find the poetry venv and use it, and other times (I think if it’s not at the head dir of the workspace?) it needs to be pointed at the python.exe. To do that, open the command palette, (ctrl-shift-p), select python interpreter, then .venv\\scripts\\python.exe wherever that venv is."
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html",
    "href": "RpyEnvs/py_r_project_overview.html",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "I’m working on a project that needs both Python and R, and they need to talk to each other. Most of the work is in R, but the first bit is Python, and more importantly, it has to call a python package (more in future, likely). I’m packaging everything up so the user doesn’t need to do everything through the repo, but can just library(package) and be off and running, but that’s complicated by the fact that any actual use of this project needs both languages.\nIt seems like there are a couple options to how I deal with the two languages.\n\nHave a python package and an R package, and make the user install both and manage the bilinguilism in their own scripts using the packages.\n\nSeems most cumbersome for user, but the users are fairly involved in the project, so could work\n\nSkip creating my own python package entirely, and just have the bits of python wrapper in inst/python\n\nLikely how I’ll start to figure out how to include python in package\n\nAnd see below- I think it’ll be easier to rapidly iterate the py code.\n\nAs python side grows, this will get very cumbersome\nIs it slower than the first option?\n\nMake my own python package, and wrap that in inst/python\n\nI think this is actually the best option, since I won’t have to maintain two copies of py code\nWill also allow a user to just use option 1 (or for the whole project to move that way in future, or shifting functions over to python).\nIt will make the R package more cumbersome than option 1, but I think it’s the best tradeoff.\nIt will also slow down initial dev- as I change the python side, I’ll have to rebuild the py package and re-install it into my environment. There’s no obvious analogue to devtools::load_all that can reach across and do all that. Whereas I think if I have my py code in inst it’ll get refreshed when I load_all.\n\n\nAll of these will follow {reticulate} docs, but those are fairly sketchy as to how to actually write code that works.\n\nhttps://rstudio.github.io/reticulate/articles/package.html (seems to conflict a bit with the next)\nhttps://rstudio.github.io/reticulate/articles/python_dependencies.html\nhttps://rstudio.github.io/reticulate/articles/calling_python.html\n\n\n\nhttps://stackoverflow.com/questions/72185273/reticulate-fails-automatic-configuration-in-r-package\nhttps://github.com/rstudio/reticulate/issues/997\nWill need to test how this works both when we already have a venv and when we don’t.\n\n\n\nas I’m developing\nThis works from console, after I put the file in the inst/python\nreticulate::source_python(file.path('inst', 'python', 'controller_functions.py'))\ntested with\nscene_namer(file.path('inst', 'extdata', 'testsmall'))\nAt first it didn’t work to have the Config/retictulate in description and the source_python in .onLoad. I didn’t get anything. Then I fixed it by adding it to the package’s global environment with the argument envir = globalenv(), and it started working. But only when I already have a python environment. If I try to get the Config/reticulate to build a python environment with the necessary dependencies in a bare project, it won’t even install the package.\nI think the issue there is that the way the config/reticulate and onload are set up, it doesn’t try to install dependencies until Python is used, and for some reason running that file in onLoad doesn’t trigger it.\nSo, somehow I need to be able to install the package without needing the py dependencies, and then install them the first time someone types library(packagename) or otherwise tries to use it.\nThat’s a whole new thing to figure out, I think.\nIf we assume the user has the python dependencies installed and accessible to reticulate, then the Config/reticulate works with the following .onLoad.\n.onLoad &lt;- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n\n  reticulate::source_python(system.file(\"python/py_functions.py\", package = 'packagename'), envir = globalenv())\n\n}\nSee some initial work I’ve done sorting this out.\n\n\n\nThe above method works to expose python functions, but the specific ones I have take dicts and lists as arguments. How do I pass those from R? We can’t just assign them to a variable in R, because those formats don’t work- e.g. we cannot create the lists and dicts in R to pass.\n\noutputType = ['summary', 'all']\n\nallowance ={'minThreshold': MINT, 'maxThreshold': MAXT, 'duration': DUR, 'drawdown': DRAW}\n\nI have sorted out how to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. See my testing of type-passing (well, if I could get it to render in quarto).\n\n\n\nIs there a way to auto-build the Config/reticulate part of DESCRIPTION? Maybe from pyproject.toml? Similar to the way usethis::use_package installs automatically add them to renv? That’s the other way though, so maybe it’s moot. Still, would be nice to automate."
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html#other-useful-sites",
    "href": "RpyEnvs/py_r_project_overview.html#other-useful-sites",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "https://stackoverflow.com/questions/72185273/reticulate-fails-automatic-configuration-in-r-package\nhttps://github.com/rstudio/reticulate/issues/997\nWill need to test how this works both when we already have a venv and when we don’t."
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html#notes",
    "href": "RpyEnvs/py_r_project_overview.html#notes",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "as I’m developing\nThis works from console, after I put the file in the inst/python\nreticulate::source_python(file.path('inst', 'python', 'controller_functions.py'))\ntested with\nscene_namer(file.path('inst', 'extdata', 'testsmall'))\nAt first it didn’t work to have the Config/retictulate in description and the source_python in .onLoad. I didn’t get anything. Then I fixed it by adding it to the package’s global environment with the argument envir = globalenv(), and it started working. But only when I already have a python environment. If I try to get the Config/reticulate to build a python environment with the necessary dependencies in a bare project, it won’t even install the package.\nI think the issue there is that the way the config/reticulate and onload are set up, it doesn’t try to install dependencies until Python is used, and for some reason running that file in onLoad doesn’t trigger it.\nSo, somehow I need to be able to install the package without needing the py dependencies, and then install them the first time someone types library(packagename) or otherwise tries to use it.\nThat’s a whole new thing to figure out, I think.\nIf we assume the user has the python dependencies installed and accessible to reticulate, then the Config/reticulate works with the following .onLoad.\n.onLoad &lt;- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n\n  reticulate::source_python(system.file(\"python/py_functions.py\", package = 'packagename'), envir = globalenv())\n\n}\nSee some initial work I’ve done sorting this out."
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html#passing-types",
    "href": "RpyEnvs/py_r_project_overview.html#passing-types",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "The above method works to expose python functions, but the specific ones I have take dicts and lists as arguments. How do I pass those from R? We can’t just assign them to a variable in R, because those formats don’t work- e.g. we cannot create the lists and dicts in R to pass.\n\noutputType = ['summary', 'all']\n\nallowance ={'minThreshold': MINT, 'maxThreshold': MAXT, 'duration': DUR, 'drawdown': DRAW}\n\nI have sorted out how to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. See my testing of type-passing (well, if I could get it to render in quarto)."
  },
  {
    "objectID": "RpyEnvs/py_r_project_overview.html#question",
    "href": "RpyEnvs/py_r_project_overview.html#question",
    "title": "Overview of bilingual python-R projects",
    "section": "",
    "text": "Is there a way to auto-build the Config/reticulate part of DESCRIPTION? Maybe from pyproject.toml? Similar to the way usethis::use_package installs automatically add them to renv? That’s the other way though, so maybe it’s moot. Still, would be nice to automate."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html",
    "href": "RpyEnvs/py_in_rpkg.html",
    "title": "Wrapping python in an R package",
    "section": "",
    "text": "I’ve been slowly sorting this out with trial and error, but haven’t really come up with a satisfactory solution. I have things that work, but they’re not ideal- often either make the user do too much, or clutter up the global environment with python objects as soon as we library in the package. So I’ve created a test repo to build a package and test different options.\nWhat do I want? There are potentially a few different things we might want to do, and I may or may not get to all of them.\n\nAccess functions in an existing python package\nAccess a small set of functions in inst/python in the R package\n\nOne option here is clearly to make these a py package and revert to (1)\n\nThe function reticulate::import_from_path just imports a module like reticulate::import, it just does so from a local path, and so these are functionally equivalent.\n\nThese may only use base python (simplest, but unlikely)\nThese may depend on python packages (most likely)\n\nAuto-install (or at least help the user install) necessary python dependencies"
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#dev-setup",
    "href": "RpyEnvs/py_in_rpkg.html#dev-setup",
    "title": "Wrapping python in an R package",
    "section": "Dev setup",
    "text": "Dev setup\nWe need a python environment for the package (needed for dev, we’ll sort out how they get configured for users below). I’ve gone with just poetry init and poetry install in the git repo/ outer project directory, which doesn’t create a full python package structure but will at least create a .venv, which is all I really need."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#wrappers-to-keep-r-clean",
    "href": "RpyEnvs/py_in_rpkg.html#wrappers-to-keep-r-clean",
    "title": "Wrapping python in an R package",
    "section": "Wrappers to keep R clean",
    "text": "Wrappers to keep R clean\nOne option might be instead of import ing in .onLoad to import within R wrapper functions. Then the python never ends up in the user’s environment. And we can document the functions in R (via the wrappers). And, we can deprecate py functions through time by simply re-writing them into the R wrappers (this applies mostly to setup-type functions, not big things like all of scipy or something). But, does that yield weird things when the package is loaded?\nThis works- writing and exporting an R function that does the onload, e.g.\nadder_wrap &lt;- function(x,y) {\n  adder &lt;- reticulate::import_from_path(\"adder\",\n                                        path = system.file(\"python\",\n                                                           package = 'PyInRpkg'),\n                                        delay_load = TRUE)\n  adder$adder(x,y)\n}\nkeeps everything clean, and we can document and test the function in R. The downside is that the first time we run that function, it is really slow. Likely on the same order as the increased time library(package) takes when the import is on .onLoad). It does get much faster subsequently, so this may not be a huge issue, though it’s possible if we were calling it thousands of times there would be significant overhead (but then maybe that would suggest a different flow, writing whatever loop in python and import ing that loop.\nThe other advantage here is that in more complex situations, where we need to pass objects of more complex types (dates, lists, dicts), we can control that in the wrapper function, letting the user pass in standard R types and converting them, rather than make the user know that named lists become dicts, as they would need to when the adder module itself is exposed."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#functions-with-py-dependencies",
    "href": "RpyEnvs/py_in_rpkg.html#functions-with-py-dependencies",
    "title": "Wrapping python in an R package",
    "section": "Functions with py dependencies",
    "text": "Functions with py dependencies\nI’m not super interested in just wrapping a python package. But what I will need to do is have the R-package-specific python depend on other python packages; it won’t be as simple as adder.\nThat means we have to do two things- managing a python environment and checking to see if the methods above (both through .onLoad or in wrapper functions) work to import python modules with dependencies.\nIn any case, the python module that comes with the package should have functions that do all the python interaction; e..g. we don’t really want to be passing in and out of python a bunch. That makes things like method chaining work how they’re supposed to.\nI think I’ll use pandas to test since it’s common, and will be a bit more interesting/complex than numpy, for example.\nThat works just as before- if we have a function in /inst/python/pdsummary.py,\nimport pandas as pd\n\ndef pdsummary(df, group):\n  summarydf = df.groupby(group).mean()\n  return(summarydf)\nWe can use .onLoad to get it into the global environment and pass it R dataframes,\n.onLoad &lt;- function(libname, pkgname) {\n  pdsummary &lt;&lt;- reticulate::import_from_path(\"pdsummary\",\n                                         path = system.file(\"python\",\n                                                            package = 'PyInRpkg'),\n                                         delay_load = TRUE)\n}\nand it works as before with the module$method notation\npdsummary$pdsummary(iris, 'Species')\nIf I have a few functions in the module, can I import the module at the top of a .R with a bunch of wrappers, and then use it in each? Or does that load immediately?\nIf we make a few py functions in one file, some of which depend on pandas\nimport pandas as pd\n\ndef pdmean(df, group):\n  df = df.groupby(group).mean()\n  return(df)\n  \ndef pdvar(df, group):\n  df = df.groupby(group).var()\n  return(df)\n\ndef pdselect(df, cols):\n  df = df[cols]\n  return(df)\n  \ndef divide(x,y):\n  return(x/y)\nThen we can import in .onLoad and have access to all of them with multipy$NAME\n.onLoad &lt;- function(libname, pkgname) {\n  multipy &lt;&lt;- reticulate::import_from_path(\"multipython\",\n                                         path = system.file(\"python\",\n                                                            package = 'PyInRpkg'),\n                                         delay_load = TRUE)\n}\nAnd we can use them\nmultipy$pdmean(iris, 'Species')\nmultipy$pdvar(iris, 'Species')\nmultipy$pdselect(iris, c('Species', 'Sepal.Length'))\nmultipy$divide(10, 5)\n\nWrapping in various ways\nAnd, as before, we can do the import inside wrapper functions, and then it doesn’t clutter up the world and only gets imported on first use. We have to @export them, and we can cheat a bit and use the … as the argument. This is a bit dangerous- the user needs to know the arguments to the python function, since they won’t really end up documented in the R wrapper. It’s not best practice, but it does work.\n#' wrap pandas grouped mean\n#'\n#' @param df\n#' @param group\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_mean &lt;- function(df, group) {\n  multipy &lt;- reticulate::import_from_path(\"multipython\",\n                                           path = system.file(\"python\",\n                                                              package = 'PyInRpkg'),\n                                           delay_load = TRUE)\n  return(multipy$pdmean(df, group))\n}\n\n#' wrap pandas grouped var\n#'\n#' @param ... df, character group column\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_var &lt;- function(...) {\n  multipy &lt;- reticulate::import_from_path(\"multipython\",\n                                           path = system.file(\"python\",\n                                                              package = 'PyInRpkg'),\n                                           delay_load = TRUE)\n  return(multipy$pdvar(...))\n}\n\n#' wrap pandas column select\n#'\n#' @param ... df, columns to select as character vector\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_select &lt;- function(...) {\n  multipy &lt;- reticulate::import_from_path(\"multipython\",\n                                           path = system.file(\"python\",\n                                                              package = 'PyInRpkg'),\n                                           delay_load = TRUE)\n  return(multipy$pdselect(...))\n}\n\n#' Wrap python divide\n#'\n#' @param ... two values\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_divide &lt;- function(...) {\n  multipy &lt;- reticulate::import_from_path(\"multipython\",\n                                           path = system.file(\"python\",\n                                                              package = 'PyInRpkg'),\n                                           delay_load = TRUE)\n  return(multipy$divide(...))\n}\nIt still is slow on the first use and fast on subsequent, even when calling different functions.\nwrap_mean(iris, 'Species')\nwrap_var(iris, 'Species')\nwrap_select(iris, c('Species', 'Sepal.Length'))\nwrap_divide(10,5)\nIf we don’t want the import inside each function, we can put it at the head of the Rscript. In the demo package, I demo this with a new set of python functions in multi2.py\nimport pandas as pd\n\ndef pdmin(df, group):\n  df = df.groupby(group).min()\n  return(df)\n  \ndef pdsum(df, group):\n  df = df.groupby(group).sum()\n  return(df)\n  \ndef minus(x,y):\n  return(x-y)\nAnd then wrap it with the import outside the functions\nmulti2 &lt;- reticulate::import_from_path(\"multi2\",\n                                       path = system.file(\"python\",\n                                                          package = 'PyInRpkg'),\n                                       delay_load = TRUE)\n\n#' Wrap pandas group sd\n#'\n#' @param df\n#' @param group\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_min &lt;- function(df, group) {\n  return(multi2$pdmin(df, group))\n}\n\n#' Wrap pandas group sum\n#'\n#' @param df\n#' @param group\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_sum &lt;- function(df, group) {\n  return(multi2$pdsum(df, group))\n}\n\n#' wrap python subtraction\n#'\n#' @param df\n#' @param group\n#'\n#' @return\n#' @export\n#'\n#' @examples\nwrap_minus &lt;- function(df, group) {\n  return(multi2$minus(df, group))\n}\nAnd that still works\nwrap_min(iris, 'Species')\nwrap_sum(iris, 'Species')\nwrap_minus(5,4)\nJust like before, that is slow the first time, and fast subsequently. It does not put the imported module into the R environment, so as far as I can tell, it’s equivalent to when we put the module import in each function. It’s just a bit cleaner to only have the import line once.\nThat made me think maybe we could do the import with &lt;- instead of &lt;&lt;- in .onLoad, and then use that in functions, but it doesn’t work that way, unfortunately."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#setup",
    "href": "RpyEnvs/py_in_rpkg.html#setup",
    "title": "Wrapping python in an R package",
    "section": "setup",
    "text": "setup\nTo make this easier on myself, the commands to set this up after I create the project are\n\nrenv::install('devtools')\n\nThe following package(s) will be installed:\n- devtools [2.4.5]\nThese packages will be installed into \"C:/Users/galen/AppData/Local/R/cache/R/renv/library/galen_website-f20f9fdb/R-4.3/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing devtools ...                       OK [linked from cache]\nSuccessfully installed 1 package in 25 milliseconds.\n\n# I have the package in /Documents, so.\ndevtools::install_local('~/py_in_rpkg')\n\nWarning in normalizePath(path.expand(path), winslash, mustWork):\npath[1]=\"C:\\Users\\galen\\Documents/py_in_rpkg\": The system cannot find the file\nspecified\n\n\nError : Could not copy `C:\\Users\\galen\\Documents\\py_in_rpkg` to `C:\\Users\\galen\\AppData\\Local\\Temp\\RtmpGkFJ2y\\file59d028c51dc1`\n\nlibrary(PyInRpkg)"
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#no-configreticulate-no-preexisting-python",
    "href": "RpyEnvs/py_in_rpkg.html#no-configreticulate-no-preexisting-python",
    "title": "Wrapping python in an R package",
    "section": "No Config/reticulate , no preexisting python",
    "text": "No Config/reticulate , no preexisting python\nInstall works, library works.\nRunning adder_wrap(1,3) kicks off a conda install of a bunch of python stuff. It then insalls a bunch of python and packages, and while the original adder_wrap call gets lost, a new one works fine.\nWhen I try to use wrap_min, though, I get\nError: Error loading Python module multi2\nThis is a cryptic error, I think because the wrap_min function is in the R file that uses import_from_path at the head of the R file. If I use wrap_mean, which is in the R file that uses import_from_path inside each wrapper function, I get\nError in py_module_import(module, convert = convert) :    ModuleNotFoundError: No module named 'pandas'\nThat’s more helpful.\nI think before I move on to trying to manage the package dependencies with Config/reticulate or pre-loading, I want to try two things-\n\nJust closing and reopening- does the whole python install have to happen again?\n\nNo.\n\nStarting another clean directory- does it grab the python env we just built, or does it build a new one per-project?\n\nIt builds a new one, but MUCH faster- must be cross-linking packages and/or python itself"
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#pre-setting-py-env",
    "href": "RpyEnvs/py_in_rpkg.html#pre-setting-py-env",
    "title": "Wrapping python in an R package",
    "section": "Pre-setting py env",
    "text": "Pre-setting py env\nInstead of starting blank, what if I manually control the python environment by initializing a poetry project? e.g. go create a poetry project poetry new prepoetry, then cd prepoetry, poetry add pandas to create a .venv with pandas installed. Then create the R project in prepoetry directory.\nNow, when I use adder_wrap(1,5), it works without building any new python envs. As usual, the first run is slow, later are fast.\nAnd both wrap_min(iris, 'Species') and wrap_mean(iris, 'Species') work (import_from_module external and internal to the functions, respectively). No additional python building has to happen. First run is slow (even when adder_wrap has been used), but later are fast, even across R functions in different files, as here. So it must have to do some behind the scenes python the first time we use pandas."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#using-configreticulate",
    "href": "RpyEnvs/py_in_rpkg.html#using-configreticulate",
    "title": "Wrapping python in an R package",
    "section": "Using Config/reticulate",
    "text": "Using Config/reticulate\nIn theory, including a Config/reticulate: entry in the DESCRIPTION should handle the dependencies. I think those docs also say that we will need a .onLoad with reticulate::configure_environment to get it to work.\nFirst, add\nConfig/reticulate:\n  list(\n    packages = list(\n      list(package = \"pandas\")\n    )\n  )\nTo the description. I’m ignoring the version and pip arguments for now.\nI’ll need to do a few tests here-\n\nBare directory, no py environments\nExisting py environments with the dependencies already\nExisting py environments without the right dependencies\n\nI’m starting without the reticulate::configure_environment in .onLoad so I can see why/when it’s necessary."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#bare-no-py",
    "href": "RpyEnvs/py_in_rpkg.html#bare-no-py",
    "title": "Wrapping python in an R package",
    "section": "Bare, no py",
    "text": "Bare, no py\nEverything works, even without the reticulate::configure_environment. It installs python and pandas on the first use of a function, and the adder_wrap, wrap_min, and wrap_mean all work."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#existing-py-env-with-pandas",
    "href": "RpyEnvs/py_in_rpkg.html#existing-py-env-with-pandas",
    "title": "Wrapping python in an R package",
    "section": "Existing py env with pandas",
    "text": "Existing py env with pandas\nAs above, set up a .venv with poetry , then put an R project in it. Everything works, but it worked before too, since there’s no python configuring that needs to happen."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#existing-py-env-without-pandas",
    "href": "RpyEnvs/py_in_rpkg.html#existing-py-env-without-pandas",
    "title": "Wrapping python in an R package",
    "section": "Existing py env without pandas",
    "text": "Existing py env without pandas\nI’ll do the same thing as above, but build a .venv with numpy but not pandas.\nIn this case, as soon as we use any function, it installs pandas into the .venv, just as it did before for both python and pandas. All the functions work.\nSo, what is the point of configure_environment? I thought it was to deal with this? Maybe it’s that I need it to alter python once I’m already using it in a session? E.g. we have a python version running with reticulate for something else, and we then want to use this package? I guess I’ll try that."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#active-in-session-python-without-pandas",
    "href": "RpyEnvs/py_in_rpkg.html#active-in-session-python-without-pandas",
    "title": "Wrapping python in an R package",
    "section": "Active in-session python without pandas",
    "text": "Active in-session python without pandas\nI’ll start a session in a project with numpy, and use reticulate to do some simple numpy to get python running,\nnp &lt;- reticulate::import('numpy')\nnp$array(list(c(1, 2, 3, 4), c(5, 6, 7, 8), c(9, 10, 11, 12)))\nThen I’ll install the package.\nWhen I try to use it, adder_wrap works, but the pandas-based functions don’t- same errors as earlier, and it hasn’t added pandas to the venv.\n\nusing .onLoad\nIf I add a .onLoad function to zzz.R, e.g.\n.onLoad &lt;- function(libname, pkgname) {\n  reticulate::configure_environment(pkgname)\n}\nThen it works. I can load up numpy as before, but as soon as I library(PyInRpkg), it installs pandas, and then the functions work.\nSo, that seems to be the way to avoid weird runtime pitfalls in interactive sessions. It’s a particular case, but likely a fairly common one, so seems like the way to go.\nDoes having that in .onLoad affect any of the other ways? e.g. bare projects or preexisting but inactive python environments? Seems to work everywhere."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#passing-types",
    "href": "RpyEnvs/py_in_rpkg.html#passing-types",
    "title": "Wrapping python in an R package",
    "section": "Passing types",
    "text": "Passing types\nWe might need to call functions with arguments of a specific type (lists, dicts) that are not necessarily the same between languages. This is where wrapper functions can be very helpful. See my testing of type-passing."
  },
  {
    "objectID": "RpyEnvs/py_in_rpkg.html#python-venv-locations",
    "href": "RpyEnvs/py_in_rpkg.html#python-venv-locations",
    "title": "Wrapping python in an R package",
    "section": "Python venv locations",
    "text": "Python venv locations\nI have run into issues with reticulate finding the right virtual env, especially if it’s not in the top-level of the package. In that case, we need to be careful and control more things manually with environment variables and .Rprofile.\n\nSys.setenv(RETICULATE_PYTHON = 'path/to/venv') (otherwise it will try to install conda and barf)\n\nI would have thought I’d have to do this before the library, but it seems to work.\nBetter is to have a path to venv in .Rprofile, anyway\nNOTE sometimes if the .venv is in the outer directory, I’m getting weird errors when I try to Sys.setenv or otherwise set the path to the venv (it’s either prepending ~/virtualenvs or C::/ unless I pass a full fixed path. Seems to work in that case to just not set the environment variable though, and {reticulate} sorts it out correctly."
  },
  {
    "objectID": "RpyEnvs/end_run_renv.html",
    "href": "RpyEnvs/end_run_renv.html",
    "title": "End-running renv",
    "section": "",
    "text": "I use {renv} to manage packages, but it is not uncommon for renv::restore to fail if enough time has passed. Maybe a package no longer exists. Maybe the package versions don’t work with the current R version (most common). We can use rig to downgrade R for that project, but lately that is failing with new versions of Rstudio. And sometimes we just want to install all the packages in a project, but all-new versions. If everything worked right, renv::update() should do that, but not if the packages haven’t been successfully restore()d.\nWe can stil use renv to solve this. Get the dependencies, make them a character vector, and install them. That’s way less work than my old method of running renv::status(), seeing what it said was missing, and installing, especially since that method included dependencies, and so had a lot of needless installs that would have happened with the outer package.\n\ndeps &lt;- renv::dependencies()\nchardeps &lt;- unique(deps$Package)\n\n# I really don't want to run this here, and only partially trust eval: false\n# renv::install(chardeps)\n\nSome end up failing anyway. To find out where those are used, use\n\ndeps[which(deps$Package == 'FAILEDPACKAGENAME'), ]\n\nThe failures are annoying because they don’t just fail that one, they fail everything and then we have to start over. One dumb solution that fixes this would be to loop over each package in turn, and save its name if it fails. I have code to do that, but will need to find it."
  },
  {
    "objectID": "RpyEnvs/R_py_type_passing.html",
    "href": "RpyEnvs/R_py_type_passing.html",
    "title": "R-py type passing",
    "section": "",
    "text": "This runs fine interactively, but when I render, everything fails. The python chunks lose what was in the previous ones, and the references to py$ in R chunks all fail. For now I’m going to just skip errors and have this not render correctly, which is unsatisfying. Not much more I can do though until I sort out the NameError issue, which is tricky because it doesn’t replicate consistently."
  },
  {
    "objectID": "RpyEnvs/R_py_type_passing.html#note--quarto-renders-fail",
    "href": "RpyEnvs/R_py_type_passing.html#note--quarto-renders-fail",
    "title": "R-py type passing",
    "section": "",
    "text": "This runs fine interactively, but when I render, everything fails. The python chunks lose what was in the previous ones, and the references to py$ in R chunks all fail. For now I’m going to just skip errors and have this not render correctly, which is unsatisfying. Not much more I can do though until I sort out the NameError issue, which is tricky because it doesn’t replicate consistently."
  },
  {
    "objectID": "RpyEnvs/R_py_type_passing.html#passing-types",
    "href": "RpyEnvs/R_py_type_passing.html#passing-types",
    "title": "R-py type passing",
    "section": "Passing types",
    "text": "Passing types\nI have a python function that takes dicts and lists as arguments. How do I pass those from R? We can’t just assign them to a variable in R, because those formats don’t work- e.g. we cannot create the lists and dicts in R to pass.\n\n```{r}\n#| eval: false\noutputType = ['summary', 'all']\n\nallowance ={'minThreshold': MINT, 'maxThreshold': MAXT, 'duration': DUR, 'drawdown': DRAW}\n```\n\nSo, let’s write a function to tell me the type of what I’m passing and try a few things.\n\n```{python}\ndef test_type(testarg):\n  \n  return(type(testarg))\n```\n\nIs a named list a dict or a list? what about just a c()? Is that a list?\n\n```{r}\nrlist &lt;- list(dict1 = 100, dict2 = 'testing')\nrc &lt;- c(100, 50)\n```\n\nThe named list is a dict. Look at it in python and R.\n\n```{python}\ntest_type(r.rlist)\n```\n\n&lt;class 'dict'&gt;\n\n\n\n```{r}\npy$test_type(rlist)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\nThe c() is a list- but see below- this fails if it’s length-one\n\n```{r}\npy$test_type(rc)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\nAn unnamed list is a list\n\n```{r}\nrlistu = list(100, 'testunname')\npy$test_type(rlistu)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\nThat is useful, since creating a length-one list doesn’t work with single values or c() wrapping single values\n\n```{r}\nrone = 100\npy$test_type(rone)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\n\n```{r}\nronec = c('testingc')\npy$test_type(ronec)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found\n\n\nWe do get a length-one list with an unnamed list of length 1.\n\n```{r}\nronel = list(100)\npy$test_type(ronel)\n```\n\nError in eval(expr, envir, enclos): object 'py' not found"
  },
  {
    "objectID": "RpyEnvs/R_py_shared_projects.html",
    "href": "RpyEnvs/R_py_shared_projects.html",
    "title": "R and python envs in same project",
    "section": "",
    "text": "Both renv and poetry want to set up project structures and work within them. And I want to do both in the same git repo because I’m using both for the same project. And have access to both everywhere within the project (ie I want to be able to use reticulate, but more importantly I want the different parts of the project to be able to have both py and R components.\nI’ve done a bit getting them to work in the same script, and setting up clean python environments with poetry. With the shared scripts, I did it with a subdirectory, and that sort of worked for testing, but won’t work for a project where they’re both used a fair amount and in both places.\n\n\nCan I just initiate them both in the base git directory? I know i can with renv, but does poetry let us stick a project in an existing repo? When I was sorting out poetry, I always made a new dir with poetry new dirname.\nIt looks like py code should be in the inner directory of the poetry structure. Let’s assume that. Which roughly matches R structure, where we’ll have code in an R/ dir if it’s a package or in some other dir structure. IE, if we can just get the environment management into the outer dir of the repo, and then all other code inside. I’m not sure though that I’ll want to split py from R at present. Think about that.\nSo, really, the question is whether I can poetry new and poetry install in a dir that already exists.\nMaybe poetry init instead of poetry new? Asks a bunch of questions.\nIt creates a pyproject.toml file, and then poetry install creates the poetry.lockand .venv, but the rest of the structure’s not there (tests dir, second level of the project dir). Will it work? Probably. Do we want that structure? Probably.\n\n\n\nSo, maybe better to poetry new somewhere else and drag over, then poetry install. Does that work? I copied over everything inside the outer dir, since I want the whole project to share the outer dir. It makes the lock and venv, but I get ‘dirname does not contain any element’. I’m guessing because I made a poetry env with a different name. Try using the same name, then again copying over the internals.\nThat seems to work. Now to build the env so everything actually works. But that’s about project details, so I’ll leave this here.\nThat seemed to have made vs code happy- it can find a venv in the workspace and use it. It didn’t do that automatically when the venv was in a subdir. (I had to command palette- select python interpreter)."
  },
  {
    "objectID": "RpyEnvs/R_py_shared_projects.html#the-issue",
    "href": "RpyEnvs/R_py_shared_projects.html#the-issue",
    "title": "R and python envs in same project",
    "section": "",
    "text": "Both renv and poetry want to set up project structures and work within them. And I want to do both in the same git repo because I’m using both for the same project. And have access to both everywhere within the project (ie I want to be able to use reticulate, but more importantly I want the different parts of the project to be able to have both py and R components.\nI’ve done a bit getting them to work in the same script, and setting up clean python environments with poetry. With the shared scripts, I did it with a subdirectory, and that sort of worked for testing, but won’t work for a project where they’re both used a fair amount and in both places.\n\n\nCan I just initiate them both in the base git directory? I know i can with renv, but does poetry let us stick a project in an existing repo? When I was sorting out poetry, I always made a new dir with poetry new dirname.\nIt looks like py code should be in the inner directory of the poetry structure. Let’s assume that. Which roughly matches R structure, where we’ll have code in an R/ dir if it’s a package or in some other dir structure. IE, if we can just get the environment management into the outer dir of the repo, and then all other code inside. I’m not sure though that I’ll want to split py from R at present. Think about that.\nSo, really, the question is whether I can poetry new and poetry install in a dir that already exists.\nMaybe poetry init instead of poetry new? Asks a bunch of questions.\nIt creates a pyproject.toml file, and then poetry install creates the poetry.lockand .venv, but the rest of the structure’s not there (tests dir, second level of the project dir). Will it work? Probably. Do we want that structure? Probably.\n\n\n\nSo, maybe better to poetry new somewhere else and drag over, then poetry install. Does that work? I copied over everything inside the outer dir, since I want the whole project to share the outer dir. It makes the lock and venv, but I get ‘dirname does not contain any element’. I’m guessing because I made a poetry env with a different name. Try using the same name, then again copying over the internals.\nThat seems to work. Now to build the env so everything actually works. But that’s about project details, so I’ll leave this here.\nThat seemed to have made vs code happy- it can find a venv in the workspace and use it. It didn’t do that automatically when the venv was in a subdir. (I had to command palette- select python interpreter)."
  },
  {
    "objectID": "RpyEnvs/RandPython.html",
    "href": "RpyEnvs/RandPython.html",
    "title": "Using R and python together",
    "section": "",
    "text": "```{r setup}\n#| warning: false\n#| message: false\n\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#the-issue",
    "href": "RpyEnvs/RandPython.html#the-issue",
    "title": "Using R and python together",
    "section": "The issue",
    "text": "The issue\nI have a project primarily in R, but needs some python. For the big python work, I’ll have a directory with a poetry environment and python code. But I’ve run into the issue that I want to run just one or two lines of python from R. The specific case is that I have python code for extracting river gauge data, and I’ve filtered some river gauges in R for something else, and rather than do the finding of the gauges again in python, I’d rather just do the extraction in R. I think that means I have to sort out {reticulate}, but also how to point reticulate at my python environment. The situation I have is a poetry project inside a directory with an Rproj (which probably needs to be split up, but it’s what I have now).\nMy python_setup sets up a very similar situation, so let’s see if I can use it.\nI’m going to try to remember to always put\nexecute:\n  echo: fenced\nin the yaml headers of mixed r-python notebooks, so it’s clear which chunks are which language (though we can usually tell by the code).\n\nAdditional issues\nThis all worked, but then stopped- python chunks separated by R chunks couldn’t share objects. This particular notebook still ran, but others would run fine interactively but when rendered would throw errors about “name ‘pyobjname’ is not defined”. I tried making sure jupyter was in the python env and set engine: knitr in yaml headers, since that’s what the help suggested. And I set the QUARTO_PYTHON environment variable in an _environments file, since that helped me previously. It’s unclear why it worked previously.\nIDE NOTE The above may be due in part because Rstudio uses the reticulate python REPL for python chunks, and so they are available to R. Quarto itself does as well. But VScode does not, and so python and R chunks are run wholly independently of each other and we can’t just pass objects between them as we do here in interactive notebooks. Instead, we need to run the python through reticulate in R chunks., rather than interactive python in the REPL. This is usually fine, but becomes a pain when we want to stay in python for a while, e.g to run something, process it, run something else, and then get it back in R."
  },
  {
    "objectID": "RpyEnvs/RandPython.html#set-up-reticulate-from-r",
    "href": "RpyEnvs/RandPython.html#set-up-reticulate-from-r",
    "title": "Using R and python together",
    "section": "Set up reticulate from R",
    "text": "Set up reticulate from R\nPoint reticulate at the venv. See stackoverflow. This seems to not be necessary if the .venv is in the outer project directory. Or if we’ve set the RETICULATE_PYTHON environment variable elsewhere (like .Rprofile).\n```{r}\nreticulate::use_virtualenv(file.path('RpyEnvs', 'pytesting', 'Scripts', '.venv'), required = TRUE)\n```\nIf this is more than a one-off and you’re using an R project, it’s usually better to set the RETICULATE_PYTHON environment variable in .Rprofile. Here, that means adding this line in .Rprofile . This has the added bonus of stopping Rstudio/R throwing warnings about conda at startup when it detects {reticulate} being used in the project.\n```{r}\nSys.setenv(RETICULATE_PYTHON = file.path('RpyEnvs', 'pytesting', '.venv'))\n```\nSee Quarto notes for some similar issues for different python-related env variables.\nLoad the library. Interestingly, the python code chunks will run without loading the library, but I can’t access their values using py$pythonobject unless I load it.\n\n```{r}\nlibrary(reticulate)\n```"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#r",
    "href": "RpyEnvs/RandPython.html#r",
    "title": "Using R and python together",
    "section": "R",
    "text": "R\nFirst, let’s create some things in R.\n\n```{r}\na &lt;- 1\nb &lt;- 2\n```"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#python",
    "href": "RpyEnvs/RandPython.html#python",
    "title": "Using R and python together",
    "section": "Python",
    "text": "Python\nDoes not just inherit the values from R, but runs.\n\n```{python}\na = 1\nb = 2\na+b\n```\n\n3\n\n\nDo I have access to packages? Yes.\n\n```{python}\nimport numpy as np\n\nx = np.arange(15, dtype=np.int64).reshape(3, 5)\nx[1:, ::2] = -99\nx\n```\n\narray([[  0,   1,   2,   3,   4],\n       [-99,   6, -99,   8, -99],\n       [-99,  11, -99,  13, -99]], dtype=int64)\n\n\nDoes access to python objects persist? Yes\nThough in lot of other docs, this has proved to be super unstable, and fails intermittently\n\n```{python}\nx.max(axis=1)\n```\n\narray([ 4,  8, 13], dtype=int64)"
  },
  {
    "objectID": "RpyEnvs/RandPython.html#moving-data-back-and-forth",
    "href": "RpyEnvs/RandPython.html#moving-data-back-and-forth",
    "title": "Using R and python together",
    "section": "Moving data back and forth",
    "text": "Moving data back and forth\n\nPython to R\nCan I access objects with R? Yes, but not quite directly. Have to use the py$pythonObject notation. But only if I’ve loaded library(reticulate) or specified with reticulate::py. That’s a pain, so probably almost always better to load the library. Even though the python chunks run fine without explictly loading it, I can’t seem to access py without loading it.\n\n```{r}\n# x\nreticulate::py$x\n```\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    2    3    4\n[2,]  -99    6  -99    8  -99\n[3,]  -99   11  -99   13  -99\n\n\n\n\nR to python\nSimilar to python objects being in py, R objects are in r, and are accessed with . instead of $.\n\n```{r}\nc &lt;- 17\n```\n\nInterestingly, the r. notation to get R into python does not need reticulate:: on it. Which I guess makes some sense- this block is actually running in python and python doesn’t know what reticulate is. But it does know what r. is, somehow. Pretty cool.\n\n```{python}\nr.c + b\n```\n\n19.0\n\n\n\n\nPython-R-python and NameErrors\nElsewhere I’m running into a new issue of getting “NameError: name ‘pyobjectname’ is not defined” when I try to access an object defined in a previous python chunk in a later python chunk. It seems to be worse when there’s an R chunk in the middle. It doesn’t seem to be happening here, since the preceding chunk could get at b, which was defined way earlier.\nDoes the issue happen when R has touched it? YES. This all runs fine interactively, but when I try to render, I get “Error in py_call_impl(callable, dots$args, dots$keywords) : NameError: name ‘b’ is not defined” in the python chunk.\n\n```{python}\nb + a\n```\n\n3\n\n\n\n```{r}\nrb &lt;- py$b + c\nrb\n```\n\n[1] 19\n\n\n\n```{python}\nb + a\n```\n\n3"
  },
  {
    "objectID": "RpyEnvs/managingprivate.html",
    "href": "RpyEnvs/managingprivate.html",
    "title": "Private data and website",
    "section": "",
    "text": "I’m getting set up to use github pages to host a website. But some content I (might) host needs to be private. A clear option is to simply mock-up data matching that private data, and that’s the way I’ll go. But because a large part of the content here will be sorting through issues, the initial sort-through will likely depend on figuring out what it is about the private data that needs to be mocked-up, and some portion of the testing will depend on that data. And I want all of that to be version controlled, but not shared publicly. In short, I need a private development location, and then go through, make a clean version based on mocked-up data, and publish that. So, how?\nThe first thing that came to mind is to just have a local-only branch that I keep private. I could have a private/ folder, where I do dev, with that folder ignored in the master .gitignore. And have a different .gitignore in another branch so development would be tracked in that other branch. Then, as things were ready to make public, I could just drop them over. However, because github requires the whole github pages repo to be public, I would never be able to push this branch. Sure, people would be unlikely to poke around in it, but it would all be there. And if I ever forgot the process, I would expose things. And I don’t want to lose a cloud-hosted version- only storing locally isn’t so great, even if it is backed up or dropboxed.\nI’m now leaning towards having a second, private repo for development, and then drag and drop into the github pages repo once the doc I’m working on is clean. That’s basically the same idea as the internal private/ folder, but as a whole different repo, and so could actually be held as a private repo on github. There are two main catches that I can see with this approach at the outset-\n\nThe actual development history of files won’t be available on the public repo. That’s kind of the point, but it is a bit annoying\nKeeping the two repos synced will be a pain. If I make a small change to a file that’s already public in the public repo, I’d need to get it back into the other. The obvious solution is to do everything in the private, and then move things over. But if I make a small change to a file that’s already moved to the public repo in the private repo, I need to make sure it moves.\n\nI think I’ve dealt with this issue before, but can’t remember the details. I had a repo as a fork of another that was upstream or something. Will need to sort that out. It’s essentially a repo-diff, but needing to check what should be diff (still private), vs. what shouldn’t be a diff (a change that needs to move over).\n\n\nIs there a better solution that allows building from somewhere other than github pages, and so could use a private repo? Netlify would work. And might be better anyway. But if the whole point is to make messy code public, then we want it on a public repo, right? And just hold the private stuff back/ do it elsewhere."
  },
  {
    "objectID": "RpyEnvs/managingprivate.html#the-issue",
    "href": "RpyEnvs/managingprivate.html#the-issue",
    "title": "Private data and website",
    "section": "",
    "text": "I’m getting set up to use github pages to host a website. But some content I (might) host needs to be private. A clear option is to simply mock-up data matching that private data, and that’s the way I’ll go. But because a large part of the content here will be sorting through issues, the initial sort-through will likely depend on figuring out what it is about the private data that needs to be mocked-up, and some portion of the testing will depend on that data. And I want all of that to be version controlled, but not shared publicly. In short, I need a private development location, and then go through, make a clean version based on mocked-up data, and publish that. So, how?\nThe first thing that came to mind is to just have a local-only branch that I keep private. I could have a private/ folder, where I do dev, with that folder ignored in the master .gitignore. And have a different .gitignore in another branch so development would be tracked in that other branch. Then, as things were ready to make public, I could just drop them over. However, because github requires the whole github pages repo to be public, I would never be able to push this branch. Sure, people would be unlikely to poke around in it, but it would all be there. And if I ever forgot the process, I would expose things. And I don’t want to lose a cloud-hosted version- only storing locally isn’t so great, even if it is backed up or dropboxed.\nI’m now leaning towards having a second, private repo for development, and then drag and drop into the github pages repo once the doc I’m working on is clean. That’s basically the same idea as the internal private/ folder, but as a whole different repo, and so could actually be held as a private repo on github. There are two main catches that I can see with this approach at the outset-\n\nThe actual development history of files won’t be available on the public repo. That’s kind of the point, but it is a bit annoying\nKeeping the two repos synced will be a pain. If I make a small change to a file that’s already public in the public repo, I’d need to get it back into the other. The obvious solution is to do everything in the private, and then move things over. But if I make a small change to a file that’s already moved to the public repo in the private repo, I need to make sure it moves.\n\nI think I’ve dealt with this issue before, but can’t remember the details. I had a repo as a fork of another that was upstream or something. Will need to sort that out. It’s essentially a repo-diff, but needing to check what should be diff (still private), vs. what shouldn’t be a diff (a change that needs to move over).\n\n\nIs there a better solution that allows building from somewhere other than github pages, and so could use a private repo? Netlify would work. And might be better anyway. But if the whole point is to make messy code public, then we want it on a public repo, right? And just hold the private stuff back/ do it elsewhere."
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html",
    "href": "RpyEnvs/py_r_dates.html",
    "title": "Complex passing py-R",
    "section": "",
    "text": "```{r setup}\n#| warning: false\n#| message: false\n\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\nOld way of giving path to venv, new way is to set it in .Rprofile\n```{r}\n# reticulate::use_virtualenv(file.path('RpyEnvs', 'pytesting', '.venv'), required = TRUE)\n```\n```{r}\nlibrary(reticulate)\n```"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#general-problem",
    "href": "RpyEnvs/py_r_dates.html#general-problem",
    "title": "Complex passing py-R",
    "section": "General problem",
    "text": "General problem\nThe real issue here is when we bring a df into R from python and it has a list column with ‘environment’ in it. Then the conversion hasn’t really happened, and doing that conversion post-hoc has to convert every single environment, which is all rows. And that takes forever. It’s not necessarily dates (and as we can see below, sometimes they do work just fine). So, if that happens, rather than taking an eternity to purrr or otherwise go through the column to translate, just put it in something easier in py, bring it over, and put it back.\nI do wish I had a better idea about why it happened. I’m sure as it happens more I’ll start to figure it out.\nIt’s possible this has been fixed in more recent {reticulate}, and that’s why I can’t reproduce? https://github.com/rstudio/reticulate/pull/1266. No, I’m still getting the error where I was before. Still struggling to replicate it, but it does happen in the original case."
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#a-demonstration-sort-of",
    "href": "RpyEnvs/py_r_dates.html#a-demonstration-sort-of",
    "title": "Complex passing py-R",
    "section": "A demonstration (sort of)",
    "text": "A demonstration (sort of)\nSay we have a pandas dataframe with a few columns of simple types (numeric, character) and 1000 rows\n\n```{python}\nimport pandas as pd\nimport random\nimport string\n\nrandnums = [random.gauss(0, 1) for _ in range(1000)]\nallchars = list(string.ascii_lowercase) + list(string.ascii_uppercase)\n\nrandchars = [random.choice(allchars) for _ in range(1000)]\n\nsimpledf = pd.DataFrame({'rand_nums': randnums, 'rand_chars': randchars}, columns=['rand_nums', 'rand_chars'], index=range(1000))\n\nsimpledf\n```\n\n     rand_nums rand_chars\n0    -0.701866          c\n1     0.459327          y\n2    -0.261080          i\n3    -0.801544          o\n4    -1.080855          S\n..         ...        ...\n995  -0.213803          J\n996   0.808282          X\n997  -0.645825          E\n998  -0.956105          I\n999  -0.208278          Q\n\n[1000 rows x 2 columns]\n\n\nNow, we can get that into R without too much fuss using py$.\n\n```{r}\nrsimple &lt;- py$simpledf\nrsimple\n```\n\n         rand_nums rand_chars\n1    -0.7018658992          c\n2     0.4593274131          y\n3    -0.2610798051          i\n4    -0.8015437130          o\n5    -1.0808549780          S\n6    -1.6438483004          D\n7    -0.3334575577          y\n8     0.4935451879          D\n9    -0.1560796095          b\n10    0.3323185565          R\n11   -2.1908387891          Y\n12   -0.2950657487          M\n13   -0.9115783643          j\n14    0.3962070865          O\n15    2.3519102656          X\n16    0.4391649467          d\n17    0.7300874167          Z\n18    0.7413467021          F\n19   -0.3111477835          e\n20    0.9823226372          Z\n21    0.3950978754          x\n22    1.1499516641          w\n23    1.2511164152          d\n24   -2.2146034662          D\n25   -0.1336232404          C\n26   -1.8382912488          v\n27    0.7784467509          B\n28   -0.1280738737          q\n29   -0.8124590539          o\n30    0.2711110996          Q\n31   -0.7441588686          y\n32    0.1285400079          t\n33   -0.6396603485          X\n34   -0.2807885652          O\n35    1.1521418770          q\n36   -1.4038714453          u\n37    2.0806655871          l\n38    0.2761555629          T\n39   -0.1991139600          L\n40   -1.3505007980          A\n41    1.1538226352          W\n42   -1.4467217299          O\n43    0.2481680067          A\n44   -1.4942114905          Q\n45   -1.2843187303          T\n46    0.2574032552          i\n47   -0.9797651017          x\n48   -0.4170744879          V\n49    0.7153474943          i\n50    0.7533982301          o\n51   -1.3336047335          G\n52   -0.9240890286          p\n53   -1.2656653882          V\n54   -1.9762137075          A\n55   -0.3760371579          Q\n56   -0.2588065075          M\n57   -2.1176771667          S\n58   -0.7189876499          I\n59   -0.5577799281          w\n60    0.9043242108          x\n61    1.0504640126          q\n62    0.1181189650          v\n63   -0.9850789312          Y\n64    0.2231129871          a\n65    0.2864486847          C\n66    1.0981813055          L\n67   -1.2706626483          N\n68   -0.7419632459          o\n69    1.4600143060          G\n70    0.5525795882          z\n71   -0.7677861242          h\n72    0.0560519001          Q\n73   -1.3960906754          O\n74    0.6167615877          o\n75    0.4096709127          I\n76   -1.2672040841          R\n77   -0.0843148257          C\n78   -2.2520123342          y\n79    0.0549257414          U\n80   -0.4799570788          C\n81   -0.2872730118          L\n82   -0.5502245618          k\n83    1.0700368331          x\n84    0.9384796491          p\n85   -0.5022452286          f\n86    0.1083143396          P\n87    0.0573921669          K\n88   -1.8781921416          C\n89    0.0126693796          M\n90    0.4316892544          p\n91   -2.1621499857          d\n92   -1.1505331351          z\n93    0.8401515501          Q\n94   -1.0210129385          w\n95   -0.0046774511          v\n96   -0.8025082850          S\n97   -0.0402267228          U\n98    0.3381054287          Y\n99    0.7180860224          o\n100  -0.3488052118          j\n101  -1.1044635022          P\n102  -1.2168672228          V\n103  -1.4036701430          P\n104   1.0592318529          X\n105   0.5640597923          X\n106   0.1379419397          y\n107   0.1984329343          a\n108  -0.7127988170          Z\n109   0.0397090526          b\n110  -0.5361383788          Y\n111  -0.5415966538          k\n112   0.0665710922          v\n113   0.6091413451          L\n114  -0.2167627362          U\n115  -0.1413768540          K\n116  -0.2783724734          l\n117  -0.1077502242          u\n118   0.3452272609          j\n119  -0.2587823903          J\n120   0.3238092422          U\n121  -1.5146103008          e\n122   0.9270940868          i\n123   0.8081201779          G\n124   0.1739384315          O\n125   0.7349655378          T\n126   2.1613856495          i\n127   1.6893585918          g\n128  -2.4704034652          d\n129  -0.0076850908          U\n130  -1.6544088363          o\n131   1.2057116016          w\n132   0.2470774154          J\n133   0.7114776251          R\n134  -0.2780996500          W\n135   0.4467569873          x\n136  -0.6754559457          d\n137  -0.9194636989          c\n138  -0.9952852360          B\n139  -0.6729531603          x\n140  -0.5007421996          j\n141  -0.5690312844          P\n142   1.0576341934          S\n143  -0.7886859079          v\n144   0.3027513158          n\n145   1.2837863601          p\n146  -1.8530146127          K\n147  -0.8919650257          u\n148   0.0077003617          u\n149   0.5816436901          y\n150   0.8569916057          E\n151  -0.0293560235          a\n152   0.3357785277          y\n153   0.0212049402          w\n154  -1.2427557518          j\n155  -0.8331603605          h\n156   1.0701914881          n\n157  -0.3933929335          n\n158  -1.8650372275          d\n159  -0.3738399269          s\n160  -1.0910719281          R\n161  -0.3232570646          B\n162  -0.0929475239          m\n163  -0.3224133134          D\n164   1.1740269486          N\n165  -0.2324599108          I\n166  -0.3045914893          u\n167  -0.4120925975          t\n168   0.2621403939          j\n169   1.7822502922          t\n170  -0.3695158113          Y\n171   1.1083349934          R\n172   0.1726439420          y\n173   2.0872526525          U\n174  -1.1796077941          T\n175  -0.4893328620          l\n176   0.3025706297          h\n177   1.1878539582          i\n178  -0.3624111552          t\n179  -0.8275963489          F\n180  -0.9210124697          G\n181  -0.2413278292          o\n182   0.1934532986          d\n183  -0.2834687745          P\n184  -1.0510165068          T\n185   0.1193487874          t\n186   0.4634951980          M\n187  -1.4310181095          M\n188   0.9795518900          K\n189  -1.5793763703          V\n190  -0.5355509895          Z\n191   0.6777743214          Y\n192  -0.9401279161          B\n193  -0.3131691286          z\n194   1.0136157751          t\n195   2.4648592649          c\n196  -0.1123349550          u\n197  -0.7634269258          y\n198  -0.1956354007          i\n199   1.6716278233          p\n200   1.3007570727          X\n201   0.4510890219          n\n202  -1.2211868160          P\n203  -0.6623963011          q\n204   0.8491075734          i\n205  -1.0316316676          t\n206  -0.4700021285          y\n207  -1.4587971310          L\n208   1.2062958906          J\n209  -1.0015108378          s\n210  -0.8828380102          H\n211  -2.4487313918          o\n212  -0.3666984680          j\n213   0.1485483294          J\n214   1.2845450427          c\n215   0.0753673755          h\n216   0.8150718076          D\n217   1.2827426854          f\n218  -0.7907764907          Z\n219  -1.8545961636          d\n220  -0.7862898331          F\n221  -0.0136131797          o\n222  -0.3806067867          V\n223   0.0059519492          B\n224   0.6045473687          s\n225   0.8885653996          Q\n226  -0.5637310411          Z\n227  -1.2974545634          K\n228   2.0696693706          j\n229   1.7089497885          g\n230  -0.5749308926          n\n231  -0.8642215550          b\n232  -1.8486022254          F\n233   0.8348355511          A\n234   0.7227545181          A\n235   0.0712407444          P\n236  -1.6883801128          s\n237  -1.3281808014          B\n238   1.3003977211          N\n239  -0.0694278847          W\n240  -0.0533318120          e\n241   0.1166895763          Q\n242   0.9160985673          n\n243   0.8403536086          U\n244   0.9100906330          f\n245   1.5016474279          c\n246   0.1074100201          C\n247  -0.9043478058          M\n248  -0.4144098200          t\n249   1.3671461975          G\n250   0.5032642282          W\n251   0.2373176981          p\n252  -1.4901168890          m\n253  -0.2410164254          R\n254   0.2226577175          J\n255   0.7616861254          b\n256   0.5102059635          s\n257  -0.5518024425          O\n258  -0.4543078980          h\n259   0.8457439613          L\n260   1.3797927721          i\n261   0.9500353404          o\n262  -1.7429733279          r\n263   1.2737253980          N\n264  -1.7093882939          C\n265  -0.0001419687          S\n266  -1.1801914111          X\n267  -0.0863132536          d\n268  -0.9672521665          C\n269   0.1448770752          H\n270  -1.6900156374          Q\n271   0.3754274577          k\n272  -0.2244144551          B\n273   0.0372876959          B\n274   1.1215990228          N\n275  -0.9304425554          I\n276  -0.6977437815          l\n277  -1.0132417840          P\n278   0.6091060315          i\n279  -0.6247144324          k\n280  -0.3418057459          D\n281  -1.3511219191          o\n282   1.1384850041          Y\n283   0.3679698073          y\n284  -1.9193555205          I\n285  -1.0827150750          b\n286  -1.0273936215          h\n287  -2.5034002378          e\n288   0.8599332090          I\n289   0.3135843293          Y\n290   0.9317228415          W\n291   1.4799162416          q\n292   0.6007748585          X\n293   0.5440643388          J\n294  -0.0411721958          X\n295  -1.7219187279          g\n296   1.1712814126          O\n297  -2.1606195632          d\n298  -0.7123967888          j\n299   0.5941765102          O\n300   0.1989054738          K\n301  -0.3293622723          p\n302  -0.8206038871          y\n303  -0.3759277960          f\n304   1.3418036345          Z\n305  -1.1895773771          r\n306  -1.6668392401          h\n307  -0.4206255195          Z\n308  -0.6176810747          e\n309   1.9349770647          F\n310   0.3855568577          K\n311  -1.1791245291          T\n312  -0.0982526941          u\n313   2.2934638826          U\n314  -0.8891313380          V\n315   1.1280570062          D\n316  -1.0639492795          J\n317  -2.4931671907          J\n318   0.0959922720          g\n319  -1.0717750282          j\n320   0.5838197279          L\n321   0.8307962898          F\n322   0.4777511111          E\n323   0.4086220498          Y\n324  -0.9186934273          Z\n325  -1.0053313253          n\n326  -0.0355969149          T\n327   0.8468154989          P\n328  -0.0342874758          k\n329   0.0996263113          a\n330   1.6751971802          u\n331   0.7043031852          K\n332  -0.7585540801          k\n333  -0.3481285096          k\n334   0.2785079868          C\n335  -0.4529136317          m\n336  -0.1858910863          j\n337  -0.2450604225          q\n338  -0.3862497247          z\n339   0.1633145299          d\n340   1.6181009784          X\n341  -0.6936999137          f\n342  -0.7368535810          t\n343   1.3829288990          G\n344   0.5797685050          I\n345  -0.3467568034          W\n346   0.7880514740          C\n347  -1.4244107843          O\n348  -0.1759525941          K\n349   0.4468652696          H\n350   1.6543187684          P\n351  -0.8825293884          K\n352   0.7700923414          A\n353   0.0523330014          y\n354   2.4227396801          L\n355  -1.2798017420          v\n356  -0.0759638223          b\n357   0.9406846383          U\n358   1.1676627172          k\n359   0.2290247072          O\n360   0.6271988953          H\n361   1.2348048225          N\n362  -0.6089331921          k\n363  -3.0367039349          u\n364  -0.9215758573          b\n365  -0.3954085762          S\n366   0.2089812392          Q\n367   1.1441932628          m\n368   0.8347874659          Y\n369   0.6546463383          n\n370  -1.7801259700          L\n371   0.1652961277          D\n372  -1.1429959338          t\n373  -0.4582644260          d\n374  -2.7222010332          P\n375   0.2762536393          q\n376  -0.2940206749          T\n377   1.9677946055          c\n378  -1.1910594613          N\n379  -1.9949688051          Q\n380  -0.2435462391          W\n381   1.6901677658          I\n382  -0.5524715813          i\n383  -0.5935018750          J\n384   0.2718732142          h\n385   0.0840018241          F\n386   1.6520550890          I\n387   2.6950840670          T\n388  -1.3906127303          e\n389  -0.5202015995          y\n390  -0.7345347763          H\n391   0.6720119786          y\n392   0.8881815896          g\n393   1.0087944240          t\n394  -1.1083758414          I\n395  -0.7971139199          C\n396  -1.1708554183          r\n397   1.6933236869          h\n398   0.6417952947          k\n399   1.2799928452          C\n400   1.1264493892          c\n401  -0.0673396153          z\n402  -0.3778595494          i\n403  -0.1444795174          C\n404   0.1200237331          E\n405  -0.2424233042          c\n406  -0.9496102702          q\n407   0.0289361040          K\n408   0.3733227270          d\n409   1.9825327169          E\n410  -1.2380831928          C\n411   1.5706059682          x\n412   0.9341718913          F\n413   1.4288302804          X\n414  -1.0542930755          B\n415  -0.5544814964          S\n416  -0.8488481371          x\n417  -0.5414563321          S\n418   0.2735410490          k\n419   1.2333797302          F\n420  -0.7076198507          V\n421   0.3962208288          Y\n422   0.7624549396          B\n423   0.7251655574          I\n424  -0.2232489899          D\n425  -0.2041434550          j\n426  -0.0956456730          S\n427  -1.2188845683          l\n428  -1.2805469475          z\n429  -1.7733196969          t\n430   2.2136059576          h\n431   0.9154488313          D\n432   1.5545204720          P\n433  -0.0233323438          Q\n434   1.4685774838          c\n435   0.1718864434          e\n436  -0.2033664451          g\n437  -1.1185697790          a\n438  -0.9345521937          F\n439  -1.6290603792          b\n440  -0.3392908425          V\n441   0.1156334067          E\n442  -0.5322578855          A\n443  -0.2875645476          X\n444   1.1559894318          q\n445  -1.6330953661          u\n446  -0.2766105267          M\n447   0.2359445926          r\n448   2.2651616552          Q\n449  -0.0498513023          z\n450  -1.0770353853          I\n451  -0.1382972676          W\n452   0.6835743663          m\n453   1.3255346990          H\n454  -1.5266829027          N\n455  -1.1605859112          p\n456  -0.0607873609          y\n457   1.4030861610          S\n458   1.3858522692          v\n459   1.1045598901          Z\n460   1.3956986715          k\n461   0.5685687068          o\n462  -2.0811546851          S\n463  -1.9517469353          X\n464   0.8418801329          X\n465   0.2390282572          q\n466   0.3996511978          q\n467   0.3662531029          j\n468   0.0976006310          e\n469  -1.7391317948          f\n470   0.1896379830          D\n471   0.6355798725          u\n472  -0.7408925358          z\n473  -0.1458139215          w\n474  -2.0875166203          a\n475   1.3290519171          F\n476   1.1224443221          x\n477  -0.1575470986          h\n478   0.7853175970          T\n479  -0.8146137505          q\n480   0.8747225844          h\n481   1.5696707878          S\n482  -1.4033403956          N\n483   0.0471532142          B\n484   0.8531992973          r\n485  -0.3352512041          Q\n486   0.2252765840          A\n487   0.4261285304          e\n488   0.2323299958          q\n489   0.1665928049          U\n490   1.0777124141          w\n491   1.0727034939          F\n492  -0.8460144976          h\n493   0.5115132926          O\n494   0.0614492065          r\n495  -0.2888240999          o\n496   0.9928980180          d\n497  -0.6765177184          k\n498   0.7350610405          C\n499  -0.1546650354          Q\n500  -1.2230531777          L\n501  -1.4974415025          t\n502   1.4347168335          M\n503  -1.4589643611          L\n504   0.8924854925          b\n505   0.1067199132          n\n506   1.0987670446          X\n507  -1.5938476932          v\n508   0.5719518650          G\n509  -0.9206472408          r\n510   1.5703154229          k\n511  -2.1534953659          t\n512   0.5062054404          e\n513  -0.8580279187          a\n514  -1.8217132150          t\n515   0.8754096284          d\n516   0.2074617933          d\n517  -1.4133275042          y\n518   1.4947165256          H\n519  -0.8015709033          U\n520   0.6536488919          R\n521  -0.9400254026          z\n522   0.8104738111          l\n523  -0.0749049234          m\n524   0.8323710646          T\n525   1.4605080246          m\n526  -1.3072713850          S\n527   0.0930232667          B\n528  -0.2227758909          D\n529   1.5235731951          R\n530  -0.4245859065          D\n531   0.6812616563          R\n532   1.1799619344          y\n533  -1.0596486272          J\n534  -1.2288719540          g\n535   0.7434582961          M\n536   1.3839611789          Y\n537   0.4801570419          H\n538  -0.8788999124          n\n539  -0.0286253656          k\n540  -0.4918207514          A\n541  -1.3343168666          E\n542  -1.0406224528          f\n543  -1.1658091029          V\n544  -0.3217346097          F\n545  -0.9510090738          S\n546  -1.2817430380          t\n547  -1.5413609497          A\n548   2.6847789526          r\n549   1.3436223615          k\n550   1.5127903929          I\n551   0.1593625244          e\n552  -0.6421099688          U\n553   0.7025693728          w\n554  -0.9941014226          r\n555  -0.8788695072          f\n556  -0.6623419813          d\n557  -0.9663591419          q\n558  -0.2747265994          y\n559   0.7240087879          w\n560   0.3194627475          h\n561  -0.2675948399          W\n562  -1.9079363447          w\n563   0.5302893728          N\n564  -0.0196478069          N\n565  -0.3027043046          u\n566   1.0934307199          s\n567  -1.0718835773          p\n568  -0.0312958618          S\n569   0.3433876974          K\n570   0.6747192680          x\n571  -0.6961926337          k\n572   0.5609310183          R\n573  -1.3006719543          h\n574   0.4106339751          N\n575   0.2105707136          s\n576  -1.1250587460          E\n577   0.6019832477          y\n578  -0.2626193156          r\n579  -0.6642793635          V\n580  -0.5546566479          X\n581  -1.0280260421          H\n582  -0.5039325967          B\n583  -0.4245075849          y\n584   0.2377904660          n\n585  -0.2335782276          e\n586  -0.1719600006          j\n587  -1.5571980967          w\n588  -0.0888406731          U\n589   0.0106486191          u\n590  -0.8723614601          S\n591  -0.0827994400          V\n592   1.3708043176          D\n593  -1.1643272771          P\n594  -0.1614700837          E\n595   0.0034743217          q\n596  -0.7204152049          n\n597   0.2694211407          y\n598   0.8899269304          E\n599   1.0433818002          T\n600   0.0537060130          R\n601  -0.5243799262          O\n602  -1.8279869799          k\n603   1.4682366623          k\n604  -0.8729703132          A\n605  -1.7422231224          N\n606   0.7733753172          K\n607  -0.4864075013          C\n608   1.0594171876          k\n609   0.7523782990          Q\n610  -1.6487707174          M\n611   0.6590369503          F\n612   0.6010557322          g\n613   0.4912401358          x\n614  -3.3612952510          i\n615  -1.3744131188          V\n616  -0.7528395018          c\n617  -0.3131836740          R\n618   1.3043994999          u\n619  -1.4248934439          a\n620  -3.4685356113          D\n621  -0.4962190397          t\n622  -0.3811621593          d\n623  -0.5060443432          F\n624  -1.2826920102          P\n625   0.6311408243          e\n626  -0.1044574746          K\n627  -0.6224602686          g\n628  -1.0000106459          b\n629  -1.4334978438          l\n630  -0.2980016920          P\n631  -0.4646188367          Q\n632   1.0898369859          F\n633  -0.2866437491          R\n634  -0.0391344521          D\n635   1.1029231676          q\n636  -0.1469066065          G\n637   1.2461424805          F\n638  -0.9918571946          B\n639  -1.5347455014          d\n640   0.9591686846          g\n641   0.8987168250          O\n642   0.3439651238          a\n643  -1.2807237464          T\n644  -0.6731718766          L\n645  -1.3836012693          B\n646   1.1228109178          v\n647  -0.5365134433          A\n648  -1.0815990341          R\n649   0.4190913052          X\n650  -0.3464250098          u\n651   1.0465781124          T\n652   0.5806992801          L\n653   0.7224270673          K\n654  -0.7600792479          p\n655   0.8825651986          f\n656  -0.8683437095          k\n657  -0.9877358359          g\n658   0.8998314948          P\n659  -1.7113022556          m\n660   0.8183336930          O\n661   0.2227827033          t\n662  -1.2197091790          M\n663  -0.2610821601          t\n664  -0.2270152995          n\n665  -0.4731897478          w\n666  -1.0551649541          M\n667  -0.9833289154          C\n668  -0.8468576421          H\n669   1.1103823729          P\n670   1.1265630160          b\n671  -0.5068817706          m\n672  -2.0808467683          L\n673   1.1194835345          v\n674   0.1341476513          G\n675   0.0983495105          D\n676  -1.0832674528          y\n677   1.5699042347          N\n678  -0.7432527090          A\n679   0.2170218643          h\n680  -2.4033770723          R\n681  -0.2498179661          u\n682   0.5608832671          d\n683   0.1782033293          w\n684  -1.7192471702          G\n685  -0.5416863775          E\n686   0.6025539680          q\n687   0.1165847154          D\n688   0.5815740845          X\n689   0.7964988515          V\n690   1.0191261823          o\n691   0.5916985712          D\n692   0.6363366225          Y\n693   0.9583644052          T\n694   0.8523915679          P\n695  -1.4502693064          p\n696  -0.2647455684          f\n697  -0.0388619754          g\n698   0.6133232429          z\n699  -0.0816385071          K\n700   1.4645606830          m\n701  -1.0004884176          R\n702  -1.1198961861          W\n703   0.3543930928          H\n704   1.6633528990          g\n705  -1.0637170180          i\n706  -0.1547361502          b\n707   1.4914445121          w\n708  -0.3974313514          K\n709   0.4103273805          D\n710  -0.1814189191          w\n711   0.3722626912          T\n712  -0.1055983585          r\n713   0.3693084068          u\n714   1.2924514710          G\n715  -0.3628344869          W\n716   0.5858424312          Z\n717  -1.1271212190          o\n718   0.2615717470          C\n719   1.1952118473          T\n720  -1.4941487123          f\n721  -0.2983214457          K\n722  -0.2535398418          E\n723   0.2031698641          m\n724  -0.3173048617          y\n725   0.7137190676          d\n726  -0.4619705391          J\n727  -0.1581049836          N\n728  -0.4448835817          F\n729  -1.2628796409          A\n730  -0.3828276769          Q\n731  -1.5621390360          B\n732   0.4054029088          J\n733  -0.0809525224          C\n734   1.1684991660          m\n735   0.3217081760          h\n736  -1.2268098900          S\n737  -0.7782858484          j\n738   1.9129822201          g\n739   1.6941103226          j\n740  -0.2749184443          S\n741   0.0971587854          L\n742   0.9573208882          n\n743   1.1378289214          j\n744   1.4297365662          P\n745   1.6402318176          T\n746  -1.5871685514          w\n747   1.4938047510          X\n748   0.3450879099          g\n749   1.2360095826          M\n750  -0.0444084679          e\n751   0.4216740394          z\n752  -0.7514039625          F\n753   1.6067325483          k\n754   1.2682869203          D\n755   0.8053719930          N\n756  -1.2336591645          L\n757   0.6339825718          N\n758  -1.2150854646          D\n759  -0.1642736670          R\n760  -0.2044802348          x\n761  -0.2696861749          D\n762   1.0515375502          C\n763   1.2208865940          K\n764   0.3315830028          C\n765  -1.3155499050          i\n766   1.0166994873          E\n767   2.5786050568          V\n768   1.4694780697          y\n769  -0.5683592467          u\n770   0.5535191756          v\n771  -0.9865936496          H\n772  -0.0397361431          m\n773  -0.8460886925          X\n774   1.1250793124          P\n775  -0.1991943347          I\n776  -0.2166001951          G\n777   0.6546357158          b\n778  -2.2093417075          o\n779   0.7878591827          i\n780   0.4917220485          M\n781   0.6210564013          v\n782  -0.2705989640          O\n783  -0.1990900076          H\n784  -1.0743543476          w\n785   0.5738445583          y\n786  -0.2557058818          F\n787   0.5946206412          b\n788   0.1729229875          l\n789   0.2204214791          H\n790   1.0493174323          T\n791  -0.3829648736          o\n792  -0.6486192239          K\n793   0.1886997312          g\n794   0.7484398952          q\n795   0.8611137091          O\n796   0.3683990620          N\n797  -1.4656359423          i\n798  -0.5843651976          e\n799   1.7059168135          s\n800   1.7033448762          v\n801   0.1941263856          v\n802  -1.0452567684          b\n803  -2.0799767651          M\n804  -0.0427886617          Y\n805   0.3286797419          z\n806   1.1213775980          w\n807  -1.6953249256          K\n808  -1.5467083399          q\n809  -0.1252895603          u\n810   0.0747370613          s\n811  -0.0064225986          E\n812  -1.4374492341          y\n813   0.5366007066          c\n814   1.6877123084          J\n815   0.8360104619          c\n816  -0.1307210086          L\n817  -0.7952735505          Z\n818   0.1739603507          U\n819   0.0547709321          T\n820   0.0382255044          y\n821  -0.7602221046          e\n822  -0.2141963671          u\n823  -0.4790809004          T\n824   0.8267706405          s\n825  -0.9291264555          p\n826   0.3806941415          B\n827  -0.5598543290          Q\n828  -1.2020293644          O\n829   0.4682327149          J\n830  -0.8210079538          l\n831   0.5494417608          B\n832   0.6086025988          j\n833   0.9517034647          M\n834  -0.3306437378          N\n835  -0.5919299997          d\n836  -1.1186468104          l\n837   0.7411656358          E\n838  -1.2584403023          L\n839   2.0120689290          M\n840   1.8756363433          y\n841   0.2184800596          w\n842  -0.4191981165          O\n843   0.3104913660          e\n844  -1.7531862285          r\n845   1.2255984350          X\n846   1.4400116479          j\n847   0.2137335697          Y\n848   1.1615000404          K\n849  -0.1075967121          J\n850  -1.5015404490          q\n851   0.3677284954          N\n852   0.8380301197          U\n853  -0.3162600932          p\n854  -0.2929429381          M\n855  -0.0234277343          H\n856   1.7969596465          o\n857   0.6964874930          z\n858  -2.1220711464          V\n859  -1.5421497886          D\n860   0.0327497236          B\n861   0.0589483661          Q\n862  -3.0078973567          r\n863  -0.8645719857          c\n864  -0.8995614767          Q\n865  -1.6409444302          o\n866   0.3251294576          N\n867  -0.7675346879          L\n868   2.2138928033          y\n869   2.7538728136          E\n870  -0.7135443489          V\n871   0.7838591176          o\n872  -0.8817111767          P\n873  -0.2245301424          c\n874   1.2647555586          Y\n875  -0.2457782272          d\n876  -0.6833531832          K\n877   0.3745479624          P\n878  -0.3248263823          u\n879  -0.8113981148          v\n880  -1.4217164966          o\n881   0.6953886215          t\n882  -0.2826541968          I\n883   0.6637339455          i\n884   1.7555702502          q\n885   0.5174354694          B\n886  -1.0020984623          p\n887  -0.0109159362          K\n888   0.2800029561          i\n889  -0.0854868183          g\n890   0.3685413684          o\n891  -1.1314595430          Z\n892  -0.2443620150          U\n893  -0.3858008365          c\n894  -1.0031454930          M\n895  -0.8535702052          x\n896   1.0769760376          X\n897   2.8705743252          Y\n898   0.3474132811          u\n899   0.6051647330          O\n900   0.3720343803          u\n901  -0.6387019753          H\n902  -0.8697792609          v\n903  -1.6533981557          A\n904   1.8048471424          s\n905   0.6234795334          W\n906   0.7771509812          e\n907   0.1163388187          G\n908   0.5814445470          N\n909   2.4434042316          H\n910   1.4113015797          B\n911   0.7339381782          j\n912   0.9456636011          W\n913   0.2005407918          s\n914   0.7121760328          T\n915  -0.8704760327          Q\n916  -0.1428665609          U\n917  -1.8805375999          e\n918   0.0540751707          k\n919  -0.5749482769          r\n920   0.4891165044          b\n921  -1.0838281894          m\n922   0.1113334137          z\n923  -1.6354372114          O\n924   0.6855392145          E\n925  -1.0737324305          Q\n926   0.0910344596          r\n927   2.5729762621          j\n928  -0.8130978791          n\n929   1.4078225644          h\n930  -0.9859281596          X\n931  -0.5867002409          k\n932   0.4157363444          C\n933   0.9920435295          y\n934   1.6798511468          L\n935   1.3106900699          y\n936   0.1975719063          G\n937  -0.5284100557          x\n938   0.5456398175          E\n939  -1.3059284470          c\n940  -0.5208386436          z\n941   0.4039497648          d\n942  -0.7542280668          n\n943   0.4078971313          d\n944   0.4872615518          M\n945  -0.2506064114          r\n946   1.9820904167          O\n947  -0.7919638631          X\n948   1.1550503916          x\n949   1.3490034869          Q\n950  -1.7343725661          Q\n951  -0.4129654441          U\n952   0.2517848760          L\n953  -0.4016405270          u\n954   1.4980043516          V\n955   0.5342719829          f\n956  -1.3625039724          w\n957  -0.6288541697          m\n958  -1.3880879828          A\n959  -0.6485266061          O\n960   0.9683390958          T\n961  -0.9152801398          A\n962  -2.1148102388          Z\n963   0.9348777206          l\n964  -0.2903792900          X\n965  -0.1549520480          z\n966  -1.0475499510          Z\n967  -0.5548963782          z\n968  -1.3817398867          T\n969   1.7781597664          L\n970   1.3849574081          S\n971   0.9122741692          F\n972  -1.0201814603          F\n973   0.4994904709          J\n974  -0.1564284306          Q\n975  -0.0182764230          h\n976  -1.8358616109          h\n977   0.6114241147          S\n978   1.5990849187          u\n979   0.0614672614          T\n980  -0.0992237223          y\n981   0.4002245399          M\n982   0.4445965781          s\n983   0.6457828217          v\n984  -0.1461903497          f\n985   0.8467565782          V\n986   0.2129414254          d\n987   0.9877007208          j\n988   1.4879031533          d\n989  -1.6791758368          J\n990   0.8906924990          p\n991   1.3154467076          u\n992   0.5150408190          O\n993   0.9562421576          l\n994   0.7088498278          n\n995   0.5124412475          c\n996  -0.2138030363          J\n997   0.8082824729          X\n998  -0.6458249916          E\n999  -0.9561045757          I\n1000 -0.2082780147          Q\n\n\nQuick and easy. I think py_to_r is supposed to do some of this, but I can never get it to work. I think maybe it would make more sense in a script where we’re moving back and forth than here where we have separate code chunks?"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#now-with-time",
    "href": "RpyEnvs/py_r_dates.html#now-with-time",
    "title": "Complex passing py-R",
    "section": "Now, with time",
    "text": "Now, with time\nLet’s add a column of dates to simpledf. First, create the dates, then add to simpledf.\n\n```{python}\nimport datetime\n\ndates = []\n\nd1 = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n\n# Because i starts at 0, the first loop is the start date\nfor i in range(1000):\n    # Add i days to the start date\n    day_new = d1 + datetime.timedelta(days=i)\n    # Append the current date string to the list of dates\n    dates.append(day_new)\n\ntimedf = simpledf.assign(date = dates)\n\n# Try another way too\ntimedf['date2'] = pd.to_datetime(timedf['date'])\n```\n\nNow, when we bring it into R, it just works? That’s not at all what happened to me when I had the original issue.\n\n```{r}\ntimedfR &lt;- py$timedf\n# tibble::as_tibble(timedfR)\n```"
  },
  {
    "objectID": "RpyEnvs/py_r_dates.html#for-future-reference",
    "href": "RpyEnvs/py_r_dates.html#for-future-reference",
    "title": "Complex passing py-R",
    "section": "For future reference",
    "text": "For future reference\nSo, I can’t seem to replicate the issue. Previously, the datetime col came in as a list-col into a tibble, and was wrapped in a python environment. It was possible to parse it with purrr (or lapply, but purrr was much faster (and weirdly, furrr was slower). Not running, because this isn’t in a python env, and so this doesn’t actually work.\n\n```{r}\n#| eval: false\ndemodates &lt;- timedfR$date\nrdates &lt;- purrr::map(demodates, py_to_r) %&gt;%\n  tibble(.name_repair = ~'Date') %&gt;%\n  unnest(cols = Date)\n```\n\nBut what was much faster was to convert the column to strings in python with\n\n```{python}\ntimedf['date_str'] = timedf['date'].astype(str)\ntimedf.info()\n```\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   rand_nums   1000 non-null   float64       \n 1   rand_chars  1000 non-null   object        \n 2   date        1000 non-null   datetime64[ns]\n 3   date2       1000 non-null   datetime64[ns]\n 4   date_str    1000 non-null   object        \ndtypes: datetime64[ns](2), float64(1), object(2)\nmemory usage: 39.2+ KB\n\n\nAnd then lubridate back to dates in R\n\n```{r}\ntimedfRstr &lt;- py$timedf\ntimedfRstr &lt;- dplyr::select(timedfRstr, -date) |&gt; \n  dplyr::mutate(date = lubridate::ymd(date_str))\nstr(timedfRstr)\n```\n\n'data.frame':   1000 obs. of  5 variables:\n $ rand_nums : num  -0.702 0.459 -0.261 -0.802 -1.081 ...\n $ rand_chars: chr  \"c\" \"y\" \"i\" \"o\" ...\n $ date2     : POSIXct, format: \"2000-01-01 11:00:00\" \"2000-01-02 11:00:00\" ...\n $ date_str  : chr  \"2000-01-01\" \"2000-01-02\" \"2000-01-03\" \"2000-01-04\" ...\n $ date      : Date, format: \"2000-01-01\" \"2000-01-02\" ...\n - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=1000, step=1)"
  },
  {
    "objectID": "RpyEnvs/python_nameerror.html",
    "href": "RpyEnvs/python_nameerror.html",
    "title": "Python NameErrors",
    "section": "",
    "text": "Issue\nI keep getting NameErrors and weird behavior with mixed R and python chunks. Usually it seems to be that python can’t find objects if there’s an R chunk in between. It only happens when I render in quarto- running this interactively in Rstudio always works fine.\nI’ve set the RETICULATE_PYTHON and QUARTO_PYTHON environment variables, and put engine: knitr in the yaml, and it doesn’t seem to have helped.\nThe issue seems to be worst when R is somehow involved with the objects.\nThis doc is to try to systematically and simply reproduce the issue. I hope that works.\nIt was working, but the more often I rendered, the more intermittent the problem became. I’ve noted something about the frequency of errors for the chunks- some error more than others.\n\n```{r}\nlibrary(reticulate)\n```\n\nAn R chunk to make sure knitr kicks off\n\n```{r}\na &lt;- 1\n```\n\nDefine a bunch of python variables to do different things with.\n\n```{python}\nb = 2\nc = 3\nd = 4\ne = 5\nf = 6\ng = 7\n```\n\nCan we see those immediately?\n\n```{python}\nb\n```\n\n2\n\n\nWhat if there’s an R chunk in the middle?\n\n```{r}\nrdummy &lt;- 1\n```\n\nCan we still see the python? Yes\n\n```{python}\nc\n```\n\n3\n\n\nWhat if the R touches the python?\n\n```{r}\nrb &lt;- py$d + 1\nrb\n```\n\n[1] 5\n\n\nThis now fails almost every time\n\n```{python}\nd\n```\n\n4\n\n\nAre the other variables unscathed? No, this also fails consistently (but not 100% of the time).\n\n```{python}\nc\n```\n\n3\n\n\nWhat about those that haven’t been used since declared? Fails again most of the time.\n\n```{python}\ne\n```\n\n5\n\n\nCan we declare more python?\n\n```{python}\nf = 6\n```\n\nagain, R in the middle\n\n```{r}\nrtest &lt;- 5\n```\n\nDoes the new python persist?\n\n```{python}\nf\n```\n\n6\n\n\nWhat if there’s a python chunk that accesses R?\n\n```{python}\ng = r.a + 1\ng\n```\n\n2.0\n\n\nCan we access the previous python variables? Sometimes. I intermittently get an error here, but sometimes it runs.\n\n```{python}\nf\n```\n\n6\n\n\nCan we access that new python variable? Also only sometimes.\n\n```{python}\ng\n```\n\n2.0\n\n\nWhat if we declare python in two chunks? Do they all get annihilated after crossing the language boundary, or only on the basis of their chunk declaration?\n\n```{python}\ng = 7\nh = 8\n```\n\n\n```{python}\nj = 9\n```\n\nContaminate with R\n\n```{r}\nrgh &lt;- py$g + 9\nrgh\n```\n\n[1] 16\n\n\nCan we see the python that was defined with g? This works sometimes. So whatever is going on is unstable. It’s strange that this and the following two work sometimes- they seem like the same thing that consistently fails above (though that’s now working too).\n\n```{python}\nh\n```\n\n8\n\n\nI wasn’t expecting that to work (and it only does sometimes). Can we see g itself? Sometimes\n\n```{python}\ng\n```\n\n7\n\n\nCan we get to the python that was defined in a different chunk? Sometimes.\n\n```{python}\nj\n```\n\n9\n\n\nIs the issue that we didn’t ask for the touched variable first? Do the same thing, but this time ask for the contaminated variable\n\n```{python}\nk = 10\nl = 11\n```\n\n\n```{python}\nm = 12\n```\n\nAccess in R\n\n```{r}\nr_m &lt;- py$k + 9\n```\n\nIs that variable there in python?\n\n```{python}\nk\n```\n\n10\n\n\nIs the other one that’s defined with it?\n\n```{python}\nl\n```\n\n11\n\n\nHow about the one in the other code chunk?\n\n```{python}\nm\n```\n\n12\n\n\nI’m not really sure what to do with this. Anything defined before any interaction across languages (either py$pyvar or r.rvar) would die in python when I rendered this the first few times, but now it’s all working after about 10 update renders. I’m throwing execute: error: true in all the R-py yaml headers, but that doesn’t actually help when I actually want them to run and there are errors intermittently."
  },
  {
    "objectID": "RpyEnvs/python_updated_functions.html",
    "href": "RpyEnvs/python_updated_functions.html",
    "title": "Updating function defs",
    "section": "",
    "text": "As I develop, I often try a function, tweak it, try again, etc. In R, I can just run the function definition to have access, or source(filewithfunction.R). In python, I could tweak the function, but just trying to use them elsewhere (e.g. in a .qmd) wasn’t working, even if I re-ran import filename. Clearly, there are differences between import in python and source in R. After poking around a bit, it looks like python caches on first import, and so subsequent ones don’t refresh.\nWhat does seem to work is to run importlib.reload(filename). Obviously we wouldn’t put that in a script, but when using an interactive session, it’s really helpful. Not sure why this requires a whole separate package, but it works. See stackoverflow. It appears to be typical to just restart, but that is really prohibitive if the testing involves processing data that took a long time to create."
  },
  {
    "objectID": "betabinomial/betadists.html",
    "href": "betabinomial/betadists.html",
    "title": "Beta distributions",
    "section": "",
    "text": "This is fairly sketchy, just trying to see how the \\(\\phi\\) from Harrison (2015) affect beta probability distributions.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\np &lt;- seq(from = 0, to = 1, by = 0.01)\nphi &lt;- seq(from = 0.1, to = 1, by = 0.1)\n\n\n# ai &lt;- p[1]/phi[1]\n# bi &lt;- (1-p[1])/phi[1]\n# \n# db &lt;- dbeta(p, ai, bi)\n\nprobdists &lt;- tibble(p = NA, db = NA, betap = NA, phi = NA, .rows = 0)\n\nfor (j in 1:length(phi)) {\n  for (i in 1:length(p)) {\n  ai &lt;- p[i]/phi[j]\n  bi &lt;- (1-p[i])/phi[j]\n  \n  db &lt;- dbeta(p, ai, bi)\n  \n  dbt &lt;- tibble(p, db, betap = p[i], phi = phi[j])\n  probdists &lt;- rbind(probdists, dbt)\n}\n}\n\n\nggplot(probdists |&gt; filter(betap %in% c(0.1, 0.25, 0.5, 0.75, 0.9)), \n       aes(x = p, y = db, color = factor(betap))) + geom_line() +\n  facet_wrap(\"phi\")\n\n\n\n\n\ntestx &lt;- seq(from = 0, to = 1, by = 0.01)\ntestbetaprob &lt;- dbeta(testx, ai, bi)\nprobdist &lt;- tibble(testx, testbetaprob)\n\nggplot(probdist, aes(x = testx, y = testbetaprob)) + geom_line()\n\n\n\n\nRandom draws\n\nintercept &lt;- -2\nbeta &lt;- 0.5\nall_x &lt;- seq(0, 10, 0.1)\nall_logit_p &lt;- intercept + beta*all_x\n\nall_p &lt;- 1/(1+exp(-all_logit_p))\n\n\nplot(all_x, all_p, type = 'l')\n\n\n\n\n\n\n\n\nReferences\n\nHarrison, Xavier A. 2015. “A Comparison of Observation-Level Random Effect and Beta-Binomial Models for Modelling Overdispersion in Binomial Data in Ecology & Evolution.” PeerJ 3 (July): e1114. https://doi.org/10.7717/peerj.1114."
  },
  {
    "objectID": "data_acquisition/ALA_data_pull.html",
    "href": "data_acquisition/ALA_data_pull.html",
    "title": "Atlas of living australia",
    "section": "",
    "text": "I need to pull some records from the Atlas of living australia. There’s an API. Can I use it so I don’t have to do this manually (and can re-do it easily?).\nWas going to set up to hit the API directly, but they have an R package (that hits a LOT of these sorts of atlases, which is nice).\n# library(httr2)\nlibrary(galah)\nHave to register at their website first- if you’re at an Australian university, it’s likely you can login with those credentials- click the AAF button.\ngalah_config(email = \"g.holt@deakin.edu.au\", atlas = \"Australia\")"
  },
  {
    "objectID": "data_acquisition/ALA_data_pull.html#try-it",
    "href": "data_acquisition/ALA_data_pull.html#try-it",
    "title": "Atlas of living australia",
    "section": "Try it",
    "text": "Try it\nI’m just following the vignette, but not filtering or selecting\n\nlippia &lt;- galah_call() |&gt;\n  galah_identify(\"lippia\") |&gt;\n  # galah_select(institutionID, group = \"basic\") |&gt;\n  atlas_occurrences()\n\nThis query will return 110 records\n\n\nChecking queue\nCurrent queue size: 1. 0 \nRunning query on selected atlas\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\nDownloading\n\nlippia\n\n# A tibble: 110 × 8\n   decimalLatitude decimalLong…¹ event…² scien…³ taxon…⁴ recor…⁵ dataR…⁶ occur…⁷\n             &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1           -41.1         175.  \"1952-… Lippia  https:… 3db5bc… New Ze… PRESENT\n 2           -36.9         175.  \"1992-… Lippia  https:… d51007… New Ze… PRESENT\n 3           -36.8         175.  \"\"      Lippia  https:… 1839dd… New Ze… PRESENT\n 4           -35.3         139.  \"2020-… Lippia  https:… 5083b7… iNatur… PRESENT\n 5           -35.0         147.  \"2018-… Lippia  https:… 40762c… iNatur… PRESENT\n 6           -34.3         142.  \"2021-… Lippia  https:… e7126f… iNatur… PRESENT\n 7           -26.5         151.  \"2010-… Lippia… https:… 42c542… WildNe… PRESENT\n 8           -25.1          27.6 \"\"      Lippia  https:… b9034a… Nation… PRESENT\n 9           -23.7         -46.4 \"1931-… Lippia  https:… e29b4d… Austra… PRESENT\n10           -23.6         149.  \"1999-… Lippia… https:… b66e2f… BRI AV… PRESENT\n# … with 100 more rows, and abbreviated variable names ¹​decimalLongitude,\n#   ²​eventDate, ³​scientificName, ⁴​taxonConceptID, ⁵​recordID, ⁶​dataResourceName,\n#   ⁷​occurrenceStatus\n\n\nFrom the website though, I know that Phyla nodiflora comes up as a synonym with way more records. Can I get that sort of info from the API before pulling?\n\nall_lippia &lt;- search_taxa('lippia')\nall_lippia\n\n# A tibble: 1 × 13\n  search_term scienti…¹ scien…² taxon…³ rank  match…⁴ kingdom phylum class order\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n1 lippia      Lippia    L.      https:… genus exactM… Plantae Charo… Equi… Lami…\n# … with 3 more variables: family &lt;chr&gt;, genus &lt;chr&gt;, issues &lt;chr&gt;, and\n#   abbreviated variable names ¹​scientific_name, ²​scientific_name_authorship,\n#   ³​taxon_concept_id, ⁴​match_type\n\n\nLooks like no- these come up in the search bar on the website, but not in the above. Oh well, I guess be aware of that.\n\npnode &lt;- search_taxa('phyla nodiflora')"
  },
  {
    "objectID": "data_acquisition/ALA_data_pull.html#can-i-ask-for-both-in-one-call",
    "href": "data_acquisition/ALA_data_pull.html#can-i-ask-for-both-in-one-call",
    "title": "Atlas of living australia",
    "section": "Can I ask for both in one call?",
    "text": "Can I ask for both in one call?\nYeszs\n\nlippia_phyla &lt;- galah_call() |&gt;\n  galah_identify(c(\"lippia\", 'phyla nodiflora')) |&gt;\n  # galah_select(institutionID, group = \"basic\") |&gt;\n  atlas_occurrences()\n\nThis query will return 1640 records\n\n\nChecking queue\nCurrent queue size: 1. 0 \nRunning query on selected atlas\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\nDownloading\n\nlippia_phyla\n\n# A tibble: 1,640 × 8\n   decimalLatitude decimalLong…¹ event…² scien…³ taxon…⁴ recor…⁵ dataR…⁶ occur…⁷\n             &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1           -43.6          173. 1976-0… Phyla … https:… 9292f3… New Ze… PRESENT\n 2           -41.3          173. 2019-0… Phyla … https:… 2ae17f… New Ze… PRESENT\n 3           -41.1          175. 1952-0… Lippia  https:… 3db5bc… New Ze… PRESENT\n 4           -39.3          178. 1976-0… Phyla … https:… ffc4c1… New Ze… PRESENT\n 5           -38.2          145. 2022-1… Phyla … https:… 4026cd… iNatur… PRESENT\n 6           -38.1          145. 2023-0… Phyla … https:… 7cd4b0… iNatur… PRESENT\n 7           -37.9          145. 2022-0… Phyla … https:… a5fc0f… iNatur… PRESENT\n 8           -37.9          145. 2021-1… Phyla … https:… 996c0d… iNatur… PRESENT\n 9           -37.9          145. 2020-1… Phyla … https:… 6d20ff… iNatur… PRESENT\n10           -37.9          145. 2022-0… Phyla … https:… 482077… iNatur… PRESENT\n# … with 1,630 more rows, and abbreviated variable names ¹​decimalLongitude,\n#   ²​eventDate, ³​scientificName, ⁴​taxonConceptID, ⁵​recordID, ⁶​dataResourceName,\n#   ⁷​occurrenceStatus\n\n\nAnd it returns the names, so I can filter."
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html",
    "href": "data_acquisition/gauge_data_pre_gauge.html",
    "title": "Gauge data and gauged period",
    "section": "",
    "text": "library(foreach)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(vicwater)\nlibrary(reticulate)\nimport mdba_gauge_getter as gg\nWe often end up needing to pull gauge data for lots of projects. If we just want flow data, and especially if we’re working in python, there’s the mdba_gauge_getter, and I’ve written {vicwater} for R and expanded it to cover Victoria, NSW, and Qld. A primary advantage of {vicwater} is that it can access more of the API calls and return any desired variables.\nOne issue we often end up having is wanting to grab gauge data for a range of dates that may or may not go earlier than a gauge was put in place. There’s actually an issue with the Kisters API for that, where it silently returns values of 0 for those dates, which is not good. Because it comes from the API itself, it affects both packages.\nFor example, we can go to the website and find the period of record for NSW gauge 410007 (gauges_to_pull[138]) is 10/01/1979 - present. We’ll show first what happens for three situations with both mdba_gauge_getter and vicwater\nand then we’ll show how to handle it in {vicwater}."
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html#all-dates-pre-gauge",
    "href": "data_acquisition/gauge_data_pre_gauge.html#all-dates-pre-gauge",
    "title": "Gauge data and gauged period",
    "section": "All dates pre-gauge",
    "text": "All dates pre-gauge\nIf we ask for the period before the gauge is operational {vicwater} passes the API error through.\n\nget_ts_traces(state = 'NSW', \n                site_list = gaugenum, \n                var_list = '141',\n                start_time = weekbefore,\n                end_time = daybefore,\n                interval = 'day',\n                data_type = 'mean')\n\nWarning: executing %dopar% sequentially: no parallel backend registered\n\n\nError in {: task 1 failed - \"API error number 126. Message: No data within specified period\"\n\n\nWe get an empty dataframe from mdba_gauge_getter\n\ndemo_levs_pre = gg.gauge_pull(r.gaugenum, start_time_user = r.weekbefore, end_time_user = r.daybefore)\n\nC:\\Users\\galen\\DOCUME~1\\Website\\GALEN_~1\\RpyEnvs\\PYTEST~1\\VENV~1\\Lib\\site-packages\\mdba_gauge_getter\\gauge_getter.py:82: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n  gauges['State'] = gauges['gauge_owner'].str.strip().str.split(' ', 1).str[0]\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 1 of 1\nERROR:mdba_gauge_getter.gauge_get:No valid data contained in response, skipping\n\ndemo_levs_pre\n\nEmpty DataFrame\nColumns: [DATASOURCEID, SITEID, SUBJECTID, DATETIME, VALUE, QUALITYCODE]\nIndex: []\n\n\nSo, that’s slightly different behavior, but neither is returning misleading data."
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html#all-dates-with-gauge",
    "href": "data_acquisition/gauge_data_pre_gauge.html#all-dates-with-gauge",
    "title": "Gauge data and gauged period",
    "section": "All dates with gauge",
    "text": "All dates with gauge\nNow {vicwater} gives a dataframe.\n\nget_ts_traces(state = 'NSW', \n                site_list = gaugenum, \n                var_list = '141',\n                start_time = gaugestart,\n                end_time = weeklater,\n                interval = 'day',\n                data_type = 'mean')\n\n# A tibble: 8 × 20\n  error_num compressed timezone site_sho…¹ longi…² site_…³ latit…⁴ org_n…⁵ value\n      &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  511.\n2         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  552.\n3         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  536.\n4         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  516.\n5         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  513.\n6         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  486.\n7         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  492.\n8         0 0          10.0     YANCO CK …    146. YANCO …   -34.7 WaterN…  533.\n# … with 11 more variables: time &lt;dttm&gt;, quality_codes_id &lt;int&gt;, site &lt;chr&gt;,\n#   variable_short_name &lt;chr&gt;, precision &lt;chr&gt;, subdesc &lt;chr&gt;, variable &lt;chr&gt;,\n#   units &lt;chr&gt;, variable_name &lt;chr&gt;, quality_codes &lt;chr&gt;, data_type &lt;chr&gt;, and\n#   abbreviated variable names ¹​site_short_name, ²​longitude, ³​site_name,\n#   ⁴​latitude, ⁵​org_name\n\n\nAs does mdba_gauge_getter\n\ndemo_levs_exists = gg.gauge_pull(r.gaugenum, start_time_user = r.gaugestart, end_time_user = r.weeklater)\n\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 1 of 1\n\ndemo_levs_exists\n\n  DATASOURCEID  SITEID SUBJECTID    DATETIME    VALUE  QUALITYCODE\n0          NSW  410007     WATER  1979-01-10  510.939          255\n1          NSW  410007     WATER  1979-01-11  551.848          130\n2          NSW  410007     WATER  1979-01-12  535.988          130\n3          NSW  410007     WATER  1979-01-13  515.685          130\n4          NSW  410007     WATER  1979-01-14  512.529          130\n5          NSW  410007     WATER  1979-01-15  486.422          130\n6          NSW  410007     WATER  1979-01-16  491.710          130\n7          NSW  410007     WATER  1979-01-17  532.730          130\n\n\nThat again returns what it should. All the dates have data."
  },
  {
    "objectID": "data_acquisition/gauge_data_pre_gauge.html#dates-spanning-gauge-start",
    "href": "data_acquisition/gauge_data_pre_gauge.html#dates-spanning-gauge-start",
    "title": "Gauge data and gauged period",
    "section": "Dates spanning gauge start",
    "text": "Dates spanning gauge start\nNow {vicwater} gives a dataframe, but that initial period has value = 0, which is wrong. It should be NA, but the API returns 0 silently.\n\nget_ts_traces(state = 'NSW', \n                site_list = gaugenum, \n                var_list = '141',\n                start_time = weekbefore,\n                end_time = weeklater,\n                interval = 'day',\n                data_type = 'mean')\n\n# A tibble: 15 × 20\n   error_num compressed timezone site_sh…¹ longi…² site_…³ latit…⁴ org_n…⁵ value\n       &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 2         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 3         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 4         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 5         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 6         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 7         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…    0 \n 8         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  511.\n 9         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  552.\n10         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  536.\n11         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  516.\n12         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  513.\n13         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  486.\n14         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  492.\n15         0 0          10.0     YANCO CK…    146. YANCO …   -34.7 WaterN…  533.\n# … with 11 more variables: time &lt;dttm&gt;, quality_codes_id &lt;int&gt;, site &lt;chr&gt;,\n#   variable_short_name &lt;chr&gt;, precision &lt;chr&gt;, subdesc &lt;chr&gt;, variable &lt;chr&gt;,\n#   units &lt;chr&gt;, variable_name &lt;chr&gt;, quality_codes &lt;chr&gt;, data_type &lt;chr&gt;, and\n#   abbreviated variable names ¹​site_short_name, ²​longitude, ³​site_name,\n#   ⁴​latitude, ⁵​org_name\n\n\nThe same thing happens with mdba_gauge_getter\n\ndemo_levs_span = gg.gauge_pull(r.gaugenum, start_time_user = r.weekbefore, end_time_user = r.weeklater)\n\nINFO:mdba_gauge_getter.gauge_get:NSW - Request 1 of 1\n\ndemo_levs_span\n\n   DATASOURCEID  SITEID SUBJECTID    DATETIME    VALUE  QUALITYCODE\n0           NSW  410007     WATER  1979-01-03    0.000          255\n1           NSW  410007     WATER  1979-01-04    0.000          255\n2           NSW  410007     WATER  1979-01-05    0.000          255\n3           NSW  410007     WATER  1979-01-06    0.000          255\n4           NSW  410007     WATER  1979-01-07    0.000          255\n5           NSW  410007     WATER  1979-01-08    0.000          255\n6           NSW  410007     WATER  1979-01-09    0.000          255\n7           NSW  410007     WATER  1979-01-10  510.939          255\n8           NSW  410007     WATER  1979-01-11  551.848          130\n9           NSW  410007     WATER  1979-01-12  535.988          130\n10          NSW  410007     WATER  1979-01-13  515.685          130\n11          NSW  410007     WATER  1979-01-14  512.529          130\n12          NSW  410007     WATER  1979-01-15  486.422          130\n13          NSW  410007     WATER  1979-01-16  491.710          130\n14          NSW  410007     WATER  1979-01-17  532.730          130\n\n\nSo, that’s not good. Especially the silent part"
  },
  {
    "objectID": "drones/overlaps_reactive.html",
    "href": "drones/overlaps_reactive.html",
    "title": "Drone flight calculations",
    "section": "",
    "text": "This is an interactive page based on more detailed exploration of drone overlaps.\nThis gives us the opportunity to plug in flight parameters and back-calculate others. For example, give us the size of the photos on the ground given height, the needed drone flight distance for a desired overlap at a given height, or the speed we need to fly at a given height to yield the right overlap given the photo interval.\nOverlap is fixed at 80% for the moment.\n\n\n\n\n\n\nimport { aq, op } from '@uwdata/arquero'\n\ndroned_aq = aq.from(transpose(droned))\ndronev_aq = aq.from(transpose(dronev))\ndronei_aq = aq.from(transpose(dronei))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof dronetype = Inputs.checkbox(\n  [\"Mini 2\", \"Phantom 4 Pro V2\"],\n  {value: [\"Mini 2\"],\n    label: \"Drone\"\n  }\n)\n\nviewof aspect = Inputs.checkbox(\n  [\"3:2\", \"4:3\", \"16:9\"],\n  {value: [\"4:3\"],\n    label: \"Aspect Ratio\"\n  }\n)\n\nviewof overlap_p = Inputs.range(\n  [0.7, 0.95],\n  {value: 0.8, step: 0.05, label: \"Overlap:\"}\n)\n\nviewof height = Inputs.range(\n  [1, 50],\n  {value: 5, step: 0.1, label: \"Altitude (m):\"}\n)\n\nviewof velocity = Inputs.range(\n  [0.1, 5],\n  {value: 1, step: 0.1, label: \"Velocity (m/s):\"}\n)\n\nviewof interval = Inputs.range(\n  [0.1, 5],\n  {value: 2, step: 0.1, label: \"Photo interval (s):\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the selected values\nGround distances (photo footprints) and needed photo distances to achieve overlap are\n\ndist_h\n  .select('altitude_m', 'overlap', 'direction', 'photo_distance_m', 'ground_distance_m')\n  .view()\n\n\n\n\n\n\nFor the selected interval and height, the needed velocity is\n\nvel_h\n  .select('altitude_m', 'overlap', 'intervals', 'velocity_ms')\n  .view()\n\n\n\n\n\n\nFor the selected velocity, the needed photo interval is\n\ni_h\n  .select('altitude_m', 'overlap', 'velocity_ms', 'photo_interval')\n  .view()\n\n\n\n\n\n\n\n\n\n\n\ndistfilter = droned_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect\n})\n  .filter((d, $) =&gt; op.includes(d.drone, $.dr))\n  .filter((d, $) =&gt; op.equal(d.overlap, $.ov))\n  .filter((d, $) =&gt; op.includes(d.aspect, $.asp))\n\ndist_h = distfilter\n  .params({\n  h: height\n})\n  .filter((d, $) =&gt; op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvfilter = dronev_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect,\n  inter: interval\n})\n  .filter((d, $) =&gt; op.includes(d.drone, $.dr))\n  .filter((d, $) =&gt; op.equal(d.overlap, $.ov))\n  .filter((d, $) =&gt; op.includes(d.aspect, $.asp))\n  .filter((d, $) =&gt; op.equal(d.intervals, $.inter))\n\nvel_h = vfilter\n  .params({\n  h: height\n})\n  .filter((d, $) =&gt; op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nifilter = dronei_aq\n  .params({\n  dr: dronetype,\n  ov: overlap_p,\n  asp: aspect,\n  v: velocity\n})\n  .filter((d, $) =&gt; op.includes(d.drone, $.dr))\n  .filter((d, $) =&gt; op.equal(d.overlap, $.ov))\n  .filter((d, $) =&gt; op.includes(d.aspect, $.asp))\n  .filter((d, $) =&gt; op.equal(d.velocity_ms, $.v))\n\ni_h = ifilter\n  .params({\n  h: height\n})\n  .filter((d, $) =&gt; op.equal(d.altitude_m, $.h))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGround distancePhoto distanceVelocityPhoto interval\n\n\n\nPlot.plot({\n  grid: false,\n  color: {\n    legend: true\n  },\n  marks: [\n    Plot.line(distfilter, {x: \"altitude_m\", y: \"ground_distance_m\", stroke: \"direction\"}),\n    Plot.dot(dist_h, {x: \"altitude_m\", y: \"ground_distance_m\", stroke: \"drone\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  color: {\n    legend: true\n  },\n  marks: [\n    Plot.line(distfilter, {x: \"altitude_m\", y: \"photo_distance_m\", stroke: \"direction\"}),\n    Plot.dot(dist_h, {x: \"altitude_m\", y: \"photo_distance_m\", stroke: \"drone\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(vfilter, {x: \"altitude_m\", y: \"velocity_ms\"}),\n    Plot.dot(vel_h, {x: \"altitude_m\", y: \"velocity_ms\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(ifilter, {x: \"altitude_m\", y: \"photo_interval\"}),\n    Plot.dot(i_h, {x: \"altitude_m\", y: \"photo_interval\"})\n  ]\n})"
  },
  {
    "objectID": "observable/trying_observable.html",
    "href": "observable/trying_observable.html",
    "title": "Trying observablejs chunks",
    "section": "",
    "text": "Quarto has the option of using Observable JS for code chunks. This gives the ability to add interactivity in-browser, without needing to do server-side calcs, as we do with Shiny. The catch is with Shiny, I can serve R objects (e.g. ggplots) that I’m familiar with. Using observable means I need to figure out how that system works. I’m also a bit unclear how much processing can happen. My understanding is that any actual processing that happens needs to happen in the ojs, not R chunks, so we can’t interactively change a setting in the observable chunk and have that kick off some R. Though I might be wrong.\nI have quite a few use cases in mind if I can get this to work- serving maps, as an interface to {vicwater}, some drone settings, playing with population models, etc."
  },
  {
    "objectID": "observable/trying_observable.html#issues",
    "href": "observable/trying_observable.html#issues",
    "title": "Trying observablejs chunks",
    "section": "ISSUES",
    "text": "ISSUES\n\nInteractive notebooks\nThe ojs chunks dont work in interactive mode, and throw errors like “Error in ojs_define(iris = iris) : could not find function”ojs_define”“. So to work on anything past the first ojs chunk requires rendering. But that brings us to the next issue:\n\n\nNo output\nObservable chunks don’t have output unless you use quarto preview, not just quarto render. And the ‘Render’ button in Rstudio renders and previews, making this more confusing. JUST RUNNING quarto render doc.qmd at the terminal yields a document with code chunks but no output. This is expected behavior, but is super counter intuitive, espcially given the Render button’s name.\nUnfortunately, there is no per-document quarto preview at the terminal like there is for quarto render. So if you’re working in a quarto project (website, book, etc), you have to preview the whole thing just to check a document.\nThat means you’ll almost certainly want to turn caching on for the project (probably do anyway if it’s big), but if caching is on for the quarto project, it won’t render because the ojs_defined object can’t be cached. So the chunk with ojs_define needs to have #| cache: false added to it. Or perhaps just turn caching off in the yaml headers for pages using ojs. Depends on how much pre-processing happens in R, probably.\n\n\nChunk options\nPython and R both use #| option: value for setting chunk options. ojs cells use //| option: value.\n\n\nColumn names\nObservable uses object.thing notation like python, but it also uses the . to reference columns in arquero. That means Sepal_Length is confusing, because it gets referenced as d.Sepal_Length. So change the names.\n\nnames(iris) &lt;- stringr::str_replace_all(names(iris), '\\\\.', '_')\n\n\n\nCode changes\nI’m running into issues where I change some R code, and it works when I run it interactively, but then when I go to render, the new R code just doesn’t happen. E.g. I’ll add code that makes a dataframe with more values, and I can see them interactively in R, but they don’t appear in the render. I think it has something to do with the cache not resetting with changes, but I’m not positive."
  },
  {
    "objectID": "observable/trying_observable.html#r-setup",
    "href": "observable/trying_observable.html#r-setup",
    "title": "Trying observablejs chunks",
    "section": "R setup",
    "text": "R setup\n\nlibrary(ggplot2)\n\nI know the cool thing to do is {palmerpenguins}, but I’m just going to use {iris}.\nI have a feeling ojs is likely just as happy plotting vectors, but I’ll tend to have dataframes from analyses, so let’s stick with that.\nTo start, can we reproduce a simple ggplot?\n\nggplot(iris, aes(x = Sepal_Length, y = Sepal_Width, color = Species)) + geom_point()\n\n\n\n\nLet’s try that without reactivity to start."
  },
  {
    "objectID": "observable/trying_observable.html#data-to-ojs",
    "href": "observable/trying_observable.html#data-to-ojs",
    "title": "Trying observablejs chunks",
    "section": "Data to ojs",
    "text": "Data to ojs\nIt seems like Arquero makes a lot of sense, since it’s dplyr-like. But the example (and all other examples I can find) use it to read data in from an external file (csv, json, etc). That’s almost never what I’m going to want to do. So, how do I get a dataframe into Arquero? I’m guessing I can’t just grab it. The data sources documentation says we need to use ojs_define in R to make things available. Let’s see if we can do that and then make it an arquero object?\nNOTE I’ve not seen this mentioned anywhere, but ojs_define cannot be found in an interactive session- it’s only available on render. So interactively it throws “Error in ojs_define(iris = iris) : could not find function”ojs_define”“. AND, if caching is on for the quarto project, it won’t render because the ojs_defined object can’t be cached.\n\nojs_define(iris = iris)\n\nCan I see that in ojs? I thought .view made tables? Maybe not if we haven’t imported arquero? But I also thought order didn’t matter for ojs.\n\niris.view()\n\n\n\n\n\n\nAnyway, we can see it as an object (once we preview instead of render). I’m still grumpy about that.\n\niris\n\n\n\n\n\n\nI think we usually need to transpose according to various stackexchanges.\n\ntiris = transpose(iris)\ntiris\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, the arquero docs seem to suggest I might be able to use from to make it arquero?\n\nimport { aq, op } from '@uwdata/arquero'\nirtab = aq.from(iris)\n\nirtab.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat does look like it’s the wrong dims, let’s used the transposed.\n\ntirtab = aq.from(tiris)\ntirtab.view()"
  },
  {
    "objectID": "observable/trying_observable.html#plots",
    "href": "observable/trying_observable.html#plots",
    "title": "Trying observablejs chunks",
    "section": "Plots",
    "text": "Plots\nTo make a first plot, do something I’m pretty sure should work, stolen directly from the penguins example that starts with a dataframe and just modifying the name and removing a facet level.\n\nPlot.rectY(tiris,\n  Plot.binX(\n    {y: \"count\"},\n    {x: \"Sepal_Width\", fill: \"Species\", thresholds: 10}\n  ))\n  .plot({\n    facet: {\n      data: tiris,\n      y: \"Species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\nScatter\nNow, can we make a scatter?\n\nPlot.dot(tiris, {x: \"Sepal_Length\", y: \"Sepal_Width\", fill: \"Species\"}).plot()\n\n\n\n\n\n\nThere’s lots of cleanup we could do to make that look different, but let’s go with that for now.\nCan I make a line? It’ll be jumbled, but whatever. Maybe I can sort it at least with arquero.\nRemember to use the arquero version of the data- this barfs with tiris.\n\ntirtab\n  .orderby('Sepal_Length')\n  .view()\n\n\n\n\n\n\nNow, how to plot that? Does the chunk above order tirtab permanently? Doesn’t seem to\n\ntirtab.view()\n\n\n\n\n\n\n\n\nLine\nLet’s try the line with the orderby\n\nPlot.line(tirtab.orderby('Sepal_Length'), {x: \"Sepal_Length\", y: \"Sepal_Width\", fill: \"Species\"}).plot()\n\n\n\n\n\n\nThat seems to have worked, but it sure is goofy looking. Oh. Is it because i’m using fill? Use stroke (not color- this isn’t ggplot).\n\nPlot.line(tirtab.orderby('Sepal_Length'), {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}).plot()\n\n\n\n\n\n\nWould be good to not do the data processing inside the plot call.\nI assume that’s as easy as\n\nirorder = tirtab.orderby('Sepal_Length')\n\nPlot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}).plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoint and line\nNeed to figure this out. Can I just do both? I think the answer might be to use a Plot.plot with mutliple marks?\nFirst, how does that syntax work? This should just recreate the above, right?\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"})\n  ]\n})\n\n\n\n\n\n\ncan we just add more Plot.marktypes?\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nCool. Would be nice if there was a ggplot-esque way to use the same x,y,color and just change the marks. Maybe there is? Look for that later.\nMoving toward reactivity, let’s say I only want dots where Sepal_Length &gt; 5 and &lt; 6\nI don’t quite seem to know the filter syntax. Not entirely sure what the d=&gt; means. Seems to be an internal reference to the data, but that feels weird and extra. I can get it to work, but doing anything complicated will require more thinking I think. Note that almost all the examples I can find use op.operation and so confused me for a bit thinking I needed op. The op access mathematical operations like abs, round, etc, and here I just need a simple &gt;&lt;.\n\nsl_filter = irorder\n  .filter(d =&gt; (d.Sepal_Length &lt; 6 & d.Sepal_Length &gt; 5))\n  \nsl_filter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow use that in a plot\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(irorder, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(sl_filter, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nThat seems to work"
  },
  {
    "objectID": "observable/trying_observable.html#reactivity",
    "href": "observable/trying_observable.html#reactivity",
    "title": "Trying observablejs chunks",
    "section": "Reactivity",
    "text": "Reactivity\nThe thing here is to use viewof and Inputs.typeofinput. But what are those types? The observable docs seem to have a good overview.\nLet’s replicate the above, but also allow selecting the species. Basically following the quarto docs, but with a couple modifications. There’s got to be a way to obtain the ranges, species names, etc in code and not hardcode them in.\n\nviewof min_sl = Inputs.range(\n  [4, 8],\n  {value: 5, step: 0.1, label: \"Min Sepal Length:\"}\n)\n\nviewof max_sl = Inputs.range(\n  [4, 8],\n  {value: 6, step: 0.1, label: \"Max Sepal Length:\"}\n)\n\nviewof sp = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI like the tabset thing they do in the help, but to keep it simple just make the plot.\nI’m going to filter the data in its own chunk to try to aid figuring this out.\nThe .params here is needed to use the reactive values, and then gets referenced as $, while the data is d.\n\nspfilter = irorder\n  .params({\n  spf: sp\n})\n  .filter((d, $) =&gt; op.includes($.spf, d.Species))\n  \nspfilter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsizefilter = spfilter\n  .params({\n  minsl: min_sl,\n  maxsl: max_sl\n})\n  .filter((d, $) =&gt; d.Sepal_Length &gt; $.minsl && d.Sepal_Length &lt; $.maxsl)\n  \nsizefilter.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd plot\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter, {x: \"Sepal_Length\", y: \"Sepal_Width\", stroke: \"Species\"}),\n    Plot.dot(sizefilter, {x: \"Sepal_Length\", y: \"Sepal_Width\"})\n  ]\n})\n\n\n\n\n\n\nThat seems to work. Can I package it up pretty like in the example?"
  },
  {
    "objectID": "observable/trying_observable.html#making-better-ux",
    "href": "observable/trying_observable.html#making-better-ux",
    "title": "Trying observablejs chunks",
    "section": "Making better UX",
    "text": "Making better UX\nLet’s build that same thing, but at least kill off displaying code. We need to use different names here because ojs is reactive and so you can’t define variables in two places. Maybe I’ll just use petals instead of sepals.\n\nviewof min_pl = Inputs.range(\n  [1, 7],\n  {value: 5, step: 0.1, label: \"Min Petal Length:\"}\n)\n\nviewof max_pl = Inputs.range(\n  [1, 7],\n  {value: 6, step: 0.1, label: \"Max Petal Length:\"}\n)\n\nviewof sp2 = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [\"versicolor\"],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspfilter2 = irorder\n  .orderby('Petal_Length')\n  .params({\n  spf: sp2\n})\n  .filter((d, $) =&gt; op.includes($.spf, d.Species))\n\npetfilter = spfilter2\n  .params({\n  minpl: min_pl,\n  maxpl: max_pl\n})\n  .filter((d, $) =&gt; d.Petal_Length &gt; $.minpl && d.Petal_Length &lt; $.maxpl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter2, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\nFancy layouts\nSee the quarto layouts docs for help here, I’ll only try a couple things.\n\nTabset\nMake a tabset with the plot and data\n\nPlotData\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter2, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\npetfilter.view()\n\n\n\n\n\n\n\n\n\n\n\nSidebar panel\nNeed to rename the inputs to avoid double-naming\n\n\nviewof min_pl3 = Inputs.range(\n  [1, 7],\n  {value: 5, step: 0.1, label: \"Min Petal Length:\"}\n)\n\nviewof max_pl3 = Inputs.range(\n  [1, 7],\n  {value: 6, step: 0.1, label: \"Max Petal Length:\"}\n)\n\nviewof sp3 = Inputs.checkbox(\n  [\"setosa\", \"versicolor\", \"virginica\"],\n  {value: [\"versicolor\"],\n    label: \"Species\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: false,\n  marks: [\n    Plot.line(spfilter3, {x: \"Petal_Length\", y: \"Petal_Width\", stroke: \"Species\"}),\n    Plot.dot(petfilter3, {x: \"Petal_Length\", y: \"Petal_Width\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\nspfilter3 = irorder\n  .orderby('Petal_Length')\n  .params({\n  spf: sp3\n})\n  .filter((d, $) =&gt; op.includes($.spf, d.Species))\n\npetfilter3 = spfilter3\n  .params({\n  minpl: min_pl3,\n  maxpl: max_pl3\n})\n  .filter((d, $) =&gt; d.Petal_Length &gt; $.minpl && d.Petal_Length &lt; $.maxpl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI think I’ll stop there. There’s lots more I could do to get better at using observable, and lots more I could do to integrate that with Quarto layouts, but my goal here was to figure out how to get it to work, and those other things will make more sense with specific use cases or their own quartos or something."
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html",
    "href": "parallelism/changing_batchtools_template.html",
    "title": "Changing batchtools template",
    "section": "",
    "text": "I got future.batchtools working, and now I have a bunch of follow-up tests.\nCan we call a different template, even if it’s not named batchtools.slurm.tmpl?\nIn my slurm testing repo, I have the default templates from future.batchtools and batchtools saved in /batchtools_templates. The one from future.batchtools is also saved as batchtools.slurm.tmpl in the outer directory (where it gets found by default and we know it works)."
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html#modify-path",
    "href": "parallelism/changing_batchtools_template.html#modify-path",
    "title": "Changing batchtools template",
    "section": "Modify path",
    "text": "Modify path\nFirst, I’ll just send a path, as in plan(batchtools_slurm, template = \"/path/to/batchtools.slurm.tmpl\") to the original template from future.batchtools that I copied to make /batchtools.slurm.tmpl.\nAnd then I’ll use the batchtools template and see how that works.\nTo do both of these, I’ll modify slurm_r_tests/testing_future_batchtools.R to take the path as an argument so I can pass it from the command line.\nI could loop over that like I did when testing single-node plans but I think I don’t for the moment.\nCheck that the default works (no path argument at command line, sbatch batchtools_R.sh testing_future_batchtools.R. That works.\nThe same template but with a different name and in a subdirectory works if we’re careful with the path-\nsbatch batchtools_R.sh testing_future_batchtools.R ./batchtools_templates/slur\nm.tmpl\nThe template that comes from batchtools doesn’t work.\nsbatch batchtools_R.sh testing_future_batchtools.R ./batchtools_templates/slur\nm-simple.tmpl\nWhy not? I think because it doesn’t actually specify any resources. It expects that to come in from elsewhere.\nSo, let’s figure out resource specification, and then try that one again."
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html#specifying-resources",
    "href": "parallelism/changing_batchtools_template.html#specifying-resources",
    "title": "Changing batchtools template",
    "section": "Specifying resources",
    "text": "Specifying resources\nRemember, this is per job.\nWhat I want to do is use tweak- e.g. tweak(batchtools_slurm, resources = list(ncpus = 10, nodes = 2)). Again, remembering this is what gets requested for every job-\n\nso does it ever make sense to request nodes &gt; 1?\nShould I only request ncpus &gt; 1 if I use list-plans to then go to multisession or multicore?\n\nBut first, does that let me use the slurm-simple template that didn’t request anything itself?\nI’ve done it by hardcoding some resource requests in tweak_resources.R in the test repo, but they could be build in a script if we wanted.\nDidn’t work. But I think the answer is in th %&lt; resources$whatever &gt;% - THOSE ARE THE NAMES. AND I CAN CHANGE WHAT THE LIST RETURNS- character instead of integer, for ex.\nSo the slurm-simple.tmpl from batchtools has\n#SBATCH --job-name=&lt;%= job.name %&gt;\n#SBATCH --output=&lt;%= log.file %&gt;\n#SBATCH --error=&lt;%= log.file %&gt;\n#SBATCH --time=&lt;%= ceiling(resources$walltime / 60) %&gt;\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=&lt;%= resources$ncpus %&gt;\n#SBATCH --mem-per-cpu=&lt;%= resources$memory %&gt;\nAnd so only lets us set those values (job-name, output, error, time, ntasks, cpus-per-task, and mem-per-cpu), and we have to do that in the right slots of resources and with the right type- e.g. resources$walltime sets --time, and has to be numeric. BUT, we could change that with a different template file that just uses the character vector “hh:mm:ss” (as I do with the non-template any_R.sh. Or passing things like 4GB instead of memory in mb.\nNow, if I run that with resources$ncpus = 12, I get the same output as before. But I think I’m using 100 cpus, but each one is also sitting on 11 others. I’m just not saving what I need to check. The ntasks instead of nodes is a bit confusing too- I thought tasks were threads on the cpu. Maybe that is the case- the hardcode nthreads = 1 here says don’t thread below the cpu level. And no node request I assume just defaults to 1.\nOR if we aren’t defining nodes, does slurm just auto-assign work to cpus across nodes? ie node-agnostic? And then we don’t have to worry about necessarily matching work to CPUs on nodes?\nI think the future.batchtools template batchtools_templates/slurm.tmpl is more flexible. Instead of individually filling parts of the slurm script as above, it just fills whatever options we want. It has the minimal set to get things to run hardcoded, but the section\n## Resources needed:\n&lt;% if (length(resources) &gt; 0) {\n  opts &lt;- unlist(resources, use.names = TRUE)\n  opts &lt;- sprintf(\"--%s=%s\", names(opts), opts)\n  opts &lt;- paste(opts, collapse = \" \") %&gt;\n#SBATCH &lt;%= opts %&gt;\n&lt;% } %&gt;\nJust writes in anything. So, can I get that to work in tweak_resources.R? It should be easier, but wasn’t working for me."
  },
  {
    "objectID": "parallelism/changing_batchtools_template.html#tweak-has-to-match-the-template",
    "href": "parallelism/changing_batchtools_template.html#tweak-has-to-match-the-template",
    "title": "Changing batchtools template",
    "section": "Tweak has to match the template!",
    "text": "Tweak has to match the template!\nSo, that means the way tweak(batchtools_slurm, resources = …) works is template-specific. Some might not have parsing for what gets passed, sometimes it might be the wrong type, etc).\n\nExamples\nplan(tweak(batchtools_slurm,\n           template = \"./batchtools_templates/slurm-simple.tmpl\",\n           resources = list(ncpus = 12,\n                            memory = 1000,\n                            walltime=60*5)))\nFor the slurm-simple.tmpl, the SLURM --time is referenced to resources$walltime and gets divided by 60 so has to be a numeric in seconds.\nThe ncpus = 12 here gets 12 CPUs that all get assigned (kinda weirdly though, with --cpus-per-task), even though we only use one- see the top of the output\n## Nodes and pids\n# A tibble: 100 × 6\n# Groups:   all_job_nodes, node, pid, taskid [100]\n    all_job_nodes node             pid taskid cpus_avail n_reps\n    &lt;chr&gt;         &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;int&gt;\n  1 gandalf-vm02  gandalf-vm02 3329413 0      12              6\n  2 gandalf-vm02  gandalf-vm02 3329479 0      12              7\n  3 gandalf-vm02  gandalf-vm02 3329545 0      12              7\n  4 gandalf-vm02  gandalf-vm02 3329611 0      12              6\n  5 gandalf-vm02  gandalf-vm02 3329679 0      12              7\n  6 gandalf-vm02  gandalf-vm02 3329747 0      12              6\n  7 gandalf-vm02  gandalf-vm02 3329811 0      12              6\nWhereas cpus_avail is 1 without that line-\n## Nodes and pids\n# A tibble: 100 × 6\n# Groups:   all_job_nodes, node, pid, taskid [100]\n    all_job_nodes node             pid taskid cpus_avail n_reps\n    &lt;chr&gt;         &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;int&gt;\n  1 gandalf-vm01  gandalf-vm01   12812 0      1               6\n  2 gandalf-vm01  gandalf-vm01   12881 0      1               6\n  3 gandalf-vm01  gandalf-vm01   12966 0      1               6\n  4 gandalf-vm01  gandalf-vm01   13043 0      1               6\n  5 gandalf-vm01  gandalf-vm01   13117 0      1               6\n  6 gandalf-vm01  gandalf-vm01   13179 0      1               6\n  7 gandalf-vm01  gandalf-vm01   13261 0      1               6\nIt looks like a major catch here is I can’t use names in the resources list with dashes, e.g. ntasks-per-node. And so to set those i’ll have to translate, as in slurm-simple, I think. Unless batchtools auto-translates under the hood, but I think not.\nGoing to have to come back to this. It’d be nice if it were possible. How did that github issue do it? It used ncpus in slurm-simple.tmpl. So maybe I’ll just do that for now. Then I can get on with checking the use of chunks and arrays and nodes and nesting. And whether i can use the ncpus thing to get (and use) more cpus than exist on single nodes.\nDoes that mean I can auto-generate jobnames???? That would be great"
  },
  {
    "objectID": "parallelism/globals_speed.html",
    "href": "parallelism/globals_speed.html",
    "title": "Foreach globals and speed",
    "section": "",
    "text": "I previously tested the impact of unused globals on speed, but only briefly. Here, I’ll be more systematic, because it gets tricky fast if we need to be super careful about what objects exist in the global environment.\nThere are a couple things to check here\nI’ll tackle these by\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nlibrary(future.apply)\nlibrary(furrr)\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nlibrary(microbenchmark)\nregisterDoFuture()\nplan(multisession)"
  },
  {
    "objectID": "parallelism/globals_speed.html#nothing-exists",
    "href": "parallelism/globals_speed.html#nothing-exists",
    "title": "Foreach globals and speed",
    "section": "Nothing exists",
    "text": "Nothing exists\nWell, almost nothing. I’m going to set a couple scalars and define a function for furrr and future.apply . I’m not using any of the globals or export arguments in the functions.\n\nBare\n\nn_reps = 100\nsize &lt;- 1000\n\nfn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n\n\nBenchmark\n\nmicrobenchmark(\n  dofut0 = {foreach(i = 1:n_reps, \n                       .combine = cbind) %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  furr0 = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE))},\n  \n  fuapply0 = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE)},\n  times = 10\n)\n\nUnit: milliseconds\n     expr      min       lq     mean   median       uq       max neval\n   dofut0 666.6571 694.0119 845.0106 718.1011 738.6922 2018.6689    10\n    furr0 696.5679 716.0437 918.4407 727.5611 745.7060 2626.3897    10\n fuapply0 672.0328 708.2940 728.8155 726.5226 762.0094  771.1695    10\n\n\nSo, doFuture and furrr are slower than future.apply, but not by a ton. The key thing here is this sets the baseline, so we can see if things slow down once we have big objects in memory.\n\n\n\nInside a function\nThese functions are from testing parallel speed, though they have different names here. I’ve added the ability to change the way they handle globals so I don’t have to write new functions for comparing that later, with the default set at the function default.\n\nforeach\n\nforeach_fun &lt;- function(n_reps = 100, size = 1000, .export = NULL, .noexport = NULL) {\n  c_foreach &lt;- foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .export = .export,\n                       .noexport = .noexport) %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  return(c_foreach)\n}\n\n\n\nfurrrr\n\nfurrr_fun &lt;- function(n_reps = 100, size = 1000, globals = TRUE) {\n  fn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  \n  c_map &lt;- future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, \n                                               globals = globals))\n  matrix(unlist(c_map), ncol = n_reps)\n}\n\n\n\nfuture.apply\n\nfuapply_fun &lt;- function(n_reps = 100, size = 1000, future.globals = TRUE) {\n    fn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n    }\n    \n  c_apply &lt;- future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE,\n                           future.globals = future.globals)\n  \n    matrix(unlist(c_apply), ncol = n_reps)\n}\n\n\n\nBenchmark\n\nmicrobenchmark(\n  dofut_fun = foreach_fun(n_reps = 100, size = 1000),\n  fur_fun = furrr_fun(n_reps = 100, size = 1000),\n  app_fun = fuapply_fun(n_reps = 100, size = 1000),\n  times = 10\n)\n\nUnit: milliseconds\n      expr      min       lq     mean   median       uq      max neval\n dofut_fun 688.9580 715.9357 725.7951 725.2115 742.3267 745.3953    10\n   fur_fun 717.4536 727.3583 745.8955 740.4063 769.3066 784.2487    10\n   app_fun 720.7231 741.9849 757.3939 751.2876 762.0890 831.7847    10\n\n\nThis sets the other baseline before we have big objects in memory, so we can see if things respond differently when used inside a function’s environment vs directly in the global. Now all three functions are basically equivalent."
  },
  {
    "objectID": "parallelism/globals_speed.html#with-big-global",
    "href": "parallelism/globals_speed.html#with-big-global",
    "title": "Foreach globals and speed",
    "section": "With big global",
    "text": "With big global\nDefault future.globals.maxsize is 500MB. Should i increase that, or just try to hit it? I think just try to get just under it.\n\n# This is 1.6GB\n# big_obj &lt;- matrix(rnorm(20000*10000), nrow = 10000)\n# 496 MB\nbig_obj &lt;- matrix(rnorm(10000*6200), nrow = 10000)\n\nNow, same tests as before, and some that reference it but don’t use it.\nThe comparisons to make here are:\n\nMatched to above- does just having the object exist slow things down, even if not called?\nReferenced and not- does it only get passed in if asked for and slow things down?\n\nNot exactly sure how I’ll check that. Maybe instead of referencing it in the function (which is hard to do without using it, especially with furrr and future.apply), I’ll explicitly send it in with their globals arguments.\n\n\n\nBare\n\nBenchmark\nI’m going to run this for default (no global argument), explicitly sending them in, and explicitly excluding them.\n\nmicrobenchmark(\n  # default- same as above, but now big_obj exists, but is not used in the actual processing\n  dofut0 = {foreach(i = 1:n_reps, \n                       .combine = cbind) %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  furr0 = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE))},\n  \n  fuapply0 = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE)},\n  \n  # Explicitly telling it not to send big global (I can't sort out getting .export to work)\n  dofut_no_g = {foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .noexport = \"big_obj\") %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  \n  furr_no_g = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, \n                                               globals = FALSE))},\n  \n  fuapply_no_g = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE,\n                           future.globals = FALSE)},\n  \n  # Explicitly telling it to send the unused global\n  dofut_g = {foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .export = 'big_obj') %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n                       }},\n  \n  furr_g = {future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, \n                                               globals = 'big_obj'))},\n  \n  fuapply_g = {future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE,\n                           future.globals = 'big_obj')},\n  \n  times = 10\n)\n\nUnit: milliseconds\n         expr       min        lq      mean    median        uq       max neval\n       dofut0  622.3521  627.1805  684.9883  703.2614  712.3879  771.1737    10\n        furr0  617.5179  651.7789  691.6123  694.2537  735.1046  776.9627    10\n     fuapply0  634.4952  641.2787  682.2679  683.2625  710.3911  742.8530    10\n   dofut_no_g  608.3405  663.3900  682.8678  669.4480  717.3878  749.9284    10\n    furr_no_g  621.6507  643.7371  673.7533  667.9665  703.6524  724.5566    10\n fuapply_no_g  622.4711  652.5555  679.7137  669.7782  690.2530  762.7595    10\n      dofut_g 7463.9133 7746.4635 7956.0593 7915.0786 8241.5531 8473.0075    10\n       furr_g 7391.4087 7488.0329 7746.6185 7685.3700 7854.3536 8301.3615    10\n    fuapply_g 7419.2943 7690.8532 7948.7242 7942.7046 8197.5952 8526.3078    10\n\n\nNow there’s a big object sitting in global memory, but it does not slow down the default run relative to the enforced-non-pass version or the version from before it existed (above). It does show major slowdown when it is explicitly passed.\nUnused globals therefore are NOT passed by default, even when code is running straight in the global environment.\n\n\n\nInside functions\nThe functions have an option to change the way globals are handled.\n\nBenchmark\n\nmicrobenchmark(\n  # default\n  dofut_default = foreach_fun(n_reps = 100, size = 1000),\n  fur_default = furrr_fun(n_reps = 100, size = 1000),\n  app_default = fuapply_fun(n_reps = 100, size = 1000),\n  \n  # No globals\n  dofut_no_g = foreach_fun(n_reps = 100, size = 1000, .noexport = 'big_obj'),\n  fur_no_g = furrr_fun(n_reps = 100, size = 1000,\n                          globals = FALSE),\n  app_no_g = fuapply_fun(n_reps = 100, size = 1000,\n                            future.globals = FALSE),\n  \n  # Explicit globals\n  dofut_g = foreach_fun(n_reps = 100, size = 1000,\n                              .export = 'big_obj'),\n  fur_g = furrr_fun(n_reps = 100, size = 1000, \n                          globals = 'big_obj'),\n  app_g = fuapply_fun(n_reps = 100, size = 1000,\n                            future.globals = 'big_obj'),\n  \n  \n  times = 10\n)\n\nUnit: milliseconds\n          expr       min        lq      mean    median        uq       max\n dofut_default  629.1086  654.8935  664.8788  669.1030  676.3397  699.0022\n   fur_default  648.5858  669.6395  689.7485  682.5734  699.5598  746.6292\n   app_default  651.2509  655.6757  667.3066  664.9745  669.3691  713.9691\n    dofut_no_g  623.7273  641.6626  661.3558  668.6618  678.1437  684.8114\n      fur_no_g  632.2700  639.9049  648.2439  643.9862  652.9461  687.4514\n      app_no_g  630.7245  653.2999  672.8701  674.8392  693.8775  712.1876\n       dofut_g 7281.1734 7421.9364 7509.7354 7506.4282 7633.5690 7718.6878\n         fur_g 7242.9499 7329.6440 7432.2407 7441.0199 7537.4374 7638.8029\n         app_g 7250.6937 7398.2939 7575.0305 7490.0815 7632.4715 8491.8494\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nUsing functions yields the same result as before- the big objects sitting in the global environment do not get passed in and slow things down if they aren’t actually used in the functions (or explicitly sent in).\nUnused globals therefore are NOT passed by default into parallelised functions."
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html",
    "href": "parallelism/initial_future_batchtools.html",
    "title": "Using future.batchtools",
    "section": "",
    "text": "I need to sort out how to use futures for parallel processing on the HPC.There’s a few things I’ve tried previously that didn’t work, and a way I’ve cobbled together that’s not ideal.\nThere are some issues with my current approach"
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#what-do-i-want",
    "href": "parallelism/initial_future_batchtools.html#what-do-i-want",
    "title": "Using future.batchtools",
    "section": "What do I want?",
    "text": "What do I want?\nThere are some improvements I want to make that will make my code work better (and faster)\n\nMinor (if any) changes when running locally or on an HPC\nIn-code splitting of work into nodes to balance the work across them\nMake sure we’re using both nodes and cores within them\n\nThe main solution seems to be using future.batchtools, but I wasn’t able to get it working quickly.\n\nSome questions\nThere are a few big-picture things I have questions about that aren’t clear from reading the docs (in addition to just ‘how do we get a run to work’)\n\nHow do jobs actually start? I think, but am not 100% sure, that the R script essentailly builds slurm bash scripts and then calls sbatch. Is that what happens?\nDo we still use sbatch or other command-line bash at all? Or is everything managed in R? If so, how do we actually start the runs? Rscript? srun on an R control script?\n\nif Rscript, do we end up with that main R process running in the login node the whole time? What about if srun or…\nThis issue implies we need to leave R running. But can it run with Rscript? srun?sinteractive? sbatch any_R.sh analysis_script.R? With any_R.sh having low resources but maybe long walltime? It looks like srun barfs to the terminal and blocks, while sbatch outputs to file and is non-blocking. So that’s likely the way to go.\n\nDoes it make sense to manage nodes and cores separately, or do we just ask for a ton of cores and it auto-manages nodes to get them?\n\nI think {slurmR} with plan(cluster) does the latter, but not positive\nI’m not actually sure what future.batchtools does by default (will check), but I think a list-plan likely makes sense.\n\ndo we use SLURM job arrays? Or does it generate a bunch of batch scripts that get called as separate jobs instead of array jobs? Does it matter?\n\nwhat are chunks.\n\nIf we feed it a big set of iterations, does it send each one to its own node? Its own core? Is there any chunking?"
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#templates",
    "href": "parallelism/initial_future_batchtools.html#templates",
    "title": "Using future.batchtools",
    "section": "Templates",
    "text": "Templates\nI’m still a bit confused by the overall workflow, but it’s clear I need a template. There’s one in future.batchtools github, and a few at the batchtools github.\nThen I think in the plan call, we tweak that? Let’s just get it working. Trying first with the one that comes from future.batchtools. Though the one from batchtools looks like it has more capability for doing things like managing cores on nodes. Maybe try them both as we go?\nI have both of those. I kind of want to test both. The docs say the template should be either at ./batchtools.slurm.tmpl (associated with a particular working directory) or ~/.batchtools.slurm.tmpl (for all processes to find it). But I want to be able to test multiple templates. I should be able to use\nplan(batchtools_slurm, template = \"/path/to/batchtools.slurm.tmpl\")\nbut its a bit unclear whether the templates still need to be named batchtools.slurm.tmpl, or can have whatever filename we want, as long as I give the path. Guess I’ll test that. Try first with it as batchtools.slurm.tmpl in the repo directory first though."
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#try-a-simple-job",
    "href": "parallelism/initial_future_batchtools.html#try-a-simple-job",
    "title": "Using future.batchtools",
    "section": "Try a simple job",
    "text": "Try a simple job\nUse the future.batchtools template, and a foreach loop using %:%.\nStart with Rscript filename.R\nIt starts printing directly to terminal, which is annoying.\nOpening another terminal and typing squeue shows that I have 4 nodes- though actually that was just at that moment.\nIt doesn’t seem to produce stdout or stderr, which is going to make it tricky to see what happened. See below- this is true unless we run the master R session through sbatch.\nI can copy in from the terminal output:\n::: {#Simple output} Loading required package: foreach Loading required package: future Warning message: package ‘future’ was built under R version 4.0.5 Loading required package: parallelly Warning messages: 1: package ‘future.batchtools’ was built under R version 4.0.5 2: package ‘parallelly’ was built under R version 4.0.5\nPlan is: List of future strategies: 1. batchtools_slurm: - args: function (expr, envir = parent.frame(), substitute = TRUE, globals = TRUE, label = NULL, template = NULL, resources = list(), workers = NULL, registry = list(), …) - tweaked: FALSE - call: plan(batchtools_slurm)\n\navailable workers:\nlocalhost localhost localhost localhost localhost localhost localhost localhost\n\n\ntotal workers:\n8\n\n\nunique workers:\nlocalhost\n\n\navailable Cores:\n\nnon-slurm\n8\n\n\nslurm method\n1\n\n\n\nMain PID:\n3195570 There were 50 or more warnings (use warnings() to see the first 50)\n\n\nUnique processes\n100\nIDs of all cores used\n238059 1524768 1518289 1500375 1513942 3264114 1269189 1345061 2623254 238128 3264182 1269259 1345124 1524852 1518372 3264251 238197 1500450 1514026 1524930 1518447 238268 3264327 1500525 1514101 238335 1525010 1518527 1500596 1514176 3264408 238401 1525084 1518604 1500666 1514254 3264478 1518681 238466 1525164 1500739 1518756 1514328 238530 1500811 1525241 1514402 1518828 1500879 238594 1525316 1514479 3264562 1518901 1500952 238662 1525391 1518977 1514555 1501019 3264640 238727 1525467 1519049 1514630 238791 1501090 1525542 3264715 1519121 1501158 238857 1525621 1519194 1514708 1501225 3264796 238922 1525693 1519271 1514786 1501292 3264863 1525766 238989 1519345 1514861 1525839 1501361 239060 3264939 1519420 1514936 1501428 1525912 239129 3265003 1519494 1501496 1525987"
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#nodes-and-pids",
    "href": "parallelism/initial_future_batchtools.html#nodes-and-pids",
    "title": "Using future.batchtools",
    "section": "Nodes and pids",
    "text": "Nodes and pids\n           238059 238128 238197 238268 238335 238401 238466 238530 238594\nbilbo 6 6 6 6 6 6 7 6 7 frodo-vs01 0 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0 0\n           238662 238727 238791 238857 238922 238989 239060 239129 1269189\nbilbo 6 7 7 6 7 6 7 6 0 frodo-vs01 0 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 7 gandalf-vm04 0 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0 0\n           1269259 1345061 1345124 1500375 1500450 1500525 1500596 1500666\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 6 6 6 6 7 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 6 0 0 0 0 0 0 0 gandalf-vm04 0 6 6 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1500739 1500811 1500879 1500952 1501019 1501090 1501158 1501225\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 6 6 6 6 6 6 6 6 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1501292 1501361 1501428 1501496 1513942 1514026 1514101 1514176\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 7 6 7 6 0 0 0 0 frodo-vs04 0 0 0 0 6 7 6 6 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1514254 1514328 1514402 1514479 1514555 1514630 1514708 1514786\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 6 7 7 6 6 6 6 6 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1514861 1514936 1518289 1518372 1518447 1518527 1518604 1518681\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 7 7 6 6 6 6 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 6 6 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1518756 1518828 1518901 1518977 1519049 1519121 1519194 1519271\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 6 6 7 7 6 7 7 6 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1519345 1519420 1519494 1524768 1524852 1524930 1525010 1525084\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 6 6 6 7 6 frodo-vs02 7 6 7 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1525164 1525241 1525316 1525391 1525467 1525542 1525621 1525693\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 6 6 6 6 6 6 6 6 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 0 0 0 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           1525766 1525839 1525912 1525987 2623254 3264114 3264182 3264251\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 6 6 6 6 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 0 0 0 0 0 6 7 6 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 6 0 0 0\n           3264327 3264408 3264478 3264562 3264640 3264715 3264796 3264863\nbilbo 0 0 0 0 0 0 0 0 frodo-vs01 0 0 0 0 0 0 0 0 frodo-vs02 0 0 0 0 0 0 0 0 frodo-vs03 0 0 0 0 0 0 0 0 frodo-vs04 0 0 0 0 0 0 0 0 gandalf-vm02 7 7 6 6 6 6 6 6 gandalf-vm03 0 0 0 0 0 0 0 0 gandalf-vm04 0 0 0 0 0 0 0 0 gandalf-vm05 0 0 0 0 0 0 0 0\n           3264939 3265003\nbilbo 0 0 frodo-vs01 0 0 frodo-vs02 0 0 frodo-vs03 0 0 frodo-vs04 0 0 gandalf-vm02 6 6 gandalf-vm03 0 0 gandalf-vm04 0 0 gandalf-vm05 0 0 :::\nThe formatting of the table is all boogered up, but that looks like I got 9 nodes, and used something like 9-14 PIDs each, each about 7-6 times. It’s a bit confusing, because sinfo --Node --long shows that the number of CPUs is different across those nodes. And that seems to be what happened here. I need to change the output so I can better see what I used. But, roughly, that looks about right- it send jobs to CPUs as needed.\n\n# number of iterations\n25*25\n\n[1] 625\n\n9*12*6\n\n[1] 648"
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#calling-and-output",
    "href": "parallelism/initial_future_batchtools.html#calling-and-output",
    "title": "Using future.batchtools",
    "section": "Calling and output",
    "text": "Calling and output\nWe seem to need to leave an R session running to operate the futures. We can probably do that interactively module load R, R, then run the code, or Rscript. But that’s all interactive. As is srun. And it all prints all printable output to the terminal, and doesn’t save it. I think an sbatch might be the way to go. Try that.\nThat does run, and spawn the others\n\nIt’s silly that we need another wrapper call, but maybe that’s ok. Gives us the option to do some auto-setup in an intermediate script.\nrepeatedly calling squeue shows something interesting- it looks like it’s creating separate jobs for each task, not doing anything node-aware (e.g. chunking jobs to a node, and then multisessioning). I guess that makes sense, based on the help and other testing, but we can almost certainly speed things up by chunking and list-planning.\nIf the passing-object overhead really becomes an issue, we might want to just use batchtools directly, which I think also could remove the need to leave a master R session running. But I think that gets rid of the major advantage of being able to use essentially the same code locally and on the HPC, just by changing the plan.\nThe output using sbatch batchtools_R.sh testing_future_batchtools.R (where batchtools_R.sh is a lightweight master job) is\nℹ Using R 4.0.3 (lockfile was generated with R 4.2.2)\n\n Plan is:\nList of future strategies:\n1. batchtools_slurm:\n   - args: function (expr, envir = parent.frame(), substitute = TRUE, globals = TRUE, label = NULL, template = NULL, resources = list(), workers = NULL, registry = list(), ...)\n   - tweaked: FALSE\n   - call: plan(batchtools_slurm)\n\n### available workers:\ngandalf-vm01\n\n\n### total workers:\n1\n\n### unique workers:\ngandalf-vm01\n\n### available Cores:\n\n#### non-slurm\n1\n\n#### slurm method\n1\n\n### Main PID:\n4135554\n\n### Unique processes\n100\n\nIDs of all cores used\n\n241089\n1528094\n1522003\n1503388\n1516888\n3269690\n1522077\n241160\n1528170\n1503459\n1516963\n3269759\n1522156\n1528243\n241230\n3269825\n1271878\n1528317\n1522235\n241297\n1503531\n1528392\n1517044\n1522308\n241367\n1503606\n1528471\n1522380\n1517120\n241436\n3269894\n1528548\n1522456\n1503681\n1517195\n241505\n1522531\n1528626\n3269975\n1271968\n1503749\n241573\n1522605\n3270042\n1528703\n1503817\n241641\n1522682\n1528778\n1517275\n3270121\n1272046\n241709\n1528853\n1522758\n1503892\n1517348\n241778\n1528932\n1522829\n1503961\n1517420\n3270199\n241843\n1529005\n1522901\n1504030\n241910\n1529082\n1517500\n3270277\n1522975\n1504098\n241978\n1529157\n1523051\n1504167\n1517581\n3270355\n1529231\n242042\n1523125\n1504238\n1529306\n1517659\n3270422\n242108\n1523197\n1529384\n1504307\n1517731\n3270488\n242173\n1523272\n1529459\n1504376\n1517812\n3270555\n242239\n1529532\n\n## Nodes and pids\n# A tibble: 100 × 3\n# Groups:   node [7]\n    node             pid n_reps\n    &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;\n  1 bilbo         241089      6\n  2 bilbo         241160      6\n  3 bilbo         241230      7\n  4 bilbo         241297      6\n  5 bilbo         241367      6\n  6 bilbo         241436      6\n  7 bilbo         241505      6\n  8 bilbo         241573      6\n  9 bilbo         241641      7\n 10 bilbo         241709      6\n 11 bilbo         241778      7\n 12 bilbo         241843      6\n 13 bilbo         241910      6\n 14 bilbo         241978      7\n 15 bilbo         242042      6\n 16 bilbo         242108      6\n 17 bilbo         242173      6\n 18 bilbo         242239      6\n 19 frodo-vs01   1528094      6\n 20 frodo-vs01   1528170      6\n 21 frodo-vs01   1528243      6\n 22 frodo-vs01   1528317      6\n 23 frodo-vs01   1528392      6\n 24 frodo-vs01   1528471      7\n 25 frodo-vs01   1528548      6\n 26 frodo-vs01   1528626      6\n 27 frodo-vs01   1528703      6\n 28 frodo-vs01   1528778      6\n 29 frodo-vs01   1528853      7\n 30 frodo-vs01   1528932      6\n 31 frodo-vs01   1529005      6\n 32 frodo-vs01   1529082      6\n 33 frodo-vs01   1529157      6\n 34 frodo-vs01   1529231      6\n 35 frodo-vs01   1529306      6\n 36 frodo-vs01   1529384      6\n 37 frodo-vs01   1529459      6\n 38 frodo-vs01   1529532      6\n 39 frodo-vs02   1522003      7\n 40 frodo-vs02   1522077      7\n 41 frodo-vs02   1522156      6\n 42 frodo-vs02   1522235      7\n 43 frodo-vs02   1522308      6\n 44 frodo-vs02   1522380      6\n 45 frodo-vs02   1522456      6\n 46 frodo-vs02   1522531      6\n 47 frodo-vs02   1522605      7\n 48 frodo-vs02   1522682      6\n 49 frodo-vs02   1522758      6\n 50 frodo-vs02   1522829      6\n 51 frodo-vs02   1522901      7\n 52 frodo-vs02   1522975      6\n 53 frodo-vs02   1523051      6\n 54 frodo-vs02   1523125      7\n 55 frodo-vs02   1523197      6\n 56 frodo-vs02   1523272      7\n 57 frodo-vs03   1503388      6\n 58 frodo-vs03   1503459      6\n 59 frodo-vs03   1503531      6\n 60 frodo-vs03   1503606      6\n 61 frodo-vs03   1503681      6\n 62 frodo-vs03   1503749      6\n 63 frodo-vs03   1503817      6\n 64 frodo-vs03   1503892      6\n 65 frodo-vs03   1503961      6\n 66 frodo-vs03   1504030      6\n 67 frodo-vs03   1504098      6\n 68 frodo-vs03   1504167      6\n 69 frodo-vs03   1504238      6\n 70 frodo-vs03   1504307      7\n 71 frodo-vs03   1504376      6\n 72 frodo-vs04   1516888      6\n 73 frodo-vs04   1516963      7\n 74 frodo-vs04   1517044      7\n 75 frodo-vs04   1517120      6\n 76 frodo-vs04   1517195      7\n 77 frodo-vs04   1517275      7\n 78 frodo-vs04   1517348      6\n 79 frodo-vs04   1517420      7\n 80 frodo-vs04   1517500      7\n 81 frodo-vs04   1517581      7\n 82 frodo-vs04   1517659      6\n 83 frodo-vs04   1517731      6\n 84 frodo-vs04   1517812      6\n 85 gandalf-vm02 3269690      6\n 86 gandalf-vm02 3269759      6\n 87 gandalf-vm02 3269825      6\n 88 gandalf-vm02 3269894      7\n 89 gandalf-vm02 3269975      7\n 90 gandalf-vm02 3270042      6\n 91 gandalf-vm02 3270121      6\n 92 gandalf-vm02 3270199      6\n 93 gandalf-vm02 3270277      6\n 94 gandalf-vm02 3270355      6\n 95 gandalf-vm02 3270422      7\n 96 gandalf-vm02 3270488      6\n 97 gandalf-vm02 3270555      7\n 98 gandalf-vm03 1271878      6\n 99 gandalf-vm03 1271968      6\n100 gandalf-vm03 1272046      6\n\nTime taken for code: 185\nSo, we can see the lightweight wrapper uses 1 cpu on one node, but spawns 100 workers, each of which gets used 6-7 times."
  },
  {
    "objectID": "parallelism/initial_future_batchtools.html#todo",
    "href": "parallelism/initial_future_batchtools.html#todo",
    "title": "Using future.batchtools",
    "section": "TODO",
    "text": "TODO\nother templates\nmanaging resources\nlist-plans and nesting\nchunks.as.array\nauto-managing plan locally vs HPC\nconsequences of master R session dying (if Rscript, if sbatched)\netc"
  },
  {
    "objectID": "parallelism/master_persistence.html",
    "href": "parallelism/master_persistence.html",
    "title": "Control process dying or timing out",
    "section": "",
    "text": "The way future.batchtools works there’s a controlling R process that starts the futures and waits for them to return. In typical use, the value() of futures are intended to be used, whereas in some cases, particularly when I use them on the cluster, I’m using them to fire off a bunch of HPC jobs that save output, and don’t care if they return. This is a very similar issue to the discussion here.\nThere are a couple issues this use-case brings up\n\nWhat happens if that master R script that creates the futures dies?\n\nDo we need to tie up a core just to run that? (YES)\nDoes it have to stay in use for the entire period all futures take to finish? (NO- see next)\nWhat if it dies once the jobs have started running vs before they’ve started\n\ne.g. does it create a queue that just runs no matter what? (NO)\nonce a job starts, does it finish no matter what? (YES, unless it times out)\n\n\nDoes it matter if the master runs through sbatch, sinteractive, or just Rscript on the login node? (Probably not, but sbatch is likely safest/most robust).\nShould I actually be using batchtools::submitJobs() instead, as suggested in that github thread? I really like the automatic control of jobs and globals etc in future.batchtools, and the ability to be portable to local computers. (Quite possibly, but that’s for another day, I think. And we’d potentially lose a lot of the advantages of future)."
  },
  {
    "objectID": "parallelism/master_persistence.html#conclusions",
    "href": "parallelism/master_persistence.html#conclusions",
    "title": "Control process dying or timing out",
    "section": "Conclusions",
    "text": "Conclusions\nThe master needs to run until all jobs have started, but not necessarily until all jobs have finished (unless it actually does something with the output).\nThat will be hard to manage, since the time it takes to start jobs depends on SLURM queues.\nThere’s no reason to use srun or sinteractive- they still need to ask for resources, and die when disconnect, so even more finicky.\nUsing Rscript on the login node is I guess a potential workaround, but I think it also dies when we disconnect and potentially times out anyway. It’s still tying up a node, just in a different place. I think that’d make people even grumpier. It’s also hard to test with short-ish runs."
  },
  {
    "objectID": "parallelism/nested_parallel.html",
    "href": "parallelism/nested_parallel.html",
    "title": "Nested parallelism",
    "section": "",
    "text": "I tend to run a lot of simulation code that consists of some core functions that are then run for a large number of different parameter values. This bit is entirely independent and clearly parallelisable. There are also typically large calculations inside the simulation functions that can be parallelised. What’s not clear to me is whether I should write them all with parallel-friendly code (foreach %dopar%, furrr, etc), or just one or the other. While my particular situation gives me the option to choose, it’s likely not uncommon to call parallelisable functions from a package in code that is itself parallelised, and so is useful to know how this works.\nI already did some tests of what sort of work makes most sense to be parallelised so I’ll try to follow those ideas as I do these tests- assuming that the internal parallel code actually makes sense to be parallel, and wouldn’t just be faster sequential anyway. To test this, I’ll attempt to build an example that is non-trivial, but still try to stay minimally complex to avoid getting into writing a complex population dynamics model."
  },
  {
    "objectID": "parallelism/nested_parallel.html#packages-and-setup",
    "href": "parallelism/nested_parallel.html#packages-and-setup",
    "title": "Nested parallelism",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}, because I tend to like writing for loops (there’s a reason I’ll try to write up sometime later). I test other packages in the {future} family (furrr, future_apply) where I try to better understand when they do and don’t give speed advantages.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nlibrary(foreach)\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nlibrary(ggplot2)\n\nregisterDoFuture()\nplan(multisession)"
  },
  {
    "objectID": "parallelism/nested_parallel.html#built-in-nesting",
    "href": "parallelism/nested_parallel.html#built-in-nesting",
    "title": "Nested parallelism",
    "section": "Built-in nesting",
    "text": "Built-in nesting\nThe foreach package provides built-in nesting, with constructions using %:%. This is designed for loops that can be written tightly together (no processing between them). For example, we might write a nested loop over two sets of parameters mean and sd and calculate the realised coefficient of variation and return it as a matrix. Situations with more complexity around dependency in these loops is here.\n\nrealised_cv &lt;- foreach(i = 1:10, .combine = cbind) %:%\n  foreach(j = seq(from = 0, to = 1, by = 0.1), .combine = rbind) %dopar% {\n    a &lt;- rnorm(1000, mean = 1, sd = j)\n    sd(a)/mean(a)\n  }\nrealised_cv\n\n                [,1]      [,2]      [,3]       [,4]       [,5]       [,6]\nresult.1  0.00000000 0.0000000 0.0000000 0.00000000 0.00000000 0.00000000\nresult.2  0.09891223 0.1008605 0.0945898 0.09907191 0.09714435 0.09994607\nresult.3  0.19514135 0.2004283 0.1978253 0.19998854 0.19953238 0.19845697\nresult.4  0.30272246 0.2979960 0.2879971 0.28725665 0.29370995 0.31131028\nresult.5  0.40790566 0.3990116 0.4000965 0.39641709 0.39208482 0.40541046\nresult.6  0.49720731 0.4974663 0.5049968 0.50798889 0.48008738 0.49096348\nresult.7  0.64000011 0.5884977 0.6004626 0.65640455 0.59444297 0.60885005\nresult.8  0.72563956 0.7155720 0.7112370 0.72276167 0.74244327 0.70035968\nresult.9  0.81651582 0.8431277 0.7823390 0.78515947 0.79606197 0.79760404\nresult.10 0.85954100 0.9478188 0.8785366 0.89448581 0.91753339 0.86162369\nresult.11 0.93010890 1.0626908 0.9827518 0.99434673 0.98176869 1.01819481\n               [,7]       [,8]       [,9]      [,10]\nresult.1  0.0000000 0.00000000 0.00000000 0.00000000\nresult.2  0.1005070 0.09780583 0.09893618 0.09814039\nresult.3  0.2078426 0.20334995 0.20473360 0.19951013\nresult.4  0.3012958 0.29666971 0.30354469 0.29686108\nresult.5  0.3995659 0.38186272 0.39905156 0.39615277\nresult.6  0.4946903 0.50200764 0.48092292 0.48294714\nresult.7  0.5863672 0.62224528 0.61382700 0.61524278\nresult.8  0.7167759 0.68353520 0.72894997 0.69285072\nresult.9  0.8086785 0.80463310 0.78535213 0.79137067\nresult.10 0.9121453 0.88604900 0.89767558 0.92785435\nresult.11 1.0691658 0.95093915 1.05077995 0.99766564\n\n\nThat can be super useful, but isn’t the goal here- I’m interested in the situation where we have\nforloop () {\n  lots of processing\n  \n  forloop2(outcomes of the processing) {\n    more processing\n  }\n  \n  more processing\n}"
  },
  {
    "objectID": "parallelism/nested_parallel.html#inner-loop",
    "href": "parallelism/nested_parallel.html#inner-loop",
    "title": "Nested parallelism",
    "section": "Inner loop",
    "text": "Inner loop\n\nParallel version\n\ninner_par &lt;- function(in_vec, size) {\n  inner_out &lt;- foreach(j = in_vec,\n                       .combine = c) %dorng% {\n    d &lt;- rnorm(size, mean = j)\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    mean(g)\n    \n  }\n}\n\n\n\nSequential version\n\ninner_seq &lt;- function(in_vec, size) {\n  inner_out &lt;- foreach(j = in_vec,\n                       .combine = c) %do% {\n    d &lt;- rnorm(size, mean = j)\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    mean(g)\n    \n  }\n}\n\n\n\nUsing preallocated for\nThis is likely to be faster than the sequential\n\ninner_for &lt;- function(in_vec, size) {\n  inner_out &lt;- vector(mode = 'numeric', length = size)\n  \n  for(j in 1:length(in_vec)) {\n    d &lt;- rnorm(size, mean = in_vec[j])\n    \n    f &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    g &lt;- d %*% f\n    \n    inner_out[j] &lt;- mean(g)\n    \n  }\n  return(inner_out)\n}"
  },
  {
    "objectID": "parallelism/nested_parallel.html#outer-loop",
    "href": "parallelism/nested_parallel.html#outer-loop",
    "title": "Nested parallelism",
    "section": "Outer loop",
    "text": "Outer loop\n\nparallel\n\nouter_par &lt;- function(size, innerfun) {\n  outer_out &lt;- foreach(i = 1:size,\n                       .combine = c) %dorng% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a &lt;- rnorm(size, mean = i)\n                         \n                         b &lt;- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec &lt;- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(in_vec = cvec, size = size)\n                         \n                         h &lt;- sd(cvec)/inner_out\n                         \n                         \n                       }\n  \n  return(outer_out)\n}\n\n\n\nsequential\n\nouter_seq &lt;- function(size, innerfun) {\n  outer_out &lt;- foreach(i = 1:size,\n                       .combine = c) %do% {\n                         \n                         # Do a matrix mult on a vector specified with i\n                         a &lt;- rnorm(size, mean = i)\n                         \n                         b &lt;- matrix(rnorm(size*size), nrow = size)\n                         \n                         cvec &lt;- a %*% b\n                         \n                         # Now iterate over the values in c to do somethign else\n                         inner_out &lt;- innerfun(in_vec = cvec, size = size)\n                         \n                         h &lt;- sd(cvec)/inner_out\n                         \n                         \n                       }\n  \n  return(outer_out)\n}\n\n\n\nPreallocated for\n\nouter_for &lt;- function(size, innerfun) {\n  outer_out &lt;- matrix(nrow = size, ncol = size)\n  for(i in 1:size) {\n    \n    # Do a matrix mult on a vector specified with i\n    a &lt;- rnorm(size, mean = i)\n    \n    b &lt;- matrix(rnorm(size*size), nrow = size)\n    \n    cvec &lt;- a %*% b\n    \n    # Now iterate over the values in c to do somethign else\n    inner_out &lt;- innerfun(in_vec = cvec, size = size)\n    \n    outer_out[, i] &lt;- sd(cvec)/inner_out\n    \n  }\n  outer_out &lt;- c(outer_out) \n  \n  return(outer_out)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html",
    "href": "parallelism/parallel_speed.html",
    "title": "Investigating parallel speedups",
    "section": "",
    "text": "I tend to run a lot of code that can be parallelised, but it’s not always clear when it’s worth it and how best to structure the paralellisation. Should it be at the outermost layer, where I’m typically looping over parameters, some intermediate layer where I might be looping over indices or iterators, or to handle large datasets?\nFor reference, I often have population dynamics models with many species and locations. At each timestep I need to make a lot of calculations on the species, including some large matrix multiplications to get dispersal. These could be parallelised over species. And after simulations are complete, I calculate a lot of covariances over species, space, and time that can be parallelised over pairwise combinations. Both of these cases operate on large arrays, and so would feed large amounts of data to parallelised functions, which would then do some limited processing on it (e.g. calculate covariances and clean them up for return). At the other extreme, each of these situations is governed by an initial set of parameters, giving, for example, environmental conditions, species growth rates, etc. These are often just vectors, and so parallelising over them would feed the parallel function small amounts of data and kick off large amounts of work.\nTo test parallel performance under these different situations, I’ll attempt to build an example that is non-trivial, but still try to stay minimally complex to avoid getting into writing a complex population dynamics model."
  },
  {
    "objectID": "parallelism/parallel_speed.html#packages-and-setup",
    "href": "parallelism/parallel_speed.html#packages-and-setup",
    "title": "Investigating parallel speedups",
    "section": "Packages and setup",
    "text": "Packages and setup\nI’ll use the {future} package, along with {dofuture} and {foreach}, because I tend to like writing for loops (there’s a reason- I’ll try to write up sometime later). I’ll also test {furrr} and {future.apply} to see if they differ in any appreciable way.\n\nlibrary(microbenchmark)\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nLoading required package: future\n\nlibrary(foreach)\nlibrary(furrr)\nlibrary(future.apply)\nlibrary(doRNG)\n\nLoading required package: rngtools\n\nlibrary(listenv)\n\nJust set up a typical doFuture situation with plan(multisession). Sorting out plans is a topic for another day.\n\nregisterDoFuture()\nplan(multisession)"
  },
  {
    "objectID": "parallelism/parallel_speed.html#foreach",
    "href": "parallelism/parallel_speed.html#foreach",
    "title": "Investigating parallel speedups",
    "section": "foreach",
    "text": "foreach\n\nmult_foreach &lt;- function(a, b) {\n  c_foreach &lt;- foreach(i = 1:ncol(a), .combine = rbind) %dopar% {\n    a[,i] %*% b\n  }\n  return(t(c_foreach))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#furrr",
    "href": "parallelism/parallel_speed.html#furrr",
    "title": "Investigating parallel speedups",
    "section": "furrr",
    "text": "furrr\npurrr (and so furrr) don’t seem to work on matrices. So, I guess have a silly pre-step to make it a list. I’m going to do that outside the function, simply because if we went this way, we’d set the data up to work.\n\nmult_furrr &lt;- function(a_list, b) {\n  c_map &lt;- future_map(a_list, \\(x) x %*% b)\n  matrix(unlist(c_map), ncol = 2)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#future.apply",
    "href": "parallelism/parallel_speed.html#future.apply",
    "title": "Investigating parallel speedups",
    "section": "future.apply",
    "text": "future.apply\n\nmult_apply &lt;- function(a, b) {\n  future_apply(a, MARGIN = 2, FUN = function(x) x %*% b)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#simple-for",
    "href": "parallelism/parallel_speed.html#simple-for",
    "title": "Investigating parallel speedups",
    "section": "simple for",
    "text": "simple for\nPreallocate, because I’m not a heathen\n\nmult_for &lt;- function(a, b) {\n  \n  c_for &lt;- a\n  for(i in 1:ncol(a)) {\n    c_for[,i] &lt;- a[,i] %*% b\n  }\n  return(c_for)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#future-for",
    "href": "parallelism/parallel_speed.html#future-for",
    "title": "Investigating parallel speedups",
    "section": "future for",
    "text": "future for\nWe can write a usual for loop if we use futures directly. the futures themselves have to go in a list, because they are futures, not values, and so can’t go straight into a matrix. That list can be preallocated.\nThere are two ways to do this- explicit and implicit- see the future docs.\n\nExplicit futures\n\nmult_for_future_e &lt;- function(a, b) {\n  \n  c_for &lt;- vector(mode = 'list', length = ncol(a))\n  \n  for(i in 1:ncol(a)) {\n    c_for[[i]] &lt;- future({a[,i] %*% b})\n  }\n  # get values and make a matrix\n  v_for &lt;- lapply(c_for, FUN = value)\n  \n  return(matrix(unlist(v_for), ncol = ncol(a)))\n}\n\n\n\nImplicit futures\nusing listenv\n\nmult_for_future_i &lt;- function(a, b) {\n  \n  c_for &lt;- listenv()\n  \n  for(i in 1:ncol(a)) {\n    c_for[[i]] %&lt;-% {a[,i] %*% b}\n  }\n  # get values and make a matrix\n  v_for &lt;- as.list(c_for)\n  \n  return(matrix(unlist(v_for), ncol = ncol(a)))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#linear-algebra",
    "href": "parallelism/parallel_speed.html#linear-algebra",
    "title": "Investigating parallel speedups",
    "section": "linear algebra",
    "text": "linear algebra\n\nmult_linear &lt;- function(a,b) {\n  t(a %*% b)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#preallocate-the-foreach",
    "href": "parallelism/parallel_speed.html#preallocate-the-foreach",
    "title": "Investigating parallel speedups",
    "section": "preallocate the foreach",
    "text": "preallocate the foreach\nI typically don’t do this, since my understanding of foreach is that it builds them with the .combine, so preallocating doesn’t do anything. But maybe?\n\nmult_foreach_pre &lt;- function(a, b) {\n  c_foreach &lt;- t(a)\n  c_foreach &lt;- foreach(i = 1:ncol(a), .combine = rbind) %dopar% {\n    a[,i] %*% b\n  }\n  return(t(c_foreach))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#return-the-foreach-as-a-list",
    "href": "parallelism/parallel_speed.html#return-the-foreach-as-a-list",
    "title": "Investigating parallel speedups",
    "section": "Return the foreach as a list",
    "text": "Return the foreach as a list\nIt is possible that using .combine is forcing slower behaviour for the foreach, and it’s optimized for a list?\n\nmult_foreach_list &lt;- function(a, b) {\n  c_foreach &lt;- foreach(i = 1:ncol(a)) %dopar% {\n    a[,i] %*% b\n  }\n  \n  # Do the binding in one step\n  return(matrix(unlist(c_foreach), ncol = 2))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#dont-preallocate-the-for",
    "href": "parallelism/parallel_speed.html#dont-preallocate-the-for",
    "title": "Investigating parallel speedups",
    "section": "Don’t preallocate the for",
    "text": "Don’t preallocate the for\nHow bad is this- I almost always DO preallocate (it’s faster, and the loop starts at 1, and it’s just cleaner), but it’s possible not to.\n\nmult_for_build &lt;- function(a, b) {\n  \n  c_for &lt;- a[,1] %*% b\n  \n  for(i in 2:ncol(a)) {\n    ctemp &lt;- a[,i] %*% b\n    c_for &lt;- rbind(c_for, ctemp)\n  }\n  return(t(c_for))\n}\n\nI’ll include the furrr and future.apply here too, I guess as reference.\n\nmicrobenchmark(\n  futurefurrr = mult_furrr(a100_l, b),\n  futureapply = mult_apply(a100, b),\n  futureforeach = mult_foreach(a100, b),\n  preallocate_foreach = mult_foreach_pre(a100, b),\n  foreach_list = mult_foreach_list(a100, b),\n  bare_for = mult_for(a100, b),\n  unallocate_for = mult_for_build(a100, b),\n  bare_linear = mult_linear(t(a100), b),\n  times = 10\n)\n\nUnit: milliseconds\n                expr      min       lq     mean    median       uq      max\n         futurefurrr 443.6282 486.7204 514.9555 499.42955 508.2764 724.0422\n         futureapply 587.7048 634.2925 673.7851 647.22095 740.1291 767.1812\n       futureforeach 244.3625 253.6442 321.1252 256.70470 287.4974 804.0015\n preallocate_foreach 242.1381 251.5804 278.5400 277.79885 282.3750 374.8727\n        foreach_list 248.4650 272.0348 290.8289 299.22400 306.9559 343.0977\n            bare_for  88.4069  89.8161 100.2756  93.38755 103.5877 133.0030\n      unallocate_for 106.7770 111.9356 119.9050 119.36350 129.2688 134.1177\n         bare_linear  35.4130  38.0087  42.1330  39.99825  43.7534  58.0615\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nThese results are interesting, and the actual slowdown (not just lack of speedup) is worrying for how I do some things. I think the overhead of shifting the data around is absolutely killing these parallel processes. And I definitely have code that does this sort of thing.\nI have two more questions now (plus one for later):\n\nDoes the foreach loop work as fast as a for if I use %do% instead of %dopar%? Or is the overhead still there?\n\nAnd same with all the futures if I set plan(sequential)?\n\nIf I don’t send prebuilt data, but just some parameters and build the data internally, how do they compare?\n\nSometimes this flow makes sense, and sometimes it doesn’t– e.g. if I’m simulating populations, the outer set of parallelisation just sends parameters. But the internal set often needs to work on things like population matrices. And so maybe that internal loop just shouldn’t be parallel.\n\nHow do other plan options affect these answers? I think this deserves its own page, and could get very complicated once I get into future.callr, future.batchtools, cluster, etc. And multicore might avoid the passing and use pointers, but i can’t test on windows?"
  },
  {
    "objectID": "parallelism/parallel_speed.html#foreach-do",
    "href": "parallelism/parallel_speed.html#foreach-do",
    "title": "Investigating parallel speedups",
    "section": "foreach %do%",
    "text": "foreach %do%\nLet’s try shifting to %do% for the foreach\n\nmult_foreach_do &lt;- function(a, b) {\n  c_foreach &lt;- foreach(i = 1:ncol(a), .combine = rbind) %do% {\n    a[,i] %*% b\n  }\n  return(t(c_foreach))\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#list-foreach-do",
    "href": "parallelism/parallel_speed.html#list-foreach-do",
    "title": "Investigating parallel speedups",
    "section": "list foreach %do%",
    "text": "list foreach %do%\n\nmult_foreach_list_do &lt;- function(a, b) {\n  c_foreach &lt;- foreach(i = 1:ncol(a)) %do% {\n    a[,i] %*% b\n  }\n  \n  # Do the binding in one step\n  return(matrix(unlist(c_foreach), ncol = 2))\n}\n\nTest against parallel foreach, bare for, and the unallocated for (since that’s kind of what the foreach is doing- building up an object). Since that’s sometimes nice behaviour and leads to cleaner code than a for loop, I’d like to see how they compare.\n\nmicrobenchmark(\n  futureforeach = mult_foreach(a100, b),\n  foreachdo = mult_foreach_do(a100, b),\n  foreachlistdo = mult_foreach_list_do(a100, b),\n  bare_for = mult_for(a100, b),\n  unallocate_for = mult_for_build(a100, b),\n  bare_linear = mult_linear(t(a100), b),\n  times = 10\n)\n\nUnit: milliseconds\n           expr      min       lq      mean   median       uq      max neval\n  futureforeach 235.1256 243.4750 264.39874 258.7047 280.2599 321.3101    10\n      foreachdo  94.8164  97.6870 100.78289 100.6124 102.1715 107.7091    10\n  foreachlistdo  96.9755  98.4346 100.76391 101.1035 103.3189 105.4224    10\n       bare_for  86.3208  87.9507  88.75037  88.3432  89.2126  92.0338    10\n unallocate_for 104.1609 108.6303 110.51359 109.9748 112.3416 116.5181    10\n    bare_linear  35.3966  36.1527  37.14453  36.3332  37.2698  42.9894    10\n\n\nInteresting. Nearly identical to the unallocated for and quite a bit faster than the parallel version, which seems to hint that it’s the data transfer that’s killing things. And backs up my assumption of what’s going on under the hood in terms of constructing the object as in an unallocated for loop. There’s no appreciable difference in using the foreach with a list and then combining vs combining as we go with .combine."
  },
  {
    "objectID": "parallelism/parallel_speed.html#plansequential",
    "href": "parallelism/parallel_speed.html#plansequential",
    "title": "Investigating parallel speedups",
    "section": "plan(sequential)",
    "text": "plan(sequential)\nHow do the parallel versions work with plan(sequential)? Do they all get a speedup from avoiding data transfer? This is the same benchmark test as above, but now run sequentially.\n\nplan(sequential)\n\nmicrobenchmark(\n  futurefurrr = mult_furrr(a100_l, b),\n  futureapply = mult_apply(a100, b),\n  futureforeach = mult_foreach(a100, b),\n  bare_for = mult_for(a100, b),\n  unallocate_for = mult_for_build(a100, b),\n  bare_linear = mult_linear(t(a100), b),\n  times = 10\n)\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:listenv':\n\n    map\n\n\nThe following objects are masked from 'package:foreach':\n\n    accumulate, when\n\n\nUnit: milliseconds\n           expr      min       lq      mean   median       uq      max neval\n    futurefurrr 110.2440 114.4588 119.01770 118.1613 121.6648 136.4901    10\n    futureapply 327.7384 328.5536 332.10152 329.8668 336.5330 341.8481    10\n  futureforeach 106.6627 107.0847 110.10954 109.0071 109.9754 120.1479    10\n       bare_for  86.4083  87.0670  88.07185  87.3572  88.7584  91.1030    10\n unallocate_for 109.3728 109.9509 113.80280 113.8166 116.2439 121.0449    10\n    bare_linear  35.3039  35.7138  37.46994  36.4234  38.2815  43.2047    10\n\n\nInteresting. foreach and furrr both sped up about as expected (furrr just seems a bit slower in general), but future.apply had much less of a speedup. It must not fall back to a simpler function, and still tries to use the parallel data shuffling? It is still much faster than with plan(multisession) (was 590 microseconds), so something is happening, but it’s not getting down to the speeds of the other futures. And the simple for is still fastest (other than just using linear algebra, obviously).\n\nThe message so far\nTest the parallel implementation at different points in the code- if there’s no way to avoid data passing, a simple for (or other sequential function like apply could be fastest."
  },
  {
    "objectID": "parallelism/parallel_speed.html#future-for-1",
    "href": "parallelism/parallel_speed.html#future-for-1",
    "title": "Investigating parallel speedups",
    "section": "future for",
    "text": "future for\nWe can write a usual for loop if we use futures directly. the futures themselves have to go in a list, because they are futures, not values, and so can’t go straight into a matrix. That list can be preallocated.\nThere are two ways to do this- explicit and implicit- see the future docs.\n\nExplicit futures\n\nmult_for_future_internal_e &lt;- function(n_reps = 100, size = 1000) {\n  \n  c_for &lt;- vector(mode = 'list', length = n_reps)\n  \n  for(i in 1:n_reps) {\n    c_for[[i]] &lt;- future({a &lt;- rnorm(size, mean = i)\n        b &lt;- matrix(rnorm(size * size), nrow = size)\n        a %*% b}, seed = TRUE)\n  }\n  # get values and make a matrix\n  v_for &lt;- lapply(c_for, FUN = value)\n  \n  return(matrix(unlist(v_for), ncol = n_reps))\n}\n\n\n\nImplicit futures\nusing listenv\nThat’s a funny way to set the seed. Good to know.\n\nmult_for_future_internal_i &lt;- function(n_reps = 100, size = 1000) {\n  \n  c_for &lt;- listenv()\n  \n  for(i in 1:n_reps) {\n    c_for[[i]] %&lt;-% {a &lt;- rnorm(size, mean = i)\n        b &lt;- matrix(rnorm(size * size), nrow = size)\n        a %*% b} %seed% TRUE\n  }\n  # get values and make a matrix\n  v_for &lt;- as.list(c_for)\n  \n  return(matrix(unlist(v_for), ncol = n_reps))\n}\n\nThere’s no reason to have a linear algebra version here, since we’re by definition not operating on existing matrices.\nTry that without adjusting what globals are passed to the futures\n\nmicrobenchmark(\n  futurefurrr = mult_furrr_internal(n_reps = 100, size = 1000),\n  futureapply = mult_apply_internal(n_reps = 100, size = 1000),\n  futureforeach = mult_foreach_internal(n_reps = 100, size = 1000),\n  futurefor_e = mult_for_future_internal_e(n_reps = 100, size = 1000),\n  futurefor_i = mult_for_future_internal_i(n_reps = 100, size = 1000),\n  bare_for = mult_for_internal(n_reps = 100, size = 1000),\n  times = 10\n)\n\nUnit: milliseconds\n          expr       min        lq      mean    median        uq       max\n   futurefurrr  638.0028  658.8124  868.5756  663.1727  673.2477 2710.5420\n   futureapply  647.0352  653.3856  779.9339  660.3188  669.0498 1852.4155\n futureforeach  637.7363  644.3180  670.8410  655.9570  677.6647  776.2754\n   futurefor_e 2639.2926 2669.8748 2688.2644 2682.2723 2702.9564 2751.7045\n   futurefor_i 2658.7272 2668.5496 2721.6388 2720.5006 2738.6566 2825.9734\n      bare_for 3137.0728 3160.3969 3193.4383 3182.6047 3211.6341 3323.5784\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nNow the parallelisation is helping quite a bit, even the for loops with futures, though they’re still much slower than the other future methods. However, this is much slower than creating the matrices as we actually should do for this particular calculation, and is even slower than passing those matrices in. So, if the operations need to happen this way anyway (parallel lots of stuff over parameters), this makes lots of sense and speeds up. If we CAN pre-generate matrices, linear algebra is fastest (unsurprisingly), and the loops are next, even if we have to eat pass-in cost. Though in that case sequential is probably better.\nI think the slower bare futures are likely because there’s no chunking being done, and so data is copied to each iteration, rather than to chunks of iterations (which is built into doFuture and I think furrr and future.apply. I could try to manually chunk to test that, but I think I’m just going to skip it."
  },
  {
    "objectID": "parallelism/parallel_speed.html#globals",
    "href": "parallelism/parallel_speed.html#globals",
    "title": "Investigating parallel speedups",
    "section": "Globals",
    "text": "Globals\nOne of the nice things about future compared to some other parallel backends is that it does pass the global environment, so i don’t have to manage what it gets- it works as it does interactively. BUT, if data passing is what’s killing the speed, maybe I do need to manage what gets passed. In theory, the test above should get even faster if we don’t pass it the global environment, but only the arguments.\nSo, a new version of the above, explicitly limiting passing.\n\nforeach\n\nmult_foreach_internal_g &lt;- function(n_reps = 100, size = 1000) {\n  c_foreach &lt;- foreach(i = 1:n_reps, \n                       .combine = cbind,\n                       .export = NULL) %dorng% {\n    a &lt;- rnorm(size, mean = i)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  return(c_foreach)\n}\n\n\n\nfurrrr\n\nmult_furrr_internal_g &lt;- function(n_reps = 100, size = 1000) {\n  fn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n  }\n  \n  c_map &lt;- future_map(1:n_reps, fn_to_call, size = size, \n                      .options = furrr_options(seed = TRUE, globals = NULL))\n  matrix(unlist(c_map), ncol = n_reps)\n}\n\n\n\nfuture.apply\n\nmult_apply_internal_g &lt;- function(n_reps = 100, size = 1000) {\n    fn_to_call &lt;- function(rep, size) {\n    a &lt;- rnorm(size, mean = rep)\n    b &lt;- matrix(rnorm(size * size), nrow = size)\n    t(a %*% b)\n    }\n    \n  c_apply &lt;- future_lapply(1:n_reps, FUN = fn_to_call, size, \n                           future.seed = TRUE, future.globals = FALSE)\n  \n    matrix(unlist(c_apply), ncol = n_reps)\n}"
  },
  {
    "objectID": "parallelism/parallel_speed.html#benchmark-1",
    "href": "parallelism/parallel_speed.html#benchmark-1",
    "title": "Investigating parallel speedups",
    "section": "Benchmark",
    "text": "Benchmark\n\nmicrobenchmark(\n  futurefurrr = mult_furrr_internal(n_reps = 100, size = 1000),\n  futureapply = mult_apply_internal(n_reps = 100, size = 1000),\n  futureforeach = mult_foreach_internal(n_reps = 100, size = 1000),\n  futurefurrr_g = mult_furrr_internal_g(n_reps = 100, size = 1000),\n  futureapply_g = mult_apply_internal_g(n_reps = 100, size = 1000),\n  futureforeach_g = mult_foreach_internal_g(n_reps = 100, size = 1000),\n  bare_for = mult_for_internal(n_reps = 100, size = 1000),\n  times = 10\n)\n\nUnit: milliseconds\n            expr       min        lq      mean    median        uq       max\n     futurefurrr  650.9615  655.6274  667.7335  665.4363  672.8008  695.4913\n     futureapply  660.9141  681.4908  691.7151  685.7910  705.6650  735.2728\n   futureforeach  643.2652  665.4859  671.3939  670.2467  672.7690  700.9367\n   futurefurrr_g  670.0894  676.9485  690.0467  684.3684  695.3350  723.5180\n   futureapply_g  658.1669  669.3657  675.3438  671.9081  682.6484  695.2184\n futureforeach_g  641.1861  654.8838  679.7260  666.7028  705.6335  748.3529\n        bare_for 3191.5845 3219.5277 3261.9446 3256.3190 3309.1928 3349.3968\n neval\n    10\n    10\n    10\n    10\n    10\n    10\n    10\n\n\nSo, that’s not hauling around a ton of extra stuff. I think, based on the doFuture vignette, that future will send the globals, but only those that are needed. So it’s not that they’re in functions, it’s that future can tell it doesn’t need to pass extra variables. This means we don’t really get performance gains because future already had our back."
  },
  {
    "objectID": "parallelism/template_modification.html",
    "href": "parallelism/template_modification.html",
    "title": "Template modification",
    "section": "",
    "text": "I don’t want to spend a ton of time on this, but if I want to add the ability to adjust any of the SLURM variables, including those that have dashes ‘-’ in their names, and I don’t want to use the format of the resources list enforced by the {batchtools} template slurm-simple.tmpl, I’ll need to modify the template.\nTo do that, it’s a bit helpful to know how {brew} works, though it’s mostly self-explanatory from reading the templates. I’m primarily confused why I can’t pass names with backticks, when somethign like\nresources &lt;- list('name-with-ticks' = 17, namenoticks = 'a')\nresources\n\n$`name-with-ticks`\n[1] 17\n\n$namenoticks\n[1] \"a\"\n\nnames(resources)\n\n[1] \"name-with-ticks\" \"namenoticks\"\nWorks, since the future.batchtools template just uses unlist and names. e.g. the relevant bit is\nopts &lt;- unlist(resources, use.names = TRUE)\nopts &lt;- sprintf(\"--%s=%s\", names(opts), opts)\nopts &lt;- paste(opts, collapse = \" \")\n\nopts\n\n[1] \"--name-with-ticks=17 --namenoticks=a\"\nIt must be enforced by the batchtools wrapper for brew, because this works\nresources &lt;- list('name-with-ticks' = 17, namenoticks = 'a')\njob.name &lt;- 'testjob'\nlog.file = 'testlog'\nuri &lt;- 'testuri'\nbrew::brew(file = file.path('batchtools_templates', 'slurm.tmpl'), output = 'temp.txt')\nyeilding\nDo I want to dig into why batchtools enforces syntactic names? Not really.\nDo I want to bypass batchtools slurm generation? Not really.\nSo, can I modify the slurm script to do what I want? I think probably."
  },
  {
    "objectID": "parallelism/template_modification.html#what-do-i-want",
    "href": "parallelism/template_modification.html#what-do-i-want",
    "title": "Template modification",
    "section": "What do I want?",
    "text": "What do I want?\n\nTo pass arbitrary slurm arguments, similar to how future.batchtools ’s slurm.tmpl works, but including dash names. I really don’t like that batchtools takes over some of those arguments and calls them different things.\nTo be able to pass the options in the formats slurm allows (e.g. --time in seconds or as 00:05:00, --mem as an integer of megabytes, or as \"1GB\")\n\nI think what might work is to use dots in the names of resources and then translate to dashes?\ne.g. insert one small gsub line in the future.batchtools script:\n\nresources_dots &lt;- list('name.with.ticks' = 17, namenoticks = 'a')\nopts &lt;- unlist(resources_dots, use.names = TRUE)\nopts &lt;- sprintf(\"--%s=%s\", names(opts), opts)\nopts &lt;- gsub('\\\\.', '-', opts)\nopts &lt;- paste(opts, collapse = \" \")\nopts\n\n[1] \"--name-with-ticks=17 --namenoticks=a\"\n\n\nI generally like to structure SLURM scripts with\n#SBATCH --option1=value1\n#SBATCH --option2=value2\nAnd this template structures them\n#SBATCH --option1=value1 --option2=value2\nI could almost certainly write a thing that put carriage returns and #SBATCH in front of each, but we never see these templates, so I think I’ll skip that.\nI’ve changed it in batchtools.slurm.tmpl, and now the question is whether it runs afoul of some other batchtools error-catcher."
  },
  {
    "objectID": "parallelism/template_modification.html#outcome",
    "href": "parallelism/template_modification.html#outcome",
    "title": "Template modification",
    "section": "Outcome",
    "text": "Outcome\nIt seems to work\nI can also change --job-name now with resources$job.name, but the catch is I don’t know if there’s a way to change it on a per-job basis, since I have to specify it in plan with the resources list, and the iteration won’t be known until later. I would have thought that’s where the default job.name was coming from in slurm.tmpl and slurm-simple.tmpl, but they seem to just call themselves ‘doFuture’."
  },
  {
    "objectID": "plotting/faded_colors.html",
    "href": "plotting/faded_colors.html",
    "title": "Faded colors",
    "section": "",
    "text": "library(tidyverse) # Overkill, but easier than picking and choosing\nlibrary(colorspace)\nlibrary(sf)\nlibrary(patchwork)\nThere are a number of reasons we might want bivariate color axes in plots. The particular use I’m looking for now is to use a faded color to indicate less certainty in a result. Other uses will be developed later or elsewhere, but should build on this fairly straightforwardly.\nI’m doing this with colorspace because it’s hue-chroma-luminance approach makes it at least appear logical to shift along those dimensions. We might want hue (or luminance) to show one thing, and intensity to show another. Though we will play around with how that looks in practice. The specific use motivatiung this is to show the predicted amount of something with hue, and certainty with chroma or luminance (in particular, we have a model that makes predictions more accurately in some places than others). But there are many other potential uses.\nIn the HCL exploration file, I figure out HOW to generate faded colors and find some palettes that might work. Here, I’m going to sort out how to go from there to using them in plots, including creating legends."
  },
  {
    "objectID": "plotting/faded_colors.html#plot-the-bivariate-colors",
    "href": "plotting/faded_colors.html#plot-the-bivariate-colors",
    "title": "Faded colors",
    "section": "Plot the bivariate colors",
    "text": "Plot the bivariate colors\nBefore trying to plot with the colors, first I want to actually plot them themselves. One reason is to test how they are being created and specified, and the other is potentially to use the plot as a legend.\nWhy? The legend() part of ggplot may not handle the bivariate nature of the colors well, so need to basically homebrew one. This is the most flexible option- make the plot, then shrink and pretend it’s a legend. But, could also make a legend in vector form, then stack. Just not sure how well that’ll work. The shrunk plot would work better for continuous variables, the legend probably works better to use other parts of ggplot and not always have to screw around with grobs or ggarrange or patchwork or cowplot. I’ll try them all, I guess.\nFirst, make a matrix of colors. Take the base palette, fade it and save the color values for the whole thing. The for loop is lame, should be a function, but I’m just looking right now.\n\nbaseramp &lt;- sequential_hcl(8, 'ag_Sunset')\n\nfadesteps &lt;- seq(0,1, by = 0.25)\n\ncolormat &lt;- matrix(rep(baseramp, length(fadesteps)), nrow = 5, byrow = TRUE)\n\nfor(i in 1:length(fadesteps)) {\n  colormat[i, ] &lt;- lighten(colormat[i, ], amount = fadesteps[i]) %&gt;%\n    desaturate(amount = fadesteps[i])\n}\n\nOption 1 is to make that into a plot that we can then smash on top\n\n# Make a tibble from the matrix to feed to ggplot\ncoltib &lt;- as_tibble(colormat, rownames = 'row') %&gt;%\n  pivot_longer(cols = starts_with('V'), names_to = 'column')\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n# coltib\n\nggplot(coltib, aes(y = row, x = column, fill = value)) + \n  geom_tile() + scale_fill_identity()\n\n\n\n\nThat’s upside-down with how I tend to think about it. How about flipping the construction?\n\nfadesteps &lt;- rev(seq(0,1, by = 0.25))\ncolormat &lt;- matrix(rep(baseramp, length(fadesteps)), nrow = 5, byrow = TRUE)\n\nfor(i in 1:length(fadesteps)) {\n  colormat[i, ] &lt;- lighten(colormat[i, ], amount = fadesteps[i]) %&gt;%\n    desaturate(amount = fadesteps[i])\n}\n\ncoltib &lt;- as_tibble(colormat, rownames = 'row') %&gt;%\n  pivot_longer(cols = starts_with('V'), names_to = 'column')\n\n\nggplot(coltib, aes(y = row, x = column, fill = value)) +\n  geom_tile() + scale_fill_identity()"
  },
  {
    "objectID": "plotting/faded_colors.html#programmatic-color-setting",
    "href": "plotting/faded_colors.html#programmatic-color-setting",
    "title": "Faded colors",
    "section": "Programmatic color setting",
    "text": "Programmatic color setting\nCreate a function basically following the above. But allow it to take palettes by name or raw hue values if they are obtained elsewhere (like from a manually specified hue ramp). hex color vals and pal names are both characters, but hex always starts with ‘#’, so should be able to auto-detect. It can take a number of fades, or a vector of specific fade levels, and returns the matrix of colors.\n\ncol2dmat &lt;- function(pal, n1, n2 = 2, dropwhite = TRUE, fadevals = NULL) {\n  # pal can be either a palette name or a vector of hex colors (or single hex color)\n  # dropwhite is there to by default knock off the bottom row that's all white\n  # fadevals is a way to bypass the n2 and specify specific fade levels (ie if nonlinear)\n\n  if (all(str_detect(pal, '#'))) {\n    baseramp &lt;- pal\n  } else {\n    baseramp &lt;- sequential_hcl(n1, pal)\n  }\n\n  if (is.null(fadevals)) {\n    if (dropwhite) {n2 = n2+1}\n\n    fadesteps &lt;- rev(seq(0,1, length.out = n2))\n\n    if (dropwhite) {fadesteps &lt;- fadesteps[2:length(fadesteps)]}\n\n  }\n\n  if (!is.null(fadevals)) {\n    fadesteps &lt;- sort(fadevals, decreasing = TRUE)\n  }\n\n  colormat &lt;- matrix(rep(baseramp, length(fadesteps)), nrow = length(fadesteps), byrow = TRUE)\n\n\n  for(i in 1:length(fadesteps)) {\n    colormat[i, ] &lt;- lighten(colormat[i, ], amount = fadesteps[i]) %&gt;%\n      desaturate(amount = fadesteps[i])\n  }\n\n  return(colormat)\n}\n\nCreate another function that plots a matrix of colors. Typically that matrix comes out of col2dmat. Why not make one big function? because we will often want to access the color values themselves, and not always just plot them.\n\nplot2dcols &lt;- function(colmat) {\n  coltib &lt;- as_tibble(colmat, rownames = 'row') %&gt;%\n    pivot_longer(cols = starts_with('V'), names_to = 'column') %&gt;%\n    mutate(row = as.numeric(row), column = as.numeric(str_remove(column, 'V')))\n\n  colplot &lt;- ggplot(coltib, aes(y = row, x = column, fill = value)) +\n    geom_tile() + scale_fill_identity()\n\n  return(colplot)\n}\n\nTest that works with a given number of fades\n\nnewcolors &lt;- col2dmat('ag_Sunset', n1 = 8, n2 = 4)\nplot2dcols(newcolors)\n\n\n\n\nTest with set fade levels. REMEMBER FADE is FADE, not intensity. ie 0 is darkest.\n\nnewcolsuneven &lt;- col2dmat('ag_Sunset', n1 = 8, fadevals = c(0, 0.33, 0.8))\nplot2dcols(newcolsuneven)\n\n\n\n\nTest with non-built in palettes- ie setting hue manually. This could be particularly useful if we want quantitative hues. This tests the ability to auto-detect a vector of colors.\nUse the manual-set colors from hcl exploration for testing.\n\nhclmat &lt;- cbind(50, max_chroma(h = seq(from = 130, to = 275, length.out = 8), l = 50, floor = TRUE),\n                seq(from = 130, to = 275, length.out = 8))\n\npg &lt;- polarLUV(hclmat)\nswatchplot(hex(pg))\n\n\n\n\nWorks!\n\npgmat &lt;- col2dmat(hex(pg), n2 = 4)\nplot2dcols(pgmat)"
  },
  {
    "objectID": "plotting/faded_colors.html#plotting-the-data",
    "href": "plotting/faded_colors.html#plotting-the-data",
    "title": "Faded colors",
    "section": "Plotting the data",
    "text": "Plotting the data\nAbove, we were trying to plot the colors. Now, we want to assign those colors to data so we can plot the data with the appropriate color.\n\nSingle datapoint\nThe above is fine for looking at a color matrix, but in general, we’ll have a dataframe with a value for each dimension, and need to assign it a single color. Step one is figuring out how to do that assignment.\nCan I take a ‘datapoint’ with arbitrary values on both axes and choose its color?\nCan we do that for both color bins or continuous color?\nWe’ll need to relativise the data, since neither hue or fade are defined on the real line, but by their endpoints.\nLet’s fake some data. Don’t use round numbers (e.g. 0, 100) to avoid making stupid mistakes relating to relativising the scale. We need to know the endpoints of the data to match the endpoints of the hue and fade, and then a datapoint somewhere in the middle to create.\n\n# what is the range of the data?\n  # don't use round numbers (e.g. 0, 100)\nmax1 &lt;- 750\nmin1 &lt;- 150\n\nmax2 &lt;- 67\nmin2 &lt;- -55\n\n\n# get color for a single value pair\nval1 &lt;- 455\nval2 &lt;- 8\n\njust use a simple linear transform to get position on the min-max axes. Could use logit or something for either, but keeping it simple. The value above the min divided by the range gives where the data point is on a 0-1 scale from min to max. In reality, we will have two vectors (well, cols in a dataframe), and this is actually easier to do in that case because we can just get the min and max directly.\n\nvalpos1 &lt;- (val1-min1)/(max1-min1)\nvalpos2 &lt;- (val2-min2)/(max2-min2)\n\nThat’s easy to vectorize, which is basically how we’ll do it with a dataframe.\nFor now, can we just get individual colors to assign to a value pair?\nNeed to specify the min and max hue- these are the hue endpoints, not data endpoints.\n\nminhue &lt;- 130\nmaxhue &lt;- 275\n\nfind the hue value at the same relative position as the datapoint\n\nmatchH1 &lt;- (maxhue-minhue)*valpos1 + minhue\n\nUsing the manual colors\n\nsinglehclmat1 &lt;- cbind(50, max_chroma(h = matchH1, l = 50, floor = TRUE),\n                matchH1)\n\npgsingle1 &lt;- polarLUV(singlehclmat1)\nswatchplot(hex(pgsingle1))\n\n\n\n\nalso need the other axis. That’s also just on 0-1 (well, 1-0, since it’s fade, not intensity) and so would be done the same way.\n\nsinglecol &lt;- col2dmat(hex(pgsingle1), fadevals = (1-valpos2))\nswatchplot(singlecol)\n\n\n\n\nIt’s clear we can write all this as functions, and that we’ll need to. So…\n\n\nProgramatically finding colors\nEarlier, we made col2dmat, which found colors and faded them. We want to do something similar here, but the goal isn’t quite the same- we don’t really care about the full matrix, but about a single point. We could modify col2dmat, but probably easier (and fewer horrible logicals) to just write purpose-built functions.\nNeed new functions to 1) find the hue, 2) adjust the fade\n\nFind the hue\nTakes either a number of bins or Inf for continuous.\n\nhuefinder &lt;- function(hueval, minhue, maxhue, n = Inf, palname = NULL) {\n\n  # If continuous, use the value\n  # If binned, find the value of the bin the value is in\n  if (is.infinite(n)) {\n    matchH &lt;- (maxhue-minhue)*hueval + minhue\n  } else if (!is.infinite(n)) {\n\n    nvec &lt;- seq(from = 0, to = 1, length.out = n)\n\n    # The nvecs need to choose the COLOR, but the last one gets dropped in\n    # findInterval, so need an n+1\n    whichbin &lt;- findInterval(hueval,\n                             seq(from = 0, to = 1, length.out = n+1),\n                             rightmost.closed = TRUE)\n\n    # Don't build if using named palette because won't have min and max\n    if (is.null(palname)) {\n      binhue &lt;- nvec[whichbin]\n      matchH &lt;- (maxhue-minhue)*binhue + minhue\n    }\n\n  }\n\n  if (is.null(palname)) {\n    h &lt;- cbind(50, max_chroma(h = matchH, l = 50, floor = TRUE),\n               matchH)\n    h &lt;- hex(polarLUV(h))\n  } else {\n    h &lt;- sequential_hcl(n, palname)[whichbin]\n  }\n\n  return(h)\n}\n\n\n\nFind the fade\nThis takes the just found hue as basehue, and fades it. Again, n specifies either a number of fade bins or if infinite it is continuous and so just fades by whatever the value is.\n\nfadefinder &lt;- function(fadeval, basehue, n = Inf) {\n\n  # If n is infinite, just use fadeval. Otherwise, bin, dropping the all-white level\n  if (is.infinite(n)) {\n    fadeval &lt;- fadeval\n  } else {\n    # The +1 drops the white level\n    fadevec &lt;- seq(from = 0, to = 1, length.out = n + 1)\n\n    # Rightmost closed fixes an issue right at 1\n    fadeval &lt;- fadevec[findInterval(fadeval, fadevec, rightmost.closed = TRUE) + 1]\n  }\n\n  fadedcol &lt;- lighten(basehue, amount = 1-fadeval) %&gt;%\n    desaturate(amount = 1-fadeval)\n}\n\n\n\nHue and fade\nThis is meant to use in a mutate to take two columns of data and find the appropriate color. Should use … to pass, but whatever\n\ncolfinder &lt;- function(hueval, fadeval, minhue, maxhue, nhue = Inf, nfade = Inf, palname = NULL) {\n  thishue &lt;- huefinder(hueval, minhue, maxhue, nhue, palname)\n  thiscolor &lt;- fadefinder(fadeval, thishue, nfade)\n}\n\nQuick tests\n\nfunhue &lt;- huefinder(valpos1, minhue = minhue, maxhue = maxhue)\nfunfaded &lt;- fadefinder(valpos2, funhue)\nswatchplot(funfaded)\n\n\n\n\nshould be the same as\n\nfunboth &lt;- colfinder(valpos1, valpos2, minhue, maxhue)\nswatchplot(funboth)\n\n\n\n\n\n\n\nCalculating for dataframes\nVectorizing the relativization calculations is straightforward.\n\nvec1 &lt;- c(150, 588, 750, 455, 234)\n\n# get it for each value in vectorized way\n(vec1 - min(vec1))/(max(vec1)-min(vec1))\n\n[1] 0.0000000 0.7300000 1.0000000 0.5083333 0.1400000\n\n\nMaking a function to get the relative position. We can use this in the mutate once we move on to dataframes.\n\nrelpos &lt;- function(vec) {\n  (vec - min(vec))/(max(vec)-min(vec))\n}\n\nNow, let’s make a dataframe of fake data, with one column that should map to hue and the other mapping to fade. This just puts points all across the space of both variables so we can make sure everything is getting assigned correctly. Then, we’ll use the functions we just created to do a few different things:\n\ncustom hue ramps and built-in palettes\nbinned hue and fade\ncontinuous hue and binned fade\nboth continuous\n\nThe ‘continuous’ examples using inbuilt palettes are only pseudo-continuous by using large numbers of bins because that’s easier for the moment given the way sequential_hcl() works. There’s probably a way around it, but for the moment I’ll ignore it.\n\ncolortibble &lt;- tibble(rvec1 = runif(10000, min = -20, max = 50),\n       rvec2 = runif(10000, min = 53, max = 99)) %&gt;%\n  mutate(rel1 = relpos(rvec1),\n         rel2 = relpos(rvec2)) %&gt;%\n  mutate(colorval = colfinder(rel1, rel2, minhue, maxhue),\n         binval = colfinder(rel1, rel2, minhue, maxhue, nhue = 8, nfade = 4),\n         # need to bypass some args\n         binsun = colfinder(rel1, rel2, nhue = 8, nfade = 4, palname = 'ag_Sunset',\n                            minhue = NULL, maxhue = NULL),\n         pseudoconsun = colfinder(rel1, rel2, nhue = 1000, nfade = 4, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL),\n         pseudoconsun2 = colfinder(rel1, rel2, nhue = 1000, nfade = Inf, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL))\n\nContinuous in both dimensions, using custom hue ramp\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = colorval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nBinned both dims, custom ramp\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = binval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nInbuilt palette, binned both dims.\nThere is a spot in this ag_Sunset palette that matches the ggplot default grey background and so hard to see, but I’ll ignore that for the moment since it doesn’t affect the main thing we’re doing. THese aren’t production plots.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = binsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nPseudo-continuous, binned fades.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = pseudoconsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nPseudo-continuous both dimensions.\n\nggplot(colortibble, aes(x = rvec1, y = rvec2, color = pseudoconsun2)) +\n  geom_point() +\n  scale_color_identity()"
  },
  {
    "objectID": "plotting/faded_colors.html#plotting-data",
    "href": "plotting/faded_colors.html#plotting-data",
    "title": "Faded colors",
    "section": "Plotting data",
    "text": "Plotting data\nNow, let’s see how that might look for some real data. I’ll use some with point data (iris) and then move on to maps, since that’s originally what this was developed for. It should easily extend to anything we can aes() on, e.g. barplot fills, etc.\n\nScatterplot\nTo keep it simple, let’s use iris\nIt won’t span the full space because of the relationship, but that’s OK, I think. We did that above. Here’s iris- now let’s color this plot.\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width)) + geom_point()\n\n\n\n\n\nFade defined by an axis\nThis is how we did it above when plotting the colors to make sure they were working.\nRelativize the x and y to define colors.\n\ncoloriris &lt;- iris %&gt;%\n  mutate(rel1 = relpos(Sepal.Length),\n         rel2 = relpos(Petal.Width)) %&gt;%\n  mutate(colorval = colfinder(rel1, rel2, minhue, maxhue),\n         binval = colfinder(rel1, rel2, minhue, maxhue, nhue = 8, nfade = 4),\n         # need to bypass some args\n         binsun = colfinder(rel1, rel2, nhue = 8, nfade = 4, palname = 'ag_Sunset',\n                            minhue = NULL, maxhue = NULL),\n         pseudoconsun = colfinder(rel1, rel2, nhue = 1000, nfade = 4, palname = 'ag_Sunset',\n                                  minhue = NULL, maxhue = NULL),\n         pseudoconsun2 = colfinder(rel1, rel2, nhue = 1000, nfade = Inf, palname = 'ag_Sunset',\n                                   minhue = NULL, maxhue = NULL))\n\nMake some plots to see the colors and fades correspond to the axis values in binned and unbinned ways.\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = colorval)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = pseudoconsun2)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\nggplot(coloriris, aes(x = Sepal.Length, y = Petal.Width, color = binsun)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\n\n\nFade as a new aesthetic\nTo actually match what I want to use this for, it’s more like we’d say versicolor is less certain. IE Species defines the fade. This is like fade is an aesthetic in ggplot, but we’re sort of manually doing it.\nLet’s set hue by sepal length, and fade by species\n\nuncertainVers &lt;- iris %&gt;%\n  mutate(rel1 = relpos(Sepal.Length),\n         faded = ifelse(Species == 'versicolor', 0.50, 1)) %&gt;%\n  mutate(binhue = huefinder(rel1, n = 8, palname = 'ag_Sunset'),\n         conhue = huefinder(rel1, n = 1000, palname = 'ag_Sunset'),\n         binfade = fadefinder(faded, binhue),\n         confade = fadefinder(faded, conhue))\n\nNow, versicolor should be faded relative to the others\n\nggplot(uncertainVers, aes(x = Sepal.Length, y = Petal.Width, color = binfade)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\nggplot(uncertainVers, aes(x = Sepal.Length, y = Petal.Width, color = confade)) +\n  geom_point() +\n  scale_color_identity()\n\n\n\n\nThat seems to be working, both binned and continous on the hue scale.\n\n\n\nMaps\nWhat I really want this for is a map, with each polygon having a value of the variable of interest mapped to hue, and a ‘certainty’ determining the fade. Though that axis could really be any other value. Can I mock that up?\nRead a map in of catchments in Australia.\n\nallbasins &lt;- read_sf(file.path('data', '42343_shp', 'rbasin_polygon.shp'))\n\nIgnoring fade for the minute, what should we color by? Probably should be random, really, for the demo.\nColoring by centroid will just put a cross-country fade on:\n\nggplot(allbasins, aes(fill = CENTROID_X)) + geom_sf() + scale_fill_continuous_sequential('ag_Sunset')\n\n\n\n\nLet’s make a column representing the value we want to plot for each basin, just chosen at random\n\nallbasins &lt;- allbasins %&gt;%\n  mutate(fakevals = runif(nrow(allbasins))) %&gt;%\n  mutate(rel1 = relpos(fakevals)) %&gt;%\n  mutate(binhue = huefinder(rel1, n = 8, palname = 'ag_Sunset'),\n         conhue = huefinder(rel1, n = 1000, palname = 'ag_Sunset'))\n\nI can use the values directly here with scale_fill_XX if I don’t care about fade\n\nggplot(allbasins, aes(fill = fakevals)) + geom_sf() + scale_fill_continuous_sequential('ag_Sunset')\n\n\n\n\nbut the hues for the faded should match the set hues. Now, I need to use scale_fill_identity(). Works for binned and pseudo-continuous. I’ll save the binned to compare later with the faded version.\n\nhuesonly &lt;- ggplot(allbasins, aes(fill = binhue)) +\n  geom_sf() +\n  scale_fill_identity()\nhuesonly\n\n\n\nggplot(allbasins, aes(fill = conhue)) +\n  geom_sf() +\n  scale_fill_identity()\n\n\n\n\nNow, fade some out (with relatively low probability)\n\nallbasins &lt;- allbasins %&gt;%\n  mutate(faded = sample(x = c(1, 0.5),\n                           size = nrow(allbasins),\n                           replace = TRUE,\n                           prob = c(0.8, 0.2))) %&gt;%\n  mutate(binfade = fadefinder(faded, binhue),\n         confade = fadefinder(faded, conhue))\n\nBinned and continuous. Again, save the binned for comparison\n\nhuefade &lt;- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity()\nhuefade\n\n\n\nggplot(allbasins, aes(fill = confade)) +\n  geom_sf() +\n  scale_fill_identity()\n\n\n\n\nplot the raw and faded next to each other using patchwork. We can now see that some of the catchments are faded versions of the original hue.\n\nhuesonly + huefade\n\n\n\n\n\nLegends\nWe need legends. Could be done by playing with the actual ggplot legend or making mini plot and gluing on.\nQuick attempt at guide fails, because the colors are mixed up because of the RGB sorting.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend') +\n  guides(fill = guide_legend(ncol = 2))\n\n\n\n\nCan I change the order by basing it on the hues and then the fades? Does ‘breaks’ work? Yeah, sort of. And need to sort them in the right way.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = unique(allbasins$binhue))\n\n\n\n\nI think that will basically work, but I’ll need to edit a bit There’s probably a way to write the functions better to just do this all in the mutates, but for now, I can create a tibble of breaks and labels using summarise.\n\nbreaksnlabels &lt;- allbasins %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(binhue) %&gt;%\n  summarize(minbin = min(fakevals),\n            maxbin = max(fakevals),\n            fromto = paste0(as.character(round(minbin, 2)),\n                            ' to ',\n                            as.character(round(maxbin, 2)))) %&gt;%\n  ungroup() %&gt;%\n  arrange(minbin)\n\nWorks for the unfaded\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = breaksnlabels$binhue,\n                      labels = breaksnlabels$fromto)\n\n\n\n\nI could now ALSO fade those, but I might be able to do it as one summarise using the faded column\n\nfadebreaks &lt;- allbasins %&gt;%\n  st_drop_geometry() %&gt;%\n  # needs to capture the color boundaries, whether or not faded\n  group_by(binhue) %&gt;%\n  mutate(minbin = min(fakevals),\n            maxbin = max(fakevals),\n            fromto = paste0(as.character(round(minbin, 2)),\n                            ' to ',\n                            as.character(round(maxbin, 2)))) %&gt;%\n  ungroup() %&gt;%\n  group_by(binfade, faded) %&gt;%\n  summarize(minbin = first(minbin),\n            maxbin = first(maxbin),\n            fromto = first(fromto)) %&gt;%\n  ungroup() %&gt;%\n  arrange(minbin, desc(faded))\n\n`summarise()` has grouped output by 'binfade'. You can override using the\n`.groups` argument.\n\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fadebreaks$binfade,\n                      labels = fadebreaks$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\nPlot tweaking\nThat’s close. Can I make the labels better? Ideally, drop from the faded, and make them at 45 or something. and fix up the size.\nFirst, drop the labels on the faded, since they are the same as the base hue.\n\nfb2 &lt;- fadebreaks %&gt;%\n  mutate(fromto = ifelse(faded == 0.5, '', fromto))\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom')\n\n\n\n\nFixing up the sizes and angles. The size doesn’t do what I want (square), because the text is too big.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value', title.position = 'top',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n      legend.background = element_blank(),\n      legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n      legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nCan I fake it on the row labels by inserting line breaks? The number of lines is really unstable across device sizes or saving the figure, so the number of line breaks will have to be adjusted every time this gets saved etc. But it might kind of work.\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = 'Value\\n\\n\\n\\nCertain\\n\\n\\nUncertain', title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nCan I bold that title?\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = expression(atop(bold('Value'),atop('Certain','Uncertain'))),\n                             title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.3, 'cm'), # This should make them square, but isn't.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nThat doesn’t work very well. Does ggtext do it? Allows markdown syntax and HTML (hence the  instead of ). It works, but still, the number of breaks will depend on the size of the figure device or file\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fb2$binfade,\n                      labels = fb2$fromto) +\n  guides(fill = guide_legend(title = '**Value**&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;Certain&lt;br&gt;&lt;br&gt;Uncertain',\n                             title.position = 'left',\n                             nrow = 2, label.position = 'top')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'bottom',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'), # This should make them square, but isn't because the angled value labels don't allow it.\n        legend.text = element_text(angle = 45, vjust = 0.4))\n\n\n\n\nIf we want square legend boxes and readable text for the value labels, might have to go vertical and that means re-doing the breaks and labels dataframe\n\nfbv &lt;- fadebreaks %&gt;%\n  mutate(fromto = ifelse(faded == 1, '', fromto)) %&gt;%\n  arrange(desc(faded), minbin)\n\n\nggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  guides(fill = guide_legend(title = '**Value**&lt;br&gt;&lt;br&gt;Certain Uncertain',\n                             title.position = 'top',\n                             ncol = 2, label.position = 'right')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'right',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'))\n\n\n\n\nThat works pretty well. If we wanted multiple levels of uncertainty (fades), a similar thing would work with just having more columns. That basically works. If I want to label the fades more robustly, I think I’ll likely need to resort to grobs, in which case I probably might as well do the figure as legend method.\n\n\nMini-figure legends\nSometimes we want to create a legend and then add it back into a figure (maybe if it’s shared, or we want a standard legend across a group of figures). Here, we might want to create a different legend for the certian and uncertain, glue them together, and then glue them back on the main figure.\nto show how this might make sense, let’s make three plots- one with just the certain, one with uncertain, and one with no legend, and then glue together Making this as vertical, but easy enough to swap\nMake the map alone\n\njustmap &lt;- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  theme(legend.position = 'none')\n\n# used later- continuous specification of color and fade\njustmapcon &lt;- ggplot(allbasins, aes(fill = confade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade,\n                      labels = fbv$fromto) +\n  theme(legend.position = 'none')\n\nGet the indices for the two fades\n\ncerts &lt;- which(fbv$faded == 1)\nuncerts &lt;- which(fbv$faded == 0.5)\n\nMake the legend for the unfaded\n\ncertleg &lt;- ggplot(allbasins, aes(fill = binfade)) +\n  geom_sf() +\n  scale_fill_identity(guide = 'legend',\n                      breaks = fbv$binfade[certs],\n                      labels = fbv$fromto[certs]) +\n  guides(fill = guide_legend(title = 'Certain',\n                             title.position = 'top',\n                             ncol = 1, label.position = 'right')) +\n  theme(legend.title = ggtext::element_markdown(),\n        legend.position = 'right',\n        legend.background = element_blank(),\n        legend.key.size = unit(0.5, 'cm'))\n\n# I don't actually want the plot, just the legend, so\n certleg &lt;- ggpubr::get_legend(certleg)\n\nAnd the faded\n\n uncertleg &lt;- ggplot(allbasins, aes(fill = binfade)) +\n   geom_sf() +\n   scale_fill_identity(guide = 'legend',\n                       breaks = fbv$binfade[uncerts],\n                       labels = fbv$fromto[uncerts]) +\n   guides(fill = guide_legend(title = 'Uncertain',\n                              title.position = 'top',\n                              ncol = 1, label.position = 'right')) +\n   theme(legend.title = ggtext::element_markdown(),\n         legend.position = 'right',\n         legend.background = element_blank(),\n         legend.key.size = unit(0.5, 'cm'))\n\n # I don't actually want the plot, just the legend, so\n uncertleg &lt;- ggpubr::get_legend(uncertleg)\n\nGlue those legends\n\nbothleg &lt;- ggpubr::ggarrange(certleg, uncertleg)\n\nand glue on the plot\n\n plotpluslegs &lt;- ggpubr::ggarrange(justmap, bothleg, widths = c(8,2))\n plotpluslegs\n\n\n\n\nThat’s not really any better than what I had before. It is useful to have this level of control sometimes though. In particular, we might want to use a PLOT as a legend, either binned or not.\nTo use a plot as a legend\nHere, binned is obviously the way to go, especially for the two fade levels, but let’s demo both.\nabove, we defined a function col2dmat that makes a plot of the color matrix. Let’s use that to demo a few options. First create the figures that will be the legends.\nBinned both dims, two fades, but just low-high labels\n\nbinnedplotmat &lt;- col2dmat('ag_Sunset', n1 = 8, fadevals = c(0, 0.5))\n bin2legqual &lt;- plot2dcols(binnedplotmat) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = c(1, 2), labels = c('Uncertain', 'Certain')) +\n   # Vague levels\n   scale_x_continuous(breaks = c(1, 8), labels = c('Low', 'High')) +\n   theme(axis.text = element_text())\n bin2legqual\n\n\n\n\nBinned both dims, but now the hue values are quantitatively labeled\n\nnamedlabs &lt;- filter(fb2, fromto != '') %&gt;% select(fromto) %&gt;% pull()\n bin2legquant &lt;- plot2dcols(binnedplotmat) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = c(1, 2), labels = c('Uncertain', 'Certain')) +\n   # Vague levels\n   scale_x_continuous(breaks = 1:8, labels = namedlabs) +\n   theme(axis.text.y = element_text(),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n   ggtitle('Value')\n bin2legquant\n\n\n\n\nA few levels of fade. Very similar to above\n\nmat4fade &lt;- col2dmat('ag_Sunset', n1 = 8, n2 = 4)\n\n fadevals &lt;- rev(seq(0,1, length.out = 4+1))[1:4]\n bin4leg &lt;- plot2dcols(mat4fade) +\n   # Breaks aren't centered on the values for this geom, so instead of 0.5 and 1, need to shift\n   theme_void() +\n   scale_y_continuous(breaks = 1:4, labels = rev(fadevals), name = 'Certainty') +\n   scale_x_continuous(breaks = 1:8, labels = namedlabs, name = 'Value') +\n   theme(axis.text.y = element_text(),\n         axis.title.y = element_text(angle = 90),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n         axis.title.x = element_text())\n bin4leg\n\n\n\n\npseudo-continuous. put the x-axis on top, because that’s what we’d expect for a legend, really. Labels can take a lambda function of the breaks, allowing us to use auto-chosen breaks. But probably better to reference the values they correspond to. It’s just that for this silly demo they are 0-1. Let’s pretend for the minute that they’re logged just for fun and to demo how to do it.\n\nmatcfade &lt;- col2dmat('ag_Sunset', n1 = 100, n2 = 100)\n conleg &lt;- plot2dcols(matcfade) +\n   theme_void() +\n   scale_y_continuous(name = 'Certainty %') +\n   #\n   scale_x_continuous(labels = ~round(log(.), 2), name = 'Value', position = 'top') +\n   theme(axis.text.y = element_text(),\n         axis.title.y = element_text(angle = 90),\n         axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n         axis.title.x = element_text())\n conleg\n\n\n\n\nNow, attach those to the map as legends.\nI’ll use patchwork for most of them, but ggpubr::ggarrange would work too, just with different tweaking. The way patchwork does insets and sizes is working better for me right now, so that’s what I’ll use.\nTaking the grey background off because it’s distracting with inset legends.\nTwo-level binned legend with high-low\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element(bin2legqual, left = 0.1, bottom = 0.1, right = 0.5, top = 0.2)\n\n\n\n\nSame, but quantitative legend labels. Text is a bit absurd.\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((bin2legquant + theme(axis.text = element_text(size = 8),\n                                       title = element_text(size = 8))),\n                 left = 0.1, bottom = 0, right = 0.5, top = 0.25)\n\n\n\n\nA 4-fade example with quantitative fades as well. That’s not our immediate need, but good to be able to do. maybe fade according to standard error or something.\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((bin4leg + theme(axis.text = element_text(size = 8),\n                                       title = element_text(size = 8))),\n                 left = 0.1, bottom = 0, right = 0.5, top = 0.25)\n\n\n\n\nContinuous values in both dimensions. Here, we use a map where colors and fades are both defined continuously.\n\n(justmapcon + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((conleg + coord_fixed() +\n                    theme(axis.text = element_text(size = 8),\n                          title = element_text(size = 8))),\n                 left = 0.1, bottom = 0.05, right = 0.5, top = 0.25)\n\n\n\n\nCan I put the legend off to the side just by specifying bigger coords? sort of- it goes but gets lost\n\n(justmap + theme_bw() + theme(legend.position = 'none')) +\n   inset_element((conleg + coord_fixed() +\n                    theme(axis.text = element_text(size = 8),\n                          title = element_text(size = 8))),\n                 left = 1, bottom = 0.4, right = 1.5, top = 0.75)\n\n\n\n\nWorks with making a small plot with spacers and then glueing that onto the big plot\n\nguidespot &lt;- plot_spacer() /\n   (conleg + coord_fixed() +\n   theme(axis.text = element_text(size = 8),\n         title = element_text(size = 8))) /\n   plot_spacer()\n\n (justmap + theme_bw() + theme(legend.position = 'none')) +\n   guidespot +\n   plot_layout(widths = c(9, 1))\n\n\n\n\nDoes that work with the simpler ones? Yeah, although the 2-fades makes more sense horizontal, so do that\n\n# I can't fiugre out why this creates a dataframe. results = 'hide' doesn't hide it, wrapping with invisible(), etc. I give up. Giving it its own code block\nguidespot2 &lt;- plot_spacer() |\n   (bin2legquant + theme(axis.text = element_text(size = 8),\n                         title = element_text(size = 8))) |\n   plot_spacer()\n\n\n (justmap + theme_bw() + theme(legend.position = 'none')) /\n   guidespot2 +\n   plot_layout(heights = c(9, 1))\n\n\n\n\nA very similar approach would work for ggpubr::ggarrange\nThere’s quite a lot more that could be done here, but this gets me what I need for now."
  },
  {
    "objectID": "plotting/faded_colors.html#notes",
    "href": "plotting/faded_colors.html#notes",
    "title": "Faded colors",
    "section": "Notes",
    "text": "Notes\nif this were truly bivariate (ie two variables of interest), could rotate 45 degrees to equally weight (and likely use different color ramps). But it’s not- it’s certainty along one axis, so leaving horiz and having a lightness axis fits what we’re doing here better."
  },
  {
    "objectID": "plotting/ggplot_themes.html",
    "href": "plotting/ggplot_themes.html",
    "title": "Custom ggplot themes",
    "section": "",
    "text": "I often want to consistently theme my ggplots across projects. I’ve developed some custom themes, but they’re usually ad-hoc, and don’t work particularly well in packages, because the simple way to do it isn’t a function.\nlibrary(ggplot2)\nFor example, we might have a theme that’s good for publication, as in Saving and theming plots, where we specify size, backgrounds, and text. We load new fonts first.\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galen_website\n\nshowtext::showtext_auto()\npubfont &lt;- 'Cambria'\nloadfonts(fontvec = pubfont)\npubtheme &lt;- theme_bw(base_size = 10) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=pubfont))\nHere, the theme is essentially hardcoded, and we can add it to a figure.\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) + \n  geom_point() + \n  facet_wrap(~Species) + \n  pubtheme"
  },
  {
    "objectID": "plotting/ggplot_themes.html#can-we-pass-other-arguments",
    "href": "plotting/ggplot_themes.html#can-we-pass-other-arguments",
    "title": "Custom ggplot themes",
    "section": "Can we pass other arguments?",
    "text": "Can we pass other arguments?\nIf the goal is to enforce a style, we might not want to allow passing other arguments to theme, but can we with …?\n\ntheme_pub_dots &lt;- function(base_size = 10, font, ...) {\n  if (!(font %in% sysfonts::font_families())) {\n      loadfonts(font)\n  }\n\n  ggplot2::theme_bw(base_size = base_size) +\n    theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=font), \n        ...)\n}\n\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) + \n  geom_point() + \n  facet_wrap(~Species) +\n  theme_pub_dots(base_size = 8, font = 'Arial', legend.position = 'none')"
  },
  {
    "objectID": "plotting/math_in_ggplot.html",
    "href": "plotting/math_in_ggplot.html",
    "title": "Math and greek in legends",
    "section": "",
    "text": "I often need math in ggplot labels. Sometimes it’s actual math, but often just things like greek letters, subscripts, fractions, hats, bars, etc. THere’s ways to do most of that with expression and paste and bquote, but none of it is ever very intuitive for me. I’d like to be able to just send it latex and have it work.\nDoes latex2exp work?\n\n\n\nlibrary(ggplot2) \nlibrary(latex2exp)\n\nLet’s just try to do some things with that. Make the usual iris plot.\n\ntestplot &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\ntestplot\n\n\n\n\nNow let’s add some math. I’d typically use labs for the x,y, and color, so try that. Spacing not great, but it works.\nBasically following the manual, it’s pretty self-explanatory.\nThe r says to use raw strings so don’t have to escape slashes.\n\ntestplot &lt;- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho})'),\n       color = TeX(r'($\\left{ \\int_0^\\inf \\exp{\\eta x} dx \\right})'))\n\ntestplot\n\n\n\n\nCan I use amsmath in latex? Maybe, but not for linebreaks- this errors.\n\ntestplot &lt;- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho}$)'),\n       color = TeX(r'($\\begin{split}\\left{ \\int_0^\\inf \\\\ \\exp{\\eta x} dx \\right}\\end{split}$)'))\n\ntestplot"
  },
  {
    "objectID": "plotting/math_in_ggplot.html#using-latex2exp",
    "href": "plotting/math_in_ggplot.html#using-latex2exp",
    "title": "Math and greek in legends",
    "section": "",
    "text": "library(ggplot2) \nlibrary(latex2exp)\n\nLet’s just try to do some things with that. Make the usual iris plot.\n\ntestplot &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\ntestplot\n\n\n\n\nNow let’s add some math. I’d typically use labs for the x,y, and color, so try that. Spacing not great, but it works.\nBasically following the manual, it’s pretty self-explanatory.\nThe r says to use raw strings so don’t have to escape slashes.\n\ntestplot &lt;- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho})'),\n       color = TeX(r'($\\left{ \\int_0^\\inf \\exp{\\eta x} dx \\right})'))\n\ntestplot\n\n\n\n\nCan I use amsmath in latex? Maybe, but not for linebreaks- this errors.\n\ntestplot &lt;- testplot + \n  labs(x = TeX(r'(Words and greek $\\Delta_1$)'),\n       y = TeX(r'($\\frac{1-\\alpha}{\\rho}$)'),\n       color = TeX(r'($\\begin{split}\\left{ \\int_0^\\inf \\\\ \\exp{\\eta x} dx \\right}\\end{split}$)'))\n\ntestplot"
  },
  {
    "objectID": "plotting/rgb_to_hex.html",
    "href": "plotting/rgb_to_hex.html",
    "title": "RGB to hex",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(colorspace)\nlibrary(ggplot2)\n\nLet’s say we have a dataframe with R, G, B values, but we want the hex. Why? Maybe we want to use scale_color_identity to plot the values in some other columns. Let’s say x and y.\n\nset.seed(17)\nrgbtib &lt;- tidyr::expand_grid(x = 1:10, y = 1:10) %&gt;% \n  mutate(R = sample(0:255, 100),\n                         G = sample(0:255, 100),\n                         B = sample(0:255, 100))\nrgbtib\n\n# A tibble: 100 × 5\n       x     y     R     G     B\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1   177   166    44\n 2     1     2   231    80    48\n 3     1     3   224   188   136\n 4     1     4   221   168   181\n 5     1     5   107    82    93\n 6     1     6   238   111   179\n 7     1     7   246   242   124\n 8     1     8   230    57   213\n 9     1     9   172   151   130\n10     1    10   166    93    84\n# … with 90 more rows\n\n\nWe can use {colorspace}, but it’s convoluted- have to make an RGB object first, and then convert to hex. And the RGB need to be on 0-1, not 0-255.\nWriting that out doesn’t work because the colorspace RGB object can’t get stuffed in the dataframe. But this is the idea\n\nrgbtib_writeout &lt;- rgbtib %&gt;% \n  # Convert to 0-1\n  mutate(across(all_of(c('R', 'G', 'B')), ~./255)) %&gt;%\n  # Create the rgb object\n  mutate(rgbobj = colorspace::RGB(R, G, B)) %&gt;% \n  # Get the hex values\n  mutate(hexval = colorspace::hex(rgbobj))\n\nSo, make a function. Have a maxval the user can pass (don’t assume it’s 1 or 255).\n\nrgb2hex &lt;- function(R, G, B, maxval = 255) {\n  rgbobj &lt;- colorspace::RGB(R/maxval, G/maxval, B/maxval)\n  hexval &lt;- colorspace::hex(rgbobj)\n  return(hexval)\n}\n\nTest that\n\nrgb2hex(177, 41, 147)\n\n[1] \"#D970C8\"\n\n\napparently quarto doesn’t do the cool printing of color thing in output, just input.\n\n\"#D970C8\"\n\n[1] \"#D970C8\"\n\n\nNow, use that in the mutate\n\nrgbtib &lt;- rgbtib %&gt;% \n  mutate(hexvals = rgb2hex(R, G, B))\nrgbtib\n\n# A tibble: 100 × 6\n       x     y     R     G     B hexvals\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  \n 1     1     1   177   166    44 #D9D373\n 2     1     2   231    80    48 #F49878\n 3     1     3   224   188   136 #F1DFC1\n 4     1     4   221   168   181 #EFD4DB\n 5     1     5   107    82    93 #AD9AA3\n 6     1     6   238   111   179 #F7B0DA\n 7     1     7   246   242   124 #FBF9B9\n 8     1     8   230    57   213 #F482EC\n 9     1     9   172   151   130 #D6CABD\n10     1    10   166    93    84 #D3A39B\n# … with 90 more rows\n\n\nPlot to show it works\n\nggplot(rgbtib, aes(x = x, y = y, fill = hexvals)) + geom_tile() + theme(legend.position = 'none')"
  },
  {
    "objectID": "plotting/tweaks_tricks.html",
    "href": "plotting/tweaks_tricks.html",
    "title": "Theming and saving",
    "section": "",
    "text": "library(tidyverse) # Overkill, but easier than picking and choosing"
  },
  {
    "objectID": "plotting/tweaks_tricks.html#theming",
    "href": "plotting/tweaks_tricks.html#theming",
    "title": "Theming and saving",
    "section": "Theming",
    "text": "Theming\nI tend to establish a theme to set the basic plot look, including font sizes. I start with theme_bw() because the default ggplot grey background doesn’t look good in pubs. I used to set the sizes separately for each sort of text (commented out), but that is typically easier to just use the base_size argument and let ggplot handle the relative adjustments.\nCan also set theme differently for presentations, including doing things like setting font to match a ppt theme.\nDeveloping themes is now done more extensively in ggplot themes, where I develop them as more flexible functions.\nTypically, very few fonts are loaded into R and available for use. See fonts.Rmd for figuring out how to work with that. The short answer is that we use showtext to load what we need (if anything). If this step is skipped, will default to the default font and throw a warning about “fontfamily not found” because we haven’t loaded the selected font yet.\nWe could load fonts by hand Using functions from showtext and sysfonts, and specify the text = element_text(family=\"Ink Free\") with a manual character vector. It’s way easier to automate though, and saves issues of loading the wrong font.\nFirst, load the function I wrote that simplifies finding the files and their names to load them. And tell R to use showtext to render fonts.\n\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galen_website\n\nshowtext::showtext_auto()\n\nThen, load the font(s) we want\n\ntalkfont &lt;- 'Ink Free'\npubfont &lt;- 'Cambria'\nloadfonts(fontvec = c(talkfont, pubfont))\n# loadfonts()\n\nNote that we could also just loadfonts() with no arguments to read in ALL available fonts\nPass talkfont and pubfont to the themes.\n\ntalktheme &lt;- theme_bw(base_size = 18) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=talkfont)) # Replace with fontname used in PPT\n\n# \n        # axis.text = element_text(size = 18),\n        # axis.title = element_text(size = 24),\n        # strip.text = element_text(size = 24),\n        # plot.title = element_text(size = 24))\n\npubtheme &lt;- theme_bw(base_size = 10) + \n  theme(strip.background = element_blank(),\n        plot.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(family=pubfont))\n\nAs an example, let’s make a simple plot with iris, and then look at the themed versions.\n\nbaseiris &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point()\nbaseiris\n\n\n\n\nNow, what does a publication version look like?\n\nbaseiris + pubtheme\n\n\n\n\nNote that further theme changes can happen later on, e.g. Note that it’s easy to get in trouble with the internal legend positions when it comes time to save- as the dimensions change on export vs whatever arbitrary size you have the Rstudio plot pane, what looks good will changes as well.\n\nbaseiris + pubtheme +\n  theme(legend.title = element_text(face = 'bold'),\n        legend.position = c(0.8,0.2))\n\n\n\n\nFor talks, we use talktheme. Terrible font, but easy to see that it’s been shifted from default.\n\nbaseiris + talktheme\n\n\n\n\nWe can update parts of the theme including the font while keeping the rest. Though if we haven’t loaded all fonts, will need to load the new ones now.\n\n# load new font\nloadfonts(fontvec = 'Elephant')\n\ntalktheme &lt;- talktheme + \n  theme(text = element_text(family = 'Elephant')) # Replace with fontname used in PPT\n\nAnd to show that it worked, plot again.\n\nbaseiris + talktheme"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My background is in theoretical community ecology, where I use analytical theory and probabilistic simulation models to better understand how environmental variation in space and time interact with life history to maintain diversity and community structure. Current work extends this approach to nonstationary environmental variation (e.g. climate change), with a particular focus on how movement and growth across a heterogeneous landscape determines population trajectories.\n\n\n\nBranching"
  },
  {
    "objectID": "research.html#analysis-and-methods-development",
    "href": "research.html#analysis-and-methods-development",
    "title": "Research",
    "section": "Analysis and methods development",
    "text": "Analysis and methods development\nCapturing the data needed for linking theory to empirical work or building large-scale management models often requires developing new methods and analytical techniques or applying them in new ways. I enjoy the creativity inherent in this process, whether it is to adapt beta-binomial logistic regression for testing density-dependence in infection rates or modifying cameras to take underwater night vision movies of insect egg-laying.\nIn the course of my work I develop R packages and analysis workflows (some of which are available publicly and some not (yet)). These typically are the core output of my theoretical and modelling work, as well as the analysis of empirical data. Others simply arise as side projects or infrastructure to allow other projects to proceed. As is likely typical of most people writing code, the final, working, code is the end product of quite a lot of experimentation and figuring out new approaches. I have begun documenting this process both for my future self and others."
  },
  {
    "objectID": "setup/R_in_VS.html",
    "href": "setup/R_in_VS.html",
    "title": "R in VS code",
    "section": "",
    "text": "I typically use Rstudio, and am very used to it. But I need to use VScode for working on Azure, and am trying to sort that out. There are also idiosyncracies with using Azure, but I’ll try to hold those for somewhere else and just keep this about VS."
  },
  {
    "objectID": "setup/R_in_VS.html#the-issue",
    "href": "setup/R_in_VS.html#the-issue",
    "title": "R in VS code",
    "section": "",
    "text": "I typically use Rstudio, and am very used to it. But I need to use VScode for working on Azure, and am trying to sort that out. There are also idiosyncracies with using Azure, but I’ll try to hold those for somewhere else and just keep this about VS."
  },
  {
    "objectID": "setup/R_in_VS.html#the-basics",
    "href": "setup/R_in_VS.html#the-basics",
    "title": "R in VS code",
    "section": "The basics",
    "text": "The basics\nThe VS code documentation gives a pretty good overview of the basics- install VScode, install languageserver, and install the R extension. That gets us up and running. Though it is worth noting that if you tend to work in renv for everything, it’s probably better to install languageserver globally (ie in a non-renv-managed session).\nNow, supposedly that provides linting, debugging, code completion, help, etc. And the add-ons (radian and httpgd look good too in terms of nicer terminal and visualisations). The question now is, HOW do we actually use all that functionality. I’m so used to Rstudio, it’ll take some playing. I’ll try to write down here what I try and how to get it to work.\nI tried to install.packages('languageserver') globally, but it gets grumpy sometimes and can’t find it inside a renv- managed repo because the .libPaths don’t have wherever the global package directory is. It seems to have worked on Windows, but not Azure/Unix.\n\nRadian\nI tried installing radian as the terminal. Assuming I don’t want it to mess up the project-level python environments, I installed it globally through the git-bash in windows terminal. It works when I type radian into VS bash, but does not run when I try to actually run something from a .R file. In the command pallette -&gt; settings -&gt; extensions -&gt; R, there’s an option for Rterm:Windows that says it can be the path to radian. I tried where radian in git-bash (which radian on unix), and it gave me two paths in py-env/shims. The one with the .bat works on windows (C:\\Users\\galen\\.pyenv\\pyenv-win\\shims\\radian.bat). On Unix, it’s just a standard usr/bin/…. I’ve turned radian back off, though, because it throws a really annoying amount of weird errors that don’t actually stop the code from running, and that don’t appear in the base R terminal. Things like ‘unexpected & in }’, when there are no ‘&’ symbols in the code (and it still completes (usually). I think it works in .R scripts, but not in quarto. It’s something about bracketed paste not working right in notebooks I think. Working on sorting that out.\n\n\nlinting\nThis is something that (weirdly, I think) isn’t included in Rstudio. It supposedly is in VScode, but I’m not seeing obvious signs of it.\nInteresting. I don’t know what changed, but it has suddenly started linting. Maybe I turned something on in the settings-&gt;extensions-&gt;R section?\nAnd now all the blue lines are super annoying. Would be nice to at least have a turn off for comments setting. Or a good way to wrap comments a la Rstudio ctrl-shift /.\nGuess I need to figure out how to step through lintr and fix issues."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html",
    "href": "simmodelling/twoDautocorr.html",
    "title": "2d autocorrelation",
    "section": "",
    "text": "knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\nlibrary(tidyverse)"
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#motivation",
    "href": "simmodelling/twoDautocorr.html#motivation",
    "title": "2d autocorrelation",
    "section": "Motivation",
    "text": "Motivation\nI often need to simulate processes that are autocorrelated in two dimensions. Sometimes that’s time and 1d space, sometimes 2d space. Clearly 3d is likely needed as well, and I’ll update this with that once I get to it.\nThis is code that builds on work I’ve done in a couple projects, both across matlab and R. I’m doing it here in R because that’s the most up to date and open-source, but the matlab translation is straightforward.\nWe want to be able to generate a set of values with given statistical properties- means, standard deviations, and correlations in both dimensions. For the moment, I’m developing this with a gaussian random variable, but extensions to other random variables that are transforms from gaussian are relatively straightforward by backcalculating the needed \\(\\mu\\) and \\(\\sigma\\). Care must be taken if the correlations need to also be defined on the final scale.\n\nFuture/elsewhere\nI’ve done the back-calculations for the lognormal to allow setting desired correlations, means, and variances on the lognormal scale, and will add it in here later as an example. Likewise, we might want to set the correlation length \\(\\tau\\) rather than the correlation \\(\\rho\\), and in that case we need to back-calculate \\(\\rho\\) from the desired \\(\\tau\\). I’ve done that as well and will add it in. Finally, I have written up the math to obtain the equations used in this function, and will add that later as well."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#process",
    "href": "simmodelling/twoDautocorr.html#process",
    "title": "2d autocorrelation",
    "section": "Process",
    "text": "Process\nThe goal is a U matrix that is 2d AC, on the normal scale\n\nSet up autocorrelation in the y dimension in U with a usual \\(y+1 = y*\\rho + a\\) formulation, where \\(a\\) is uncorrelated errors\nSet up autocorrelation in the x dimension\n\n\n\nthe errors here (\\(\\varepsilon\\) matrix) need to be correlated in the y dimension\nthese errors are thus generated by an AC process and so need their own set of errors (which are uncorrelated) for that AC\n\nVariances are set for all error matrices (\\(a\\), \\(\\varepsilon\\), and sub-errors (\\(z\\) matrix)) according to the relationships between normVar (the desired \\(\\sigma^2\\) of the final distribution) and the \\(\\rho_y\\) and \\(\\rho_x\\) (the desired correlations in both dimensions)."
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#function",
    "href": "simmodelling/twoDautocorr.html#function",
    "title": "2d autocorrelation",
    "section": "Function",
    "text": "Function\nI usually do a bunch of demos, but here I’ve developed this and just want it available more easily. So I’ll lead with the function and then demonstrate it and a few extensions.\n\nac2d &lt;- function(n_x, n_y, \n                 rho_y = 0, rho_x = 0, \n                 normVar = 1,\n                 printStats = FALSE,\n                 returnStats = FALSE) {\n  # n_x = number of sites along the x-dimension\n  # n_y = number of sites along the y-dimension\n  # rho_y = desired autocorr in the x direction\n  # rho_x = desired autocorr in the y direction\n  # normVar = desired variance of the underlying normal distribution\n  \n  # The goal is a U matrix that is 2d AC, on the normal scale\n  \n  # make the U matrix as rnorms to initialise\n  U &lt;- matrix(rnorm(n_x*n_y)*sqrt(normVar), nrow = n_y)\n  \n  # Set up the errors for the y process alone\n  # generate the errors - set the SD of these (hence the sqrt around the\n  # variance)\n  a &lt;- rnorm(n_y) * sqrt((normVar*(1-rho_y^2)))\n  \n  # Make the y ac for the U matrix\n  for (i in 1:(n_y-1)) {\n    U[i+1, ] &lt;- (rho_y * U[i, ]) + a[i]\n  }\n  \n  # Set up for the x-autocorr, which needs to have errors autocorred in the y-dimension\n  \n  # first, generate a z error matrix- these are the errors for epsilon, which\n  # are in turn the errors for U(t,x).\n  # What should var(z) be theoretically?\n  varZ &lt;- normVar*(1-rho_y^2)*(1-rho_x^2)\n  \n  # Make z, adjusting its standard deviation\n  # should have 'y' rows\n  z &lt;- matrix(rnorm(n_x*n_y), nrow = n_y) * \n    (sqrt(normVar * (1-rho_y^2) * (1-rho_x^2)))\n  \n  # now let's generate an epsilon matrix\n  # These are the errors for x part of the 2d ac process. These errors are\n  # themselves autocorrelated in the y dimension.\n  vareps &lt;- normVar * (1-rho_x^2)\n  eps &lt;- matrix(rnorm(n_x*n_y), nrow = n_y) * sqrt(vareps)\n  \n  # Now, generate the eps matrix y-autocorrelated (that is, going down rows within each column)\n  # eps is already created, so just write into the rows\n  for (i in 1:(n_y-1)) {\n    eps[i+1, ] &lt;- (rho_y * eps[i, ]) + z[i, ]\n  }\n  \n  # Now, make the U matrix x-autocorrelated\n  for (t in 1:(n_x-1)) {\n    U[ ,t+1] &lt;- (rho_x * U[ ,t]) + eps[ ,t]\n    \n  }\n  \n  # Check the stats if asked\n  if (printStats | returnStats) {\n    # calc stats in both dimensions\n    acstats &lt;- ac2dstats(U)\n    \n    if (printStats) {\n      print(paste0('Mean of all points is ', round(mean(c(U)), 3)))\n      print(paste0('Var of all points is ', round(var(c(U)), 3)))\n      print(paste0('Mean y AC is ', round(mean(acstats$ac_y), 3)))\n      print(paste0('Mean x AC is ', round(mean(acstats$ac_x), 3)))\n    }\n  }\n  \n  # usually don't want a list with the stats, and can always get later if needed, I suppose\n  if (returnStats) {\n    return(lst(U, acstats))\n  } else {\n    return(U)\n  }\n  \n}\n\nThat potentially calls another function to get the stats, which is here.\n\n# 2d ac stats function, useful for calling elsewhere\nac2dstats &lt;- function(acmatrix) {\n  # Calculate the autocorrs in both dimensions\n  \n  # Conditionals on 0 variance are because ar throws an error if there's no variance. Could have set up a try, but this is clearer\n  # Using 1 as the ac in that case because with no variance each value is the same as previous and so perfectly correlated. NA would be another option.\n  \n  # Get the ac in x-dimension: do this for each y (row)\n  ac_x &lt;- vector(mode = 'numeric', length = nrow(acmatrix)-1)\n  for (i in 1:(nrow(acmatrix)-1)) {\n    if (sd(acmatrix[i, ]) == 0) {\n      ac_x &lt;- 1\n    } else {\n      ac_x[i] &lt;- acf(acmatrix[i, ], lag.max = 1, type = 'correlation', plot = FALSE, demean = TRUE)$acf[2]\n    }\n    \n  } \n  \n  # Get the ac acorss the stream: do this for each x (column)\n  ac_y &lt;- vector(mode = 'numeric', length = ncol(acmatrix)-1)\n  for (i in 1:(ncol(acmatrix)-1)) {\n    \n    if (sd(acmatrix[,i]) == 0) {\n      ac_y[i] &lt;- 1\n    } else {\n      ac_y[i] &lt;- acf(acmatrix[ ,i], lag.max = 1, type = 'correlation', plot = FALSE, demean = TRUE)$acf[2]\n    }\n    \n  } \n  \n  return(lst(ac_y, ac_x))\n}"
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#testing",
    "href": "simmodelling/twoDautocorr.html#testing",
    "title": "2d autocorrelation",
    "section": "Testing",
    "text": "Testing\nA couple edge cases to make sure it doesn’t break. 0 and 1 correlations.\n\nacmatrix_0_1 &lt;- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0, rho_y = 1,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is 0.028\"\n[1] \"Var of all points is 0.976\"\n[1] \"Mean y AC is 1\"\n[1] \"Mean x AC is 0.043\"\n\n\n0 variance, but try to set autocorrelations- forces all points equal, which is right.\n\nacmatrix_0_1 &lt;- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 0, printStats = TRUE)\n\n[1] \"Mean of all points is 0\"\n[1] \"Var of all points is 0\"\n[1] \"Mean y AC is 1\"\n[1] \"Mean x AC is 1\""
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#demonstration",
    "href": "simmodelling/twoDautocorr.html#demonstration",
    "title": "2d autocorrelation",
    "section": "Demonstration",
    "text": "Demonstration\nHow do we use that? Let’s say we want to create an environment that is 1000 x 500 sites, with \\(\\rho_y = 0.9\\) and \\(\\rho_x = 0.7\\), with the whole environment having a variance of 1 (for simplicity).\nSetting printstats = TRUE prints out the statistics and confirms the final matrix has been created with the desired correlations.\n\nacmatrix_7_9 &lt;- ac2d(n_x = 1000, n_y = 500,\n        rho_x = 0.7, rho_y = 0.9,\n        normVar = 1, printStats = TRUE)\n\n[1] \"Mean of all points is 0.021\"\n[1] \"Var of all points is 0.993\"\n[1] \"Mean y AC is 0.89\"\n[1] \"Mean x AC is 0.698\"\n\n\nWe can plot that up, easiest is to use ggplot because that’s what I’m used to. First, make it a tibble\n\nactib_7_9 &lt;- tibble::as_tibble(acmatrix_7_9) %&gt;%\n  mutate(y = row_number()) %&gt;%\n  pivot_longer(cols = starts_with('V')) %&gt;%\n  mutate(x = as.numeric(str_remove(name, 'V'))) %&gt;%\n  select(-name)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\nPlot it two different ways. It’s a monster though, so cut it to just a 100x100 block.\nFirst, a contour\n\nggplot(filter(actib_7_9, x &gt; 100 & x &lt;= 200 & y &gt; 300 & y &lt; 400), aes(x = x, y = y, z = value)) +\n  geom_contour_filled()\n\n\n\n\nAnd a tiled version, which is more precisely the data.\n\nggplot(filter(actib_7_9, x &gt; 100 & x &lt;= 200 & y &gt; 300 & y &lt; 400), aes(x = x, y = y, fill = value)) + \n  geom_tile() +\n  viridis::scale_fill_viridis(option = 'viridis')"
  },
  {
    "objectID": "simmodelling/twoDautocorr.html#extensions",
    "href": "simmodelling/twoDautocorr.html#extensions",
    "title": "2d autocorrelation",
    "section": "Extensions",
    "text": "Extensions\n\n2 species\nA crude step toward 3d autocorr is to say we want 2d autocorr for two species (or really, just a second set of 2d autocorrelated values) with known correlation to the first set. I’ve done that, but it’s very task-specific and so not including here until I generalise a bit better.\n\n\nCross-correlation\nBy definition, the 2d autocorrelated matrices here have embedded nonzero cross-correlations at different lags (see analytical work for what they are once I put it in here). As a quick example, we can use ccf to get the cross correlation between two adjacent vectors along the x-dimension (columns), or the same along the y-dimension (rows).\nColumns\n\nccf(x = acmatrix_7_9[,100], y = acmatrix_7_9[,101], lag.max = 10, type = 'correlation')\n\n\n\n\nRows\n\nccf(x = acmatrix_7_9[100,], y = acmatrix_7_9[101,], lag.max = 10, type = 'correlation')"
  },
  {
    "objectID": "small_helpers/error_handling.html",
    "href": "small_helpers/error_handling.html",
    "title": "Catching, passing, handling errors",
    "section": "",
    "text": "There’s a lot out there on handling errors. This will mostly be testing things as they come up when I need to use them. Will rely heavily on Hadley, as usual."
  },
  {
    "objectID": "small_helpers/error_handling.html#try-and-passing",
    "href": "small_helpers/error_handling.html#try-and-passing",
    "title": "Catching, passing, handling errors",
    "section": "Try and passing",
    "text": "Try and passing\nIf we just use try, the error should get printed but everything keeps moving\n\noutvec &lt;- vector(mode = 'numeric', length = 10)\nfor (i in 1:10) {outvec[i] &lt;- try(err_even(i))}\n\nError in err_even(i) : Even numbers are error\nError in err_even(i) : Even numbers are error\nError in err_even(i) : Even numbers are error\nError in err_even(i) : Even numbers are error\nError in err_even(i) : Even numbers are error\n\noutvec\n\n [1] \"1\"                                              \n [2] \"Error in err_even(i) : Even numbers are error\\n\"\n [3] \"3\"                                              \n [4] \"Error in err_even(i) : Even numbers are error\\n\"\n [5] \"5\"                                              \n [6] \"Error in err_even(i) : Even numbers are error\\n\"\n [7] \"7\"                                              \n [8] \"Error in err_even(i) : Even numbers are error\\n\"\n [9] \"9\"                                              \n[10] \"Error in err_even(i) : Even numbers are error\\n\"\n\n\nHuh. I thought try just printed the values but let things keep going. Changing the non-failures to character isn’t ideal. But I guess then I’d use tryCatch? For now though, this is exactly what i need, so I’ll stop here."
  },
  {
    "objectID": "small_helpers/error_handling.html#trycatch",
    "href": "small_helpers/error_handling.html#trycatch",
    "title": "Catching, passing, handling errors",
    "section": "tryCatch",
    "text": "tryCatch\nI actually want to capture errors, warnings, or passing to assess some code\n\nerr_even_warn5 &lt;- function(x) {\n  if ((x %% 2) == 0) {\n    stop('Even numbers are error')\n  } else if (x == 5) {\n    warning('5 throws a warning')\n  } else {x}\n}\n\nI want to use this for recording, so\n\nrecorder &lt;- vector(mode = 'character', length = 10)\nfor (i in 1:10) {\n  recorder[i] &lt;- tryCatch(err_even_warn5(i),\n                        error = function(c) c$message,\n                        warning = function(c) c$message,\n                        message = function(c) c$message)\n}\nrecorder\n\n [1] \"1\"                      \"Even numbers are error\" \"3\"                     \n [4] \"Even numbers are error\" \"5 throws a warning\"     \"Even numbers are error\"\n [7] \"7\"                      \"Even numbers are error\" \"9\"                     \n[10] \"Even numbers are error\"\n\n\nAnd to be even more explicit, can I do some mods in the call to just say if it passed?\n\nrecorder2 &lt;- vector(mode = 'character', length = 10)\nfor (i in 1:10) {\n  recorder2[i] &lt;- tryCatch(if(is.numeric(err_even_warn5(i))) {'pass'},\n                        error = function(c) c$message,\n                        warning = function(c) c$message,\n                        message = function(c) c$message)\n}\nrecorder2\n\n [1] \"pass\"                   \"Even numbers are error\" \"pass\"                  \n [4] \"Even numbers are error\" \"5 throws a warning\"     \"Even numbers are error\"\n [7] \"pass\"                   \"Even numbers are error\" \"pass\"                  \n[10] \"Even numbers are error\""
  },
  {
    "objectID": "small_helpers/error_handling.html#asidespecific-case",
    "href": "small_helpers/error_handling.html#asidespecific-case",
    "title": "Catching, passing, handling errors",
    "section": "Aside/specific case",
    "text": "Aside/specific case\nThe function purrr::safely is useful when using purrr::map and similar, as it returns a list with a result and error item. This means that purrring over things where some may fail doesn’t kill everything, but we need to unpack it a bit.\n\nerrpurr &lt;- purrr::map(1:10, purrr::safely(err_even_warn5))\n\nWarning in .f(...): 5 throws a warning\n\nerrpurr\n\n[[1]]\n[[1]]$result\n[1] 1\n\n[[1]]$error\nNULL\n\n\n[[2]]\n[[2]]$result\nNULL\n\n[[2]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[3]]\n[[3]]$result\n[1] 3\n\n[[3]]$error\nNULL\n\n\n[[4]]\n[[4]]$result\nNULL\n\n[[4]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[5]]\n[[5]]$result\n[1] \"5 throws a warning\"\n\n[[5]]$error\nNULL\n\n\n[[6]]\n[[6]]$result\nNULL\n\n[[6]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[7]]\n[[7]]$result\n[1] 7\n\n[[7]]$error\nNULL\n\n\n[[8]]\n[[8]]$result\nNULL\n\n[[8]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\n[[9]]\n[[9]]$result\n[1] 9\n\n[[9]]$error\nNULL\n\n\n[[10]]\n[[10]]$result\nNULL\n\n[[10]]$error\n&lt;simpleError in .f(...): Even numbers are error&gt;\n\n\nNote that safely only deals with errors, the ‘warning’ at index 5 just passes through and is included in the result. We could use quietly instead if we want to capture all possibilities except errors, which still cause quietly to fail. We can do things like look for the values with or without errors\n\nwhicherrors &lt;- purrr::map(errpurr, \\(x) !is.null(x$error)) |&gt; \n  unlist() |&gt; \n  which()\n\nwhicherrors\n\n[1]  2  4  6  8 10\n\n\nThose without errors (or with a non-null result can be used to extract the clean outputs. Note that this includes the warning.\n\nnoterrors &lt;- purrr::map(errpurr,\n           \\(x) purrr::pluck(x, 'result')) |&gt;  \n    unlist()\n\nnoterrors\n\n[1] \"1\"                  \"3\"                  \"5 throws a warning\"\n[4] \"7\"                  \"9\"                 \n\n\nThe above is really handy if we want to read the errors. If not, and we just want to save the non-errors, possibly with a default is likely better.\n\nerrpurrP &lt;- purrr::map(1:10, purrr::possibly(err_even_warn5, NA)) |&gt; unlist()\n\nWarning in .f(...): 5 throws a warning\n\nerrpurrP\n\n [1] \"1\"                  NA                   \"3\"                 \n [4] NA                   \"5 throws a warning\" NA                  \n [7] \"7\"                  NA                   \"9\"                 \n[10] NA                  \n\n\nNote that in use we’d likely still want to have a cleanup step/function to chuck out the warnings before concatenating the rest."
  },
  {
    "objectID": "small_helpers/lifecycle_warnings.html",
    "href": "small_helpers/lifecycle_warnings.html",
    "title": "Intermittent warnings",
    "section": "",
    "text": "I’ve been dealing with debugging a package with a dplyr warning that only appears every 8 hours with the message\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nIt causes warnings every time I test the whole package with devtools::test(), but only every 8 hours otherwise, which makes tracing and debugging impossible.\n{rlang} provides a way to set {lifecycle} warning frequencies (docs) so I set in my test script\n\nrlang::local_options(lifecycle_verbosity = \"warning\")\n\nI think this should do it too\n\noptions(lifecycle_verbosity = \"warning\")"
  },
  {
    "objectID": "small_helpers/private_github_notes.html",
    "href": "small_helpers/private_github_notes.html",
    "title": "Github notes",
    "section": "",
    "text": "Authorisation can happen a couple ways- ssh and https with a github PAT.\n\n\nGo to https://github.com/settings/tokens and create a PAT with at least repo scope and copy it. SAVE IT SOMEWHERE OTHER THAN PLAINTEXT. Then, to authorise, the simplest but most dangerous is to use auth_token = 'YOUR GITHUB PAT'.\nThe better option is to use credentials::set_github_pat() to set your PAT using the github signin, which doesn’t require you to have it in plaintext. That sets the GITHUB_PAT environment variable, which is the default for auth_token.\nSo, assuming you’ve created a PAT in github,\n# install.packages(\"devtools\")\ncredentials::set_github_pat()\ndevtools::install_github(\"USER/repo\")\n\n\n\nSee the github documents for creating ssh keys. Note that R and helpers want them to be the default names (and often the RSA version). So don’t do anything different with names, and if get some errors about not finding them, check if you made ed25519 and not rsa keys.\nThen, to install,\n# install.packages(\"devtools\")\ndevtools::install_git(\"git@github.com:MDBAuth/WERP_toolkit.git\", ref = 'master', force = TRUE, upgrade = 'ask')\nBut, if using R 4.3, the {git2r} package does not support ssh, and so you have to clone the directory, and use\n`devtools::install_local('path/to/repo', force = TRUE, upgrade = 'ask')`\nHopefully that’s fixed soon (or install_git moves away from {git2r}- {gert} works with ssh, but install_git can’t use it)"
  },
  {
    "objectID": "small_helpers/private_github_notes.html#private-repos-and-access",
    "href": "small_helpers/private_github_notes.html#private-repos-and-access",
    "title": "Github notes",
    "section": "",
    "text": "Authorisation can happen a couple ways- ssh and https with a github PAT.\n\n\nGo to https://github.com/settings/tokens and create a PAT with at least repo scope and copy it. SAVE IT SOMEWHERE OTHER THAN PLAINTEXT. Then, to authorise, the simplest but most dangerous is to use auth_token = 'YOUR GITHUB PAT'.\nThe better option is to use credentials::set_github_pat() to set your PAT using the github signin, which doesn’t require you to have it in plaintext. That sets the GITHUB_PAT environment variable, which is the default for auth_token.\nSo, assuming you’ve created a PAT in github,\n# install.packages(\"devtools\")\ncredentials::set_github_pat()\ndevtools::install_github(\"USER/repo\")\n\n\n\nSee the github documents for creating ssh keys. Note that R and helpers want them to be the default names (and often the RSA version). So don’t do anything different with names, and if get some errors about not finding them, check if you made ed25519 and not rsa keys.\nThen, to install,\n# install.packages(\"devtools\")\ndevtools::install_git(\"git@github.com:MDBAuth/WERP_toolkit.git\", ref = 'master', force = TRUE, upgrade = 'ask')\nBut, if using R 4.3, the {git2r} package does not support ssh, and so you have to clone the directory, and use\n`devtools::install_local('path/to/repo', force = TRUE, upgrade = 'ask')`\nHopefully that’s fixed soon (or install_git moves away from {git2r}- {gert} works with ssh, but install_git can’t use it)"
  },
  {
    "objectID": "small_helpers/smallpieces.html",
    "href": "small_helpers/smallpieces.html",
    "title": "Small pieces",
    "section": "",
    "text": "This is mostly quick little code snippets to copy-paste and avoid re-writing. load tidyverse and get going.\n\nlibrary(tidyverse)\n\n\n\nWe often want to set the root directory not to the file but to the project. In Rmarkdown, we use the following in the setup chunk. Quarto typically uses a different method, but see the Quarto notes for some exceptions. Converting from Rmarkdown to quarto with knitr::convert_chunk_header kills this block, and it’s annoying to always have the header. In both Rmarkdown and Quarto, this has to be in a setup chunk.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nI thought it’d be easiest to set in the global options, but that doesn’t seem to persist to render or knit.\n\n\n\n\n\nnewdir &lt;- file.path('output', 'testdir')\nif (!dir.exists(newdir)) {dir.create(newdir, recursive = TRUE)}\n\n\n\n\nWindows paths come in with \\, which R treats as an escape character. We can use file.path to just avoid them, or replace them with / or \\\\. But sometimes we just want to paste a path in quickly and be done with it. As of R 4.0, we can do that with r. It requires the parentheses to be in a funny place- inside the quotes.\n\npastepath &lt;- r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\"\npastepath\n\n[1] \"C:\\\\Users\\\\username\\\\path\\\\to\\\\somewhere.csv\"\n\n\nAnd we can feed that straight into functions that need paths, eg.\n\nreadr::read_csv(r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\")\n\n\n\n\nFunctions like duplicated give the second (and greater) values that match. e.g.\n\nx &lt;- c(1,2,1,3,4,2)\nduplicated(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nBut we often want to grab all values that are repeated- ie if everything matches in one column what’s going on in the others. do do that we can use group_by and filter to get those with &gt; 1 row.\nIE, let’s compare cars with duplicated mpg values\n\nmtcars %&gt;%\n  dplyr::group_by(mpg) %&gt;%\n  dplyr::filter(n() &gt; 1) %&gt;%\n  dplyr::arrange(mpg) # makes the comparisons easier\n\n\n\n  \n\n\n\nWhy is that useful? We can see not only that these aren’t fully duplicated rows (which we also could have done with duplicated on the whole table), but also actually look at what differs easily.\n\n\nWe might have a list with internal duplicates, e.g.\n\nduplist &lt;- list(a = c('a', 'b'), b = c('b', 'c'), d = c('a', 'c'), e = c('f', 'g'), f = c('f', 'h'), g = c('a', 'l'))\nduplist\n\n$a\n[1] \"a\" \"b\"\n\n$b\n[1] \"b\" \"c\"\n\n$d\n[1] \"a\" \"c\"\n\n$e\n[1] \"f\" \"g\"\n\n$f\n[1] \"f\" \"h\"\n\n$g\n[1] \"a\" \"l\"\n\n\nWe can see which values in the first position are duplicated, but again, not the first instances.\n\nthedups &lt;- duplist[duplicated(purrr::map_chr(duplist, \\(x) x[1]))] |&gt;\n      purrr::map_chr(\\(x) x[1])\nthedups\n\n  d   f   g \n\"a\" \"f\" \"a\" \n\n\nWe can get all of them by mapping whether the first value is in thedups and then dropping empties\n\nall_duplicated &lt;- purrr::map(duplist, \\(x) x[x[1] %in% thedups]) |&gt; \n  purrr::discard(\\(x) length(x) == 0)\nall_duplicated\n\n$a\n[1] \"a\" \"b\"\n\n$d\n[1] \"a\" \"c\"\n\n$e\n[1] \"f\" \"g\"\n\n$f\n[1] \"f\" \"h\"\n\n$g\n[1] \"a\" \"l\"\n\n\n\n\n\n\nSometimes with long csvs, readr’s guess of col type based on the first thousand rows is wrong. But only for some cols. If we want to not have to specify all of them, we can use .default and only specify the offending col.\nFirst, save dummy data\n\ndumtib &lt;- tibble(c1 = 1:3000, c2 = rep(letters, length.out = 3000), c3 = c(c1[1:2000], c2[2001:3000]))\n\nwrite_csv(dumtib, file = file.path(newdir, 'colspectest.csv'))\n\nIf we read in without the cols, it assumes c3 is numeric and we get errors. But it doesn’t. why not? It keeps getting me elsewhere, but now I can’t create the problem. FIgure this out later, I guess\n\nfilein &lt;- read_csv(file.path(newdir, 'colspectest.csv'), guess_max = 100)\n\nRows: 3000 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): c2, c3\ndbl (1): c1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTell it the third col is character.\n\nfilein &lt;- readr::read_csv(file.path(newdir, 'colspectest.csv'), col_types = cols(.default = \"?\", c3 = \"c\"))\n\n\n\n\nYes, we should be building as a library in this case, but it’s often easier at least initially to not deal with the overhead. If, for example, all functions are in the ‘functions’ directory,\n\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galen_website\n\n\n\n\n\nRender in quarto defaults to making dfs text, and so often we can’t see all the columns (or rows), or access them. setting the df-print option to paged allows them to work. The header should look like this (commented out because this isn’t a header)\n\n# title: \"TITLE\"\n# author: \"AUTHOR\"\n# format:\n#   html:\n#     df-print: paged\n\n\n\n\nconvert_chunk_headers is the main thing, but I want to apply it to a full directory. Let’s get the dir for here.\n\nallrmd &lt;- list.files(rprojroot::find_rstudio_root_file(), pattern = '.Rmd', recursive = TRUE, full.names = TRUE)\n\nallrmd &lt;- allrmd[!stringr::str_detect(allrmd, 'renv')]\n\nallqmd &lt;- stringr::str_replace(allrmd, '.Rmd', '.qmd')\n\nCan I vectorize? No, but a loop works. Git commit first!\n\nfor (i in 1:length(allrmd)) {\n  knitr::convert_chunk_header(input = allrmd[i], output = allqmd[i])\n}\n\nNow, if you want to really go for it, delete the rmds. That makes git happier because then it can treat this as a rename and keep tracking the files.\nDangerous- make sure you’ve git-committed. I’m commenting out and eval: false ing this\n\n# file.remove(allrmd)"
  },
  {
    "objectID": "small_helpers/smallpieces.html#what-is-this",
    "href": "small_helpers/smallpieces.html#what-is-this",
    "title": "Small pieces",
    "section": "",
    "text": "This is mostly quick little code snippets to copy-paste and avoid re-writing. load tidyverse and get going.\n\nlibrary(tidyverse)\n\n\n\nWe often want to set the root directory not to the file but to the project. In Rmarkdown, we use the following in the setup chunk. Quarto typically uses a different method, but see the Quarto notes for some exceptions. Converting from Rmarkdown to quarto with knitr::convert_chunk_header kills this block, and it’s annoying to always have the header. In both Rmarkdown and Quarto, this has to be in a setup chunk.\n\n```{r setup}\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\n```\n\nI thought it’d be easiest to set in the global options, but that doesn’t seem to persist to render or knit.\n\n\n\n\n\nnewdir &lt;- file.path('output', 'testdir')\nif (!dir.exists(newdir)) {dir.create(newdir, recursive = TRUE)}\n\n\n\n\nWindows paths come in with \\, which R treats as an escape character. We can use file.path to just avoid them, or replace them with / or \\\\. But sometimes we just want to paste a path in quickly and be done with it. As of R 4.0, we can do that with r. It requires the parentheses to be in a funny place- inside the quotes.\n\npastepath &lt;- r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\"\npastepath\n\n[1] \"C:\\\\Users\\\\username\\\\path\\\\to\\\\somewhere.csv\"\n\n\nAnd we can feed that straight into functions that need paths, eg.\n\nreadr::read_csv(r\"(C:\\Users\\username\\path\\to\\somewhere.csv)\")\n\n\n\n\nFunctions like duplicated give the second (and greater) values that match. e.g.\n\nx &lt;- c(1,2,1,3,4,2)\nduplicated(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\nBut we often want to grab all values that are repeated- ie if everything matches in one column what’s going on in the others. do do that we can use group_by and filter to get those with &gt; 1 row.\nIE, let’s compare cars with duplicated mpg values\n\nmtcars %&gt;%\n  dplyr::group_by(mpg) %&gt;%\n  dplyr::filter(n() &gt; 1) %&gt;%\n  dplyr::arrange(mpg) # makes the comparisons easier\n\n\n\n  \n\n\n\nWhy is that useful? We can see not only that these aren’t fully duplicated rows (which we also could have done with duplicated on the whole table), but also actually look at what differs easily.\n\n\nWe might have a list with internal duplicates, e.g.\n\nduplist &lt;- list(a = c('a', 'b'), b = c('b', 'c'), d = c('a', 'c'), e = c('f', 'g'), f = c('f', 'h'), g = c('a', 'l'))\nduplist\n\n$a\n[1] \"a\" \"b\"\n\n$b\n[1] \"b\" \"c\"\n\n$d\n[1] \"a\" \"c\"\n\n$e\n[1] \"f\" \"g\"\n\n$f\n[1] \"f\" \"h\"\n\n$g\n[1] \"a\" \"l\"\n\n\nWe can see which values in the first position are duplicated, but again, not the first instances.\n\nthedups &lt;- duplist[duplicated(purrr::map_chr(duplist, \\(x) x[1]))] |&gt;\n      purrr::map_chr(\\(x) x[1])\nthedups\n\n  d   f   g \n\"a\" \"f\" \"a\" \n\n\nWe can get all of them by mapping whether the first value is in thedups and then dropping empties\n\nall_duplicated &lt;- purrr::map(duplist, \\(x) x[x[1] %in% thedups]) |&gt; \n  purrr::discard(\\(x) length(x) == 0)\nall_duplicated\n\n$a\n[1] \"a\" \"b\"\n\n$d\n[1] \"a\" \"c\"\n\n$e\n[1] \"f\" \"g\"\n\n$f\n[1] \"f\" \"h\"\n\n$g\n[1] \"a\" \"l\"\n\n\n\n\n\n\nSometimes with long csvs, readr’s guess of col type based on the first thousand rows is wrong. But only for some cols. If we want to not have to specify all of them, we can use .default and only specify the offending col.\nFirst, save dummy data\n\ndumtib &lt;- tibble(c1 = 1:3000, c2 = rep(letters, length.out = 3000), c3 = c(c1[1:2000], c2[2001:3000]))\n\nwrite_csv(dumtib, file = file.path(newdir, 'colspectest.csv'))\n\nIf we read in without the cols, it assumes c3 is numeric and we get errors. But it doesn’t. why not? It keeps getting me elsewhere, but now I can’t create the problem. FIgure this out later, I guess\n\nfilein &lt;- read_csv(file.path(newdir, 'colspectest.csv'), guess_max = 100)\n\nRows: 3000 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): c2, c3\ndbl (1): c1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTell it the third col is character.\n\nfilein &lt;- readr::read_csv(file.path(newdir, 'colspectest.csv'), col_types = cols(.default = \"?\", c3 = \"c\"))\n\n\n\n\nYes, we should be building as a library in this case, but it’s often easier at least initially to not deal with the overhead. If, for example, all functions are in the ‘functions’ directory,\n\n# Load local functions\ndevtools::load_all()\n\nℹ Loading galen_website\n\n\n\n\n\nRender in quarto defaults to making dfs text, and so often we can’t see all the columns (or rows), or access them. setting the df-print option to paged allows them to work. The header should look like this (commented out because this isn’t a header)\n\n# title: \"TITLE\"\n# author: \"AUTHOR\"\n# format:\n#   html:\n#     df-print: paged\n\n\n\n\nconvert_chunk_headers is the main thing, but I want to apply it to a full directory. Let’s get the dir for here.\n\nallrmd &lt;- list.files(rprojroot::find_rstudio_root_file(), pattern = '.Rmd', recursive = TRUE, full.names = TRUE)\n\nallrmd &lt;- allrmd[!stringr::str_detect(allrmd, 'renv')]\n\nallqmd &lt;- stringr::str_replace(allrmd, '.Rmd', '.qmd')\n\nCan I vectorize? No, but a loop works. Git commit first!\n\nfor (i in 1:length(allrmd)) {\n  knitr::convert_chunk_header(input = allrmd[i], output = allqmd[i])\n}\n\nNow, if you want to really go for it, delete the rmds. That makes git happier because then it can treat this as a rename and keep tracking the files.\nDangerous- make sure you’ve git-committed. I’m commenting out and eval: false ing this\n\n# file.remove(allrmd)"
  },
  {
    "objectID": "stats_probability/expected_shannon.html",
    "href": "stats_probability/expected_shannon.html",
    "title": "Expected Shannon Diversity",
    "section": "",
    "text": "We have a community that we’re describing with Shannon diversity, \\(-\\sum{p_ilogp_i}\\) , but we’re doing so for a lot of sub-communities of different size. These diversities change with size in a way that is clearly not what we’d expect if they were simply samples from the regional pool, but to show that we need to see that expectation.\nWe should be able to get an analytical expected value given a set of regional proportions \\(p_i…p_k\\) and size of the sub-community \\(N\\). Most likely as a transformation of a multinomial distribution. But we’re under a bit of a time crunch, so I’ll throw together a bootstrap version.\nFundamentally, the null expectation for a sub-community of size \\(N\\) is defined as the identity of each individual being chosen from a categorical distribution with regional probabilities (pooled over all sub-communities) \\(p_1…p_k\\) , where \\(k\\) is the number of species. We could construct this with the Bernouilli, but the {extraDistr} package just has a random number generator.\n\nextraDistr::rcat(10, c(0.1, 0.2, 0.3, 0.4))\n\n [1] 2 2 2 4 4 4 3 2 4 3\n\n\nSo, to bootstrap the expected shannon, for each n, we will do an rcat some large number of times, calculate shannon. That will give the probability distribution of the shannon, and the mean will be approximately the expectation. We might want other moments too, as sd should shrink as \\(N\\) increases.\n\nexpected_shannon &lt;- function(N, regional_probs, \n                             n_boots = 1000, returntype = 'mean') {\n  \n  H &lt;- matrix(NA, ncol = length(N), nrow = n_boots)\n  colnames(H) &lt;- paste0('N_', as.character(N))\n  \n  for (i in 1:length(N)) {\n    for (j in 1:n_boots) {\n      # This loses the names, but that doesn't matter.\n      localcomp &lt;- extraDistr::rcat(N[i], regional_probs)\n      localprops &lt;- table(localcomp)/N[i]\n      H[j,i] &lt;- -sum(localprops*log(localprops))\n    }\n  }\n  \n  if (returntype == 'dist') {return(H)}\n  \n  if (returntype == 'mean') {return(apply(H, 2, mean))}\n  \n}\n\nA quick test\n\npooled_probs &lt;- c(0.1, 0.5, 0.1, 0.3)\nEs &lt;- expected_shannon(1:100, regional_probs = pooled_probs)\n\n\nplot(Es)\n\n\n\n\nLarger N makes that take much larger, so we might want to do something like\n\nEslong &lt;- expected_shannon(c(1:50, 100, 250, 500), regional_probs = pooled_probs)\n\n\nplot(Eslong, type = 'lines')\n\nWarning in plot.xy(xy, type, ...): plot type 'lines' will be truncated to first\ncharacter"
  },
  {
    "objectID": "stats_probability/fitting_truncated.html",
    "href": "stats_probability/fitting_truncated.html",
    "title": "Fitting partial distributions",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(fitdistrplus)\n\nWarning: package 'fitdistrplus' was built under R version 4.2.3\n\n\nLoading required package: MASS\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nLoading required package: survival\n\nlibrary(foreach)\n\n\nAttaching package: 'foreach'\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\nWe have some data that has a lot of zeros in it, but is otherwise reasonably modelled by a lognormal. We want to be able to shift the distribution to change the number of zeros, and so we need to fit that lognormal, and then shift it around.\nOne way to think about the problem is that there is some detection limit or process limit below which we get zeros in the data, but where theoretically the distribution continues. Specifically, we’re dealing with water flow, and so maybe it drops below the ability of the gauges to detect, but continues dropping below that. It may even drop underground (e.g river is dry), but to some approximation, water remains underground, and the amount can continue dropping.\nThus, we want to take the data we have, and say that any zeros are actually some small but unknown number, and fit the distribution. We can identify the zero/undetectable threshold as the smallest nonzero number. Then, we can simulate new data with a shifted distribution, which will have a different number of points below that threshold.\nHere, we want to figure out how to shift the data."
  },
  {
    "objectID": "stats_probability/fitting_truncated.html#generate-data",
    "href": "stats_probability/fitting_truncated.html#generate-data",
    "title": "Fitting partial distributions",
    "section": "Generate data",
    "text": "Generate data\nMake the ‘true’ data\n\ntruedata &lt;- rlnorm(10000, 5, 1.5)\n\nSet the detection limit. Making this a variable since we need it in a few places and might want to change it.\n\ndetectionlimit &lt;- 10\n\n\ncensdata &lt;- truedata\ncensdata[censdata &lt; detectionlimit] &lt;- 0\n\nPlot those densities- big spike at 0 in censored (obviously).\n\ntc &lt;- tibble(truedata, censdata)\n\nggplot(tc) +\n  geom_density(aes(x = truedata), color = 'black') + \n  geom_density(aes(x = censdata), color = 'firebrick') + \n  xlim(-1, 100)\n\nWarning: Removed 6136 rows containing non-finite values (`stat_density()`).\nRemoved 6136 rows containing non-finite values (`stat_density()`)."
  },
  {
    "objectID": "stats_probability/fitting_truncated.html#naive-fit",
    "href": "stats_probability/fitting_truncated.html#naive-fit",
    "title": "Fitting partial distributions",
    "section": "Naive fit",
    "text": "Naive fit\nWe can just use fitdistr to get a naive fit- it should be right for truedata but wrong for censdata. It’s not way off, but it is off.\n\nfit_true &lt;- fitdistr(truedata, densfun = 'lognormal')\nfit_cens &lt;- fitdistr(censdata[censdata &gt; 0], densfun = 'lognormal')\n\nfit_true\n\n    meanlog       sdlog   \n  5.02722644   1.49624675 \n (0.01496247) (0.01058006)\n\nfit_cens\n\n     meanlog        sdlog   \n  5.152259790   1.372372006 \n (0.013981250) (0.009886236)\n\n\nHow far off?\n\ndf_cdf &lt;- tibble(x = seq(0,1000, by = 0.1), \n                 cdf_true = plnorm(x, \n                             fit_true$estimate['meanlog'],\n                             fit_true$estimate['sdlog']),\n                 cdf_cens = plnorm(x, \n                             fit_cens$estimate['meanlog'],\n                             fit_cens$estimate['sdlog']))\n\n\nggplot() + \n  stat_ecdf(data = tc, mapping = aes(x = truedata), color = 'forestgreen') + \n  stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_true), color = 'darkseagreen') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens), color = 'deeppink2') +\n  coord_cartesian(xlim = c(-1, 2*detectionlimit), ylim = c(0, 0.1))\n\n\n\n\nThe empirical cdfs fit exactly after 10, because of the probability mass at 0 for the censored data. The fitted distributions are clearly very good for the true data, but quite a ways off for the censored."
  },
  {
    "objectID": "stats_probability/fitting_truncated.html#censored-fit",
    "href": "stats_probability/fitting_truncated.html#censored-fit",
    "title": "Fitting partial distributions",
    "section": "Censored fit",
    "text": "Censored fit\nWe should be able to use fitdistrplus::fitdistcens, though we need some weird data manipulation. Our data is either left-censored or true. From the help, left should be NA for left-censored, or observed for non-censored, and right should be the right bound of the interval for interval-censored or the data for non-censored. I think that means we set it to the detection limit for the censored observations- e.g. it’s the right bound of a censoring interval from -infinity to detection limit. But the interval censoring is confusing me.\n\ncensframe &lt;- tc |&gt; \n  dplyr::mutate(left = ifelse(censdata == 0, NA, censdata),\n                right = ifelse(censdata == 0, detectionlimit, censdata)) |&gt; \n  dplyr::select(left, right)\n\nNow, fit that. There’s a bug where an internal error check fails for tibbles, so send it an old-fashioned df. (it asks for length(censdata[,1]), which is the length of the vector for data.frames, but because indexing remains a tibble, fails for tibbles).\n\nfit_cens2 &lt;- fitdistcens(censdata = data.frame(censframe), distr = 'lnorm')\n\nfit_cens2\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n        estimate\nmeanlog 5.026718\nsdlog   1.497242\n\n\nAdd that to the cdfs\n\ndf_cdf &lt;- df_cdf |&gt; \n  mutate(cdf_cens2 = plnorm(x, \n                             fit_cens2$estimate['meanlog'],\n                             fit_cens2$estimate['sdlog']),)\n\nAnd plot it\n\nggplot() + \n  stat_ecdf(data = tc, mapping = aes(x = truedata), color = 'forestgreen') + \n  stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_true), color = 'darkseagreen') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens), color = 'deeppink2') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens2), color = 'purple') +\n  coord_cartesian(xlim = c(-1, 2*detectionlimit), ylim = c(0, 0.1))"
  },
  {
    "objectID": "stats_probability/fitting_truncated.html#shifted-to-censor",
    "href": "stats_probability/fitting_truncated.html#shifted-to-censor",
    "title": "Fitting partial distributions",
    "section": "Shifted to censor",
    "text": "Shifted to censor\nThe above is working, but when I go to apply it, there’s a slightly different issue. The data above is still bounded by 0, we’re just saying we can’t see it below some limit. And so the censored fitting allows us to fill in that bit of distribution between 0 and the limit.\nThe issue I actually encounter with my real data is that it looks like the curve simply cuts off at zero, but should go below. In other words, it looks like this:\n\ntc &lt;- tc |&gt; \n  mutate(shiftdata = truedata-detectionlimit,\n         shiftcens = ifelse(shiftdata &gt; 0, shiftdata, 0))\n\n\nggplot(tc) +\n  stat_ecdf(mapping = aes(x = shiftdata), color = 'grey80') +\n  stat_ecdf(mapping = aes(x = shiftcens), color = 'black') +\n  stat_ecdf(mapping = aes(x = truedata), color = 'forestgreen') + \n  stat_ecdf(mapping = aes(x = censdata), color = 'firebrick') +\n  coord_cartesian(xlim = c(-detectionlimit-1, 10*detectionlimit), ylim = c(0, 0.5))\n\n\n\n\nSo, it looks like the same problem, but we run into problems when we try to fit it. Because any function we try to fit will still be bounded by 0, not allow some unknown pseudo-tail to go lower than 0.\nFit it. where do we put the detection limit in this case? It shouldn’t be the same as above, necessarily, because we do have numbers between 0 and 10 (or whatever it is). I’ll choose 1, but even that’s arbitrary. Changing the left to NA anything less than whatever that value is so we don’t have a few numbers and then the spike.\n\nshiftdetect &lt;- 1\ncensframeshift &lt;- tc |&gt; \n  dplyr::mutate(left = ifelse(shiftcens &lt; shiftdetect, NA, shiftcens),\n                right = ifelse(shiftcens == 0, shiftdetect, shiftcens)) |&gt; \n  dplyr::select(left, right)\n\nNow, fit that.\n\nfit_censshift &lt;- fitdistcens(censdata = data.frame(censframeshift), distr = 'lnorm')\n\nfit_censshift\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n        estimate\nmeanlog 4.759746\nsdlog   1.934907\n\n\nThat is quite a bit different than the previous\n\nfit_cens2\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n        estimate\nmeanlog 5.026718\nsdlog   1.497242\n\n\nAdd that to the cdfs\n\ndf_cdf &lt;- df_cdf |&gt; \n  mutate(cdf_censshift = plnorm(x, \n                             fit_censshift$estimate['meanlog'],\n                             fit_censshift$estimate['sdlog']))\n\nAnd plot it\n\nggplot() + \n  stat_ecdf(data = tc, mapping = aes(x = truedata), color = 'forestgreen') + \n  stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  stat_ecdf(data = tc, mapping = aes(x = shiftcens), color = 'black') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_true), \n            color = 'darkseagreen') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens), \n            color = 'deeppink2') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_cens2),\n            color = 'purple') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_censshift),\n            color = 'magenta') +\n  coord_cartesian(xlim = c(-1, 2*detectionlimit), ylim = c(0, 0.1))\n\n\n\n\nWe can see that the magenta line is a terrible fit, because it is trying to fit an actual lognormal, which is necessarily bound by 0.\nSo, we want to find a way to fit the bit of the lognormal we have, assuming that some portion has gone below 0.\n\nShift the distribution\nThis approach exactly parallels how we created the dummy data- we shift the distribution up some amount, call that the detection limit, and fit (and then shift the fit back down). Let’s first do that for the case we know should work- exactly mirroring the shift we used to create the data. Assuming that works, we’ll need to then explore the situation where we don’t know how far to shift.\nThis is a bit silly, because we are just recovering the test data, but to make the process clear (and retain the zeros in shiftcens), we create a shifted version of it. This should match the censored true data censdata for this precise re-shift, with the slight change in the location of the initial probability mass.\n\ntc &lt;- tc |&gt; \n  mutate(shiftcensback = shiftcens + detectionlimit)\n\n\nggplot(tc) + \n    stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  stat_ecdf(data = tc, mapping = aes(x = shiftcensback), color = 'cyan', linetype = 2) + xlim(c(0,100))\n\nWarning: Removed 6136 rows containing non-finite values (`stat_ecdf()`).\nRemoved 6136 rows containing non-finite values (`stat_ecdf()`).\n\n\n\n\n\nNow, we fit that shifted data. We’re back to censoring at the detectionlimit (or whatever the shift back is). Now, instead of a mass at 0, all that mass gets pushed to detectonlimit (e.g. min(tc$shiftcensback) is 10, not 0). So we have to use that as the censoring value.\n\ncensshiftback &lt;- tc |&gt; \n  dplyr::mutate(left = ifelse(shiftcensback &lt;= detectionlimit, NA, shiftcensback),\n                right = ifelse(shiftcensback &lt;= detectionlimit, detectionlimit, shiftcensback)) |&gt; \n  dplyr::select(left, right)\n\nfit_censshiftback &lt;- fitdistcens(censdata = data.frame(censshiftback), distr = 'lnorm')\n\nfit_censshiftback\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n        estimate\nmeanlog 5.026718\nsdlog   1.497242\n\n\nThat is the same as fit_cens2, which we know works. Plot to double check. Need to add to df_cdf. And make a shifted x that we can use to re-backshift the new cdf. If that makes any sense.\n\ndf_cdf &lt;- df_cdf |&gt; \n  mutate(cdf_censshiftback = plnorm(x, \n                             fit_censshiftback$estimate['meanlog'],\n                             fit_censshiftback$estimate['sdlog']),\n         x_shiftback = x-detectionlimit)\n\nNow, cdf_censshiftback should match the shifted-up distribution, and shifting it back down should match the ‘original’ distribution that looks like it should go below 0.\n\nggplot() + \n  stat_ecdf(data = tc, mapping = aes(x = censdata), color = 'firebrick') +\n  stat_ecdf(data = tc, mapping = aes(x = shiftcens), color = 'black') +\n   stat_ecdf(data = tc, mapping = aes(x = shiftcensback), \n             color = 'cyan', linetype = 2) +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_censshiftback), \n            color = 'deeppink2') +\n  geom_line(data = df_cdf, mapping = aes(x = x, y = cdf_censshift),\n            color = 'magenta') +\n    geom_line(data = df_cdf, mapping = aes(x = x_shiftback, y = cdf_censshiftback),\n            color = 'dodgerblue') +\n  coord_cartesian(xlim = c(-1, 2*detectionlimit), ylim = c(0, 0.1))\n\n\n\n\nSo, that seems to be working. Now, though, what if we don’t know how much to shift? E.g. we’ll have data with no obvious ‘true’ shift. Can we just shift it way off to make sure the cdf hits zero, and then shift it back?\nI think at this point I probably need to use a clean dataframe and set of variables- detectionlimit doesn’t really mean the same thing as ‘arbitrary shift’.\n\n\nUnknown shift\nI’m going to start all the way back at the beginning with a clean set of truedata and then shift it.\n\ntrueshift &lt;- 10\n\n\ndf_shift &lt;- tibble(truedata = truedata, \n                   shiftdata = truedata-trueshift, \n                   shiftcens = ifelse(shiftdata &gt; 0, shiftdata, 0))\n\nIs that what I think it is? Yes\n\nggplot(df_shift) +\n  stat_ecdf(aes(x = truedata), color = 'black') +\n  stat_ecdf(aes(x = shiftdata), color = 'firebrick') +\n  stat_ecdf(aes(x = shiftcens), color = 'dodgerblue') +\n  coord_cartesian(xlim = c(-10, 100), ylim = c(0, 0.5))\n\n\n\n\nSO, the blue line there (shiftcens) is what we think we have for the real data. We know from above that it works to fit it if we bump it back up by exactly what we bumped down to make it, because, unsurprisingly, it becomes exactly what it was.\nSo the question now is, can we shift that blue line back up some arbitrary amount (as long as it’s greater than the downshift), and still recover the distribution?\n\nupshift &lt;- 100\n\nNow, the min of that is upshift, not 0.\n\ndf_shift &lt;- df_shift |&gt; \n  mutate(shiftup = shiftcens + upshift)\n\nGet the fit for that. It clearly does not give the same values, by necessity.\n\nupcens &lt;- df_shift |&gt; \n  dplyr::mutate(left = ifelse(shiftup &lt;= upshift, NA, shiftup),\n                right = ifelse(shiftup &lt;= upshift, upshift, shiftup)) |&gt; \n  dplyr::select(left, right)\n\nfit_upcens &lt;- fitdistcens(censdata = data.frame(upcens), distr = 'lnorm')\n\nfit_upcens\n\nFitting of the distribution ' lnorm ' on censored data by maximum likelihood \nParameters:\n         estimate\nmeanlog 5.6976127\nsdlog   0.9454364\n\n\nDo we at least do a good job fitting that shifted distribution?\n\ncdf_up &lt;- tibble(x = seq(0,1000, by = 0.1), \n                 cdf_shiftup = plnorm(x, \n                             fit_upcens$estimate['meanlog'],\n                             fit_upcens$estimate['sdlog']))\n\nThat’s terrible.\n\nggplot() +\n  stat_ecdf(data = df_shift, mapping = aes(x = shiftup)) +\n  geom_line(data = cdf_up, mapping = aes(x = x, y = cdf_shiftup), color = 'purple') +\n  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 0.5))\n\n\n\n\nWrite a function and loop to look at this over some range of shifts and clean up the dataframe.\n\nshiftfun &lt;- function(indata, upshift) {\n  # Shift the data\n  shiftdf &lt;- tibble(clipped = indata, shiftdat = indata + upshift, upshift)\n  \n  # Create censored dataset and fit\n  \n  # Handle the zero case- we just use the next value up\n  if (upshift == 0) {rightlim &lt;- min(indata[indata&gt;0])} else {rightlim &lt;- upshift}\n  \n  upcens &lt;- shiftdf |&gt; \n  dplyr::mutate(left = ifelse(shiftdat &lt;= upshift, NA, shiftdat),\n                right = ifelse(shiftdat &lt;= upshift, rightlim, shiftdat)) |&gt; \n  dplyr::select(left, right)\n\n  fit_up &lt;- fitdistcens(censdata = data.frame(upcens), distr = 'lnorm')\n\n  # This isn't ideal, but we can shove the cdf on here too, it just has rows that don't mean the same thing. prevents us saving a list though.\n  shiftdf &lt;- shiftdf |&gt; \n    mutate(x = row_number()/10,\n           cdf_up = plnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           pdf_up = dlnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           # Some diagnostics\n           fitloglik = fit_up$loglik,\n           fitsemean = fit_up$sd['meanlog'],\n           fitsesd = fit_up$sd['sdlog'])\n\n}\n\nLoop over a few values of the shift, from as-is, below true, true, and above true. Indata is the dummy for the data we actually have (ending at 0, but too high). Here, it’s in df_shift$shiftcens.\n\nabshift &lt;- foreach(upshift = c(0, 1, 5, 10, 20, 50, 100), .combine = bind_rows) %do% {\n  shiftfun(df_shift$shiftcens, upshift)\n}\n\n\nggplot(abshift) +\n  stat_ecdf(aes(x = shiftdat, color = factor(upshift))) +\n  geom_line(aes(x = x, y = cdf_up, color = factor(upshift)), linetype = 2) +\n  coord_cartesian(xlim = c(0,200), ylim = c(0, 0.5))\n\n\n\n\nI think maybe we need to minimize Kullback-Leibler, but we might be able to just look at the fit diagnostics\n\nabshift |&gt; \n  distinct(upshift, fitloglik, fitsemean, fitsesd) |&gt; \nggplot(aes(x = upshift, y = fitloglik)) + \n  geom_line() + geom_point() + \n  geom_label(aes(label = upshift), hjust = -0.1, vjust = -0.1, alpha = 0.2)\n\n\n\n\n\nabshift |&gt; \n  distinct(upshift, fitloglik, fitsemean, fitsesd) |&gt; \nggplot(aes(x = upshift)) + \n  geom_line(aes(y = fitsemean)) + geom_point(aes(y = fitsemean)) + \n  geom_line(aes(y = fitsesd), color = 'firebrick') + \n  geom_point(aes(y = fitsesd), color = 'firebrick') + \n  geom_label(aes(y = fitsemean, label = upshift), hjust = -0.1, vjust = -0.1, alpha = 0.2)\n\n\n\n\nSo, unsurprisingly, the thing to do is maximise log-likelihood. I could probably figure out a way to include a shift in the main function and use whatever the optimiser is doing internally.\nOr, I can use optim on the output?\n\nindata &lt;- df_shift$shiftcens\nshiftoptim &lt;- function(upshift) {\n  # Shift the data\n  shiftdf &lt;- tibble(clipped = indata, shiftdat = indata + upshift, upshift)\n  \n  # Create censored dataset and fit\n  \n  # Handle the zero case- we just use the next value up\n  if (upshift == 0) {rightlim &lt;- min(indata[indata&gt;0])} else {rightlim &lt;- upshift}\n  \n  upcens &lt;- shiftdf |&gt; \n  dplyr::mutate(left = ifelse(shiftdat &lt;= upshift, NA, shiftdat),\n                right = ifelse(shiftdat &lt;= upshift, rightlim, shiftdat)) |&gt; \n  dplyr::select(left, right)\n\n  suppressWarnings(fit_up &lt;- fitdistcens(censdata = data.frame(upcens), distr = 'lnorm'))\n  \n  return(-fit_up$loglik)\n}\n\nThis says to use optimize\n\noptim(1, shiftoptim)\n\nWarning in optim(1, shiftoptim): one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\n$par\n[1] 11.125\n\n$value\n[1] 67693.8\n\n$counts\nfunction gradient \n      28       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\noptimize(shiftoptim, interval = c(0, 1000))\n\n$minimum\n[1] 11.1132\n\n$objective\n[1] 67693.8\n\n\nLet’s write a version that takes the data, then optimizes to get the right upshift.\n\nfitshift &lt;- function(rawdata, shift_up) {\n  # Handle the zero case- we just use the next value up\n    if (shift_up == 0) {rightlim &lt;- min(rawdata[rawdata&gt;0])\n    } else {\n      rightlim &lt;- shift_up}\n  \n  inshift &lt;- rawdata + shift_up\n    \n    upcens &lt;- tibble(left = ifelse(inshift &lt;= shift_up, NA, inshift),\n                right = ifelse(inshift &lt;= shift_up, rightlim, inshift))\n    \n    suppressWarnings(fit_up &lt;- fitdistcens(censdata = data.frame(upcens),\n                                           distr = 'lnorm'))\n    \n    return(fit_up)\n}\n\n\nopt_up &lt;- function(shift_up, rawdata) {\n  \n  fit_up &lt;- fitshift(rawdata, shift_up)\n  \n  return(-fit_up$loglik)\n}\n\n\noptshift &lt;- function(rawdata) {\n  \n  \n  # get the optimal shift\n  shift &lt;- optimize(opt_up, interval = c(0, 1000), rawdata = rawdata)\n  \n  # Get the fit at that shift (would be nice to kick this out of opt_up somehow)\n  \n  fit_up &lt;- fitshift(rawdata, shift$minimum)\n  \n # Create a df for output\n  # The shifted data\n  shiftdf &lt;- tibble(orig_data = rawdata, \n                    shift_data = rawdata + shift$minimum, \n                    optimum_shift = shift$minimum)\n  \n\n   # This isn't ideal, but we can shove the cdf on here too, it just has rows that don't mean the same thing. prevents us saving a list though.\n  shiftdf &lt;- shiftdf |&gt; \n    mutate(x = row_number()/10,\n           cdf_up = plnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           pdf_up = dlnorm(x, \n                             fit_up$estimate['meanlog'],\n                             fit_up$estimate['sdlog']),\n           # Some diagnostics\n           fitloglik = fit_up$loglik)\n  \n  # and a shifted-back version of the cdf/pdf just needs a shifted x. The\n  # backshift of the data is just the original `rawdata`.\n  shiftdf &lt;- shiftdf |&gt; \n    mutate(x_back = x-shift$minimum)\n\n}\n\n\noptimal_fit &lt;- optshift(df_shift$shiftcens)\n\nThat’s not exactly 10, but is it close? The CDF looks pretty good.\n\nggplot(optimal_fit) +\n  stat_ecdf(aes(x = shift_data)) +\n  geom_line(aes(x = x, y = cdf_up), linetype = 2) +\n  coord_cartesian(xlim = c(0,400), ylim = c(0, 0.5))\n\n\n\n\nPDF not as perfect, but OK.\n\nggplot(optimal_fit) +\n  geom_density(aes(x = shift_data)) +\n  geom_line(aes(x = x, y = pdf_up), linetype = 2) +\n  xlim(c(0,1000))\n\nWarning: Removed 1039 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nAnd the backshift (e.g. back to the original position of the data)\n\nggplot(optimal_fit) +\n  stat_ecdf(aes(x = orig_data)) +\n  geom_line(aes(x = x_back, y = cdf_up), linetype = 2) +\n  coord_cartesian(xlim = c(0,400), ylim = c(0, 0.5))"
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html",
    "href": "tidyprogramming/tidy_programs.html",
    "title": "Tidy programming",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#the-issue",
    "href": "tidyprogramming/tidy_programs.html#the-issue",
    "title": "Tidy programming",
    "section": "The issue",
    "text": "The issue\nTidyverse, and particularly dplyr and ggplot, are great for quickly doing very powerful rearrangements and calculations of data and making plots. One of the main way they achieve this is by allowing us to use bare variable names- unquoted, no $ syntax. However, that becomes tricky when programming and we might want to pass variables as an argument. Passing other things as arguments can also be a pain, e.g. functions for summarize. I’ve encountered many different things that trip me up, depending on what I’m trying to pass, but my fixes are typically ad-hoc and scattered around my code. I’ll use this doc as a central place to sort out solutions to various problems as they come up. There’s quite a lot of answers from dplyr itself, but for some reason I always have to figure things out for myself."
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#passing-to-group_by",
    "href": "tidyprogramming/tidy_programs.html#passing-to-group_by",
    "title": "Tidy programming",
    "section": "Passing to group_by",
    "text": "Passing to group_by\nlet’s say we want to allow the user to pass which functions to group_by. The two usual ways I end up doing this are double-embracing or just using character vectors. Let’s demo and test with a grouped mean for mtcars. Embracing allows the user to pass bare names, chars makes them pass characters and we have to use across(all_of()) which is annoying syntax.\n\n# embracing\ngroupbrace &lt;- function(data, groupers) {\n  gm &lt;- data %&gt;%\n    group_by({{groupers}}) %&gt;%\n    summarise(meanmpg = mean(mpg)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n# characters\ngroupchar &lt;- function(data, groupers) {\n  gm &lt;- data %&gt;%\n    group_by(across(all_of(groupers))) %&gt;%\n    summarise(meanmpg = mean(mpg)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nHow do we use those for a single grouping variable?\n\ngroupbrace(mtcars, groupers = gear)\n\n# A tibble: 3 × 2\n   gear meanmpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     3    16.1\n2     4    24.5\n3     5    21.4\n\ngroupchar(mtcars, groupers = 'gear')\n\n# A tibble: 3 × 2\n   gear meanmpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     3    16.1\n2     4    24.5\n3     5    21.4\n\n\nWhat happens when we try to group by more than one column?\n\n# groupbrace(mtcars, groupers = c(gear, carb))\n# \n# groupchar(mtcars, groupers = c('gear', 'carb'))\n\nworks with the characters, but the embracing fails (unsurprisingly).\nThe website says to use …, so we can do that as follows:\n\ngroupdots &lt;- function(data, ...) {\n  gm &lt;- data %&gt;%\n    group_by(...) %&gt;%\n    summarise(meanmpg = mean(mpg)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\ngroupdots(mtcars, gear, carb)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb meanmpg\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1     3     1    20.3\n 2     3     2    17.2\n 3     3     3    16.3\n 4     3     4    12.6\n 5     4     1    29.1\n 6     4     2    24.8\n 7     4     4    19.8\n 8     5     2    28.2\n 9     5     4    15.8\n10     5     6    19.7\n11     5     8    15  \n\n\nThat works, but it becomes an issue if we’re ALSO supplying arguments for other things in the function. See below.\nThe website only uses the dots example, but across() works like it does with summarize. This I think ends up being the answer for bare variable names that don’t get mixed up between grouping and summarizing. See below.\n\ngroupacross &lt;- function(data, groupers) {\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(meanmpg = mean(mpg)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\ngroupacross(mtcars, c(gear, carb))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb meanmpg\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1     3     1    20.3\n 2     3     2    17.2\n 3     3     3    16.3\n 4     3     4    12.6\n 5     4     1    29.1\n 6     4     2    24.8\n 7     4     4    19.8\n 8     5     2    28.2\n 9     5     4    15.8\n10     5     6    19.7\n11     5     8    15"
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#passing-to-summarisemutate",
    "href": "tidyprogramming/tidy_programs.html#passing-to-summarisemutate",
    "title": "Tidy programming",
    "section": "Passing to summarise/mutate",
    "text": "Passing to summarise/mutate\nI’m going to set this up with a simple group_by in all cases because it sets up the combo, and I almost never actually call summarise on a full dataset anyway.\n\nColumns to operate on\nIf we just want one column, but the user supplies its name, we can again embrace or quote.\nNames is an issue here too. They can just be left as a fixed value, but if we want to have the name of the new column reflect what’s being passed in, we handle that in different ways. With the braces we use the glue :=, and the .names argument if characters.\nNow, the dots don’t seem to work to pass multiple bare names, I think probably because of issues with names? But we can modify the simple embraced version to use across(), making it more similar to the character version.\n\n# embracing\nsumbrace &lt;- function(data, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(gear) %&gt;%\n    summarise(\"mean_{{sumcols}}\" := mean({{sumcols}})) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n# characters\nsumchar &lt;- function(data, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(gear) %&gt;%\n    summarise(across(all_of(sumcols), mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n# mulitple bare\nsumbaremulti &lt;- function(data, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(gear) %&gt;%\n    summarise(across({{sumcols}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nWith a single user-supplied column\n\nsumbrace(mtcars, sumcols = mpg)\n\n# A tibble: 3 × 2\n   gear mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     3     16.1\n2     4     24.5\n3     5     21.4\n\nsumchar(mtcars, sumcols = 'mpg')\n\n# A tibble: 3 × 2\n   gear mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     3     16.1\n2     4     24.5\n3     5     21.4\n\n\nMultiple user-supplied cols\n\nsumbaremulti(mtcars, sumcols = c(mpg, hp))\n\n# A tibble: 3 × 3\n   gear mean_mpg mean_hp\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     3     16.1   176. \n2     4     24.5    89.5\n3     5     21.4   196. \n\nsumchar(mtcars, sumcols = c('mpg', 'hp'))\n\n# A tibble: 3 × 3\n   gear mean_mpg mean_hp\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     3     16.1   176. \n2     4     24.5    89.5\n3     5     21.4   196. \n\n\n\n\nCombine with group_by\nI often want to pass a set of variable names to group_by and a set of names to summarize. If we use the dots method, these would get all jumbled together. So the options are embracing or characters, and when embracing we still need the c(bare1, bare2, …, bareN) so each component is a single argument.\n\n# characters\ngsumchar &lt;- function(data, groupers, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(across(all_of(groupers))) %&gt;%\n    summarise(across(all_of(sumcols), mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n# mulitple bare\ngsumbaremulti &lt;- function(data, groupers, sumcols) {\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nNow we can feed it multiple grouping columns and multiple summary columns\n\ngsumbaremulti(mtcars, groupers = c(gear, carb), sumcols = c(mpg, hp))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_mpg mean_hp\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     3     1     20.3   104  \n 2     3     2     17.2   162. \n 3     3     3     16.3   180  \n 4     3     4     12.6   228  \n 5     4     1     29.1    72.5\n 6     4     2     24.8    79.5\n 7     4     4     19.8   116. \n 8     5     2     28.2   102  \n 9     5     4     15.8   264  \n10     5     6     19.7   175  \n11     5     8     15     335  \n\ngsumchar(mtcars, groupers = c('gear', 'carb'), sumcols = c('mpg', 'hp'))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_mpg mean_hp\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     3     1     20.3   104  \n 2     3     2     17.2   162. \n 3     3     3     16.3   180  \n 4     3     4     12.6   228  \n 5     4     1     29.1    72.5\n 6     4     2     24.8    79.5\n 7     4     4     19.8   116. \n 8     5     2     28.2   102  \n 9     5     4     15.8   264  \n10     5     6     19.7   175  \n11     5     8     15     335  \n\n\nIt’s really not clear why I’d ever use the dots version, or why we wouldn’t always use the across() wrap to give us generality. I guess if that generality isn’t needed? But while dots can be handy, they’re vague and it’s not like the across() wrap is hard to type.\nWhat this makes very clear is the similarity between the two methods- they’re really just using the select() syntax in the across(), but one has to embrace bare names and the other uses the all_of() modifier we always have to include when we want to select() with a character vector.\n\n\nPassing select syntax\nSince we’re using that across, is it possible to pass other select() syntax than variable names? e.g. is.numeric, starts_with() or b:f? Let’s test it just with the summarize bit.\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = is.numeric)\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(is.numeric, mean, .names = \"mean_{.col}\")`.\nCaused by warning:\n! Use of bare predicate functions was deprecated in tidyselect 1.1.0.\nℹ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %&gt;% select(is.numeric)\n\n  # Now:\n  data %&gt;% select(where(is.numeric))\n\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 11\n    gear  carb mean_mpg mean_cyl mean_disp mean_hp mean_drat mean_wt mean_qsec\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     20.3     5.33     201.    104        3.18    3.05      19.9\n 2     3     2     17.2     8        346.    162.       3.04    3.56      17.1\n 3     3     3     16.3     8        276.    180        3.07    3.86      17.7\n 4     3     4     12.6     8        416.    228        3.22    4.69      16.9\n 5     4     1     29.1     4         84.2    72.5      4.06    2.07      19.2\n 6     4     2     24.8     4        121.     79.5      4.16    2.68      20.0\n 7     4     4     19.8     6        164.    116.       3.91    3.09      17.7\n 8     5     2     28.2     4        108.    102        4.1     1.83      16.8\n 9     5     4     15.8     8        351     264        4.22    3.17      14.5\n10     5     6     19.7     6        145     175        3.62    2.77      15.5\n11     5     8     15       8        301     335        3.54    3.57      14.6\n# ℹ 2 more variables: mean_vs &lt;dbl&gt;, mean_am &lt;dbl&gt;\n\n\nThat works but is angry about missing where(). Just throwing the bare select syntax straight in works though, for the where() type arguments but seems to be general- works for col:col and starts_with() as well.\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = where(is.numeric))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 11\n    gear  carb mean_mpg mean_cyl mean_disp mean_hp mean_drat mean_wt mean_qsec\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     20.3     5.33     201.    104        3.18    3.05      19.9\n 2     3     2     17.2     8        346.    162.       3.04    3.56      17.1\n 3     3     3     16.3     8        276.    180        3.07    3.86      17.7\n 4     3     4     12.6     8        416.    228        3.22    4.69      16.9\n 5     4     1     29.1     4         84.2    72.5      4.06    2.07      19.2\n 6     4     2     24.8     4        121.     79.5      4.16    2.68      20.0\n 7     4     4     19.8     6        164.    116.       3.91    3.09      17.7\n 8     5     2     28.2     4        108.    102        4.1     1.83      16.8\n 9     5     4     15.8     8        351     264        4.22    3.17      14.5\n10     5     6     19.7     6        145     175        3.62    2.77      15.5\n11     5     8     15       8        301     335        3.54    3.57      14.6\n# ℹ 2 more variables: mean_vs &lt;dbl&gt;, mean_am &lt;dbl&gt;\n\n\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = mpg:disp)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 5\n    gear  carb mean_mpg mean_cyl mean_disp\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     20.3     5.33     201. \n 2     3     2     17.2     8        346. \n 3     3     3     16.3     8        276. \n 4     3     4     12.6     8        416. \n 5     4     1     29.1     4         84.2\n 6     4     2     24.8     4        121. \n 7     4     4     19.8     6        164. \n 8     5     2     28.2     4        108. \n 9     5     4     15.8     8        351  \n10     5     6     19.7     6        145  \n11     5     8     15       8        301  \n\n\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = starts_with('d'))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_disp mean_drat\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     201.       3.18\n 2     3     2     346.       3.04\n 3     3     3     276.       3.07\n 4     3     4     416.       3.22\n 5     4     1      84.2      4.06\n 6     4     2     121.       4.16\n 7     4     4     164.       3.91\n 8     5     2     108.       4.1 \n 9     5     4     351        4.22\n10     5     6     145        3.62\n11     5     8     301        3.54\n\n\n\n\nSelect syntax issues\nSometimes we might want to pass a vector of columns to select, but have those that don’t exist get ignored- basically, select however many of this set of columns exist in the dataset. With a character vector, that’s straightforward with any_of. But it fails with bare names, and any_of requires characters.\n\n# These both fail\nmtcars %&gt;% select(c(mpg, fakecolumn))\n\nError in `select()`:\n! Can't subset columns that don't exist.\n✖ Column `fakecolumn` doesn't exist.\n\nmtcars %&gt;% select(any_of(mpg, fakecolumn))\n\nError in `select()`:\n! Problem while evaluating `any_of(mpg, fakecolumn)`.\nCaused by error in `any_of()`:\n! `...` must be empty.\nℹ Did you forget `c()`?\nℹ The expected syntax is `any_of(c(\"a\", \"b\"))`, not `any_of(\"a\", \"b\")`\n\n\nAn obvious solution is to use character vectors.\n\nmtcars %&gt;% select(any_of(c('mpg', 'fakecolumn')))\n\n                     mpg\nMazda RX4           21.0\nMazda RX4 Wag       21.0\nDatsun 710          22.8\nHornet 4 Drive      21.4\nHornet Sportabout   18.7\nValiant             18.1\nDuster 360          14.3\nMerc 240D           24.4\nMerc 230            22.8\nMerc 280            19.2\nMerc 280C           17.8\nMerc 450SE          16.4\nMerc 450SL          17.3\nMerc 450SLC         15.2\nCadillac Fleetwood  10.4\nLincoln Continental 10.4\nChrysler Imperial   14.7\nFiat 128            32.4\nHonda Civic         30.4\nToyota Corolla      33.9\nToyota Corona       21.5\nDodge Challenger    15.5\nAMC Javelin         15.2\nCamaro Z28          13.3\nPontiac Firebird    19.2\nFiat X1-9           27.3\nPorsche 914-2       26.0\nLotus Europa        30.4\nFord Pantera L      15.8\nFerrari Dino        19.7\nMaserati Bora       15.0\nVolvo 142E          21.4\n\n\nBut does that then preclude using other tidyselect syntax such as :, starts_with, etc? Sure, we can swap back and forth if we’re accessing select directly, but not if this is embedded in a function. The answer is sometimes- it works with starts_with but not : (not really shown here because it fails).\n\nmtcars %&gt;% select(any_of(starts_with('d')))\n\n                     disp drat\nMazda RX4           160.0 3.90\nMazda RX4 Wag       160.0 3.90\nDatsun 710          108.0 3.85\nHornet 4 Drive      258.0 3.08\nHornet Sportabout   360.0 3.15\nValiant             225.0 2.76\nDuster 360          360.0 3.21\nMerc 240D           146.7 3.69\nMerc 230            140.8 3.92\nMerc 280            167.6 3.92\nMerc 280C           167.6 3.92\nMerc 450SE          275.8 3.07\nMerc 450SL          275.8 3.07\nMerc 450SLC         275.8 3.07\nCadillac Fleetwood  472.0 2.93\nLincoln Continental 460.0 3.00\nChrysler Imperial   440.0 3.23\nFiat 128             78.7 4.08\nHonda Civic          75.7 4.93\nToyota Corolla       71.1 4.22\nToyota Corona       120.1 3.70\nDodge Challenger    318.0 2.76\nAMC Javelin         304.0 3.15\nCamaro Z28          350.0 3.73\nPontiac Firebird    400.0 3.08\nFiat X1-9            79.0 4.08\nPorsche 914-2       120.3 4.43\nLotus Europa         95.1 3.77\nFord Pantera L      351.0 4.22\nFerrari Dino        145.0 3.62\nMaserati Bora       301.0 3.54\nVolvo 142E          121.0 4.11\n\n# mtcars %&gt;% select(any_of(hp:wt))\n\nIs the trick to pass it the whole any_of expression? that IS a tidyselect call. Try it in the function directly, to get all the across in there correctly. First, this fails if we just pass extra columns:\n\n# gsumbaremulti(mtcars, \n#               groupers = c(gear, carb), \n#               sumcols = c(mpg, fakecol))\n\nIf we know some might not exist, we can instead pass the whole any_of and character names. Is this cleaner? No, now we’re back to characters, but ALSO needing to pass the any_of. So why do it? if we sometimes also need to pass other tidyselect syntax.\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = any_of(c('mpg', 'fakecol')))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\nNow, what if that is in turn buried in a function, so we need to set the argument outside the call? This might happen if we have a user interface where they choose columns. For example, they might set the cols, and then call a function that calls what we have above.\n\nwhichcols &lt;- c('mpg', 'fakecol')\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = any_of(whichcols))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\nThat’s easy enough. But what if whichcols could be tidyselect syntax? That can’t be saved to an object. It can be saved with expr, but then that has to be unpacked with !!.\n\n# Fails\n# whichcols &lt;- starts_with('m')\nwhichcols &lt;- expr(starts_with('m'))\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = !!whichcols)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\nThat allows passing tidyselect, but does it break the any_of situation? Not if we wrap it in expr.\n\nwhichcols &lt;- expr(any_of(c('mpg', 'fakecol')))\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = !!whichcols)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\nThat means that if we might have a character vector and might have tidyselect, we can have a multi-step process to create the expression and pass it to the function. Ie the user can set whichcols directly as an expr-wrapped tidyselect, OR if a character vector it makes it itself. See the next two code blocks.\n\ncolstosum &lt;- c('mpg', 'fakecol')\n# colstosum &lt;- expr(starts_with('d'))\n\nif (is.character(colstosum)) {\n  whichcols &lt;- expr(any_of(colstosum))\n} else {\n  whichcols &lt;- colstosum\n}\n\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = !!whichcols)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 3\n    gear  carb mean_mpg\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     1     20.3\n 2     3     2     17.2\n 3     3     3     16.3\n 4     3     4     12.6\n 5     4     1     29.1\n 6     4     2     24.8\n 7     4     4     19.8\n 8     5     2     28.2\n 9     5     4     15.8\n10     5     6     19.7\n11     5     8     15  \n\n\n\ncolstosum &lt;- expr(starts_with('d'))\n\nif (is.character(colstosum)) {\n  whichcols &lt;- expr(any_of(colstosum))\n} else {\n  whichcols &lt;- colstosum\n}\n\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = !!whichcols)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_disp mean_drat\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     201.       3.18\n 2     3     2     346.       3.04\n 3     3     3     276.       3.07\n 4     3     4     416.       3.22\n 5     4     1      84.2      4.06\n 6     4     2     121.       4.16\n 7     4     4     164.       3.91\n 8     5     2     108.       4.1 \n 9     5     4     351        4.22\n10     5     6     145        3.62\n11     5     8     301        3.54\n\n\nBecause that’s ugly, I’m not going to spend more time on it, but it is a workaround for sometimes needing to pass tidyselect syntax and sometimes column names that might not exist. There’s likely a more general way to do this using tidyselect::eval_select, but what I have here will work for now.\n\n\ntidyselect::eval_select\nI’m now running into issues where the approach above isn’t working well, because sometimes the expression ends up including the name of an object (e.g. a passed-in character vector), and by the time we get to the {{}}, we’re too far into the call stack and it ends up failing because it essentially tries to do something like group_by(starts_with(NAME_OF_VECTOR)) instead of group_by(starts_with(VALUES_IN_VECTOR).\nSo, one way to handle this is to in the outer layer use tidyselect::eval_select in the outer layer to get column names and indices. Then we can just pass those around rather than all the promises that get lost doing it other ways. It’s a bit cruder, but i think will involve less gymnastics.\n\nHow does eval_select work?\nFirst, how does eval_select work? What do we need to feed it?\nA bare tidyselect function fails\n\ncolstosum &lt;- starts_with('d')\ntidyselect::eval_select(colstosum, mtcars)\n\nWorks if wrapped in expr\n\ncolstosum &lt;- expr(starts_with('d'))\n\ntidyselect::eval_select(colstosum, mtcars)\n\ndisp drat \n   3    5 \n\n\nWorks with character vectors.\n\ncolstosum &lt;- c('disp', 'mpg')\ntidyselect::eval_select(colstosum, mtcars)\n\ndisp  mpg \n   3    1 \n\n\nDoes not work if there are values in the character vector that aren’t in the data.\n\ncolstosum &lt;- c('disp', 'mpg', 'notinmtcars')\ntidyselect::eval_select(colstosum, mtcars)\n\nSo we likely still need the conditional to use any_of\n\ncolstosum &lt;- c('disp', 'mpg', 'notinmtcars')\n\nif (is.character(colstosum)) {\n  whichcols &lt;- expr(any_of(colstosum))\n} else {\n  whichcols &lt;- colstosum\n}\n\ntidyselect::eval_select(whichcols, mtcars)\n\ndisp  mpg \n   3    1 \n\n\nAnd, what if we pass an argument to a tidyselect? I don’t think this is enough to break the original way without some intervening function calls, but it’s the same idea that’s breaking it as we move down a stack.\n\nstartletter &lt;- 'd'\n\ncolstosum &lt;- expr(starts_with(startletter))\ntidyselect::eval_select(colstosum, mtcars)\n\ndisp drat \n   3    5 \n\n\nWhat actually is that returning? A named vector of indices.\n\ntsout &lt;- tidyselect::eval_select(colstosum, mtcars)\nstr(tsout)\n\n Named int [1:2] 3 5\n - attr(*, \"names\")= chr [1:2] \"disp\" \"drat\"\n\n\nSo, with the conditional in there to guard against grabbing things that don’t exist, that looks like it should work by basically transporting around our selects as character vectors or indices if we evaluate them early enough. In the sort of uses I’m imagining- evaluating this early, and then passing in to further functions- I’d be really nervous about using indices, and so would tend to use the names. How might that work?\n\ngsumbaremulti(mtcars, \n              groupers = c(gear, carb), \n              sumcols = names(tsout))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 11 × 4\n    gear  carb mean_disp mean_drat\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1     3     1     201.       3.18\n 2     3     2     346.       3.04\n 3     3     3     276.       3.07\n 4     3     4     416.       3.22\n 5     4     1      84.2      4.06\n 6     4     2     121.       4.16\n 7     4     4     164.       3.91\n 8     5     2     108.       4.1 \n 9     5     4     351        4.22\n10     5     6     145        3.62\n11     5     8     301        3.54\n\n\n\n\nA function to parse eval_select\nWhat if I actually make the function do the parsing? So I can pass it the characters, bare names, or expr(selectsyntax)?\n\ngsumtidy &lt;- function(data, groupers, sumcols) {\n  \n  if (is.character(groupers)) {\n    whichg &lt;- expr(any_of(groupers))\n  } else {\n    whichg &lt;- groupers\n  }\n  \n  if (is.character(sumcols)) {\n    whichs &lt;- expr(any_of(sumcols)) \n  } else {\n    whichs &lt;- sumcols\n  }\n  \n  gnames &lt;- whichg %&gt;% \n    tidyselect::eval_select(data) %&gt;% \n    names()\n  snames &lt;- whichs %&gt;% \n    tidyselect::eval_select(data) %&gt;% \n    names()\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{gnames}})) %&gt;%\n    summarise(across({{snames}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n  \n}\n\nTest that with different sorts of things.\n\ngsumtidy(mtcars, \n         groupers = 'cyl', \n         sumcols = expr(starts_with('d')))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nHow about if we include extra cols? works fine.\n\ngsumtidy(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = expr(starts_with('d')))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nThe whole top part of that could be its own function, and get run at any point in a call stack. Returning the names and not the indices, but could return the whole thing I guess, depending on safety of indices.\n\nselectnames &lt;- function(data, selector) {\n  \n  if (is.character(selector)) {\n    whichg &lt;- expr(any_of(selector))\n  } else {\n    whichg &lt;- selector\n  }\n  \n  selnames &lt;- whichg %&gt;% \n    tidyselect::eval_select(data) %&gt;% \n    names()\n  \n  return(selnames)\n}\n\n\ngtidysimple &lt;- function(data, groupers, sumcols) {\n  \n  gnames &lt;- selectnames(data, groupers)\n  snames &lt;- selectnames(data, sumcols)\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{gnames}})) %&gt;%\n    summarise(across({{snames}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n  \n}\n\n\ngtidysimple(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = expr(starts_with('d')))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\n\n\nexpr() vs enquo()\nThe above needs to wrap tidyselect syntax with expr to work- passing the bare starts_with fails\n\ngtidysimple(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = starts_with('d'))\n\nLikewise with bare names\n\ngtidysimple(mtcars, \n         groupers = c(cyl, notinmtcars), \n         sumcols = expr(starts_with('d')))\n\nThat’s because things other than character vectors need to be “defused” (see ?enquo). expr defuses ‘your own local expressions’, while enquo defuses function arguments. So, there are two options- defuse locally when giving the argument to the funciton with expr (as I’ve done above), or defuse internally with enquo.\nIn that case, we re-write the outer function to enquo its arguments.\n\ngtidyquo &lt;- function(data, groupers, sumcols) {\n  \n  gnames &lt;- selectnames(data, enquo(groupers))\n  snames &lt;- selectnames(data, enquo(sumcols))\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{gnames}})) %&gt;%\n    summarise(across({{snames}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n  \n}\n\nNow, that should work without wrapping tidyselect syntax in expr(), and take bare names or character vectors.\n\ngtidyquo(mtcars, \n         groupers = cyl, \n         sumcols = starts_with('d'))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nIt also takes characters\n\ngtidyquo(mtcars, \n         groupers = 'cyl', \n         sumcols = c('disp', 'drat'))\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nBut it’s no longer ignoring values not in the data\n\ngtidyquo(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = expr(starts_with('d')))\n\nThat’s because the internal enquo(groupers) in gtidyquo means that selectnames is always seeing selector as language, not character, and so bypassing the any_of() conditional. I don’t want to drop that whole conditional section from selectnames, because that keeps selectnames more general (doesn’t have to be fed enquo’d arguments). Instead, we can use the strict argument in eval_select to decide whether to fail or silently ignore missings. This choice is probably good to have, rather than enforce one or the other- it’s often the case that we should fail if missing columns are called, rather than just ignore silently. The same argument can also be used in the conditional as a switch to make the situation with character selector fail or pass.\n\nselectnames &lt;- function(data, selector, failmissing = TRUE) {\n  \n  if (is.character(selector)) {\n    if (failmissing) {\n      whichg &lt;- expr(all_of(selector))\n    } else {\n      whichg &lt;- expr(any_of(selector))\n    }\n    \n  } else {\n    whichg &lt;- selector\n  }\n  \n  selnames &lt;- whichg %&gt;% \n    tidyselect::eval_select(data, strict = failmissing) %&gt;% \n    names()\n  \n  return(selnames)\n}\n\nWe also need to rewrite gtidyquo to pass failmissing. Could use …, but that’s vague.\n\ngtidyquo &lt;- function(data, groupers, sumcols, failmissing = TRUE) {\n  \n  gnames &lt;- selectnames(data, enquo(groupers), failmissing)\n  snames &lt;- selectnames(data, enquo(sumcols), failmissing)\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{gnames}})) %&gt;%\n    summarise(across({{snames}}, mean, .names = 'mean_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n  \n}\n\nNow, does that work with values not in the data?\n\ngtidyquo(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = starts_with('d'),\n         failmissing = FALSE)\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nas bare names\n\ngtidyquo(mtcars, \n         groupers = c(cyl, notinmtcars), \n         sumcols = starts_with('d'),\n         failmissing = FALSE)\n\n# A tibble: 3 × 3\n    cyl mean_disp mean_drat\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     4      105.      4.07\n2     6      183.      3.59\n3     8      353.      3.23\n\n\nand it should fail if failmissing = TRUE (or left off, since that’s the default).\n\ngtidyquo(mtcars, \n         groupers = c('cyl', 'notinmtcars'), \n         sumcols = starts_with('d'))\n\n\n\nConclusions\nThat seems a bit lame to just translate to characters, but it ends up being a very robust and flexible workaround for situations where passing an object into a tidyselect ends up trying to select the object instead of its contents once we’re further down a call stack, and lets us use characters, bare names, and tidyselect and choose whether or not to fail when columns don’t exist."
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#functions-to-use",
    "href": "tidyprogramming/tidy_programs.html#functions-to-use",
    "title": "Tidy programming",
    "section": "Functions to use",
    "text": "Functions to use\nSometimes we want to tell the function how to summarise the data. Sometimes we want to do this including arguments, e.g. mean with na.rm = TRUE. Sometimes we want to pass multiple functions and have the names appended, and sometimes those functions are user-defined. Further, sometimes they have an argument internal to the data (such as a weighting column) that they need to access.\nWe’ll start simple, though I’ll keep the multi-group and multi-col syntax from above because it keeps things general, and allows testing with multiple summarise cols. I’ll use the bare names and embracing for the grouping and summarise variables, but that shouldn’t affect the way function-passing works if we used the character version instead.\n\nPassing a function by name\nIt’s typically a good idea to name the resulting column with the function when we don’t know what the function will be. And that sets us up for multi-functions.\nIn the simplest case we can just use a FUN argument. While using the all-caps “FUN” as the argument name seems to be a convention, this isn’t a special argument name and it could be whatever we want.\nPreviously, we had defined the function to apply inside our function, and so we had hardcoded the naming, e.g. 'mean_{.col}. But now, we won’t know what it is. We thus need to get the name of the function as well, using as.character(substitute).\n\nfunpass &lt;- function(data, groupers, sumcols,\n                    FUN) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN, .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\nfunpass(mtcars,\n        groupers = gear,\n        sumcols = mpg,\n        FUN = mean)\n\n# A tibble: 3 × 2\n   gear mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     3     16.1\n2     4     24.5\n3     5     21.4\n\n\nWe run into problems as soon as we try to pass arguments to that function, for example when there are NA and we want to use na.rm\n\nnacars &lt;- mtcars %&gt;%\n  mutate(randnum = rnorm(n()),\n         nampg = ifelse(randnum &gt;= 0, mpg, NA))\n\n\n#| error:false\n\n# funpass(nacars,\n#         groupers = gear,\n#         sumcols = nampg,\n#         FUN = mean, na.rm = TRUE)\n\nUsing dots syntax works to allow arguments.\n\nfunpasst &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN,..., .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\nfunpasst(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(nampg, FUN, ..., .names = funcolname)`.\nℹ In group 1: `gear = 3`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 3 × 2\n   gear mean_nampg\n  &lt;dbl&gt;      &lt;dbl&gt;\n1     3       15.7\n2     4       23.6\n3     5       15.8\n\n\nAs usual, dots can be an issue if we’re doing several things. But we’ll get to that. One solution that is also relevant generally is to specify a custom function. In a simple case this could be mean with na.rm = TRUE, but it could be anything.\n\n\nCustom function\nMaybe we want a custom function. That might be as simple as changing the na.rm default, or it might be something complicated with a few arguments. Here, I’ll demo a version with a swapped na.rm default, illustrating a way to avoid passing arguments, and a more complex function that lags values and multiplies them.\n\nmeanna &lt;- function(x) {\n  mean(x, na.rm = TRUE)\n}\n\ncustomfun &lt;- function(x, lag_k = 1, na.rm = TRUE, multiplier) {\n  xl &lt;- lag(x, lag_k)\n  xs &lt;- sum(xl, na.rm = na.rm)*multiplier\n  return(xs)\n}\n\n\nfunpasst(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = meanna)\n\n# A tibble: 3 × 2\n   gear meanna_nampg\n  &lt;dbl&gt;        &lt;dbl&gt;\n1     3         15.7\n2     4         23.6\n3     5         15.8\n\n\n\nfunpasst(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = customfun, lag_k = 0, multiplier = 10)\n\n# A tibble: 3 × 2\n   gear customfun_nampg\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     3            1415\n2     4            1179\n3     5             158\n\n\nand that works with multiple columns and groupers as well\n\nfunpasst(nacars,\n        groupers = c(gear, am),\n        sumcols = c(nampg, hp),\n        FUN = customfun, lag_k = 0, multiplier = 10)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n   gear    am customfun_nampg customfun_hp\n  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1     3     0            1415        26420\n2     4     0             178         4030\n3     4     1            1001         6710\n4     5     1             158         9780\n\n\n\n\nFunction with internal data argument\nSometimes we might want to use a function that relies on multiple columns- for example, the mean of one column using weights in another.\nIn the simplest case, we can hardcode that column. Here in a silly example of finding the mean hp weighted by wt. I’ve removed the dots for now, we’ll get to other arguments next.\n\nfuninternal &lt;- function(data, groupers, sumcols,\n                    FUN) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN, wt, .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\nfuninternal(nacars,\n        groupers = gear,\n        sumcols = mpg,\n        FUN = weighted.mean)\n\n# A tibble: 3 × 2\n   gear weighted.mean_mpg\n  &lt;dbl&gt;             &lt;dbl&gt;\n1     3              15.6\n2     4              23.6\n3     5              19.7\n\n\nand yes, that is weighting- if we just pass mean we get\n\nfunpasst(nacars,\n        groupers = gear,\n        sumcols = mpg,\n        FUN = mean)\n\n# A tibble: 3 × 2\n   gear mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     3     16.1\n2     4     24.5\n3     5     21.4\n\n\nBut what if we need to specify other arguments? We can use dots again.\n\nfuninternald &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN, wt, ..., .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\nfuninternald(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                15.0\n2     4                22.4\n3     5                15.8\n\n\nAnother way to do this that might be a bit clearer, especially as the number of arguments grows is to use tilde function specification. This is nearly the same, but makes it clear what arguments belong to the FUN.\n\nfuninternaldt &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~FUN(., wt, ...), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThat yields the same result, we’ve just specified the summary function differently.\n\nfuninternaldt(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                15.0\n2     4                22.4\n3     5                15.8\n\n\n\n\nPassing internal columns by name\nSo far, the internal columns have been hardcoded, and at a known position in the arguments to the FUN. What if we want to specify them on calling the function?\nCan we just use the dots? Not with a bare name.\n\n# funpasst(nacars,\n#         groupers = gear,\n#         sumcols = nampg,\n#         FUN = weighted.mean, wt, na.rm = TRUE)\n\nDoes it work to use the tilde version?\n\nfuntildedots &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~FUN(., ...), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nNo, that still can’t find the bare name- it looks for an object, not something internal to the data.\n\nfuntildedots(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, wt, na.rm = TRUE)\n\nIf we know that there will be a second data-variable argument to the function, we might be able to embrace.\n\nfuninteralembrace &lt;- function(data, groupers, sumcols,\n                    FUN, arg2, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUN, {{arg2}}, ..., .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nfuninteralembraceT &lt;- function(data, groupers, sumcols,\n                    FUN, arg2, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~FUN(., {{arg2}}, ...), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThat works for both the tilde and non-tilde versions.\n\nfuninteralembrace(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, \n        arg2 = wt, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                15.0\n2     4                22.4\n3     5                15.8\n\n\n\nfuninteralembraceT(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, \n        arg2 = wt, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                15.0\n2     4                22.4\n3     5                15.8\n\n\nBut what if we want a function that works with FUNS that may or may not require a second data-variable argument? Do the above functions work with something like mean that won’t have an arg2? No.\n\nfuninteralembrace(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\nfuninteralembraceT(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\nIs there a way to write a function that may have any number from 0 to n internal data arguments, as well as other non-data arguments (e.g. na.rm etc)? It will be tricky, because some unknown number of items will need to be embraced. Usual methods to unpack the ellipses using list(...) won’t work, I don’t think. And if they do, it’s still unclear how many of the items in the list should be embraced. Does it even work if we know how many need to be embraced? Test with a simple case of whether we can even do the list(…).\n\ntestdots &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  \n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  dots &lt;- list(...)\n  \n  print(dots)\n  \n  # gm &lt;- data %&gt;%\n  #   group_by(across({{groupers}})) %&gt;%\n  #   summarise(across({{sumcols}}, ~FUN(., {{dots[1]}}, ...), .names = funcolname)) %&gt;%\n  #   ungroup()\n  # return(gm)\n}\n\nEven that doesn’t work- including bare names in the dots and then embracing doesn’t work because list() needs them as objects.\n\ntestdots(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, wt, na.rm = TRUE)\n\nWhat is it I’m actually trying to do here? Write a function that takes an arbitrary number of data-variable arguments and an arbitrary number of passed env-arguments. That’s always going to be tricky, and will get trickier to sort things out like the order of the arguments. Is it possible? Almost certainly. But I think I’ll leave sorting it out for later. We have a version that works for a known number of arguments in a known order, which is enough in some situations. A workaround will become apparent anyway after the next section, where I pass in external vectors.\n\n\nFunction with vector argument passed in\nOne way to get around the issue above is instead of passing the name of a data variable, pass in the vector itself as an object. This also allows passing in vectors unattached to the dataframe being operated on, though since the’ll need to have the same nrows, in most cases they’ll be attached.\nHow does this work? We write the main function to do the grouping and summarizing, and within it define the function to evaluate in the summarize, accounting for the various types of arguments and the grouping. This works because the … are all env-variables (vectors and scalars) instead of bare names of data-variables. This is all based on funpasst above, with the addition of the internal function creation. Because the function we define may be grouped, it needs to be passed the indices for the current group rows so it only operates on those. I’m using tilde notation to keep it clearer how that function gets called in the summarise.\nWe could write the function that creates the function to evaluate inside the main function, or elsewhere. Writing it inside allows us to take some shortcuts because it can access objects in the outer function environment and avoid explicitly passing as many objects around. Though that can be dangerous.\nThe !!! unpacks a list of function arguments.\n\narbvecscal &lt;- function(data, groupers, sumcols,\n                    FUN, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  # Define the function to evaluate\n  thisfun &lt;- function(x, indices) {\n    elip &lt;- list(...)\n    \n    # deal with the case of no passed arguments\n    if (length(elip) == 0) {\n      return(rlang::exec(FUN, x))\n    } else {\n      \n      # clip vector ... arguments (e.g. weights) to just the group\n      for (i in 1:length(elip)) {\n        if (length(elip[[i]]) == nrow(data)) {\n          elip[[i]] &lt;- elip[[i]][indices]\n        }\n      }\n      \n      return(rlang::exec(FUN, x, !!!elip))\n    }\n  }\n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~thisfun(., cur_group_rows()), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nSo, for something like the weighted average with an na.rm argument, we specify the vector of weights, rather than their bare name in the dataframe.\n\narbvecscal(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, nacars$wt, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                15.0\n2     4                22.4\n3     5                15.8\n\n\nAnd that also works if we want a function without any data-variables\n\narbvecscal(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear mean_nampg\n  &lt;dbl&gt;      &lt;dbl&gt;\n1     3       15.7\n2     4       23.6\n3     5       15.8\n\n\nIf we don’t want to pass vectors but pass bare names, we might be able to do that with the same approach, but will need to specify which are which. Then we’d create the vectors internal to the function using the same select syntax as before.\nNow the internal function has to be a bit different (simpler) since it doesn’t have to do the checking for length since we’ve specified dataargs.\n\narbdatanames &lt;- function(data, groupers, sumcols,\n                         FUN, dataargs, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  # make a tibble so it doesn't collapse to vector if only one column\n  datavecs &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs}})\n  \n  # Define the function to evaluate\n  thisfun &lt;- function(x, indices) {\n    elip &lt;- list(...)\n    \n    # deal with the case of no passed arguments\n    if (length(elip) == 0 & nrow(datavecs) == 0) {\n      return(rlang::exec(FUN, x))\n    } else {\n      \n      # clip data arguments (e.g. weights) to just the group\n      thisdata &lt;- datavecs[indices, ]\n      \n      # make all the arguments a list so we can call it\n      allargs &lt;- c(as.list(thisdata), elip)\n      \n      return(rlang::exec(FUN, x, !!!allargs))\n    }\n  }\n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~thisfun(., cur_group_rows()), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nNow, that should work for the weighted mean as well. Note that now the data variables have to be part of the dataframe- this function does not accept vectors passed in from elsewhere.\nBUT, it doesn’t work because the names of the arguments need to be the names in the list of arguments following the !!!. And here, wt is the name, but weighted.mean wants w. So we not only need to specify the data-variable name, but the function-argument name for that variable as well. This is getting very in the weeds.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, dataargs = wt, na.rm = TRUE)\n\nFor example, arguments with the wrong names just get ignored. Names are essential, the execution does not just rely on order like if we called a function directly. Which makes sense for safety, but makes things harder here.\n\nvals = rnorm(10)\narglist &lt;- list(x = vals, wt = 1:10, na.rm = TRUE)\nrlang::exec(weighted.mean, !!!arglist)\n\n[1] -0.2123889\n\narglist2 &lt;- list(x = vals, w = 1:10, na.rm = TRUE)\nrlang::exec(weighted.mean, !!!arglist2)\n\n[1] -0.1297073\n\n\nIt would be nice to pass name-value pairs, but the bare names are going to trip us up, I think. Could do it with paired characters I guess, but we’ve just spent quite a lot of time trying to avoid that. Would work though. Kind of a pain to setup- would make most sense as two paired columns or vectors. And if we do that, it’d end up being roughly equivalent to just adding another argument to the function for the matched names.\nSkipping the rename if dataargnames aren’t specified allows ignoring it if the columns have the correct names, and helps it work more smoothly if there aren’t dataargs at all.\n\narbdatanames &lt;- function(data, groupers, sumcols,\n                         FUN, dataargs, dataargnames = NULL, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  # make a tibble so it doesn't collapse to vector if only one column\n  datavecs &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs}})\n  \n  if (!is.null(dataargnames)) {\n    names(datavecs) &lt;- dataargnames\n  }\n  \n  \n  # Define the function to evaluate\n  thisfun &lt;- function(x, indices) {\n    elip &lt;- list(...)\n    \n    # deal with the case of no passed arguments\n    if (length(elip) == 0 & nrow(datavecs) == 0) {\n      return(rlang::exec(FUN, x))\n    } else {\n      \n      # clip data arguments (e.g. weights) to just the group\n      thisdata &lt;- datavecs[indices, ]\n      \n      # make all the arguments a list so we can call it\n      allargs &lt;- c(as.list(thisdata), elip)\n      \n      return(rlang::exec(FUN, x, !!!allargs))\n    }\n  }\n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~thisfun(., cur_group_rows()), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nNow that works. The alternative would be to have a table of matched dataargs and dataargnames, and have that table be a single argument to arbdatanames, but we’d still have to created it and that’d involve more overhead.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, dataargs = wt, dataargnames = 'w', \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                15.0\n2     4                22.4\n3     5                15.8\n\n\nAnd it works in situations without data args.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = mean, na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear mean_nampg\n  &lt;dbl&gt;      &lt;dbl&gt;\n1     3       15.7\n2     4       23.6\n3     5       15.8\n\n\nThat should work for &gt;1 data variable as well. Let’s define a function that needs multiple data variables. This is very contrived with just some division and multiplication, but works as a check.\n\nmultidat &lt;- function(x, w, d, m, na.rm) {\n  preprep &lt;- x/d*m\n  outcome &lt;- weighted.mean(preprep, w, na.rm = na.rm)\n}\n\nThat works. Note that the dependence on argument names means we can specify out of order- we get two very different answers depending on whether we call cyl and hp d and m or m and d.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = multidat, dataargs = c(wt, cyl, hp), dataargnames = c('w', 'd', 'm'), \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear multidat_nampg\n  &lt;dbl&gt;          &lt;dbl&gt;\n1     3           373.\n2     4           435.\n3     5           521.\n\narbdatanames(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = multidat, dataargs = c(wt, cyl, hp), dataargnames = c('w', 'm', 'd'), \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear multidat_nampg\n  &lt;dbl&gt;          &lt;dbl&gt;\n1     3          0.615\n2     4          1.22 \n3     5          0.479\n\n\nIs there any reason to specify thisfun externally to the main function? I guess maybe? It forces us to specify arguments, and potentially makes things clearer.\n\n# Define the function to evaluate within the summary\nsumfun &lt;- function(x, indices, FUN, datavecs, ...) {\n  elip &lt;- list(...)\n  \n  # deal with the case of no passed arguments\n  if (length(elip) == 0 & nrow(datavecs) == 0) {\n    return(rlang::exec(FUN, x))\n  } else {\n    \n    # clip data arguments (e.g. weights) to just the group\n    thisdata &lt;- datavecs[indices, ]\n    \n    # make all the arguments a list so we can call it\n    allargs &lt;- c(as.list(thisdata), elip)\n    \n    return(rlang::exec(FUN, x, !!!allargs))\n  }\n}\n\n\nnewfun &lt;- function(data, groupers, sumcols,\n                         FUN, dataargs, dataargnames = NULL, ...) {\n  # function name as character\n  funname &lt;- as.character(substitute(FUN))\n  # This just avoids clutter in the summarise\n  funcolname &lt;- paste0(funname, '_{.col}')\n  \n  # make a tibble so it doesn't collapse to vector if only one column\n  datavecs &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs}})\n  \n  if (!is.null(dataargnames)) {\n    names(datavecs) &lt;- dataargnames\n  }\n  \n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, ~sumfun(., indices = cur_group_rows(), FUN = FUN, datavecs = datavecs, ...), .names = funcolname)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThat does work just as above. I’m not sure which will be cleaner in practice, but I like that this relies less on borrowing variables from the creating environment.\n\nnewfun(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = weighted.mean, dataargs = wt, dataargnames = 'w', \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear weighted.mean_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;\n1     3                15.0\n2     4                22.4\n3     5                15.8\n\nnewfun(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN = multidat, dataargs = c(wt, cyl, hp), dataargnames = c('w', 'm', 'd'), \n        na.rm = TRUE)\n\n# A tibble: 3 × 2\n   gear multidat_nampg\n  &lt;dbl&gt;          &lt;dbl&gt;\n1     3          0.615\n2     4          1.22 \n3     5          0.479"
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#multiple-functions--with-appropriate-named-outputs",
    "href": "tidyprogramming/tidy_programs.html#multiple-functions--with-appropriate-named-outputs",
    "title": "Tidy programming",
    "section": "Multiple functions- with appropriate named outputs",
    "text": "Multiple functions- with appropriate named outputs\n\nSimple - hardcoded number of functions\nSometimes we might want to calculate multiple summary or mutate functions for the same set of data, and so rather than repeating the above functions multiple times with different FUN arguments, it would be good to be able to send them all at once for one run-through. The simplest way to do this is to have a known number of functions and write that number of summaries, e.g.\n\nsimplemultifun &lt;- function(data, groupers, sumcols,\n                         FUN1, dataargs1, dataargnames1 = NULL,\n                         FUN2, dataargs2, dataargnames2 = NULL, ...) {\n  # function name as character\n  funname1 &lt;- as.character(substitute(FUN1))\n  # This just avoids clutter in the summarise\n  funcolname1 &lt;- paste0(funname1, '_{.col}')\n  \n    # function name as character\n  funname2 &lt;- as.character(substitute(FUN2))\n  # This just avoids clutter in the summarise\n  funcolname2 &lt;- paste0(funname2, '_{.col}')\n  \n  # make a tibble so it doesn't collapse to vector if only one column\n  datavecs1 &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs1}})\n  \n  if (!is.null(dataargnames1)) {\n    names(datavecs1) &lt;- dataargnames1\n  }\n  \n    # make a tibble so it doesn't collapse to vector if only one column\n  datavecs2 &lt;- data %&gt;%\n    as_tibble() %&gt;%\n    select({{dataargs2}})\n  \n  if (!is.null(dataargnames2)) {\n    names(datavecs2) &lt;- dataargnames2\n  }\n  \n  \n  # The main group and summarise\n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}},\n                     ~sumfun(., indices = cur_group_rows(), \n                             FUN = FUN1, datavecs = datavecs1, ...),\n                     .names = funcolname1),\n              across({{sumcols}},\n                     ~sumfun(., indices = cur_group_rows(),\n                             FUN = FUN2, datavecs = datavecs2, ...),\n                     .names = funcolname2)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThen as an example, let’s do a weighted mean but unweighted sd. note that they need to share the dots.\n\nsimplemultifun(nacars,\n        groupers = gear,\n        sumcols = nampg,\n        FUN1 = weighted.mean, dataargs1 = wt, dataargnames1 = 'w',\n        FUN2 = sd,\n        na.rm = TRUE)\n\n# A tibble: 3 × 3\n   gear weighted.mean_nampg sd_nampg\n  &lt;dbl&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1     3                15.0     3.90\n2     4                22.4     5.13\n3     5                15.8    NA   \n\n\nThat works, but is really hardcoded in terms of what we can do. It has to have two functions. So, let’s try to say we can pass an arbitrary set of functions from 1 to n.\nNote that a different data structure out the end is likely to be warranted, especially if we calculate these functions on multiple variables.- making this long with a column for the variable name and then the values of the functions might be the way to go if we do this for multiple variables.\n\n\nVariable number of functions\nWhat we really want here is to be able to pass in an arbitrary number of functions. That will get complicated if they have things like different data-variable arguments. In the simplest case, we can make the FUNS a list, and summarise just handles it. However, this breaks the names and the dots for arguments- the list needs to have all the info in it.\n\nfunmulti &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUNS)) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nIf the list is named (using lst here, but list(mean = mean, sd = sd) would work too), those names get appended.\n\nfunmulti(nacars,\n        groupers = gear,\n        sumcols = mpg,\n        FUNS = lst(mean, sd))\n\n# A tibble: 3 × 3\n   gear mpg_mean mpg_sd\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1     3     16.1   3.37\n2     4     24.5   5.28\n3     5     21.4   6.66\n\n\nThat approach should work for arbitrary arguments if I use the tilde notation, and even allows data variables. This is a bit messier in the function call than I’d like, and there’s a bit less control over the names, but I think neither of those are major issues. Would be hard to be less verbose, really, and still have argument specification make any sense across multiple functions.\nActually, can I control the names with .names after all?\n\nfunmulti &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n# nameparser &lt;- paste0('prefix_{.fn}_{.col}')\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, FUNS, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nAs of {dplyr} 1.1, this no longer works- it looks for wt as an object, not a data-variable. We’ll need to find a new solution.\n\nfunmulti(nacars, \n         groupers = gear, \n         sumcols = nampg,\n         FUNS = list(mean = ~mean(., na.rm = TRUE), \n                     sd = ~sd(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE)))\n\nError in `summarise()`:\nℹ In argument: `across(nampg, FUNS, .names = \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_nampg`.\nCaused by error:\n! object 'wt' not found\n\n\nNote that now the function args in the list work with data-variables and scalars, not with vectors passed in. This is not solely because of the grouping needing to be handled as we did above with the sumfun cutting to the correct indices, because even if we don’t group, we get errors about promise evals. This is because the FUNS list is being evaluated inside the summarise, and so thinks everything is a data-variable. There is probably a way to sort that out by using .env[['variablename']] in the specification, but that’ll just get more complex than just adding the column to the dataframe if we hit this situation. Especially since we’d have to pass the vector in so it’s available inside the funmulti environment, not just the global environment.\n\nouterweights &lt;- 1:nrow(nacars)\n\nfunmulti(nacars,\n         # groupers = gear,\n         sumcols = nampg,\n         FUNS = list(mean = ~mean(., na.rm = TRUE),\n                     sd = ~sd(., na.rm = TRUE),\n                     wm = ~weighted.mean(., w = outerweights, \n                                         na.rm = TRUE)))\n\n# A tibble: 1 × 3\n  prefix_mean_nampg prefix_sd_nampg prefix_wm_nampg\n              &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1              18.3            5.56            18.9\n\n\nCould we do something fancy with a list of FUNS and lists of arglists, parallelling how we did things above? Probably. I think in most instances though, this approach will work. I’ll develop that more complex situation only if needed."
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#bringing-it-all-together",
    "href": "tidyprogramming/tidy_programs.html#bringing-it-all-together",
    "title": "Tidy programming",
    "section": "Bringing it all together",
    "text": "Bringing it all together\nNow, let’s choose a couple grouping columns, a selection of cols to summarise, and multiple summary functions, some with data arguments and some that are custom.\nNone of this works as of dplyr 1.1. The issue is that with new behaviour in dplyr, it is looking for the additional arguments not in the column names but as objects. See section below.\n\ncomplexSummary &lt;- funmulti(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = list(mean = ~mean(., na.rm = TRUE), \n                     sd = ~sd(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE),\n                     custom = ~multidat(., w = wt, d = cyl, m = hp,\n                                        na.rm = FALSE)))\n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), FUNS, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3`, `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error:\n! object 'wt' not found\n\ncomplexSummary\n\nError in eval(expr, envir, enclos): object 'complexSummary' not found\n\n\nThis is complex enough it’s probably useful to pivot_longer\n\nlongsums &lt;- complexSummary %&gt;% \n  pivot_longer(cols = -c(gear, carb), \n               names_to = c('variable', 'summary_statistic'),\n               names_sep = '_',\n               values_to = 'value') \n\nError in eval(expr, envir, enclos): object 'complexSummary' not found\n\nlongsums\n\nError in eval(expr, envir, enclos): object 'longsums' not found\n\n\nBut that actually puts a lot of values with different meaning in the same value column. What’s probably better is to give different statistics their own columns, as sort of an intermediate long/wide.\n\nlongwide &lt;- longsums %&gt;% \n  pivot_wider(names_from = summary_statistic, values_from = value) \n\nError in eval(expr, envir, enclos): object 'longsums' not found\n\nlongwide\n\nError in eval(expr, envir, enclos): object 'longwide' not found\n\n\nAnyway, this sort of arrangement isn’t the point of this document, so I’ll stop there."
  },
  {
    "objectID": "tidyprogramming/tidy_programs.html#adjusting-to-dplyr-1.1",
    "href": "tidyprogramming/tidy_programs.html#adjusting-to-dplyr-1.1",
    "title": "Tidy programming",
    "section": "Adjusting to dplyr 1.1",
    "text": "Adjusting to dplyr 1.1\nAs of dplyr 1.1, new behaviour means that if we pass multi-argument functions, it looks for the additional arguments not as data-variables (column names), but as objects. E.g., we now get errors for all the weighted.mean calls above, since it cannot find a wt object when wt is a column name.\nThis is discussed as a dplyr github issue, where there is a workaround using rlang::quo, but I really don’t like it for a couple reasons, primarily that it forces a user to wrap their code in rlang::quo, and it matters where in the call stack the function gets defined. I’m not sure I’ll figure anything out that works better for me, since the tidyverse people came up with the workaround, but I need to try.\n\nRe-demoing the issue\nThat workaround uses {{}} around FUNS in the function. Building on funmulti above,\n\nfunbrace &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nThat actually works when we define the function to call inside the function argument.\n\nbracecheck &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE)))\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\nbracecheck\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nHowever, if we define the functions to call in an object, it fails\n\nfunstocall &lt;- list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., rlang::data_sym('wt'), na.rm = TRUE))\n\nbracecheck2 &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funstocall)\n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), funstocall, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3`, `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `x * w`:\n! non-numeric argument to binary operator\n\nbracecheck2\n\nError in eval(expr, envir, enclos): object 'bracecheck2' not found\n\n\nAnd the ‘solution’ is to use rlang::quo , followed by !! in the call\n\nfunstocallq &lt;- rlang::quo(list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE)))\n\nbracecheckq &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = !!funstocallq)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\nbracecheckq\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nThat works, but it sure requires a lot of fiddling by the user with quosures.\nWe can bring the !! inside the function, which seems to work. I’ve run into issues before where this then requires quosures for everything, but it seems to be working here for mean, which doesn’t need the quosure because it doesn’t reference data-variables.\nThe !! method is\n\nfundefuse &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, !!FUNS, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\nAnd so now we don’t have to defuse in the function call.\n\nfunstocallq &lt;- rlang::quo(c(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE)))\n\ndefusecheckb &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funstocallq)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ndefusecheckb\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nand mean works as well, even when it’s not wrapped in rlang::quo because it doesn’t reference data-variables.\n\nfunmean &lt;- list(mean = ~mean(., na.rm = TRUE))\n\ndefusecheckm &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funmean)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ndefusecheckm\n\n# A tibble: 11 × 5\n    gear  carb prefix_mean_disp prefix_mean_drat prefix_mean_nampg\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1     3     1            201.              3.18              21.5\n 2     3     2            346.              3.04              19.0\n 3     3     3            276.              3.07              16.8\n 4     3     4            416.              3.22              12.1\n 5     4     1             84.2             4.06              27.3\n 6     4     2            121.              4.16              25.9\n 7     4     4            164.              3.91              19.4\n 8     5     2            108.              4.1              NaN  \n 9     5     4            351               4.22              15.8\n10     5     6            145               3.62             NaN  \n11     5     8            301               3.54             NaN  \n\n\n\n\nSearching for a solution\nI really don’t want to require quosures. And I want to be able to pass character function names.\n\nMake reference internal to a custom function?\nAttempt 1: can I simply define a function with the data-var internally referenced so it only takes one argument? I doubt it, but that might be the easiest.\n\nweightcars &lt;- function(x) {\n  weighted.mean(x, w = wt, na.rm = TRUE)\n}\n\nThat doesn’t work with either the !! or {{}} method.\n\nfunscustom &lt;- list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weightcars(.))\n\ndefusecheckc &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funscustom)\n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nℹ In group 1: `gear = 3`, `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weightcars()`:\n! object 'wt' not found\n\ndefusecheckc\n\nError in eval(expr, envir, enclos): object 'defusecheckc' not found\n\n\n\nbracecheckc &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funscustom)\n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), funscustom, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3`, `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weightcars()`:\n! object 'wt' not found\n\nbracecheckc\n\nError in eval(expr, envir, enclos): object 'bracecheckc' not found\n\n\nAnd that doesn’t even work with the rlang::quo wrapper (unsurprisingly, I suppose).\n\nfunscustom &lt;- rlang::quo(list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weightcars(.)))\n\ndefusecheckc &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = funscustom)\n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nℹ In group 1: `gear = 3`, `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weightcars()`:\n! object 'wt' not found\n\ndefusecheckc\n\nError in eval(expr, envir, enclos): object 'defusecheckc' not found\n\n\n\n\nModify the aggregation function somehow\nMy basic thought here is whether I can auto-build the data referencing. I’ve tried using rlang::data_sym in the weighted mean function, and doing a bunch of other things, but I haven’t come up with anything yet. Maybe rlang::inject?\nEven if I specify the FUNS as a list inside the function, I need the rlang::quo. Which is surprising, since I don’t need it if they’re specified as a function argument. I’m missing something about quoting, I think.\n\nfunbrace &lt;- function(data, groupers, sumcols,\n                    FUNS, ...) {\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\n\nDifferent formulat specification\nWhat if instead of using the formula version of anonymous functions, we use \\(x)? I think this will behave like the custom weightcars above, but maybe we can have more control inside the aggregation function?\nFirst, does it work with the quo?\n\nanonq &lt;- rlang::quo(list(mean = \\(x) mean(x, na.rm = TRUE),\n                     wm = \\(x) weighted.mean(x, wt, na.rm = TRUE)))\n\n\ndefusechecka &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = anonq)\n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ndefusechecka\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nWorks with the !! but not {{}}.\n\nbracechecka &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = anonq)\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'gear', 'carb'. You can override using the\n`.groups` argument.\n\nbracechecka\n\n# A tibble: 22 × 5\n    gear  carb prefix_1_disp prefix_1_drat prefix_1_nampg\n   &lt;dbl&gt; &lt;dbl&gt; &lt;named list&gt;  &lt;named list&gt;  &lt;named list&gt;  \n 1     3     1 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 2     3     1 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 3     3     2 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 4     3     2 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 5     3     3 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 6     3     3 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 7     3     4 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 8     3     4 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n 9     4     1 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n10     4     1 &lt;fn&gt;          &lt;fn&gt;          &lt;fn&gt;          \n# ℹ 12 more rows\n\n\nNow, can we get it to work without quo???\n\nanonbare &lt;- list(mean = \\(x) mean(x, na.rm = TRUE),\n                     wm = \\(x) weighted.mean(x, wt, na.rm = TRUE))\n\nanonbare &lt;- list(mean = \\(x) mean(x, na.rm = TRUE),\nNot immediately. but can we modify those functions?\n\ndefusecheckab &lt;- fundefuse(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = anonbare)\n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nℹ In group 1: `gear = 3`, `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error:\n! object 'wt' not found\n\ndefusecheckab\n\nError in eval(expr, envir, enclos): object 'defusecheckab' not found\n\nbracecheckab &lt;- funbrace(nacars, \n         groupers = c(gear, carb), \n         sumcols = c(starts_with('d'), nampg),\n         FUNS = anonbare)\n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), anonbare, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3`, `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error:\n! object 'wt' not found\n\nbracecheckab\n\nError in eval(expr, envir, enclos): object 'bracecheckab' not found\n\n\n\n\nIs the answer to drop dplyr?\nI thought about moving to stats::aggregate, but it seems like that is going to cause just as many problems, especially when we get to passing it arbitrary lists of functions. The syntax is just so clumsy (at least to me).\n\n\nDoes it just work if I give it the vector?\nThis won’t solve the whole problem, and I think it still won’t actually work with the groupings, but should test.\n\nanonbarevec &lt;- list(mean = \\(x) mean(x, na.rm = TRUE),\n                    wm = \\(x) weighted.mean(x, nacars$wt, na.rm = TRUE))\n\nAs expected, that fails because the external vector doesn’t get broken up by the groups.\n\ndefusecheckab &lt;- fundefuse(nacars,  \n                           groupers = c(gear, carb),\n                           sumcols = c(starts_with('d'), nampg),\n                           FUNS = anonbarevec) \n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nℹ In group 1: `gear = 3`, `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weighted.mean.default()`:\n! 'x' and 'w' must have the same length\n\ndefusecheckab  \n\nError in eval(expr, envir, enclos): object 'defusecheckab' not found\n\nbracecheckab &lt;- funbrace(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = anonbarevec) \n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), anonbarevec, .names =\n  \"prefix_{.fn}_{.col}\")`.\nℹ In group 1: `gear = 3`, `carb = 1`.\nCaused by error in `across()`:\n! Can't compute column `prefix_wm_disp`.\nCaused by error in `weighted.mean.default()`:\n! 'x' and 'w' must have the same length\n\nbracecheckab\n\nError in eval(expr, envir, enclos): object 'bracecheckab' not found\n\n\n\n\nBuild and feed a character string\nWe know we need the rlang::quo to get this to work, but we can see the expressions we need in the list inside the function while debugging. So can we build the list wrapped in rlang::quo inside the function? Not very directly, as far as I can tell. But eval(parse(STRING)) seems to be a crude way forward.\nIt works to feed it a character string\n\ncharfuns &lt;- \"rlang::quo(list(mean = function(x) mean(x, na.rm = TRUE), wm = function(x) weighted.mean(x, wt, na.rm = TRUE)))\"\n\n# seems to work. NOW, how can I do that, and do it safely?\n# Likely turn the list into characters, then put rlang::quo on it, and round and round we go. Going to need lots of testing.\n\nAnd a function that parses that\n\nfunchar &lt;- function(data, groupers, sumcols,\n                     FUNS, ...) {\n  \n  FUNS &lt;- eval(parse(text = FUNS))\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  return(gm)\n}\n\n\ncharcheck &lt;- funchar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = charfuns) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharcheck\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nSo, that works. This is getting very messy though. We certianly don’t want to make a user send us that string- that’s far worse than just wrapping in rlang::quo.\nBUT, does this allow us to programatically build that string inside the function? Should try without it first, and then if it fails, build the string. Make a function that does that.\n\nfunbracechar &lt;- function(data, groupers, sumcols,\n                     FUNS, ...) {\n  \n  gm &lt;- try(data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup(), silent = TRUE)\n  \n  if (inherits(gm, 'try-error')) {\n    fchar &lt;- paste0(c(\"rlang::quo(\", deparse(FUNS), \")\"), collapse = '')\n    # FUNS2 &lt;- eval(parse(text = fchar)) # base R\n    FUNS3 &lt;- rlang::eval_tidy(rlang::parse_expr(fchar)) # rlang claims to be faster?\n  }\n  \n  gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS3}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  \n  return(gm)\n}\n\nWill need to test this with ~ functions, bare names, and \\(x) anonymous functions. I don’t think I expect it to work with character names. But it might work with character specification of the whole function?\n\nanonbare &lt;- list(mean = \\(x) mean(x, na.rm = TRUE),\n                    wm = \\(x) weighted.mean(x, wt, na.rm = TRUE))\n\nit works with the \\(x) style anonymous function\n\ncharcheck &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = anonbare) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharcheck\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nworks with tilde-style anonymous functions\n\nfunstilde &lt;- list(mean = ~mean(., na.rm = TRUE),\n                     wm = ~weighted.mean(., wt, na.rm = TRUE))\n\nchartilde &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funstilde) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\nchartilde\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nand unsurprisingly with the long form anonymous\n\nfunsfullanon &lt;- list(mean = function(x) mean(x, na.rm = TRUE),\n                     wm = function(x) weighted.mean(x, wt, na.rm = TRUE))\n\ncharfullanon &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsfullanon) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharfullanon\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nIt works with custom functions with the argument inside. If we look at what the deparse does inside the debugger, we can see that it expands those functions out, and so the thing that gets quoted is actually exactly the same as the previous version in funsfullanon.\n\nweightcustom &lt;- function(x) {\n  weighted.mean(x, w = wt, na.rm = TRUE)\n}\n\nmeancustom &lt;- function(x) {\n  mean(x, na.rm = TRUE)\n}\n\nfunscustom &lt;- list(mean = meancustom,\n                     wm = weightcustom)\n\ncharweightcustom &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funscustom) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharweightcustom\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_wm_disp prefix_mean_drat prefix_wm_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1            201.           208.              3.18           3.13\n 2     3     2            346.           347.              3.04           3.03\n 3     3     3            276.           276.              3.07           3.07\n 4     3     4            416.           425.              3.22           3.19\n 5     4     1             84.2           85.3             4.06           4.05\n 6     4     2            121.           128.              4.16           4.05\n 7     4     4            164.           164.              3.91           3.91\n 8     5     2            108.           110.              4.1            4.16\n 9     5     4            351            351               4.22           4.22\n10     5     6            145            145               3.62           3.62\n11     5     8            301            301               3.54           3.54\n# ℹ 2 more variables: prefix_mean_nampg &lt;dbl&gt;, prefix_wm_nampg &lt;dbl&gt;\n\n\nIt works when there’s a single function, not a list, too. If we look in the debugger, this does still fail with the simple {{}}, triggers the try loop, and gets deparsed.\n\nfunsnolist &lt;- weightcustom\n\ncharweightcustom &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsnolist) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharweightcustom\n\n# A tibble: 11 × 5\n    gear  carb prefix_1_disp prefix_1_drat prefix_1_nampg\n   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n 1     3     1         208.           3.13           21.5\n 2     3     2         347.           3.03           19.0\n 3     3     3         276.           3.07           16.8\n 4     3     4         425.           3.19           11.8\n 5     4     1          85.3          4.05           27.3\n 6     4     2         128.           4.05           24.7\n 7     4     4         164.           3.91           19.2\n 8     5     2         110.           4.16          NaN  \n 9     5     4         351            4.22           15.8\n10     5     6         145            3.62          NaN  \n11     5     8         301            3.54          NaN  \n\n\nI expect it not to work for a character vector, and it doesn’t.\n\nfunsnolistchar &lt;- 'weightcustom'\n\ncharnolistchar &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsnolistchar) \n\nError in `summarise()`:\nℹ In argument: `across(c(starts_with(\"d\"), nampg), \"weightcustom\",\n  .names = \"prefix_{.fn}_{.col}\")`.\nCaused by error in `across()`:\n! `.fns` must be a function, a formula, or a list of functions/formulas.\n\ncharnolistchar\n\nError in eval(expr, envir, enclos): object 'charnolistchar' not found\n\n\nBut, does it work if we add an mget line?\n\nfunbracechar &lt;- function(data, groupers, sumcols,\n                     FUNS, ...) {\n  if (is.character(FUNS)) {\n    FUNS &lt;- mget(FUNS, inherits = TRUE)\n  }\n  gm &lt;- try(data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup(), silent = TRUE)\n  \n  if (inherits(gm, 'try-error')) {\n    fchar &lt;- paste0(c(\"rlang::quo(\", deparse(FUNS), \")\"), collapse = '')\n    # FUNS2 &lt;- eval(parse(text = fchar)) # base R\n    FUNS3 &lt;- rlang::eval_tidy(rlang::parse_expr(fchar)) # rlang claims to be faster?\n    gm &lt;- data %&gt;%\n    group_by(across({{groupers}})) %&gt;%\n    summarise(across({{sumcols}}, {{FUNS3}}, \n                     .names = 'prefix_{.fn}_{.col}')) %&gt;%\n    ungroup()\n  }\n  \n  \n  \n  return(gm)\n}\n\nIt works for a single function\n\nfunsnolistchar &lt;- 'weightcustom'\n\ncharnolistchar &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsnolistchar) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharnolistchar\n\n# A tibble: 11 × 5\n    gear  carb prefix_weightcustom_disp prefix_weightcustom_drat\n   &lt;dbl&gt; &lt;dbl&gt;                    &lt;dbl&gt;                    &lt;dbl&gt;\n 1     3     1                    208.                      3.13\n 2     3     2                    347.                      3.03\n 3     3     3                    276.                      3.07\n 4     3     4                    425.                      3.19\n 5     4     1                     85.3                     4.05\n 6     4     2                    128.                      4.05\n 7     4     4                    164.                      3.91\n 8     5     2                    110.                      4.16\n 9     5     4                    351                       4.22\n10     5     6                    145                       3.62\n11     5     8                    301                       3.54\n# ℹ 1 more variable: prefix_weightcustom_nampg &lt;dbl&gt;\n\n\nAnd for multiple functions if they are in a character vector\n\nfunsmultichar &lt;- c('mean', 'weightcustom')\n\ncharmultichar &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsmultichar) \n\n`summarise()` has grouped output by 'gear'. You can override using the\n`.groups` argument.\n\ncharmultichar\n\n# A tibble: 11 × 8\n    gear  carb prefix_mean_disp prefix_weightcustom_disp prefix_mean_drat\n   &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;                    &lt;dbl&gt;            &lt;dbl&gt;\n 1     3     1            201.                     208.              3.18\n 2     3     2            346.                     347.              3.04\n 3     3     3            276.                     276.              3.07\n 4     3     4            416.                     425.              3.22\n 5     4     1             84.2                     85.3             4.06\n 6     4     2            121.                     128.              4.16\n 7     4     4            164.                     164.              3.91\n 8     5     2            108.                     110.              4.1 \n 9     5     4            351                      351               4.22\n10     5     6            145                      145               3.62\n11     5     8            301                      301               3.54\n# ℹ 3 more variables: prefix_weightcustom_drat &lt;dbl&gt;, prefix_mean_nampg &lt;dbl&gt;,\n#   prefix_weightcustom_nampg &lt;dbl&gt;\n\n\nBut not for a list. This is not unexpected- the mget is in if(is.character(FUNS)) , and so the list won’t get mgot. I think that’s good enough for now. It would be doable obviously to purrr over the list and mget the items that are characters, but that’s not really the focus here. We have figured out an (ugly) workaround for the dplyr 1.1 issue, and that will have to do for now- applying it over all possible organisations of FUNS will have to be for another day.\n\nfunsmulticharl &lt;- list(m = 'mean', wm = 'weightcustom')\n\ncharmulticharl &lt;- funbracechar(nacars,  \n                         groupers = c(gear, carb), \n                         sumcols = c(starts_with('d'), nampg),   \n                         FUNS = funsmulticharl) \n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nCaused by error in `across()`:\n! `.fns` must be a function, a formula, or a list of functions/formulas.\n\ncharmulticharl\n\nError in eval(expr, envir, enclos): object 'charmulticharl' not found\n\n\n\n\neval_tidy\nI keep feeling like eval_tidy should work somehow, since it allows the .data pronoun, but I can’t seem to get my head around how it would work here. I’d happily write something like \\(x) eval_tidy(weighted.mean(x, .data$wt, na.rm = TRUE))). Maybe I can get that to work with the right sort of enquoing? I tried for a while and couldn’t figure it out, but maybe come back fresh later on."
  },
  {
    "objectID": "vicwater/vicwater_testing.html",
    "href": "vicwater/vicwater_testing.html",
    "title": "Testing VicWater API",
    "section": "",
    "text": "# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#section",
    "href": "vicwater/vicwater_testing.html#section",
    "title": "Testing VicWater API",
    "section": "",
    "text": "# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#access-victoria-water-data-through-api",
    "href": "vicwater/vicwater_testing.html#access-victoria-water-data-through-api",
    "title": "Testing VicWater API",
    "section": "Access Victoria water data through API",
    "text": "Access Victoria water data through API\nThis document is my testing and development of functions to include in the {vicwater} package. Basically, it’s where I interactively sorted through how to hit the API functions, the formats of the lists, and how to unpack the returned lists. It is a work in progress, since that package is under development.\nWe want to access victorian water data for a set of sites. That requires using the api at https://data.water.vic.gov.au/cgi/webservice.exe?[JSON_request] , but it’s poorly documented. I think I got it mostly figured out in a testing document, but there’s a lot of extra testing in there that needs to be skipped over and cleaned up. My plan is to make this a package, but it needs more development. I’m moving further testing here so I can get to the point a bit quicker.\nLibraries. Do I still need jsonlite now that I’ve moved ot httr2?\n\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.2.2\n\nlibrary(httr2)\n\nWarning: package 'httr2' was built under R version 4.2.2\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#set-up-params",
    "href": "vicwater/vicwater_testing.html#set-up-params",
    "title": "Testing VicWater API",
    "section": "Set up params",
    "text": "Set up params\n\nURL\nThe base url that everything gets attached to is: and we use httr2::request to start building the request.\n\nvicurl &lt;- \"https://data.water.vic.gov.au/cgi/webservice.exe?\"\nreqvic &lt;- request(vicurl)\n\n\n\nSite lists\nI want to test with one, two, and several sites in a site list. I had tried to do \"sitelist\" = c('site', 'site') , and that failed. But it works to have \"site, site\"\nThe upper steavenson is 405328, Barwon is 233217 (and has Temp), Taggerty 405331 only ran 2010-2013, And the Marysville golf course 405837 (only rainfall). That hits some things we want to make sure we pick up- no longer running gauges, gauges with only rain, gauges with lots of variables, etc.\nI only make one site_list here with multiple, but can do the str_c inside the calls usually.\n\nbarwon &lt;- '233217'\nsteavenson &lt;- '405328'\ntaggerty &lt;- '405331'\ngolf &lt;- '405837'\n\nallsites &lt;- str_c(barwon, steavenson, taggerty, golf, sep = \", \")"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#api-functions",
    "href": "vicwater/vicwater_testing.html#api-functions",
    "title": "Testing VicWater API",
    "section": "API functions",
    "text": "API functions\nAnd how to call each- including multiple values.\nI finally found a couple sources of documentation that will hopefully be helpful: https://kisters.com.au/doco/hydllp.htm and https://water-monitoring.information.qld.gov.au/wini/Documents/RDMW_API_doco.pdf.\nThe first thing to do is to figure out what basic information is there, so we can ask for it. What we really want is get_ts_traces, but it has a lot of parameters (see Kisters docs). Some are relatively straightforward to meaning, though how to get them to be correct JSON can be tricky (e.g. site_list, while others are opaque, e.g. varfrom, varto, datasource, either to their meaning or what the options are we can ask for. We can try to figure that out with some querying of the other functions.\n\nDatasources\nCan we figure out what datasource means by asking for some by site?\nversion has to be 1.\n\nds_s_params &lt;- list(\"function\" = 'get_datasources_by_site',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites))\n\nreqvic %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 108\n\n{\"function\":\"get_datasources_by_site\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\"}}\n\nresp_ds_s &lt;- reqvic %&gt;% \n  req_body_json(ds_s_params) %&gt;% \n  req_perform()\n\nrbody_ds_s &lt;- resp_ds_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_ds_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 4\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"233217\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405328\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405331\"\n  .. .. ..$ datasources:List of 2\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. ..$ :List of 2\n  .. .. ..$ site       : chr \"405837\"\n  .. .. ..$ datasources:List of 3\n  .. .. .. ..$ : chr \"A\"\n  .. .. .. ..$ : chr \"TELEM\"\n  .. .. .. ..$ : chr \"TELEMCOPY\"\n\n\nI’ll need to sort out how to unpack that list later, but for now, let’s just look at it and see that all of them have options A and TELEM, and a couple have TELEMCOPY.\nAccording to the QLD pdf, the datasource distinguishes things like Archive and Telemetry. That’s similar in Vic, though QLD also had codes for back-filled holes, which don’t seem to be here (at least at these sites).\nSeems like it will be safest to ask for ‘A’ or ‘TELEM’.\nAnd the different variables can be in a var_list or varto and varfrom (though not always- see below). The numbers are for different variables, but again, no guarantee they’re the same in Vic.\n\nunpacking the list\nI might as well do this and build the function. Should be able to do it with unnest, and then maybe drop dumb columns? Nope, some of the lists unpack into lists of mixed type. But unnest_wider and unnest_longer might be the trick. Will need to test with single sites in case the structure changes.\nFor the function, we probably want to just print the error value or something, and not return it in the df. Or if it errors, return that, if it doesn’t, just give df. That’s probably best.\n\na &lt;- as_tibble(rbody_ds_s[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # error and a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # error, site, and a `datasources` list\n  unnest_longer(col = where(is.list)) # fully unpacked into a long df\na\n\n# A tibble: 11 × 2\n   site   datasources\n   &lt;chr&gt;  &lt;chr&gt;      \n 1 233217 A          \n 2 233217 TELEM      \n 3 233217 TELEMCOPY  \n 4 405328 A          \n 5 405328 TELEM      \n 6 405328 TELEMCOPY  \n 7 405331 A          \n 8 405331 TELEM      \n 9 405837 A          \n10 405837 TELEM      \n11 405837 TELEMCOPY  \n\n\nMight actually be better as a table or pivot_wider? Depends what the point is? Pivot wider is kind of a pain, use table? But table actually unpacks longer when I as_tibble or as.data.frame it. Which is annoying.\n\nb &lt;- table(a$site, a$datasources)\nb\n\n        \n         A TELEM TELEMCOPY\n  233217 1     1         1\n  405328 1     1         1\n  405331 1     1         0\n  405837 1     1         1\n\n\nI think just return the long tibble, and do the table as a plot or explicitly a table or something. Which orientation makes most sense? Not sure. Probably gauges on x? But plots of actual data will have gauges on y, time on x, so maybe stay consistent.\n\nc &lt;- b %&gt;% \n  as_tibble(.name_repair = 'unique') %&gt;% \n  rename(gauge = `...1`, datasource = `...2`)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n\nc %&gt;% \n  mutate(n = as.logical(n)) %&gt;% \nggplot2::ggplot(ggplot2::aes(x = datasource, y = gauge, fill = n)) + \n  ggplot2::geom_tile(colour=\"white\", size=0.25) +\n  ggplot2::scale_fill_discrete(type = c('firebrick', 'dodgerblue')) +\n  ggplot2::labs(fill = NULL) +\n  ggplot2::coord_equal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nCan I just build that plot from a? Yes, but would be just as annoying. maybe.\n\nallopts &lt;- a %&gt;%\n  expand(site, datasources)\n\na2 &lt;- a %&gt;% \n  mutate(indata = TRUE) %&gt;% \n  dplyr::right_join(allopts) %&gt;% \n  mutate(indata = ifelse(is.na(indata), FALSE, indata))\n\nJoining, by = c(\"site\", \"datasources\")\n\n  ggplot(a2, aes(x = datasources, y = site, fill = indata)) + \n  ggplot2::geom_tile(colour=\"white\", linewidth=0.25) +\n  ggplot2::scale_fill_discrete(type = c('firebrick', 'dodgerblue')) +\n  ggplot2::labs(fill = NULL) +\n  ggplot2::coord_equal()\n\n\n\n\nGood enough for this one- turn that into a function in a package.\n\n\n\nTest from the package\nI’m using devtools::load_all() to load the package repo in here for interactive testing and poking.\nObviously, hard paths are a bad idea, but I’m going to do it here since I typically do relative within repos, and these are across repos. Still, temporary and hacky.\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\n\n\nreturntib &lt;- get_datasources_by_site(vicurl, allsites)\n\n\nreturntib\n\n# A tibble: 11 × 2\n   site   datasource\n   &lt;chr&gt;  &lt;chr&gt;     \n 1 233217 A         \n 2 233217 TELEM     \n 3 233217 TELEMCOPY \n 4 405328 A         \n 5 405328 TELEM     \n 6 405328 TELEMCOPY \n 7 405331 A         \n 8 405331 TELEM     \n 9 405837 A         \n10 405837 TELEM     \n11 405837 TELEMCOPY \n\nplot_datasources_by_site(returntib)\n\nJoining, by = c(\"site\", \"datasource\")\n\n\n\n\n\n\n\nSites by datasource\nHaven’t written this one before, might blow things up. But if we want a list of sites, it might be better than the way I did this before of just asking for everything in the db, lots of which had no data.\nAnd now we know the datasource options. I think? I suppose it’s possible there’s another type I’m not aware of.\nFor some weird reason, sitelists should be \"site, site, site\", while the datasources should be c('source', 'source'). The latter yields JSON array ['source', 'source'], while the former yields JSON 'site', 'site'\nThis works. The list truncates, but it did work.\n\nds_wanted &lt;- c('A', 'TELEM')\ns_ds_params &lt;- list(\"function\" = 'get_sites_by_datasource',\n               \"version\" = \"1\",\n               \"params\" = list(\"datasources\" = ds_wanted))\n\nreqvic %&gt;% \n  req_body_json(s_ds_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 91\n\n{\"function\":\"get_sites_by_datasource\",\"version\":\"1\",\"params\":{\"datasources\":[\"A\",\"TELEM\"]}}\n\nresp_s_ds &lt;- reqvic %&gt;% \n  req_body_json(s_ds_params) %&gt;% \n  req_perform()\n\nrbody_s_ds &lt;- resp_s_ds %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_s_ds)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ datasources:List of 2\n  .. ..$ :List of 2\n  .. .. ..$ datasource: chr \"A\"\n  .. .. ..$ sites     :List of 4044\n  .. .. .. ..$ : chr \"100023\"\n  .. .. .. ..$ : chr \"100500\"\n  .. .. .. ..$ : chr \"100503\"\n  .. .. .. ..$ : chr \"100504\"\n  .. .. .. ..$ : chr \"101708\"\n  .. .. .. ..$ : chr \"102621\"\n  .. .. .. ..$ : chr \"102827\"\n  .. .. .. ..$ : chr \"102828\"\n  .. .. .. ..$ : chr \"102829\"\n  .. .. .. ..$ : chr \"102830\"\n  .. .. .. ..$ : chr \"102831\"\n  .. .. .. ..$ : chr \"103811\"\n  .. .. .. ..$ : chr \"104929\"\n  .. .. .. ..$ : chr \"104930\"\n  .. .. .. ..$ : chr \"105134\"\n  .. .. .. ..$ : chr \"105222\"\n  .. .. .. ..$ : chr \"105287\"\n  .. .. .. ..$ : chr \"105317\"\n  .. .. .. ..$ : chr \"105480\"\n  .. .. .. ..$ : chr \"105484\"\n  .. .. .. ..$ : chr \"105936\"\n  .. .. .. ..$ : chr \"107631\"\n  .. .. .. ..$ : chr \"107970\"\n  .. .. .. ..$ : chr \"107971\"\n  .. .. .. ..$ : chr \"108201\"\n  .. .. .. ..$ : chr \"108203\"\n  .. .. .. ..$ : chr \"108319\"\n  .. .. .. ..$ : chr \"108320\"\n  .. .. .. ..$ : chr \"108321\"\n  .. .. .. ..$ : chr \"108898\"\n  .. .. .. ..$ : chr \"108899\"\n  .. .. .. ..$ : chr \"108917\"\n  .. .. .. ..$ : chr \"108944\"\n  .. .. .. ..$ : chr \"109133\"\n  .. .. .. ..$ : chr \"109461\"\n  .. .. .. ..$ : chr \"109462\"\n  .. .. .. ..$ : chr \"109644\"\n  .. .. .. ..$ : chr \"109645\"\n  .. .. .. ..$ : chr \"109769\"\n  .. .. .. ..$ : chr \"109770\"\n  .. .. .. ..$ : chr \"109778\"\n  .. .. .. ..$ : chr \"109779\"\n  .. .. .. ..$ : chr \"110151\"\n  .. .. .. ..$ : chr \"110152\"\n  .. .. .. ..$ : chr \"110153\"\n  .. .. .. ..$ : chr \"110171\"\n  .. .. .. ..$ : chr \"110186\"\n  .. .. .. ..$ : chr \"110464\"\n  .. .. .. ..$ : chr \"110721\"\n  .. .. .. ..$ : chr \"110724\"\n  .. .. .. ..$ : chr \"110731\"\n  .. .. .. ..$ : chr \"110739\"\n  .. .. .. ..$ : chr \"110745\"\n  .. .. .. ..$ : chr \"110943\"\n  .. .. .. ..$ : chr \"110978\"\n  .. .. .. ..$ : chr \"111543\"\n  .. .. .. ..$ : chr \"111549\"\n  .. .. .. ..$ : chr \"111551\"\n  .. .. .. ..$ : chr \"111691\"\n  .. .. .. ..$ : chr \"111692\"\n  .. .. .. ..$ : chr \"112182\"\n  .. .. .. ..$ : chr \"112235\"\n  .. .. .. ..$ : chr \"112236\"\n  .. .. .. ..$ : chr \"112237\"\n  .. .. .. ..$ : chr \"112459\"\n  .. .. .. ..$ : chr \"112708\"\n  .. .. .. ..$ : chr \"112803\"\n  .. .. .. ..$ : chr \"112804\"\n  .. .. .. ..$ : chr \"113004\"\n  .. .. .. ..$ : chr \"113124\"\n  .. .. .. ..$ : chr \"113125\"\n  .. .. .. ..$ : chr \"113467\"\n  .. .. .. ..$ : chr \"113694\"\n  .. .. .. ..$ : chr \"113695\"\n  .. .. .. ..$ : chr \"113705\"\n  .. .. .. ..$ : chr \"113706\"\n  .. .. .. ..$ : chr \"114158\"\n  .. .. .. ..$ : chr \"114169\"\n  .. .. .. ..$ : chr \"115732\"\n  .. .. .. ..$ : chr \"115872\"\n  .. .. .. ..$ : chr \"116382\"\n  .. .. .. ..$ : chr \"116459\"\n  .. .. .. ..$ : chr \"116460\"\n  .. .. .. ..$ : chr \"116802\"\n  .. .. .. ..$ : chr \"116803\"\n  .. .. .. ..$ : chr \"119329\"\n  .. .. .. ..$ : chr \"119330\"\n  .. .. .. ..$ : chr \"119337\"\n  .. .. .. ..$ : chr \"119338\"\n  .. .. .. ..$ : chr \"119339\"\n  .. .. .. ..$ : chr \"119340\"\n  .. .. .. ..$ : chr \"119341\"\n  .. .. .. ..$ : chr \"119342\"\n  .. .. .. ..$ : chr \"119347\"\n  .. .. .. ..$ : chr \"119348\"\n  .. .. .. ..$ : chr \"119366\"\n  .. .. .. ..$ : chr \"119367\"\n  .. .. .. ..$ : chr \"119377\"\n  .. .. .. ..$ : chr \"120248\"\n  .. .. .. .. [list output truncated]\n  .. ..$ :List of 2\n  .. .. ..$ datasource: chr \"TELEM\"\n  .. .. ..$ sites     :List of 2093\n  .. .. .. ..$ : chr \"100023\"\n  .. .. .. ..$ : chr \"100500\"\n  .. .. .. ..$ : chr \"100503\"\n  .. .. .. ..$ : chr \"100504\"\n  .. .. .. ..$ : chr \"100731\"\n  .. .. .. ..$ : chr \"101708\"\n  .. .. .. ..$ : chr \"102621\"\n  .. .. .. ..$ : chr \"102827\"\n  .. .. .. ..$ : chr \"102828\"\n  .. .. .. ..$ : chr \"102829\"\n  .. .. .. ..$ : chr \"102830\"\n  .. .. .. ..$ : chr \"102831\"\n  .. .. .. ..$ : chr \"103811\"\n  .. .. .. ..$ : chr \"104929\"\n  .. .. .. ..$ : chr \"104930\"\n  .. .. .. ..$ : chr \"105134\"\n  .. .. .. ..$ : chr \"105222\"\n  .. .. .. ..$ : chr \"105484\"\n  .. .. .. ..$ : chr \"105936\"\n  .. .. .. ..$ : chr \"107631\"\n  .. .. .. ..$ : chr \"107970\"\n  .. .. .. ..$ : chr \"108201\"\n  .. .. .. ..$ : chr \"108203\"\n  .. .. .. ..$ : chr \"108319\"\n  .. .. .. ..$ : chr \"108320\"\n  .. .. .. ..$ : chr \"108321\"\n  .. .. .. ..$ : chr \"108944\"\n  .. .. .. ..$ : chr \"109133\"\n  .. .. .. ..$ : chr \"109462\"\n  .. .. .. ..$ : chr \"109644\"\n  .. .. .. ..$ : chr \"109645\"\n  .. .. .. ..$ : chr \"109769\"\n  .. .. .. ..$ : chr \"109770\"\n  .. .. .. ..$ : chr \"110151\"\n  .. .. .. ..$ : chr \"110152\"\n  .. .. .. ..$ : chr \"110153\"\n  .. .. .. ..$ : chr \"110171\"\n  .. .. .. ..$ : chr \"110186\"\n  .. .. .. ..$ : chr \"110464\"\n  .. .. .. ..$ : chr \"110721\"\n  .. .. .. ..$ : chr \"110724\"\n  .. .. .. ..$ : chr \"110731\"\n  .. .. .. ..$ : chr \"110739\"\n  .. .. .. ..$ : chr \"110745\"\n  .. .. .. ..$ : chr \"110943\"\n  .. .. .. ..$ : chr \"110978\"\n  .. .. .. ..$ : chr \"111543\"\n  .. .. .. ..$ : chr \"111549\"\n  .. .. .. ..$ : chr \"111551\"\n  .. .. .. ..$ : chr \"111691\"\n  .. .. .. ..$ : chr \"111692\"\n  .. .. .. ..$ : chr \"112182\"\n  .. .. .. ..$ : chr \"112185\"\n  .. .. .. ..$ : chr \"112235\"\n  .. .. .. ..$ : chr \"112236\"\n  .. .. .. ..$ : chr \"112237\"\n  .. .. .. ..$ : chr \"112459\"\n  .. .. .. ..$ : chr \"112708\"\n  .. .. .. ..$ : chr \"112803\"\n  .. .. .. ..$ : chr \"112804\"\n  .. .. .. ..$ : chr \"113004\"\n  .. .. .. ..$ : chr \"113124\"\n  .. .. .. ..$ : chr \"113125\"\n  .. .. .. ..$ : chr \"113467\"\n  .. .. .. ..$ : chr \"113694\"\n  .. .. .. ..$ : chr \"113695\"\n  .. .. .. ..$ : chr \"113705\"\n  .. .. .. ..$ : chr \"113706\"\n  .. .. .. ..$ : chr \"114158\"\n  .. .. .. ..$ : chr \"114169\"\n  .. .. .. ..$ : chr \"114975\"\n  .. .. .. ..$ : chr \"115732\"\n  .. .. .. ..$ : chr \"116382\"\n  .. .. .. ..$ : chr \"116802\"\n  .. .. .. ..$ : chr \"116803\"\n  .. .. .. ..$ : chr \"119329\"\n  .. .. .. ..$ : chr \"119337\"\n  .. .. .. ..$ : chr \"119340\"\n  .. .. .. ..$ : chr \"119341\"\n  .. .. .. ..$ : chr \"119342\"\n  .. .. .. ..$ : chr \"119347\"\n  .. .. .. ..$ : chr \"119348\"\n  .. .. .. ..$ : chr \"119366\"\n  .. .. .. ..$ : chr \"119367\"\n  .. .. .. ..$ : chr \"119377\"\n  .. .. .. ..$ : chr \"120248\"\n  .. .. .. ..$ : chr \"121019\"\n  .. .. .. ..$ : chr \"122152\"\n  .. .. .. ..$ : chr \"123140\"\n  .. .. .. ..$ : chr \"126975\"\n  .. .. .. ..$ : chr \"129744\"\n  .. .. .. ..$ : chr \"129746\"\n  .. .. .. ..$ : chr \"134949\"\n  .. .. .. ..$ : chr \"137194\"\n  .. .. .. ..$ : chr \"137195\"\n  .. .. .. ..$ : chr \"137197\"\n  .. .. .. ..$ : chr \"137198\"\n  .. .. .. ..$ : chr \"137199\"\n  .. .. .. ..$ : chr \"137200\"\n  .. .. .. .. [list output truncated]\n\n\nCan I tibble that up?\n\ns &lt;- as_tibble(rbody_s_ds[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # sites, and a `datasource` list\n  unnest_longer(col = where(is.list)) # fully unpacked into a long df\ns\n\n# A tibble: 6,137 × 2\n   datasource sites \n   &lt;chr&gt;      &lt;chr&gt; \n 1 A          100023\n 2 A          100500\n 3 A          100503\n 4 A          100504\n 5 A          101708\n 6 A          102621\n 7 A          102827\n 8 A          102828\n 9 A          102829\n10 A          102830\n# … with 6,127 more rows\n\n\nWhat are the number of sites in each?\n\ns %&gt;% group_by(datasource) %&gt;% summarise(n = n())\n\n# A tibble: 2 × 2\n  datasource     n\n  &lt;chr&gt;      &lt;int&gt;\n1 A           4044\n2 TELEM       2093\n\n\nWay more in Archive. Are there any in Telem that aren’t in A?\n\ntsites &lt;- s %&gt;% \n  filter(datasource == 'TELEM') %&gt;% \n  select(sites) %&gt;% \n  pull()\n\nasites &lt;- s %&gt;% \n  filter(datasource == 'A') %&gt;% \n  select(sites) %&gt;% \n  pull()\n\nall(tsites %in% asites)\n\n[1] FALSE\n\nsum(!(tsites %in% asites))\n\n[1] 96\n\n\nsee if I can blow up ggplot. Oof the plurals\n\ns &lt;- s %&gt;% \n  rename(site = sites, datasources = datasource)\n\nUnreadable. Not surprisingly.\n\nplot_datasources_by_site(s) + coord_flip()\n\nyikes.\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\nsxd &lt;- get_sites_by_datasource(datasources = c('A', 'TELEM'))\n\n\nsxd\n\n# A tibble: 6,172 × 2\n   datasource site  \n   &lt;chr&gt;      &lt;chr&gt; \n 1 A          100023\n 2 A          100500\n 3 A          100503\n 4 A          100504\n 5 A          101708\n 6 A          102621\n 7 A          102827\n 8 A          102828\n 9 A          102829\n10 A          102830\n# … with 6,162 more rows\n\n\nPlot still needs work.\n\nplot_datasources_by_site(sxd) + coord_flip()\n\nJoining, by = c(\"datasource\", \"site\")"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#variables",
    "href": "vicwater/vicwater_testing.html#variables",
    "title": "Testing VicWater API",
    "section": "Variables",
    "text": "Variables\nor do I go straight for get_ts_ and then back this back out? Or get_db_info?\nI think I’m going to want to use get_variable_list both in get_ts_traces and to generate a set of possible variables.\nI’d like to get all sites, then make a master list of datasources and variables. But I need to get this usable for get_ts_traces, I think.\n\nGet_variable_list\nFeeding this a c(datasource, datasource) makes it return only some of the results, but throws no errors. So do one at a time.\n\nv_s_params &lt;- list(\"function\" = 'get_variable_list',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites,\n                               \"datasource\" = \"A\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 119\n\n{\"function\":\"get_variable_list\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\",\"datasource\":\"A\"}}\n\nresp_v_s &lt;- req %&gt;% \n  req_body_json(v_s_params) %&gt;% \n  req_perform()\n\nrbody_v_s &lt;- resp_v_s %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_v_s)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ sites:List of 4\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ variables   :List of 6\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"19610306171500\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"210.00\"\n  .. .. .. .. ..$ units       : chr \"pH\"\n  .. .. .. .. ..$ name        : chr \"Acidity/Alkalinity (pH)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"215.00\"\n  .. .. .. .. ..$ units       : chr \"ppm\"\n  .. .. .. .. ..$ name        : chr \"Dissolved Oxygen (ppm)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221107070000\"\n  .. .. .. .. ..$ period_start: chr \"20100706123100\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"233217\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221202090000\"\n  .. .. .. .. ..$ period_start: chr \"20091119170800\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. ..$ site        : chr \"405328\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"TAGGERTY R LADY TALT\"\n  .. .. .. ..$ name      : chr \"TAGGERTY RV @ LADY TALBOT DRIVE NEAR MARYSVILLE\"\n  .. .. ..$ variables   :List of 4\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"100.00\"\n  .. .. .. .. ..$ units       : chr \"metres\"\n  .. .. .. .. ..$ name        : chr \"Stream Water Level (m)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"450.00\"\n  .. .. .. .. ..$ units       : chr \"Degrees celsius\"\n  .. .. .. .. ..$ name        : chr \"Water Temperature (°C)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"810.00\"\n  .. .. .. .. ..$ units       : chr \"NTU\"\n  .. .. .. .. ..$ name        : chr \"Turbidity (NTU)\"\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20130211110700\"\n  .. .. .. .. ..$ period_start: chr \"20100729122000\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"820.00\"\n  .. .. .. .. ..$ units       : chr \"µS/cm@25°C\"\n  .. .. .. .. ..$ name        : chr \"Conductivity (µS/cm)\"\n  .. .. ..$ site        : chr \"405331\"\n  .. ..$ :List of 3\n  .. .. ..$ site_details:List of 3\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"R.G. MARYSVILLE\"\n  .. .. .. ..$ name      : chr \"RAINGAUGE @ MARYSVILLE GOLF CLUB\"\n  .. .. ..$ variables   :List of 1\n  .. .. .. ..$ :List of 6\n  .. .. .. .. ..$ period_end  : chr \"20221202081730\"\n  .. .. .. .. ..$ period_start: chr \"20010621142700\"\n  .. .. .. .. ..$ subdesc     : chr \"Available for release\"\n  .. .. .. .. ..$ variable    : chr \"10.00\"\n  .. .. .. .. ..$ units       : chr \"mm\"\n  .. .. .. .. ..$ name        : chr \"Rainfall (mm)\"\n  .. .. ..$ site        : chr \"405837\"\n\n\nUnpack that\n\ns &lt;- as_tibble(rbody_v_s[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # sites, and a `datasource` list\n  unnest_wider(col = site_details) %&gt;% # site details in new cols\n  unnest_longer(col = variables) %&gt;% # one line per variable, details of variables in a list\n  rename(long_name = name) %&gt;% # variables have names too, avoid conflicts\n  unnest_wider(col = variables) %&gt;% # columns for each attribute of the variables\n  rename(var_name = name)\ns\n\n# A tibble: 12 × 10\n   timezone short_…¹ long_…² perio…³ perio…⁴ subdesc varia…⁵ units var_n…⁶ site \n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n 1 10.0     BARWON … BARWON… 202211… 196103… Availa… 100.00  metr… Stream… 2332…\n 2 10.0     BARWON … BARWON… 202211… 201007… Availa… 210.00  pH    Acidit… 2332…\n 3 10.0     BARWON … BARWON… 202211… 201007… Availa… 215.00  ppm   Dissol… 2332…\n 4 10.0     BARWON … BARWON… 202211… 201007… Availa… 450.00  Degr… Water … 2332…\n 5 10.0     BARWON … BARWON… 202211… 201007… Availa… 810.00  NTU   Turbid… 2332…\n 6 10.0     BARWON … BARWON… 202211… 201007… Availa… 820.00  µS/c… Conduc… 2332…\n 7 10.0     STEAVEN… STEAVE… 202212… 200911… Availa… 100.00  metr… Stream… 4053…\n 8 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 100.00  metr… Stream… 4053…\n 9 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 450.00  Degr… Water … 4053…\n10 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 810.00  NTU   Turbid… 4053…\n11 10.0     TAGGERT… TAGGER… 201302… 201007… Availa… 820.00  µS/c… Conduc… 4053…\n12 10.0     R.G. MA… RAINGA… 202212… 200106… Availa… 10.00   mm    Rainfa… 4058…\n# … with abbreviated variable names ¹​short_name, ²​long_name, ³​period_end,\n#   ⁴​period_start, ⁵​variable, ⁶​var_name\n\n\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\nvl &lt;- get_variable_list(site_list = allsites, datasource = 'A')\n\n\nvl\n\n# A tibble: 12 × 11\n   site   short_…¹ long_…² varia…³ units var_n…⁴ perio…⁵ perio…⁶ subdesc datas…⁷\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1 233217 BARWON … BARWON… 100.00  metr… Stream… 196103… 202211… Availa… A      \n 2 233217 BARWON … BARWON… 210.00  pH    Acidit… 201007… 202211… Availa… A      \n 3 233217 BARWON … BARWON… 215.00  ppm   Dissol… 201007… 202211… Availa… A      \n 4 233217 BARWON … BARWON… 450.00  Degr… Water … 201007… 202211… Availa… A      \n 5 233217 BARWON … BARWON… 810.00  NTU   Turbid… 201007… 202211… Availa… A      \n 6 233217 BARWON … BARWON… 820.00  µS/c… Conduc… 201007… 202211… Availa… A      \n 7 405328 STEAVEN… STEAVE… 100.00  metr… Stream… 200911… 202212… Availa… A      \n 8 405331 TAGGERT… TAGGER… 100.00  metr… Stream… 201007… 201302… Availa… A      \n 9 405331 TAGGERT… TAGGER… 450.00  Degr… Water … 201007… 201302… Availa… A      \n10 405331 TAGGERT… TAGGER… 810.00  NTU   Turbid… 201007… 201302… Availa… A      \n11 405331 TAGGERT… TAGGER… 820.00  µS/c… Conduc… 201007… 201302… Availa… A      \n12 405837 R.G. MA… RAINGA… 10.00   mm    Rainfa… 200106… 202212… Availa… A      \n# … with 1 more variable: timezone &lt;chr&gt;, and abbreviated variable names\n#   ¹​short_name, ²​long_name, ³​variable, ⁴​var_name, ⁵​period_start, ⁶​period_end,\n#   ⁷​datasource\n\n\nTry with two datasources\n\nv2 &lt;- get_variable_list(site_list = allsites, datasource = c('A', 'TELEM'))\n\n\nv2\n\n# A tibble: 27 × 11\n   site   short_…¹ long_…² varia…³ units var_n…⁴ perio…⁵ perio…⁶ subdesc datas…⁷\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1 233217 BARWON … BARWON… 100.00  metr… Stream… 196103… 202211… Availa… A      \n 2 233217 BARWON … BARWON… 210.00  pH    Acidit… 201007… 202211… Availa… A      \n 3 233217 BARWON … BARWON… 215.00  ppm   Dissol… 201007… 202211… Availa… A      \n 4 233217 BARWON … BARWON… 450.00  Degr… Water … 201007… 202211… Availa… A      \n 5 233217 BARWON … BARWON… 810.00  NTU   Turbid… 201007… 202211… Availa… A      \n 6 233217 BARWON … BARWON… 820.00  µS/c… Conduc… 201007… 202211… Availa… A      \n 7 405328 STEAVEN… STEAVE… 100.00  metr… Stream… 200911… 202212… Availa… A      \n 8 405331 TAGGERT… TAGGER… 100.00  metr… Stream… 201007… 201302… Availa… A      \n 9 405331 TAGGERT… TAGGER… 450.00  Degr… Water … 201007… 201302… Availa… A      \n10 405331 TAGGERT… TAGGER… 810.00  NTU   Turbid… 201007… 201302… Availa… A      \n# … with 17 more rows, 1 more variable: timezone &lt;chr&gt;, and abbreviated\n#   variable names ¹​short_name, ²​long_name, ³​variable, ⁴​var_name,\n#   ⁵​period_start, ⁶​period_end, ⁷​datasource\n\n\nI think now go to get_ts_traces, and then go back and write some helpers that can call get_datasources and get_variable and geo-locate, and use them to allow passing things like variables = ‘all’"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#get-traces",
    "href": "vicwater/vicwater_testing.html#get-traces",
    "title": "Testing VicWater API",
    "section": "get traces",
    "text": "get traces\nThe basic format is this, will need to do some testing.\n\nb1params &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(b1params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 219\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb1 &lt;- req %&gt;% \n  req_body_json(b1params) %&gt;% \n  req_perform()\n\nrbodyb1 &lt;- respb1 %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb1)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nand with two params\n\nb2params &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(b2params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb2 &lt;- req %&gt;% \n  req_body_json(b2params) %&gt;% \n  req_perform()\n\nrbodyb2 &lt;- respb2 %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb2)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 2\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n\n\nTwo params, two sites (one site only has one var, but not an error, I hope)\n\nb22params &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = str_c(barwon, steavenson, sep = \", \"),\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(b22params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 231\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217, 405328\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb22 &lt;- req %&gt;% \n  req_body_json(b22params) %&gt;% \n  req_perform()\n\nrbodyb22 &lt;- respb22 %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb22)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 3\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.80\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.82\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.94\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.90\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.71\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.59\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.52\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.64\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.95\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.11\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.01\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"8.14\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.85\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.92\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"7.79\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Field pH\"\n  .. .. .. ..$ precision : chr \"0.010000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"210.00\"\n  .. .. .. ..$ units     : chr \"pH\"\n  .. .. .. ..$ name      : chr \"Acidity/Alkalinity (pH)\"\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"STEAVENSON R @ FALLS\"\n  .. .. .. ..$ longitude : chr \"145.773503100\"\n  .. .. .. ..$ name      : chr \"STEAVENSON RIVER @ FALLS ROAD MARYSVILLE\"\n  .. .. .. ..$ latitude  : chr \"-37.525797590\"\n  .. .. .. ..$ org_name  : chr \"Victorian Rural Water Corporation\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 2: chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.738\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.736\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.734\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.741\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.755\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.739\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.735\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.759\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.745\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.742\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.757\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"405328\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nI wasn’t able to get discharge (140, 141) from a varlist. Double check\n\nbparamsd &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,140\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparamsd) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,140\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbd &lt;- req %&gt;% \n  req_body_json(bparamsd) %&gt;% \n  req_perform()\n\nrbodybd &lt;- respbd %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodybd)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nKisters has an example of asking for 140.01? Try that? No, just do the varfrom/varto method for discharge and stage.\n\nbparams &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,140.01\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 226\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"var_list\":\"100,140.01\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespb &lt;- req %&gt;% \n  req_body_json(bparams) %&gt;% \n  req_perform()\n\nrbodyb &lt;- respb %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodyb)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 2\n  .. .. .. ..$ 2 : chr \"Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction\"\n  .. .. .. ..$ 15: chr \"Minor editing. +/-11mm - 20mm drift correction\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.838\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.834\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.827\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.821\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.816\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.814\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.802\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.791\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.805\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.831\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.824\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.820\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.812\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.817\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 2\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n\n\nvarfrom-varto check. Does it give us both, or do we have to ask for the from separately`?\n\nbparamsft &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = barwon,\n                               \"start_time\" = \"20200101000000\",\n                               \"varfrom\" = \"100\",\n                               \"varto\" = \"140\", \n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparamsft) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"233217\",\"start_time\":\"20200101000000\",\"varfrom\":\"100\",\"varto\":\"140\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbft &lt;- req %&gt;% \n  req_body_json(bparamsft) %&gt;% \n  req_perform()\n\nrbodybft &lt;- respbft %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodybft)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ traces:List of 1\n  .. ..$ :List of 8\n  .. .. ..$ error_num      : int 0\n  .. .. ..$ compressed     : chr \"0\"\n  .. .. ..$ site_details   :List of 6\n  .. .. .. ..$ timezone  : chr \"10.0\"\n  .. .. .. ..$ short_name: chr \"BARWON @ GEELONG\"\n  .. .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. .. ..$ name      : chr \"BARWON RIVER @ GEELONG\"\n  .. .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. .. ..$ org_name  : chr \"Dept. Sustainability and Environment\"\n  .. .. ..$ quality_codes  :List of 1\n  .. .. .. ..$ 150: chr \"Rating extrapolated above 1.5x maximum flow gauged.\"\n  .. .. ..$ trace          :List of 15\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.18443\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.16332\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.12997\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.10202\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.08706\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07811\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07044\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.04782\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.02872\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07961\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.14628\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.11425\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.09740\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.07464\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. .. ..$ :List of 3\n  .. .. .. .. ..$ v: chr \"0.11269\"\n  .. .. .. .. ..$ t: num 2.02e+13\n  .. .. .. .. ..$ q: int 150\n  .. .. ..$ varfrom_details:List of 6\n  .. .. .. ..$ short_name: chr \"Water Level (m)\"\n  .. .. .. ..$ precision : chr \"0.001000\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"100.00\"\n  .. .. .. ..$ units     : chr \"metres\"\n  .. .. .. ..$ name      : chr \"Stream Water Level (m)\"\n  .. .. ..$ site           : chr \"233217\"\n  .. .. ..$ varto_details  :List of 6\n  .. .. .. ..$ short_name: chr \"Discharge (m3/sec)\"\n  .. .. .. ..$ precision : chr \"0.000010\"\n  .. .. .. ..$ subdesc   : chr \"Available for release\"\n  .. .. .. ..$ variable  : chr \"140.00\"\n  .. .. .. ..$ units     : chr \"cubic metres/second\"\n  .. .. .. ..$ name      : chr \"Stream Discharge (m3/s)\"\n\n\nThat looks like it might have only given us varto.\nWhat happens if we ask for a varto/from for a site that doesn’t have it? Earlier we saw that the varlist just skips, but does this?\n\nbparamse &lt;- list(\"function\" = 'get_ts_traces',\n               \"version\" = \"2\",\n               \"params\" = list(\"site_list\" = golf,\n                               \"start_time\" = \"20200101000000\",\n                               \"var_list\" = \"100,210\",\n                               \"interval\" = \"day\",\n                               \"datasource\" = \"A\", \n                               \"end_time\" = \"20200115000000\",\n                               \"data_type\" = \"mean\",\n                               \"multiplier\" = \"1\"))\n\nreq &lt;- request(vicurl)\n\nreq %&gt;% \n  req_body_json(bparamse) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 223\n\n{\"function\":\"get_ts_traces\",\"version\":\"2\",\"params\":{\"site_list\":\"405837\",\"start_time\":\"20200101000000\",\"var_list\":\"100,210\",\"interval\":\"day\",\"datasource\":\"A\",\"end_time\":\"20200115000000\",\"data_type\":\"mean\",\"multiplier\":\"1\"}}\n\nrespbe &lt;- req %&gt;% \n  req_body_json(bparamse) %&gt;% \n  req_perform()\n\nrbodybe &lt;- respbe %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbodybe)\n\nList of 2\n $ error_num: int 125\n $ error_msg: chr \"No data for specified variable in file\"\n\n\nOk, that errors. So I’ll need to capture and skip errors.\n\nunpack different formats\n(varlist with multiple, varfrom/to, etc). b1params (one param, one site), b2params (two params, varlist), b22params (two params, two sites- only one param at one site) bparamsft (varfrom/varto), bparamse (error),\nstart simple- grab errors. Should have been doing this all along.\n\ner1 &lt;- rbodyb1[1]\nere &lt;- rbodybe[1]\ner1\n\n$error_num\n[1] 0\n\nere\n\n$error_num\n[1] 125\n\n\nUnpack the single. Yeesh\nI’m ignoring quality codes for the moment. They’re a definition list, and so maybe should be unpacked separately, matched to varto (I think not varfrom), and then joined? But I want to see how they work with more complex structures first.\n\ns &lt;- as_tibble(rbodyb1[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %&gt;% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\nTry two variables. still works. still not sure why I need both varfrom and varto when they match.\n\ns &lt;- as_tibble(rbodyb2[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %&gt;% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\nTwo variables, two sites\n\ns &lt;- as_tibble(rbodyb22[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %&gt;% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\nAnd finally, the one where we do have a varto\n\ns &lt;- as_tibble(rbodybft[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. Not sure we need both varfrom and varto?\n  unnest_wider(col = varfrom_details) %&gt;% \n  rename_with(~(paste0('varfrom_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('varto_', .)), \n              c(short_name, precision, subdesc, variable, units, name)) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\nFinally, some new q values. and pretty clear we don’t need the varfroms.\nNeed to change the time column to dates\nDo we want to split up or return stacked or return wide? Make an option.\nDo we want to return NA days as NA or skip them?\n\n\nCleaning that up\nsafest is code x site x varto. though I think don’t need site, we’ll have expanded there by the time we get varto.\n\ns &lt;- as_tibble(rbodyb22[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% # complex set of lists\n  unnest_wider(col = site_details) %&gt;% # columns of info about the site\n  rename(site_name = name, site_short_name = short_name) %&gt;% \n  # there are name conflicts between site and varfrom and varto. \n  # and we can drop varfrom\n  select(-varfrom_details) %&gt;% \n  unnest_wider(col = varto_details) %&gt;% \n  rename_with(~(paste0('variable_', .)), \n              c(short_name, name)) \n\n# break in here to get the quality codes to match\nqc &lt;- s %&gt;% \n  select(quality_codes, site, variable) %&gt;% \n  unnest_longer(col = quality_codes) %&gt;% \n  mutate(quality_codes_id = as.integer(quality_codes_id))\n\n# finish unpacking\ns &lt;- s %&gt;%\n  select(-quality_codes) %&gt;% \n  unnest_longer(col = trace) %&gt;% \n  unnest_wider(col = trace)\n\n# clean up\ns &lt;- s %&gt;% \n  rename(value = v, time = t, quality_codes_id = q) %&gt;% \n  mutate(time = lubridate::ymd_hms(time)) %&gt;% \n  left_join(qc, by = c('quality_codes_id', 'site', 'variable')) %&gt;% \n  mutate(across(c(longitude, latitude, value), as.numeric)) # leaving some others because they either are names (gauges, variable) or display better (precision)\n\nCan I make that wide? Works without using id_cols but messy because too many info cols. Would end up being better to cut and join the info back on. But then I lose the quality codes, because they apply to each variable differently. Just return like this for now with a warning.\n\nsw &lt;- s %&gt;% pivot_wider(names_from = variable, values_from = value, id_cols = c(time, site))\n\nWhat I could do though is break it up into a list, potentially by sites and/or variables.\n\nslist &lt;- split(s, s$site)\nvlist &lt;- split(s, s$variable)\nsvlist &lt;- split(s, interaction(s$site, s$variable))\n\n\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\n\nOne site, one variable\n\nbs &lt;- get_ts_traces(site_list = barwon, datasource = 'A', var_list = '100', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nWarning: executing %dopar% sequentially: no parallel backend registered\n\n\nCan I pass decimals? it’s how they come out of get_variable_list\n\nbsdec &lt;- get_ts_traces(site_list = barwon, datasource = 'A', var_list = '100.00', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOne site, derived variables\n\nbsd &lt;- get_ts_traces(site_list = barwon, datasource = 'A', var_list = c('100', '140'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOnly derived\n\nbsod &lt;- get_ts_traces(site_list = barwon, datasource = 'A', var_list = '140', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nSome more variables, derived and not\n\nbsdv &lt;- get_ts_traces(site_list = barwon, datasource = 'A', var_list = c('100', '140', '210', '450'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nAnd multi-sites too- does it correctly collapse the vector?\n\nbsdvs &lt;- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nAnd finally everything including rain at golf. Careful though- does mean make sense for that? Probably better as a sum? Tried that and threw an error but told me the options:\nMean/Max/Min/Start/End/First/Last/Tot/MaxMin/Point/Cum\nDefinitely need two calls if need two different values at least for now- total temp is nonsense.\n\nbsdvs &lt;- get_ts_traces(site_list = allsites, \n                       datasource = 'A', \n                       var_list = c('10', '100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nand return a list\n\nbsdvsl &lt;- get_ts_traces(site_list = allsites, \n                       datasource = 'A', \n                       var_list = c('10', '100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'sxvlist')"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#a-variable-and-time-aware-version",
    "href": "vicwater/vicwater_testing.html#a-variable-and-time-aware-version",
    "title": "Testing VicWater API",
    "section": "A variable and time-aware version",
    "text": "A variable and time-aware version\n\npossibles &lt;- get_variable_list(site_list = allsites, datasource = 'A') %&gt;% \n  dplyr::select(site, datasource, variable, period_start, period_end)\n\n\nposs140 &lt;- possibles[possibles$variable == '100.00', ] \n\nposs141 &lt;- poss140\nposs140$variable &lt;- '140.00'\nposs141$variable &lt;- '141.00'\n\npossibles &lt;- bind_rows(possibles, poss140, poss141)\n\nall the tests above should run\n\nTest package version\n\ndevtools::load_all('C:/Users/Galen/Documents/vicwater')\n\nℹ Loading vicwater\n\n\nOne site, one variable\n\nbs &lt;- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '100', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nCan I pass decimals? it’s how they come out of get_variable_list\n\nbsdec &lt;- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '100.00', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOne site, derived variables\n\nbsd &lt;- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = c('100', '140', '141'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nOnly derived\n\nbsod &lt;- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = '140', start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nSome more variables, derived and not\n\nbsdv &lt;- get_ts_traces2(site_list = barwon, datasource = 'A', var_list = c('100', '140', '210', '450'), start_time = '20200101', end_time = '20200105', interval = 'day', data_type = 'mean', multiplier = 1, returnformat = 'df')\n\nAnd multi-sites too- does it correctly collapse the vector?\n\nbsdvs &lt;- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nDo the ‘all’ settings work? Let’s bump to year so I don’t have so much data\n\nbsdvs &lt;- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = \"all\", \n                       start_time = \"all\", \n                       end_time = \"all\", \n                       interval = 'year', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nWarning: `var_list = 'all'` is *very* dangerous, since it applies the same\n`data_type` to all variables, which is rarely appropriate. Check the variables\navailable for your sites and make sure you want to do this.\n\n\nCan I throw something wrong to interval to see if it tells me what it can do? Kisters says\nyear, month, day, hour, minute, second,\nperiod,\ndefault\n\nbiw &lt;- get_ts_traces2(site_list = barwon, \n                       datasource = 'A', \n                       var_list = \"100\", \n                       start_time = \"20200101\", \n                       end_time = \"20211231\", \n                       interval = 'eon', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df')\n\nerror is Invalid interval, must be YEAR, MONTH, DAY, HOUR, MINUTE or SECOND"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#benchmark",
    "href": "vicwater/vicwater_testing.html#benchmark",
    "title": "Testing VicWater API",
    "section": "Benchmark",
    "text": "Benchmark\nThis likely varies a lot depending on what I’m asking for. Should be done more systematically, and use microbenchmark.\nThey should be roughly the same for a single?\n\nsystem.time(b1 &lt;- get_ts_traces(site_list = barwon, \n                       datasource = 'A', \n                       var_list = '100', \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.10    0.00    0.86 \n\nsystem.time(b2 &lt;- get_ts_traces2(site_list = barwon, \n                       datasource = 'A', \n                       var_list = '100', \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.17    0.01    1.50 \n\n\ninteresting. so the second is faster locally, but higher network, I think.\n\nsystem.time(bsdvs1 &lt;- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.16    0.02    1.71 \n\nsystem.time(bsdvs2 &lt;- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.25    0.01    5.69 \n\n\nOof. That’s pretty bad. Can I speed it up? probably.\nHow about parallel?\n\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\nWarning: package 'foreach' was built under R version 4.2.2\n\n\nLoading required package: future\n\n\nWarning: package 'future' was built under R version 4.2.2\n\nregisterDoFuture()\nplan(multisession)\n\nsystem.time(bsdvs1p &lt;- get_ts_traces(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.04    0.00    2.81 \n\nsystem.time(bsdvs2p &lt;- get_ts_traces2(site_list = c(barwon, steavenson), \n                       datasource = 'A', \n                       var_list = c('100', '140', '210', '450'), \n                       start_time = '20200101', end_time = '20200105', \n                       interval = 'day', data_type = 'mean', \n                       multiplier = 1, returnformat = 'df'))\n\n   user  system elapsed \n   0.06    0.00    3.67"
  },
  {
    "objectID": "vicwater/vicwater_testing.html#get-db-info",
    "href": "vicwater/vicwater_testing.html#get-db-info",
    "title": "Testing VicWater API",
    "section": "Get db info",
    "text": "Get db info\n\nGeofiltering\nThe get_db_info API function can do a LOT. I’m primarily looking for using it for geofiltering, but it may turn out that that’s better done in other ways anyway. The filter_values sitelist_filter, etc are likely also useful, and will be worth adding as we go.\nThe area from Teesdale (-38, 144 at top left to Leopold (-38.2, 144.5) should contain 5 sites.\nOh no it doesn’t. It actually contains a bazillion, because there’s a bunch of stntype: \"GW\". Turning eval: false to we don’t do that again.\n\n# need a matrix to get the double brackets.\ntopleft &lt;- c('-38', '144')\nbottomright &lt;- c('-38.2', '144.5')\n\nrectbox &lt;- rbind(topleft, bottomright)\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nSo, that is WAAY too much, so we need to filter by surface water station types. Assuming the structure is the same, let’s figure out how to parse that into a table, and then get the stntype that matches surface and develop that capacity. And document how to choose which.\nAnd use the field_list to not return all the ‘category’ nonsense.\n\ndb &lt;- as_tibble(rbody_geo[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% \n  select(station, stntype, stname, everything(), -starts_with('category'))\n\nSo, what are the possible stntypes?\n\ntable(db$stntype)\n\nWhat are those? GW has got to be groundwater. HYD is Hydrology, presumably??? SOB I think is the State Observation Bore Network. VIR???.\nLet’s knock off the GW and see if we can figure the others out.\n\n##| rows.print: 30\ndbNoGW &lt;- db %&gt;% \n  filter(stntype != 'GW') %&gt;% \n  select(station, stntype, stname, shortname, parent, commence, cease, everything())\n\ndbNoGW\n\nSo, the SOB don’t have any info, the HYD seem to be A and B versions of the VIR. Do they have different data??? If we get traces for 233217A and B, do they differ from 233217?\nLet’s clean that up a bit first before figuring that out.\n\n##| rows.print = 30\ndbNoGW &lt;- dbNoGW %&gt;% \n  filter(stntype != 'SOB') %&gt;% \n  arrange(station)\ndbNoGW\n\nInteresting.\nSo, some things to do\nTODO-\n\nsort out the filters (filter_values I think?) to only get HYD and VIR (or whatever the user wants)\n\nand tell the user what the options are- apparently, ‘GW’, ‘SOB’, ‘HYD’, and ‘VIR’\n\nsort out field_list to only get a subset of useful fields\n\nAnd alert the user about what fields are being dropped\n\nfigure out whether the data differs between HYD and VIR (should be able to just pass the A,B, numbers to the get_ts_traces)?\nOther geo filters\n\n\n\nOther filtering\nTry filter_values and field_list. \"filter_values\" = list(\"stntype\" = 'HYD, VIR') does not work- figure out why.\n\ntopleft &lt;- c('-38', '144')\nbottomright &lt;- c('-38.2', '144.5')\n\nrectbox &lt;- rbind(topleft, bottomright)\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"stntype\" = 'VIR'),\n                               \"field_list\" = \"station, stntype, stname, shortname, commence, cease, active, northing, easting, longitude, latitude, lldatum\",\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 313\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"stntype\":\"VIR\"},\"field_list\":\"station, stntype, stname, shortname, commence, cease, active, northing, easting, longitude, latitude, lldatum\",\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 6\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5780762.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"MOORABOOL @ BATESFOR\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"MOORABOOL RIVER @ BATESFORD\"\n  .. .. ..$ category9 : chr \"YES\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"0\"\n  .. .. ..$ category4 : chr \"50\"\n  .. .. ..$ category3 : chr \"S/U\"\n  .. .. ..$ category2 : chr \"V_93F3\"\n  .. .. ..$ category1 : chr \"0\"\n  .. .. ..$ elevacc   : chr \"9\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19080101\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"03 5276 1201\"\n  .. .. ..$ spare1    : chr \"Derwent Hotel\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"232\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.278224920\"\n  .. .. ..$ comment   : chr \"\\r\\n\\r\\n\\r\\nDerwent Hotel BatesfordFrom Geelong West, head west along Ballarat Highway to Batesford. Just befor\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"232202\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Batesford\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"29.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.089311520\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"261303.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"\"\n  .. .. ..$ active    : logi FALSE\n  .. .. ..$ northing  : chr \"5779117.700\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"WASTEWATER DR @ FORD\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ category9 : chr \"\"\n  .. .. ..$ category8 : chr \"\"\n  .. .. ..$ category7 : chr \"\"\n  .. .. ..$ category6 : chr \"\"\n  .. .. ..$ category5 : chr \"\"\n  .. .. ..$ category4 : chr \"\"\n  .. .. ..$ category3 : chr \"\"\n  .. .. ..$ category2 : chr \"\"\n  .. .. ..$ category1 : chr \"\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"N\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 20060630\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEELONG\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"\"\n  .. .. ..$ posacc    : chr \"1\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"232\"\n  .. .. ..$ grdatum   : chr \"ANG\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.355800000\"\n  .. .. ..$ comment   : chr \"\\r\\nStation operated for Ford Aus. Project No 1563532011/07/17 Virtual site entry generated by DSEVIRTUALSITE.H\"| __truncated__\n  .. .. ..$ lldatum   : chr \"AGD66\"\n  .. .. ..$ station   : chr \"232711\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"VRW\"\n  .. .. ..$ barcode   : chr \"\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 20071029\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.105900000\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"N\"\n  .. .. ..$ easting   : chr \"268159.900\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5774521.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON POLLOCKSFORD\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"B\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"4WD\"\n  .. .. ..$ category5 : chr \"50\"\n  .. .. ..$ category4 : chr \"150\"\n  .. .. ..$ category3 : chr \"GRAVEL\"\n  .. .. ..$ category2 : chr \"V_93D4\"\n  .. .. ..$ category1 : chr \"18\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19060701\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"N/A\"\n  .. .. ..$ spare1    : chr \"N/A\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.187100410\"\n  .. .. ..$ comment   : chr \"\\r\\nFrom Inverliegh, travel east along Hamilton Highway. Turn south down Pollocksford Road and travel 3.1 km to\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233200\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Moriac\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.143395970\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"253492.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5779017.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"LEIGH @ INVERLEIGH\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"20\"\n  .. .. ..$ category4 : chr \"100\"\n  .. .. ..$ category3 : chr \"SEALED\"\n  .. .. ..$ category2 : chr \"V_93B3\"\n  .. .. ..$ category1 : chr \"1\"\n  .. .. ..$ elevacc   : chr \"9\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"N\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19460315\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"N/A\"\n  .. .. ..$ spare1    : chr \"N/A\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1101\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.062212190\"\n  .. .. ..$ comment   : chr \"\\r\\nIt is underneath the Hamilton Highway Bridge that passes over the Leigh River.2011/07/17 Virtual site entry\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233209\"\n  .. .. ..$ datemod   : int 20220427\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Inverleigh\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"64.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.099828090\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"n\"\n  .. .. ..$ easting   : chr \"242392.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"THIESS\"\n  .. .. ..$ active    : logi TRUE\n  .. .. ..$ northing  : chr \"5772691.000\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON @ GEELONG\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ category9 : chr \"N/A\"\n  .. .. ..$ category8 : chr \"G\"\n  .. .. ..$ category7 : chr \"G\"\n  .. .. ..$ category6 : chr \"2WD\"\n  .. .. ..$ category5 : chr \"0\"\n  .. .. ..$ category4 : chr \"150\"\n  .. .. ..$ category3 : chr \"SEALED\"\n  .. .. ..$ category2 : chr \"V_93G4\"\n  .. .. ..$ category1 : chr \"0\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"Y\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19601118\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEE/SW\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"BW\"\n  .. .. ..$ posacc    : chr \"9\"\n  .. .. ..$ timemod   : int 1359\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"UTM\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.346892190\"\n  .. .. ..$ comment   : chr \"\\r\\n\\r\\n\\r\\nBarwon Water flood monitoring stationFrom the intersection of the Fyans St and La Trobe Terrace, he\"| __truncated__\n  .. .. ..$ lldatum   : chr \"WGS84\"\n  .. .. ..$ station   : chr \"233217\"\n  .. .. ..$ datemod   : int 20220513\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"DSE\"\n  .. .. ..$ barcode   : chr \"Geelong\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 18991230\n  .. .. ..$ local_map : chr \"GEELONG\"\n  .. .. ..$ latitude  : chr \"-38.163605590\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"Y\"\n  .. .. ..$ easting   : chr \"267562.000\"\n  .. .. ..$ stntype   : chr \"VIR\"\n  .. ..$ :List of 62\n  .. .. ..$ category20: chr \"\"\n  .. .. ..$ category19: chr \"\"\n  .. .. ..$ category18: chr \"\"\n  .. .. ..$ category17: chr \"\"\n  .. .. ..$ category16: chr \"\"\n  .. .. ..$ category15: chr \"\"\n  .. .. ..$ category14: chr \"\"\n  .. .. ..$ category13: chr \"\"\n  .. .. ..$ category12: chr \"\"\n  .. .. ..$ category11: chr \"\"\n  .. .. ..$ category10: chr \"\"\n  .. .. ..$ active    : logi FALSE\n  .. .. ..$ northing  : chr \"5777388.300\"\n  .. .. ..$ timezone  : chr \"10.0\"\n  .. .. ..$ shortname : chr \"BARWON RIVER\"\n  .. .. ..$ datecreate: int 18991230\n  .. .. ..$ elevdatum : chr \"\"\n  .. .. ..$ stname    : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ category9 : chr \"\"\n  .. .. ..$ category8 : chr \"\"\n  .. .. ..$ category7 : chr \"\"\n  .. .. ..$ category6 : chr \"\"\n  .. .. ..$ category5 : chr \"\"\n  .. .. ..$ category4 : chr \"\"\n  .. .. ..$ category3 : chr \"\"\n  .. .. ..$ category2 : chr \"\"\n  .. .. ..$ category1 : chr \"\"\n  .. .. ..$ elevacc   : chr \"1\"\n  .. .. ..$ dbver47   : logi FALSE\n  .. .. ..$ quarter   : chr \"N\"\n  .. .. ..$ section   : int 0\n  .. .. ..$ commence  : int 19680208\n  .. .. ..$ parent    : chr \"\"\n  .. .. ..$ mapname   : chr \"GEELONG\"\n  .. .. ..$ meridian  : chr \"\"\n  .. .. ..$ spare5    : chr \"\"\n  .. .. ..$ spare4    : chr \"\"\n  .. .. ..$ spare3    : chr \"\"\n  .. .. ..$ spare2    : chr \"\"\n  .. .. ..$ spare1    : chr \"\"\n  .. .. ..$ posacc    : chr \"1\"\n  .. .. ..$ timemod   : int 1558\n  .. .. ..$ region    : chr \"233\"\n  .. .. ..$ grdatum   : chr \"\"\n  .. .. ..$ township  : chr \"\"\n  .. .. ..$ longitude : chr \"144.150000000\"\n  .. .. ..$ comment   : chr \"2011/07/17 Virtual site entry generated by DSEVIRTUALSITE.HSC from details in 233219A\\r\\n\"\n  .. .. ..$ lldatum   : chr \"\"\n  .. .. ..$ station   : chr \"233219\"\n  .. .. ..$ datemod   : int 20141001\n  .. .. ..$ timecreate: int 0\n  .. .. ..$ orgcode   : chr \"VRW\"\n  .. .. ..$ barcode   : chr \"\"\n  .. .. ..$ zone      : int 55\n  .. .. ..$ elev      : chr \"0.000\"\n  .. .. ..$ cease     : int 19690911\n  .. .. ..$ local_map : chr \"QUEENSCLIFF\"\n  .. .. ..$ latitude  : chr \"-38.116666670\"\n  .. .. ..$ range     : chr \"\"\n  .. .. ..$ qquarter  : chr \"N\"\n  .. .. ..$ easting   : chr \"250148.900\"\n  .. .. ..$ stntype   : chr \"VIR\"\n\n\ncleanup\n\ndb &lt;- as_tibble(rbody_geo[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% \n  select(station, stntype, stname, everything(), -starts_with('category'))\ndb\n\n# A tibble: 6 × 42\n  station stntype stname  active north…¹ timez…² short…³ datec…⁴ elevd…⁵ elevacc\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;lgl&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  \n1 232202  VIR     MOORAB… TRUE   578076… 10.0    MOORAB…  1.90e7 \"\"      9      \n2 232711  VIR     WASTEW… FALSE  577911… 10.0    WASTEW…  1.90e7 \"\"      1      \n3 233200  VIR     BARWON… TRUE   577452… 10.0    BARWON…  1.90e7 \"\"      1      \n4 233209  VIR     LEIGH … TRUE   577901… 10.0    LEIGH …  1.90e7 \"\"      9      \n5 233217  VIR     BARWON… TRUE   577269… 10.0    BARWON…  1.90e7 \"\"      1      \n6 233219  VIR     BARWON… FALSE  577738… 10.0    BARWON…  1.90e7 \"\"      1      \n# … with 32 more variables: dbver47 &lt;lgl&gt;, quarter &lt;chr&gt;, section &lt;int&gt;,\n#   commence &lt;int&gt;, parent &lt;chr&gt;, mapname &lt;chr&gt;, meridian &lt;chr&gt;, spare5 &lt;chr&gt;,\n#   spare4 &lt;chr&gt;, spare3 &lt;chr&gt;, spare2 &lt;chr&gt;, spare1 &lt;chr&gt;, posacc &lt;chr&gt;,\n#   timemod &lt;int&gt;, region &lt;chr&gt;, grdatum &lt;chr&gt;, township &lt;chr&gt;,\n#   longitude &lt;chr&gt;, comment &lt;chr&gt;, lldatum &lt;chr&gt;, datemod &lt;int&gt;,\n#   timecreate &lt;int&gt;, orgcode &lt;chr&gt;, barcode &lt;chr&gt;, zone &lt;int&gt;, elev &lt;chr&gt;,\n#   cease &lt;int&gt;, local_map &lt;chr&gt;, latitude &lt;chr&gt;, range &lt;chr&gt;, …\n\n\n\nnames(db)\n\n [1] \"station\"    \"stntype\"    \"stname\"     \"active\"     \"northing\"  \n [6] \"timezone\"   \"shortname\"  \"datecreate\" \"elevdatum\"  \"elevacc\"   \n[11] \"dbver47\"    \"quarter\"    \"section\"    \"commence\"   \"parent\"    \n[16] \"mapname\"    \"meridian\"   \"spare5\"     \"spare4\"     \"spare3\"    \n[21] \"spare2\"     \"spare1\"     \"posacc\"     \"timemod\"    \"region\"    \n[26] \"grdatum\"    \"township\"   \"longitude\"  \"comment\"    \"lldatum\"   \n[31] \"datemod\"    \"timecreate\" \"orgcode\"    \"barcode\"    \"zone\"      \n[36] \"elev\"       \"cease\"      \"local_map\"  \"latitude\"   \"range\"     \n[41] \"qquarter\"   \"easting\"   \n\n\nThat seems to have worked for filter vaues but not field list. and it doesn’t work for filtering to multiple values. Figure both those things out.\nHow about c() on both. Shorten the field list for the moment. Runs but no data.\nc() on the field list works for a single stntype. Now to figure out the stntypes\n\ntopleft &lt;- c('-38', '144')\nbottomright &lt;- c('-38.2', '144.5')\n\nrectbox &lt;- rbind(topleft, bottomright)\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"filter_values\" = list(\"stntype\" = 'HYD'),\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 232\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"filter_values\":{\"stntype\":\"HYD\"},\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 12\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"MOORABOOL RIVER AT BATESFORD\"\n  .. .. ..$ station: chr \"232202A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ station: chr \"232711A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ station: chr \"233209A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217C\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217D\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ station: chr \"233219A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Waurn Ponds Creek @ Brearly Reserve\"\n  .. .. ..$ station: chr \"233221A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Native Hut Creek @ Turtle Bend Park Teesdale\"\n  .. .. ..$ station: chr \"233242A\"\n  .. .. ..$ stntype: chr \"HYD\"\n\n\n\ndb &lt;- as_tibble(rbody_geo[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list)) %&gt;% \n  select(station, stntype, stname, everything(), -starts_with('category'))\ndb\n\n# A tibble: 12 × 3\n   station stntype stname                                      \n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                                       \n 1 232202A HYD     MOORABOOL RIVER AT BATESFORD                \n 2 232711A HYD     WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG \n 3 233200A HYD     BARWON RIVER @ POLLOCKSFORD                 \n 4 233200B HYD     BARWON RIVER @ POLLOCKSFORD                 \n 5 233209A HYD     LEIGH RIVER @ INVERLEIGH                    \n 6 233217A HYD     BARWON RIVER @ GEELONG                      \n 7 233217B HYD     BARWON RIVER @ GEELONG                      \n 8 233217C HYD     BARWON RIVER @ GEELONG                      \n 9 233217D HYD     BARWON RIVER @ GEELONG                      \n10 233219A HYD     BARWON RIVER @ MURGHEBOLUC                  \n11 233221A HYD     Waurn Ponds Creek @ Brearly Reserve         \n12 233242A HYD     Native Hut Creek @ Turtle Bend Park Teesdale\n\n\n\n\nTrying for the stntype\nTried to use sitelist_filter = \"GROUP(PLUVIO_STATIONS), which qld made look like would work, but nothing. maybe get_groups is a way to figure out what the groups are?\n\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"sitelist_filter\" = \"GROUP(PLUVIO_STATIONS)\",\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 241\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"sitelist_filter\":\"GROUP(PLUVIO_STATIONS)\",\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows: list()\n\n\n\n\nAside- get_groups\nCan we use hydstra groupings, as defined in the db? What are those groupings?\n\ngrp_params &lt;- list(\"function\" = 'get_groups',\n               \"version\" = \"1\",\n               \"params\" = list(\"site_list\" = allsites))\n\nreqvic %&gt;% \n  req_body_json(grp_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 95\n\n{\"function\":\"get_groups\",\"version\":\"1\",\"params\":{\"site_list\":\"233217, 405328, 405331, 405837\"}}\n\nresp_grp &lt;- reqvic %&gt;% \n  req_body_json(grp_params) %&gt;% \n  req_perform()\n\nrbody_grp &lt;- resp_grp %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_grp)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 31\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"to be at the top of the list\"\n  .. ..$ group       : chr \"AB\"\n  .. ..$ value       : chr \"CTS\"\n  .. ..$ value_decode: chr \"list of site A files that have changed in the last 24hrs\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"to be at the top of the list\"\n  .. ..$ group       : chr \"AB\"\n  .. ..$ value       : chr \"FLOOD\"\n  .. ..$ value_decode: chr \"All VIR Sites for Flood Zoom\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"10PLUS\"\n  .. ..$ value_decode: chr \"sites with records longer than 10 years\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"30PLUS\"\n  .. ..$ value_decode: chr \"sites with records longer than 30 years\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"ARCHIVE\"\n  .. ..$ value_decode: chr \"Archive files\"\n  .. ..$ stations    :List of 4\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"FLOWA\"\n  .. ..$ value_decode: chr \"All sites with flow (Vir and Hyd)\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"LEVEL\"\n  .. ..$ value_decode: chr \"All sites with level (100.00)\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"All sites in archive\"\n  .. ..$ group       : chr \"ALLA\"\n  .. ..$ value       : chr \"VFLOW\"\n  .. ..$ value_decode: chr \"Virtual sites with flow\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Goups for the CMA areas\"\n  .. ..$ group       : chr \"CMA\"\n  .. ..$ value       : chr \"CCMA\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"daily on change generated site lists\"\n  .. ..$ group       : chr \"HYVIRTUAL\"\n  .. ..$ value       : chr \"TS\"\n  .. ..$ value_decode: chr \"list of site A files that have changed in the last 24hrs\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Telemetry Sites\"\n  .. ..$ group       : chr \"TELEMETRY\"\n  .. ..$ value       : chr \"RAWDATA\"\n  .. ..$ stations    :List of 4\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Yukkie\"\n  .. ..$ group       : chr \"WDTF\"\n  .. ..$ value       : chr \"LEVEL\"\n  .. ..$ value_decode: chr \"All sites with level (100.00)\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Yukkie\"\n  .. ..$ group       : chr \"WDTF\"\n  .. ..$ value       : chr \"RAIN\"\n  .. ..$ value_decode: chr \"All rain monitoring sites\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Yukkie\"\n  .. ..$ group       : chr \"WDTF\"\n  .. ..$ value       : chr \"WQSITES\"\n  .. ..$ value_decode: chr \"all sites that have WQ in sample table\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"CMA\"\n  .. ..$ group       : chr \"WEB_GW_CMA\"\n  .. ..$ value       : chr \"CMA01\"\n  .. ..$ value_decode: chr \"Corangamite CMA\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"CMA\"\n  .. ..$ group       : chr \"WEB_GW_CMA\"\n  .. ..$ value       : chr \"CMA04\"\n  .. ..$ value_decode: chr \"Goulburn CMA\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"RWC\"\n  .. ..$ group       : chr \"WEB_GW_RWC\"\n  .. ..$ value       : chr \"GMW\"\n  .. ..$ value_decode: chr \"Goulburn Murray Water\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"RWC\"\n  .. ..$ group       : chr \"WEB_GW_RWC\"\n  .. ..$ value       : chr \"SRW\"\n  .. ..$ value_decode: chr \"Southern Rural Water\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites\"\n  .. ..$ group       : chr \"WEB_SW\"\n  .. ..$ value       : chr \"B_233_BARWON\"\n  .. ..$ value_decode: chr \"233-Barwon Basin\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites\"\n  .. ..$ group       : chr \"WEB_SW\"\n  .. ..$ value       : chr \"B_405_GOULBURN\"\n  .. ..$ value_decode: chr \"405-Goulburn Basin\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites - active telem only\"\n  .. ..$ group       : chr \"WEB_SW_TELEM\"\n  .. ..$ value       : chr \"B_233_BARWON\"\n  .. ..$ value_decode: chr \"233-Barwon Basin\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"Surface Water Sites - active telem only\"\n  .. ..$ group       : chr \"WEB_SW_TELEM\"\n  .. ..$ value       : chr \"B_405_GOULBURN\"\n  .. ..$ value_decode: chr \"405-Goulburn Basin\"\n  .. ..$ stations    :List of 2\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 4\n  .. ..$ group_decode: chr \"Test Sites\"\n  .. ..$ group       : chr \"WEB_TEST\"\n  .. ..$ value       : chr \"WEB_TEST\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites\"\n  .. ..$ group       : chr \"WMIS2\"\n  .. ..$ value       : chr \"RF\"\n  .. ..$ value_decode: chr \"Rainfall\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites\"\n  .. ..$ group       : chr \"WMIS2\"\n  .. ..$ value       : chr \"SW\"\n  .. ..$ value_decode: chr \"Surface Water\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites\"\n  .. ..$ group       : chr \"WMIS2\"\n  .. ..$ value       : chr \"WQ\"\n  .. ..$ value_decode: chr \"Water Quality\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites version 2\"\n  .. ..$ group       : chr \"WMIS2_2\"\n  .. ..$ value       : chr \"RF\"\n  .. ..$ value_decode: chr \"Rainfall\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"405837\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites version 2\"\n  .. ..$ group       : chr \"WMIS2_2\"\n  .. ..$ value       : chr \"SW\"\n  .. ..$ value_decode: chr \"Surface Water\"\n  .. ..$ stations    :List of 3\n  .. .. ..$ : chr \"233217\"\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405331\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 Sites version 2\"\n  .. ..$ group       : chr \"WMIS2_2\"\n  .. ..$ value       : chr \"WQ\"\n  .. ..$ value_decode: chr \"Water Quality\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 CMAs\"\n  .. ..$ group       : chr \"WMIS2_CMA\"\n  .. ..$ value       : chr \"CMA01\"\n  .. ..$ value_decode: chr \"Corangamite CMA\"\n  .. ..$ stations    :List of 1\n  .. .. ..$ : chr \"233217\"\n  ..$ :List of 5\n  .. ..$ group_decode: chr \"WMIS2 CMAs\"\n  .. ..$ group       : chr \"WMIS2_CMA\"\n  .. ..$ value       : chr \"CMA04\"\n  .. ..$ value_decode: chr \"Goulburn CMA\"\n  .. ..$ stations    :List of 2\n  .. .. ..$ : chr \"405328\"\n  .. .. ..$ : chr \"405837\"\n\n\nUnpack\n\ndb &lt;- as_tibble(rbody_grp[2]) %&gt;% # the [2] drops the error column\n  unnest_wider(col = where(is.list)) %&gt;% # a `return` list\n  unnest_longer(col = where(is.list))\n\nDrop the stations- what are the groups?\n\ndb %&gt;% select(-stations) %&gt;% distinct()\n\n# A tibble: 31 × 4\n   group_decode                         group     value   value_decode          \n   &lt;chr&gt;                                &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                 \n 1 to be at the top of the list         AB        CTS     list of site A files …\n 2 to be at the top of the list         AB        FLOOD   All VIR Sites for Flo…\n 3 All sites in archive                 ALLA      10PLUS  sites with records lo…\n 4 All sites in archive                 ALLA      30PLUS  sites with records lo…\n 5 All sites in archive                 ALLA      ARCHIVE Archive files         \n 6 All sites in archive                 ALLA      FLOWA   All sites with flow (…\n 7 All sites in archive                 ALLA      LEVEL   All sites with level …\n 8 All sites in archive                 ALLA      VFLOW   Virtual sites with fl…\n 9 Goups for the CMA areas              CMA       CCMA    &lt;NA&gt;                  \n10 daily on change generated site lists HYVIRTUAL TS      list of site A files …\n# … with 21 more rows\n\n\nNo obvious groupings there to select on except maybe surface water sites, but that drops rainfall.\n\n\nHow about sitelist_filter?\nwith one site\n\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"sitelist_filter\" = \"233217\",\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 225\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"sitelist_filter\":\"233217\",\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 1\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217\"\n  .. .. ..$ stntype: chr \"VIR\"\n\n\nwith multiple sites, it just returns the first one. And using c(\"site1\", \"site2\") errors out.\n\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"sitelist_filter\" = \"233217, 405331\",\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 233\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"sitelist_filter\":\"233217, 405331\",\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 1\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217\"\n  .. .. ..$ stntype: chr \"VIR\"\n\n\n\n\nusing complex_filter\nis this easier or harder than just running it twice (or however many times)\nIt’s more flexible, but pretty terrible to sort out. And absurdly slow.\n\ngeo_params &lt;- list(\"function\" = 'get_db_info',\n               \"version\" = \"3\",\n               \"params\" = list(\"table_name\" = \"site\",\n                               \"return_type\" = \"array\",\n                               \"complex_filter\" = list(list('fieldname' = 'stntype',\n                                                         'value' = \"HYD\",\n                                                         'operator' = 'EQ'),\n                                                    list('combine' = 'OR',\n                                                         'fieldname' = 'stntype',\n                                                         'value' = \"VIR\",\n                                                         'operator' = 'EQ')),\n                               \"field_list\" = c(\"station\", \"stntype\", \"stname\"),\n                               \"geo_filter\" = list('rectangle' = rectbox)))\n\nreqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_dry_run()\n\nPOST /cgi/webservice.exe? HTTP/1.1\nHost: data.water.vic.gov.au\nUser-Agent: httr2/0.2.2 r-curl/4.3.3 libcurl/7.64.1\nAccept: */*\nAccept-Encoding: deflate, gzip\nContent-Type: application/json\nContent-Length: 340\n\n{\"function\":\"get_db_info\",\"version\":\"3\",\"params\":{\"table_name\":\"site\",\"return_type\":\"array\",\"complex_filter\":[{\"fieldname\":\"stntype\",\"value\":\"HYD\",\"operator\":\"EQ\"},{\"combine\":\"OR\",\"fieldname\":\"stntype\",\"value\":\"VIR\",\"operator\":\"EQ\"}],\"field_list\":[\"station\",\"stntype\",\"stname\"],\"geo_filter\":{\"rectangle\":[[\"-38\",\"144\"],[\"-38.2\",\"144.5\"]]}}}\n\nresp_geo &lt;- reqvic %&gt;% \n  req_body_json(geo_params) %&gt;% \n  req_perform()\n\nrbody_geo &lt;- resp_geo %&gt;% resp_body_json(check_type = FALSE)\n\nstr(rbody_geo)\n\nList of 2\n $ error_num: int 0\n $ return   :List of 1\n  ..$ rows:List of 18\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"MOORABOOL RIVER @ BATESFORD\"\n  .. .. ..$ station: chr \"232202\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"MOORABOOL RIVER AT BATESFORD\"\n  .. .. ..$ station: chr \"232202A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ station: chr \"232711\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"WASTEWATER O/F DRAIN @ FORD FACTORY GEELONG\"\n  .. .. ..$ station: chr \"232711A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ POLLOCKSFORD\"\n  .. .. ..$ station: chr \"233200B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ station: chr \"233209\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"LEIGH RIVER @ INVERLEIGH\"\n  .. .. ..$ station: chr \"233209A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217B\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217C\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ GEELONG\"\n  .. .. ..$ station: chr \"233217D\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ station: chr \"233219\"\n  .. .. ..$ stntype: chr \"VIR\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"BARWON RIVER @ MURGHEBOLUC\"\n  .. .. ..$ station: chr \"233219A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Waurn Ponds Creek @ Brearly Reserve\"\n  .. .. ..$ station: chr \"233221A\"\n  .. .. ..$ stntype: chr \"HYD\"\n  .. ..$ :List of 3\n  .. .. ..$ stname : chr \"Native Hut Creek @ Turtle Bend Park Teesdale\"\n  .. .. ..$ station: chr \"233242A\"\n  .. .. ..$ stntype: chr \"HYD\"\n\n\nthat seems to have worked. Unpack- remember we’ve used a field_list here so it’s simpler and we don’t need to have the select we had above. In the package we should probably allow doing that both ways.\n\ndb &lt;- as_tibble(rbody_geo[2]) %&gt;% # the [2] drops the error column\n  unnest_longer(col = where(is.list)) %&gt;% # a `return` list\n  unnest_wider(col = where(is.list))\n\nTODO clean that up into a function and include. And test where we know there’s rain gauges- am I losing those with the stntypes I’ve gone with?\nallow field_list\nand geo_filter- all three kinds\nhow to do stntype- could have one option that allows actually directly passing lists to complex filter (or filter_values).\nOR (and?) could have an option that builds that list for the user somehow. would need to limit it to a few common things like stntype. *could I do this by saying if filter_values gets a c() of things for any of its keys, build an OR complex_filter. ie filter_values can only have one stntype. So if it gets c(stntype1, stntype2), just short-circuit and use complex instead. Would need to be careful if user passes more than one filtering thing in. In that case, just make them build the complex filter themselves. maybe provide a test_complex_filter function that just returns req_dry_run?"
  },
  {
    "objectID": "website_notes/multiple_render_formats.html",
    "href": "website_notes/multiple_render_formats.html",
    "title": "Multiple render formats",
    "section": "",
    "text": "It’s typically nicest to work with Quarto rendered to html. But sometimes we need other formats (collaborators, publishing, etc). We can render to different formats individually (e.g. word), but we can also produce multiple formats from the same qmd.\nThis multi-render is really nice, in that it provides the html with a link to download the others (word, pdf, etc- see top right of this page). It’s very close to the new Manuscripts project type that is coming in Quarto 1.4, though that keeps breaking for me. As an example, this (the header for this file) will produce an html, docx, and pdf, with links to the docx and pdf files at the top right.\nformat:\n  html:\n    toc: true\n    comments:\n      hypothesis: true\n  docx:\n    toc: true\n    toc-depth: 2\n    prefer-html: true\n    # setting these to the html defaults so they don't get jumbled between format\n    fig-width: 7\n    fig-height: 5\n  pdf: \n    toc: true\n    colorlinks: true\nA nice thing about this approach is that it lets a single page on a website have downloadable word/pdf files, whereas I couldn’t get the manuscript to work as a subset of a website (though I didn’t try very hard). Manuscripts do look like they’ll be nicer for author info and formatting, but I kept breaking the development version, so I’ll try that again later.\nI’ve also turned on Hypothes.is comments for this page, dealt with in more detail elsewhere. The short story is you can make annotations and highlights on this page, which can be public or private, depending on your settings.\nJust to make that fancier across outputs, I’ll throw a figure on and maybe some math\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt(2\\pi)}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\\]\n\nggplot2::ggplot(iris, ggplot2::aes(x = Sepal.Length, \n                                   y = Petal.Length, \n                                   color = Species)) +\n  ggplot2::geom_point()"
  },
  {
    "objectID": "website_notes/quarto_images_misc.html",
    "href": "website_notes/quarto_images_misc.html",
    "title": "Images, divs, and similar",
    "section": "",
    "text": "To include an image with a link, use the usual image insertion code, e.g. this is not clickable\n![Surus](../_images/surus.png)\n\n\n\nSurus\n\n\nbut modified with square brackets and parentheses similarly to text links makes it clickable.\n[![Surus](../_images/surus.png)](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\n\n\nSurus\n\n\nIf we want to adjust the size, it needs to go in the square brackets, and not have spaces.\n[![Surus](../_images/surus.png){width=200}](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\n\n\nSurus\n\n\nWe can also use a figure div to make it cross-referenceable.\n::: {#fig-elephant}\n\n[![Surus](../_images/surus.png)](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\nSurus\n\n:::\n\n\n\n\n\nSurus\n\n\nFigure 1: Surus"
  },
  {
    "objectID": "website_notes/quarto_images_misc.html#links-from-images",
    "href": "website_notes/quarto_images_misc.html#links-from-images",
    "title": "Images, divs, and similar",
    "section": "",
    "text": "To include an image with a link, use the usual image insertion code, e.g. this is not clickable\n![Surus](../_images/surus.png)\n\n\n\nSurus\n\n\nbut modified with square brackets and parentheses similarly to text links makes it clickable.\n[![Surus](../_images/surus.png)](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\n\n\nSurus\n\n\nIf we want to adjust the size, it needs to go in the square brackets, and not have spaces.\n[![Surus](../_images/surus.png){width=200}](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\n\n\nSurus\n\n\nWe can also use a figure div to make it cross-referenceable.\n::: {#fig-elephant}\n\n[![Surus](../_images/surus.png)](https://github.com/quarto-dev/quarto-web/tree/main/docs/authoring/_figure-examples)\n\nSurus\n\n:::\n\n\n\n\n\nSurus\n\n\nFigure 1: Surus"
  },
  {
    "objectID": "website_notes/quarto_images_misc.html#icons",
    "href": "website_notes/quarto_images_misc.html#icons",
    "title": "Images, divs, and similar",
    "section": "Icons",
    "text": "Icons\nWe can include icon items in the yaml to include icons from bootstrap, but what if we want to include others? The impetus for this is google scholar, but I could also foresee using something like phylopic.\nThe trick it looks like is to install (or write, but let’s leave that for later) a Quarto extension. Academicons comes up early in google, but Iconify seems to contain that set and a bunch more.\nTo install, use quarto add mcanouil/quarto-iconify at the terminal.\nTo use, include \"\". Note that when you find the icon, Iconify gives the name as setname:iconname, and we need to quote it in the yaml.\nThis seems to work here, e.g. the google scholar icon is\n\n\n\nbut I can’t get it to work in the yaml, even with quotes.\nSimply downloading the image and giving a path in the yaml doesn’t work either."
  },
  {
    "objectID": "website_notes/quarto_website_github.html",
    "href": "website_notes/quarto_website_github.html",
    "title": "Quarto website",
    "section": "",
    "text": "I want to use quarto to build a website hosted on github pages. I have a few goals for it, but step one is to figure out how to do it.\nI’ve already started a github pages repo, and had started putting things in it before I realised I was probably not working in the best way (and some things needed to be private). So before any commits, I moved the work out and want to just start clean and see how to do it. I’ll walk through the process here.\nI’ll start by following the quarto docs, but may diverge. Using the Rstudio version, but will likely use a bit of VS too for python."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#working-on-setting-up-a-quarto-website",
    "href": "website_notes/quarto_website_github.html#working-on-setting-up-a-quarto-website",
    "title": "Quarto website",
    "section": "",
    "text": "I want to use quarto to build a website hosted on github pages. I have a few goals for it, but step one is to figure out how to do it.\nI’ve already started a github pages repo, and had started putting things in it before I realised I was probably not working in the best way (and some things needed to be private). So before any commits, I moved the work out and want to just start clean and see how to do it. I’ll walk through the process here.\nI’ll start by following the quarto docs, but may diverge. Using the Rstudio version, but will likely use a bit of VS too for python."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#set-up-github-pages",
    "href": "website_notes/quarto_website_github.html#set-up-github-pages",
    "title": "Quarto website",
    "section": "Set up github pages",
    "text": "Set up github pages\nI did this a while ago, will come back to it.\nClone the repo locally."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#start-as-a-website-project",
    "href": "website_notes/quarto_website_github.html#start-as-a-website-project",
    "title": "Quarto website",
    "section": "Start as a website project",
    "text": "Start as a website project\nCould I have converted from a normal project? Probably. And I could only get to the ‘quarto website’ option if I made a new directory. So even though I already cloned the repo from github, I put the project in a new dir, and then will copy it into the repo (I guess?).\nI actually made it in a dir, then started a git repo in that dir and set its remote to hit the url of my github.io repo following the instructions on github for starting a local repo and pointing it to github.\nProject seems to work when I click render, though it renders in browser not in Viewer pane (which is actually nicer, just not what the docs say).\nI had my repo in dropbox, as that seems like it usually works fine for other repos and gives another layer of backup. But it was failing here with lots of errors about files being in use by other processes. Moved it to Documents and seems to work fine."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#setting-up-nav",
    "href": "website_notes/quarto_website_github.html#setting-up-nav",
    "title": "Quarto website",
    "section": "Setting up nav",
    "text": "Setting up nav\nI’m not entirely sure what I want the structure to be, but likely a brief home page, navbar at top with things like ‘Research’, ‘About’, ‘Code examples’, etc. Lots of options here, I guess just cobble something together quickly.\nOne question I have is what happens when I start committing to git. Does it auto-publish? It looks like no, according to quarto, if I set up to render to docs and don’t push master. Or if I publish from a gh-pages branch though that’s not working on windows.\nThe .yaml seems to be where all the website structure goes- nav bars, search, etc.\nI think I’m going to end up with something fairly complex for nav, but for now, maybe try broad categories across the top, then specifics down the side. Add additional nesting later.\nSeems reasonably ok, with ability to have sections within contents in the sidebar (I think)."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#questions",
    "href": "website_notes/quarto_website_github.html#questions",
    "title": "Quarto website",
    "section": "Questions",
    "text": "Questions\nIf I render a single file, does it render the whole website? seems like yes. If I want to render single pages (like to test them without having to re-render everything), can use the terminal quarto render filename.qmd or a subdir quarto render subdir/. The output ends up in the _site directory."
  },
  {
    "objectID": "website_notes/quarto_website_github.html#pushing-to-github",
    "href": "website_notes/quarto_website_github.html#pushing-to-github",
    "title": "Quarto website",
    "section": "Pushing to github",
    "text": "Pushing to github\nThe publishing the gh-pages branch seems the nicest, but when I set this up there was a big warning not to do that on Windows. So, I guess I did it the render to docs way. I think I’ll move to the gh-pages way now that the warning is gone, but for posterity:\n\nDocs\nadd output-dir: docs to the _quarto.yml and then create a .nojekyll file. Then quarto render to render to docs. I think I’ll also add a _quarto.yml.local with\nexecute:\n  cache: true\nto cache output and avoid long re-renders ( I hope). Seems to- re-clicking render was much faster.\nTo set to docs, go to repo, then settings –&gt; Pages (on left) –&gt; deploy from a branch, and choose the branch (likely Main) and /docs instead of /root.\nSo, I’ve been developing on dev, I guess I’ll merge main and see what happens.\nDon’t forget to merge and push ‘main’ if using the publish to /docs method. Otherwise no changes will actually appear.\n\n\nSwitching to gh-pages\nI think I’ll just toss all the stuff I made above and follow the instructions for gh-pages.\nIn short I stopped tracking docs/ and _files and other things that happen when the site renders.\nThen I run the code to create a gh-pages branch\ngit checkout --orphan gh-pages\ngit reset --hard # make sure you've committed changes before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\nAnd then go change the source branch to gh-pages as in the docs and delete the .nojekyll file. Note that I had to explicitly push the gh-pages branch after creating it before github saw it."
  },
  {
    "objectID": "website_notes/render_word.html",
    "href": "website_notes/render_word.html",
    "title": "Quarto word render",
    "section": "",
    "text": "Rendering to word is never as straightfoward as I’d like. In theory, it should be as easy as adding\nor\nSometimes that does work. If you have html output in any figures, e.g. mermaid or graphviz diagrams, it will fail. The solution is to put\nin the _quarto.yml. In theory it could go in the file header yaml, but it seems to not work there. The catch is, this will just skip those items when render, so if you want them, you’ll need to export somehow and read back in. In practice I find this really annoying, because it means we need extra code to make the same plot in different formats, which has to be maintained and never ends up looking the same.\nIf you want figure sizes to match html versions of the document, use\nIf you want to use a word template, (since pandoc’s default is old), put a new file somewhere and link it.\nThe font and similar are included in the template if it’s blank. But things like heading styles and tables of contents actually have to occur in the doc. So put in some headers, table of contents, figs, and tables if you want those formatting structures to work. Adding figs and tables doesn’t actually seem to adjust the way the captioning works, because they’re in boxes. I think this works better in Quarto 1.4, but that’s still in beta and breaks other things for me.\nThis template file can also do useful things like set line numbering, spacing, and page numbers."
  },
  {
    "objectID": "website_notes/render_word.html#multiple-formats",
    "href": "website_notes/render_word.html#multiple-formats",
    "title": "Quarto word render",
    "section": "Multiple formats",
    "text": "Multiple formats\nOne thing that’s very handy is having multiple render targets. I’m not doing that here to keep this doc clean for word–specific issues."
  }
]